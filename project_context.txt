--- START OF FILE project_context.txt ---

--- START OF PROJECT CONTEXT BUNDLE ---

--- START OF FILE ./.gitignore ---
# Python
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.so
*.egg-info/
__pypackages__/

# Virtual environments
.venv/
.env

# Typing / linting
.mypy_cache/
.ruff_cache/

# Testing
.pytest_cache/
.coverage
htmlcov/
*.log

# Editors
.vscode/
.idea/
*.swp

# System/OS
.DS_Store
Thumbs.db

# CORE-specific
logs/
demo/
.intent/keys/
pending_writes/
sandbox/
knowledge_graph.json
*.jsonl
*.lock
reports/

# Cache or checkpoints
*.bak
*.tmp

work/*
!work/.gitkeep

--- END OF FILE ./.gitignore ---

--- START OF FILE ./.intent/charter/constitution/ACTIVE ---
v2

--- END OF FILE ./.intent/charter/constitution/ACTIVE ---

--- START OF FILE ./.intent/charter/constitution/amendment_process.md ---
# .intent/charter/constitution/amendment_process.md
#
# This document is the single, canonical source of truth for the process of
# amending the CORE Constitution. Adherence is mandatory for all changes to
# files governed by the Charter.

## SECTION 1: CORE PRINCIPLES OF AMENDMENT

1.  **Safety First:** The process is designed to prevent accidental or unauthorized changes. All critical changes require explicit, verifiable human approval.
2.  **Clarity and Intent:** Every proposed change must be accompanied by a clear justification that links it to the system's core principles or mission.
3.  **Auditability:** Every step of the amendment process, from proposal to ratification, must be traceable and recorded.

## SECTION 2: THE STANDARD AMENDMENT PROCESS

This process applies to any modification of a file within the `.intent/charter/` directory.

1.  **Proposal Creation:**
    *   An authorized operator MUST create a formal proposal file (`cr-*.yaml`) according to the `proposal_schema.json`.
    *   The `target_path` MUST be the canonical path to the Charter file being changed.
    *   The `justification` MUST clearly state the reason for the change and which CORE principle it serves.

2.  **Signature and Quorum:**
    *   The proposal MUST be signed by one or more authorized approvers as defined in `approvers.yaml`.
    *   The number of valid signatures MUST meet the quorum requirements defined in `approvers.yaml` for the current operational mode (`development` or `production`).
    *   For changes targeting files listed in `critical_paths.yaml`, the **critical** quorum is required. For all other Charter files, the **standard** quorum applies.

3.  **Validation and Ratification:**
    *   The proposed change MUST pass a full constitutional audit (`core-admin check ci audit`).
    *   The change MAY be subject to a canary deployment as defined in the `canary_policy.yaml`.
    *   Once all checks pass and the quorum is met, the change is considered ratified and can be merged.

## SECTION 3: EMERGENCY PROCEDURES

Emergency procedures, such as the revocation of a compromised key, are detailed in `charter/constitution/operator_lifecycle.md`. Such actions are considered critical amendments and always require the **critical** quorum.

--- END OF FILE ./.intent/charter/constitution/amendment_process.md ---

--- START OF FILE ./.intent/charter/constitution/approvers.yaml ---
# .intent/charter/constitution/approvers.yaml
#
# This file defines the human operators authorized to approve constitutional
# changes and the rules governing that process.

approvers:
  - identity: "core-team@core-system.ai"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEA3dK7Jt4jJh6+QvZvY6XcGx3q8R0e7m5JwqYk8qFtU9U=
      -----END PUBLIC KEY-----
    created_at: "2025-08-05T15:50:53.995534+00:00"
    role: "maintainer"
    description: "Primary CORE development team"

  - identity: "security-audit@core-system.ai"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEApJ+8mNvL7wY2XfDcR9q3Q5t4yZx7v6hB8gKj0sF3T5U=
      -----END PUBLIC KEY-----
    created_at: "2025-08-05T15:50:53.995534+00:00"
    role: "security"
    description: "Security audit team for constitutional changes"

  - identity: "d.newecki@gmail.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VwAyEAmcbNEgYFEUUNf8XYGZEscamfzqrYHpgKoPHehtPuiDQ=
      -----END PUBLIC KEY-----
    created_at: "2025-08-12T10:36:49.000000Z"
    role: "maintainer"
    description: "Mentor"

quorum:
  # The operational mode should reflect the CORE_ENV variable from mind/config/runtime_requirements.yaml.
  # The system MUST enforce the 'production' quorum rules when CORE_ENV=production.
  current_mode: development
  development:
    standard: 1
    critical: 1
  production:
    standard: 2
    critical: 3

# The list of paths requiring the 'critical' quorum is now managed centrally.
critical_paths_source: "charter/constitution/critical_paths.yaml"

emergency_contact: "security-emergency@core-system.ai"

--- END OF FILE ./.intent/charter/constitution/approvers.yaml ---

--- START OF FILE ./.intent/charter/constitution/approvers.yaml.example ---
# .intent/constitution/approvers.yaml
#
# PURPOSE: This file enables cryptographic verification of constitutional approvals,
# preventing unauthorized changes. It contains the public keys of all authorized
# constitutional approvers for this instance of CORE.
#
# TO ADD A NEW APPROVER:
# 1. Run the command: `core-admin keygen "your.email@example.com"`
# 2. The command will output a JSON/YAML block.
# 3. Paste that block into the 'approvers' list below.

approvers:
  - identity: "your.name@example.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
      -----END PUBLIC KEY-----
    created_at: "YYYY-MM-DDTHH:MM:SSZ"
    role: "maintainer"
    description: "Primary maintainer of this CORE instance"

  - identity: "another.approver@example.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB=
      -----END PUBLIC KEY-----
    created_at: "YYYY-MM-DDTHH:MM:SSZ"
    role: "contributor"
    description: "Authorized contributor"

# Minimum number of valid signatures required to approve a constitutional amendment.
quorum:
  # Regular amendments (e.g., changing a policy) require 1 signature.
  standard: 1
  # Critical changes (e.g., altering this file) require 2 signatures.
  critical: 2

# A list of file paths that are considered "critical". Any proposal targeting
# these files will require the 'critical' quorum to be met.
critical_paths:
  - ".intent/policies/intent_guard.yaml"
  - ".intent/constitution/approvers.yaml"
  - ".intent/meta.yaml"

--- END OF FILE ./.intent/charter/constitution/approvers.yaml.example ---

--- START OF FILE ./.intent/charter/constitution/critical_paths.yaml ---
# .intent/charter/constitution/critical_paths.yaml
#
# This is the single source of truth for all file paths within the constitution
# that are considered critical to the system's safety, integrity, and governance.
#
# Any proposed change targeting one of these paths requires the "critical" quorum
# of approvers as defined in `approvers.yaml`.

paths:
  # The master index of the constitution.
  - ".intent/meta.yaml"

  # Core governance and identity files.
  - "charter/constitution/approvers.yaml"
  - "charter/constitution/operator_lifecycle.md"
  - "charter/constitution/amendment_process.md" # This file itself is critical.

  # The system's core mission statement.
  - "charter/mission/northstar.yaml"
  - "charter/mission/principles.yaml"

  # The most fundamental safety and governance policies.
  - "charter/policies/safety_policy.yaml"
  - "charter/policies/intent_guard_policy.yaml"

  # The definition of enforcement itself.
  - "charter/policies/enforcement_model_policy.yaml"

--- END OF FILE ./.intent/charter/constitution/critical_paths.yaml ---

--- START OF FILE ./.intent/charter/constitution/operator_lifecycle.md ---
# Human Operator Lifecycle Procedures

This document defines the formal, constitutionally-mandated procedures for managing human operators who have the authority to approve changes to the CORE constitution. Adherence to these procedures is mandatory and enforced by peer review during any change to the `approvers.yaml` file.

## Onboarding a New Approver

1.  **Key Generation:** The candidate operator MUST generate a new, secure Ed25519 key pair using the following command from the CORE repository root:
    ```bash
    poetry run core-admin keygen "candidate.email@example.com"
    ```
2.  **Proposal Creation:** A currently active, authorized approver MUST create a formal constitutional amendment proposal.
    - The `target_path` of the proposal MUST be `.intent/constitution/approvers.yaml`.
    - The `justification` MUST clearly state the reason for adding the new approver, including their role and identity.
    - The `content` of the proposal MUST be the complete `approvers.yaml` file with the new approver's YAML block appended.
3.  **Ratification:** The proposal must be signed and approved, meeting the required quorum as defined in `approvers.yaml`. Upon successful canary validation and approval, the new operator is considered active.

## Standard Revocation of an Approver

1.  **Proposal Creation:** An authorized approver MUST create a formal proposal to remove the target operator's block from the `approvers` list in `approvers.yaml`.
2.  **Justification:** The `justification` MUST clearly state the non-emergency reason for revocation (e.g., operator has left the project).
3.  **Ratification:** The proposal must be signed and approved, meeting the required quorum.

## Emergency Revocation of a Compromised Key

1.  **Proposal Creation:** In the event of a suspected or confirmed private key compromise, any active approver MUST immediately create an emergency revocation proposal targeting `approvers.yaml`.
2.  **Quorum:** This proposal requires the **critical** quorum to be met for approval.
3.  **Immediate Invalidation:** The moment a revocation proposal is created for an identity, that identity's signature is considered invalid for all quorum calculations on that proposal and any subsequent proposals until the matter is resolved.

--- END OF FILE ./.intent/charter/constitution/operator_lifecycle.md ---

--- START OF FILE ./.intent/charter/mission/manifesto.md ---
# CORE Manifesto

CORE is not a framework. It is not a plugin system.
CORE is a system that evolves itself — governed by explicit declarations of purpose, constraints, and structure.

---

### Beliefs

- Software is never done, but it must always be explainable.
- Changes must serve a declared intent — or challenge it explicitly.
- LLMs are powerful, but must be contained by purpose, not left to guess.
- Governance is not bureaucracy; it’s alignment between vision and action.

---

### CORE exists to:

- Understand its own capabilities
- Apply changes safely and transparently
- Validate every change against declared rules
- Learn from its outcomes and improve
- Inspire new systems to govern themselves

---

CORE does not pretend to be perfect.
But it refuses to be mindless.

--- END OF FILE ./.intent/charter/mission/manifesto.md ---

--- START OF FILE ./.intent/charter/mission/northstar.yaml ---
# .intent/mission/northstar.yaml

# PURPOSE: This fulfills evolvable_structure and defines CORE's NorthStar.
name: CORE
version: "0.1.0"
purpose: >
  CORE is a self-improving, intent-aware development system.
  Its purpose is to orchestrate safe, meaningful, and governed changes
  to its own codebase and related artifacts through intent bundles and introspective loops.

scope:
  - Planning and decomposition of tasks
  - Code generation via LLMs
  - Change validation and governance enforcement
  - Self-introspection and structural analysis
  - Knowledge management via manifests and graphs
  - Continuous self-evaluation and auditability

values:
  - Clarity over cleverness
  - Safety before speed
  - Traceability of every action
  - Alignment with declared purpose
  - Capability-driven reasoning

notes:
  - CORE evolves iteratively, but never silently.
  - All changes must fulfill a declared intent or generate a proposal to revise that intent.

--- END OF FILE ./.intent/charter/mission/northstar.yaml ---

--- START OF FILE ./.intent/charter/mission/principles.yaml ---
# .intent/charter/mission/principles.yaml
#
# CORE's Constitution: clear, enforceable, and readable by humans and LLMs.
# Any agent (including future LLMs) must understand and obey these rules.
# This file contains high-level, aspirational values. Specific, machine-enforceable
# rules are defined in the relevant policy files.

principles:

  - id: clarity_first
    description: >
      Prioritize clear, understandable code and documentation that effectively
      communicates its intent to both humans and machines. If something is
      ambiguous, it must be simplified.

  - id: safe_by_default
    description: >
      Every change must assume rollback or rejection unless explicitly validated.
      No file write, code execution, or intent update may proceed without confirmation.
      Rollback must be possible at every stage.

  - id: reason_with_purpose
    description: >
      Every autonomous planning step must be traceable to a core constitutional
      principle or a declared high-level goal, ensuring all actions are deliberate
      and auditable.

  - id: evolvable_structure
    description: >
      The system's structure and constitution must be designed to evolve safely.
      Self-modification must be governed by a formal, secure, and auditable
      amendment process.

  - id: no_orphaned_logic
    description: >
      No function, file, or rule may exist without being discoverable and traceable
      through the system's operational database. All logic must serve a declared purpose.

  - id: use_intent_bundle
    description: >
      All significant autonomous actions must be executed via a structured
      IntentBundle that reflects the Ten-Phase Loop of Reasoned Action. No phase
      may be skipped.

  - id: minimalism_over_completeness
    description: >
      Prefer small, focused changes. Do not generate stubs, placeholders, or
      unused functions. Unused or untestable logic is a liability and must be removed.

  - id: dry_by_design
    description: >
      "Don't Repeat Yourself." No logic or configuration may be duplicated. If a
      function, pattern, or rule exists in one place, it must be reused or
      referenced, not rewritten.

  - id: single_source_of_truth
    description: >
      The `.intent/` directory is the single source of constitutional truth (the laws).
      The operational database is the single source of operational truth (the current state).
      Derived artifacts (e.g., reports) must be generated from these sources.

  - id: separation_of_concerns
    description: >
      Each architectural domain must have a single, clearly defined responsibility.
      Inter-domain communication must be explicitly declared and governed by the
      constitution.

  - id: predictable_side_effects
    description: >
      Any action that modifies the system's state (e.g., a file write) must be
      explicit, logged, and reversible. Silent or unlogged changes are forbidden.

  - id: policy_change_requires_human_review
    description: >
      Any change to a policy file within the `.intent/policies/` directory must be
      ratified through the formal constitutional amendment process, requiring
      human review and approval.

--- END OF FILE ./.intent/charter/mission/principles.yaml ---

--- START OF FILE ./.intent/charter/policies/agent/agent_policy.yaml ---
policy_id: 18f048cb-b084-4faa-ac62-17fca55fed77
id: agent_policy
version: "1.3.0" # Version bump for new runtime validation rule
title: "Agent Governance Policy"
status: active
purpose: >
  The single source of truth for all rules governing the behavior, reasoning,
  and operational safety of all AI agents within the CORE system.

scope:
  applies_to: ["agents"]

owners:
  accountable: "Core Maintainer"

review:
  frequency: "annual"

rules:
  # --- Safety & Compliance Rules ---
  - id: agent.compliance.no_write_intent
    statement: "Agents MUST NOT write directly to '.intent/charter/**'. Constitutional changes require a formal, human-approved proposal."
    enforcement: error

  - id: agent.compliance.respect_cli_registry
    statement: "All tool invocations and system actions MUST be routed through commands registered in the `core.cli_commands` database table."
    enforcement: error

  # --- Reasoning & Auditing Rules ---
  - id: agent.reasoning.trace_required
    statement: "Agents MUST produce a concise, inspectable trace for non-trivial tasks, including inputs, tools used, and outcomes for auditability."
    enforcement: warn

  - id: agent.reasoning.source_attribution
    statement: "All claims, especially those influencing code generation or policy changes, MUST provide source attribution (e.g., file paths, policy IDs)."
    enforcement: warn

  # --- Execution & Safety Rules ---
  - id: agent.execution.no_unverified_code
    statement: "Agents MUST NOT execute or commit any generated or refactored code without first passing it through the full validation pipeline (lint, test, constitutional audit)."
    enforcement: error

  # --- START OF NEW RULE ---
  - id: agent.execution.require_runtime_validation
    statement: "All autonomously generated or refactored code MUST pass the project's test suite in an isolated environment before being committed."
    enforcement: error
  # --- END OF NEW RULE ---

  - id: agent.execution.fail_closed
    statement: "If an agent encounters ambiguous instructions or a high-risk uncertainty, it MUST halt its current task and escalate for human clarification rather than proceeding."
    enforcement: warn

  - id: agent.execution.limit_scope
    statement: "Agent actions MUST be limited to the immediate scope of the declared goal. Broad, opportunistic refactoring requires a separate, explicit intent and plan."
    enforcement: warn

# --- Resource Selection Logic (Merged from deduction_policy) ---
resource_selection:
  scoring_weights:
    cost: 0.5
    speed: 0.3
    quality: 0.1
    reasoning: 0.1

  task_specific_overrides:
    - task_keywords: ["docstringwriter", "propose a domain name", "label cluster"]
      weights:
        cost: 0.9
        speed: 0.1
        quality: 0.0
        reasoning: 0.0
    - task_keywords: ["refactor", "architect", "planner", "generate"]
      weights:
        cost: 0.1
        speed: 0.1
        quality: 0.4
        reasoning: 0.4

--- END OF FILE ./.intent/charter/policies/agent/agent_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/agent/micro_proposal_policy.yaml ---
policy_id: a5cbb67a-bbe6-4221-b187-1d88692c1124
id: micro_proposal_policy
version: "1.1.0" # Version bump for more granular path rules
title: "Micro-Proposal Policy (Autonomous Fast Track)"
status: active
purpose: >
  Defines the rules and scope for low-risk, autonomous changes that can be
  auto-approved without requiring the full human-in-the-loop constitutional
  amendment process. This is the primary gate for A1 autonomy.

scope:
  applies_to: [agents, cli]

owners:
  primary: "Governance Lead"
  reviewers: ["Core Maintainer"]

review:
  frequency: "6 months"

rules:
  - id: safe_actions
    description: "A list of capability keys that are permitted in micro-proposals."
    enforcement: error
    allowed_actions:
      - "autonomy.self_healing.fix_docstrings"
      - "autonomy.self_healing.format_code"
      - "autonomy.self_healing.fix_headers"

  - id: safe_paths
    description: "Glob patterns for file paths that are safe for autonomous modification."
    enforcement: error
    # --- START MODIFICATION ---
    # We now explicitly allow directory targets for the format_code action,
    # because the underlying tool (black/ruff) is designed to operate on them safely.
    allowed_paths:
      - "docs/**/*.md"
      - "tests/**/*.py"
      - "src/**/*.py"
      - "src" # Allow targeting the whole directory
      - "tests" # Allow targeting the whole directory
    # --- END MODIFICATION ---
    forbidden_paths:
      - ".intent/**"
      - "src/system/governance/**"
      - "src/core/**"
      - "pyproject.toml"
      - "Makefile"

  - id: require_validation
    description: "A micro-proposal must include evidence of a successful pre-flight validation (lint, test, audit)."
    enforcement: error
    required_evidence:
      - "validation_report_id"

--- END OF FILE ./.intent/charter/policies/agent/micro_proposal_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/capability_linter_policy.yaml ---
policy_id: cb081a61-5c2b-4623-8249-f26796e68d40
id: capability_linter_policy
version: "1.1.0" # Version bump to reflect #ID change
title: "Capability Linter Policy"
status: active
purpose: >
  To keep the capability catalog clean, owned, and useful by enforcing meaningful
  descriptions, owners, and the correct identity linking mechanism.
scope:
  applies_to: [code, governance, discovery]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "semiannual"

rules:
  - id: caps.meaningful_description
    statement: Capability descriptions in the database MUST be specific and non-placeholder.
    enforcement: error
  - id: caps.owner_required
    statement: Active capabilities in the database MUST have an assigned owner (agent/team).
    enforcement: error
  - id: caps.no_placeholder_text
    statement: Descriptions such as "TBD" or "N/A" are forbidden in the database.
    enforcement: error
  - id: caps.id_format
    statement: "Source code linkers MUST use the form '# ID: <uuid>'."
    enforcement: error

--- END OF FILE ./.intent/charter/policies/code/capability_linter_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/code_health_policy.yaml ---
policy_id: f7680a46-87ad-4e1a-8bdb-d6751d787309
id: code_health_policy
version: "1.0.0"
title: "Code Health Policy"
status: active
purpose: >
  To maintain small, focused modules and functions, limit complexity, and encourage
  refactoring before entropy accumulates.
scope:
  applies_to: [code, agents, auditor]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "semiannual"

rules:
  max_cognitive_complexity: 15
  max_nesting_depth: 4
  max_line_length:
    limit: 120
    enforcement: soft
  max_module_lloc: 300
  max_function_lloc: 80
  outlier_standard_deviations: 2.0
  enforce_dead_public_symbols: true

--- END OF FILE ./.intent/charter/policies/code/code_health_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/code_style_policy.yaml ---
target_path: charter/policies/code/code_style_policy.yaml
action: replace_file
justification: >
  Extend existing code_style_policy with a gentle nudge toward shared.universal
  to prevent helper-duplication (DRY, clarity_first, single_source_of_truth).
content: |
  policy_id: aaa0a228-125d-4013-bfed-c1b58cec0f66
  id: code_style_policy
  version: "1.2.0"   # ← only change: +0.0.1
  title: "Code Style Policy"
  status: active
  purpose: >
    To ensure consistent, readable code across the repository by standardizing
    tooling and expectations for contributors and agents.
  scope:
    applies_to: [code, agents, cli]
  owners:
    accountable: "Core Maintainer"
  review:
    frequency: "annual"

  rules:
    - id: style.linter_required
      statement: All changes MUST pass ruff (lint) before merge.
      enforcement: error

    - id: style.formatter_required
      statement: All changes MUST be formatted by black; CI runs black --check.
      enforcement: error

    - id: style.docstrings_public_apis
      statement: Public APIs MUST have docstrings summarizing intent and parameters; private/dunder excluded.
      enforcement: warn

    - id: style.import_order
      statement: Imports MUST follow grouping/order and avoid unused imports (enforced by linter).
      enforcement: warn

    - id: style.fail_on_style_in_ci
      statement: CI MUST fail on style or lint violations (no auto-fixing in CI).
      enforcement: error

    - id: style.capability_id_placement
      statement: "Only primary public symbols (top-level functions and classes) that represent a distinct, governable capability require an '# ID:' tag. Internal methods, properties, and private symbols SHOULD NOT be tagged."
      enforcement: warn

    - id: style.universal_helper_first
      statement: |
        Before creating any new helper function, check shared.universal.
        If a function with identical intent exists, you MUST reuse it.
        If a near-match exists, you MUST extend shared.universal via micro-proposal instead of duplicating logic.
      enforcement: warn

--- END OF FILE ./.intent/charter/policies/code/code_style_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/dependency_injection_policy.yaml ---
policy_id: "a1b2c3d4-e5f6-7a8b-9c0d-e6f7a8b9c0d1"
id: dependency_injection_policy
version: "1.1.0" # Version bump for new category field
title: "Dependency Injection Policy"
status: active
purpose: >
  To enforce a strict Dependency Injection (DI) pattern across the service and
  feature layers. This ensures loose coupling, high testability, and a clear
  flow of dependencies from a single composition root.

scope:
  applies_to: [code, auditor]

owners:
  primary: "Core Maintainer"
  reviewers: ["Governance Lead"]

review:
  frequency: "annual"

rules:
  - id: di.no_direct_instantiation
    statement: "Services and features MUST NOT directly instantiate other major services. Dependencies MUST be injected via the constructor."
    enforcement: error
    category: architectural # <-- ADD THIS
    scope:
      - "src/features/**/*.py"
      - "src/services/**/*.py"
    exclusions:
      - "src/cli/admin_cli.py"
      - "src/features/governance/runtime_validator.py"
    forbidden_instantiations:
      - "CognitiveService"
      - "GitService"
      - "ConstitutionalAuditor"
      - "QdrantService"
      - "ActionRegistry"
      - "PlanExecutor"
      - "SelfHealingAdvisor"
      - "CapabilityInvoker"
      - "CoderAgent"

  - id: di.no_global_session_import
    statement: "Modules within 'features' and 'services' MUST NOT directly import `get_session`. The database session MUST be injected."
    enforcement: error
    category: architectural # <-- ADD THIS
    scope:
      - "src/features/**/*.py"
      - "src/services/repositories/**/*.py"
    forbidden_imports:
      - "services.database.session_manager.get_session"
      - "services.repositories.db.engine.get_session"

  - id: di.constructor_injection_preferred
    statement: "Services SHOULD receive their dependencies through the `__init__` constructor, with type hints."
    enforcement: warn
    category: architectural # <-- ADD THIS

--- END OF FILE ./.intent/charter/policies/code/dependency_injection_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/naming_conventions_policy.yaml ---
policy_id: fae5215d-a1ee-424f-b5e1-46b08cecc5b9
# .intent/charter/policies/naming_conventions_policy.yaml
id: naming_conventions_policy
version: "1.0.0"
title: "Constitutional Naming Conventions"
status: active
purpose: >
  To provide a single, machine-readable source of truth for all naming
  conventions across the entire repository. This policy governs the structure of
  the constitution itself (.intent/) and the codebase (src/), ensuring clarity
  and predictability as the system evolves.

scope:
  applies_to: ["auditor", "repository"]

owners:
  accountable: "Core Maintainer"

review:
  frequency: "annual"

rules:
  # ==============================================================================
  # PART 1: GOVERNANCE OF THE CONSTITUTION ITSELF (.intent/)
  # ==============================================================================

  - id: intent.policy_file_naming
    description: "All policy files must use snake_case and end with '_policy.yaml'."
    enforcement: error
    scope: ".intent/charter/policies/*.yaml"
    target: "filename"
    pattern: "^[a-z0-9_]+_policy\\.yaml$"

  - id: intent.policy_schema_naming
    description: "Schemas for policy files must end with '_policy_schema.json'."
    enforcement: error
    scope: ".intent/charter/schemas/*_policy_schema.json"
    target: "filename"
    pattern: "^[a-z0-9_]+_policy_schema\\.json$"

  - id: intent.artifact_schema_naming
    description: "Schemas for non-policy artifacts must end with '_schema.[json|yaml]'."
    enforcement: error
    scope: ".intent/charter/schemas/*"
    target: "filename"
    pattern: "^[a-z0-9_]+_schema\\.(json|yaml)$"
    exclusions:
      - "*_policy_schema.json" # Exclude policy schemas to avoid rule conflict.

  - id: intent.prompt_file_naming
    description: "All prompt files must use snake_case and end with '.prompt'."
    enforcement: error
    scope: ".intent/mind/prompts/*.prompt"
    target: "filename"
    pattern: "^[a-z0-9_]+\\.prompt$"

  - id: intent.proposal_file_naming
    description: "All proposal files must follow the 'cr-*.yaml' naming convention."
    enforcement: warn
    scope: ".intent/proposals/*.yaml"
    target: "filename"
    pattern: "^cr-[a-zA-Z0-9_-]+\\.yaml$"
    exclusions:
      - "README.md" # The README is not a proposal.

  # ==============================================================================
  # PART 2: GOVERNANCE OF THE CODEBASE (src/ and tests/)
  # ==============================================================================

  - id: code.python_module_naming
    description: "All Python source files must use snake_case naming."
    enforcement: error
    scope: "src/**/*.py"
    target: "filename"
    pattern: "^[a-z0-9_]+\\.py$"
    exclusions:
      - "__init__.py"

  - id: code.python_test_module_naming
    description: "All Python test files must be prefixed with 'test_'."
    enforcement: error
    scope: "tests/**/*.py"
    target: "filename"
    pattern: "^test_[a-z0-9_]+\\.py$"
    exclusions:
      - "__init__.py"
      - "conftest.py"

--- END OF FILE ./.intent/charter/policies/code/naming_conventions_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/refactoring_patterns_policy.yaml ---
policy_id: 17725f7a-25e0-4747-85c4-8d98ea03310e
id: refactoring_patterns_policy
version: "1.0.0"
title: "Refactoring Patterns Policy"
status: active
purpose: >
  Provide safe, repeatable refactoring patterns for agents and developers to
  reduce risk during structural changes.
scope:
  applies_to: [code, agents, cli]
owners:
  primary: "Core Maintainer"
  reviewers: ["Governance Lead"]
review:
  frequency: "12 months"

patterns:
  - id: extract_function
    description: Move a coherent block of logic into a new function with a clear name and docstring.
    guardrails:
      - must_keep_behavior: true
      - add_unit_tests: true
      - run_audit: true

  - id: extract_module
    description: Move related functions/classes into a new module; update imports and domain boundaries.
    guardrails:
      - must_keep_behavior: true
      - update_import_map: true
      - run_audit: true

  - id: introduce_facade
    description: Add a facade/API layer to hide complexity behind a small, stable surface.
    guardrails:
      - document_contract: true
      - avoid_breaking_changes: true
      - run_audit: true

rules:
  - id: refactor.requires_tests
    statement: Any refactor that changes public behavior MUST include tests or proof of equivalence.
    enforcement: error
  - id: refactor.update_capabilities
    statement: When moving symbols, update capability tags and manifests accordingly.
    enforcement: warn
  - id: refactor.audit_after
    statement: A constitutional audit MUST run after refactors before merge.
    enforcement: error

checklist:
  - Confirm chosen pattern’s guardrails are satisfied.
  - Validate imports/domains after moves (no boundary violations).
  - Run tests + audit and attach results to the change.

--- END OF FILE ./.intent/charter/policies/code/refactoring_patterns_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/data/database_policy.yaml ---
policy_id: 1fb8949c-02db-486a-8b9d-556191456de3
id: database_policy
version: "4.0.0" # Major version bump for SSOT mandate
title: "Consolidated Database Governance Policy"
purpose: >
  To declare the database as the Single Source of Truth (SSOT) for all
  operational data and the Working Mind. The repository contains deterministic,
  read-only exports of this data for review and replication.
scope:
  applies_to: [postgresql]
owners:
  primary: "Security Lead"
  reviewers: ["Governance Lead"]
review:
  frequency: "12 months"
engine:
  type: postgresql
  schema: core
migrations:
  directory: sql
  order:
    - "001_consolidated_schema.sql"

rules:
  - id: db.ssot_for_operational_data
    statement: "The database is the authoritative source for all operational data (capabilities, symbols, links, audits, etc.). Files in `.intent/mind_export/` are read-only mirrors."
    enforcement: error
  - id: db.schema_declared
    statement: "All tables and columns MUST be declared in the database schema/migrations and validated at CI time."
    enforcement: error
  - id: db.migrations_logged
    statement: "Schema migrations MUST have an id, description, created_at, and approver recorded in the repo."
    enforcement: warn
  - id: db.write_via_governed_cli
    statement: "All writes MUST originate from registered CLI commands (see CLI Governance Policy)."
    enforcement: error
  - id: db.domains_in_db
    statement: "Capability domains MUST be stored in and queried from the database, with `.intent/mind/knowledge/domains.yaml` as a read-only export."
    enforcement: error
  - id: db.vector_index_in_db
    statement: "Vector index data MUST be stored in the database with metadata for EMBED_MODEL_REVISION and LOCAL_EMBEDDING_DIM, with file-based indices as read-only exports."
    enforcement: error
  - id: db.privacy.no_pii_or_secrets
    statement: "Personal data and secrets MUST NOT be stored in operational tables unless explicitly exempted."
    enforcement: error
  - id: db.privacy.masking
    statement: "Logs and audit records MUST redact tokens, keys, and secrets before persistence."
    enforcement: error
  - id: db.privacy.access_least_privilege
    statement: "Access to operational data MUST follow least-privilege via roles/groups."
    enforcement: warn
  - id: db.cli_registry_in_db
    statement: "The canonical list of CLI commands MUST be stored in and queried from the database. `.intent/mind/knowledge/cli_registry.yaml` is deprecated and considered a read-only export."
    enforcement: error
  - id: db.llm_resources_in_db
    statement: "The manifest of available LLM resources MUST be stored in and queried from the database. `.intent/mind/knowledge/resource_manifest.yaml` is deprecated and considered a read-only export."
    enforcement: error
  - id: db.cognitive_roles_in_db
    statement: "The definition of cognitive roles MUST be stored in and queried from the database. `.intent/mind/knowledge/cognitive_roles.yaml` is deprecated and considered a read-only export."
    enforcement: error

retention:
  audit_runs_days: 180
  cli_runs_days: 90
  capability_history_days: 365
  proposals_days: 1095
drift:
  development: warn
  production: block
backup_restore:
  cadence: daily
  test_restore_quarterly: true
quorum:
  changes_require_critical_paths: true

--- END OF FILE ./.intent/charter/policies/data/database_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/data/secrets_management_policy.yaml ---
policy_id: ffb2dcec-3a90-45e6-aea7-691dd3b339b3
id: secrets_management_policy
version: "1.0.0"
title: "Secrets Management Policy"
status: active
purpose: >
  Define rules for handling secrets to prevent accidental exposure in source code,
  logs, and outputs.
scope:
  applies_to: [code, ci, logs]
owners:
  primary: "Security Lead"
  reviewers: ["Core Maintainer"]
review:
  frequency: "12 months"

rules:
  - id: no_hardcoded_secrets
    statement: Source code MUST NOT contain hardcoded secrets (API keys, passwords). Use environment variables.
    enforcement: error
    detection:
      patterns:
        - "(A|B|S|G)K[0-9A-Za-z]{30,}" # Common API key patterns
        - 'password\s*[:=]\s*[''""].+[''""]'
      exclude:
        - "tests/**"
        - ".env.example"

  - id: redact_secrets_in_logs
    statement: Logs and telemetry MUST redact sensitive data (tokens, keys, passwords) before persistence.
    enforcement: warn

checklist:
  - Auditor scans for hardcoded secret patterns and fails the build if found outside excluded paths.
  - Periodic manual review of logs to ensure redaction is working as expected.

--- END OF FILE ./.intent/charter/policies/data/secrets_management_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/audit_ignore_policy.yaml ---
# .intent/charter/policies/governance/audit_ignore_policy.yaml
policy_id: 232934da-44d6-4ddc-9404-162382caddb7
id: audit_ignore_policy
version: 1.2.0   # Use correct symbol key format for ignores
title: Audit Ignore Policy
status: active
purpose: >
  To allow narrow, explicit exceptions for files or symbols to reduce audit noise
  without weakening the Constitution. Every ignore must have a reason and an expiry
  date.
scope:
  applies_to: [governance, ci]
owners:
  accountable: Core Maintainer
review: semannual

rules:
  - id: ignore.must_expire
    statement: Every ignore MUST include an 'expires' date to force review.
    enforcement: warn
  - id: ignore.must_have_reason
    statement: Every ignore MUST include a reason.
    enforcement: error

ignores: []

symbol_ignores:
  # --- START OF AMENDMENT ---
  # The duplicated set_context functions have been removed, so their ignores are now obsolete.
  - key: cli.commands.run.py::develop_command
    reason: Boilerplate CLI wrapper. Implementation is in logic/run.py.
    expires: '2026-01-01'
  - key: cli.commands.run.py::vectorize_command
    reason: Boilerplate CLI wrapper. Implementation is in logic/run.py.
    expires: '2026-01-01'
  - key: cli.commands.fix.py::complexity_command
    reason: Boilerplate CLI wrapper. Implementation is in
      self_healing/complexity_service.py.
    expires: '2026-01-01'
  - key: cli.logic.system.py::integrate_command
    reason: Boilerplate CLI wrapper. Implementation is in
      project_lifecycle/integration_service.py.
    expires: '2026-01-01'
  - key: cli.logic.system.py::process_crates_command
    reason: Boilerplate CLI wrapper. Implementation is in
      core/crate_processing_service.py.
    expires: '2026-01-01'
  # --- END OF AMENDMENT ---

  # --- UNASSIGNED BUT VALID (Internal Helpers) ---
  - key: cli.interactive.run_command
    reason: Internal helper for the interactive menu, not a governable
      capability.
    expires: '2026-01-01'

  # --- ACTION HANDLERS (Structurally Similar by Design Pattern) ---
  - key: core.actions.code_actions.CreateFileHandler
    reason: Boilerplate Action Handler structure.
    expires: '2026-01-01'
  - key: core.actions.code_actions.EditFileHandler
    reason: Boilerplate Action Handler structure.
    expires: '2026-01-01'
  - key: core.actions.file_actions.DeleteFileHandler
    reason: Boilerplate Action Handler structure.
    expires: '2026-01-01'
  - key: core.actions.file_actions.ListFilesHandler
    reason: Boilerplate Action Handler structure.
    expires: '2026-01-01'
  - key: core.actions.file_actions.ReadFileHandler
    reason: Boilerplate Action Handler structure.
    expires: '2026-01-01'
  - key: core.actions.healing_actions.FixDocstringsHandler
    reason: Boilerplate Action Handler structure.
    expires: '2026-01-01'
  - key: core.actions.healing_actions.FixHeadersHandler
    reason: Boilerplate Action Handler structure.
    expires: '2026-01-01'
  - key: core.actions.healing_actions.FormatCodeHandler
    reason: Boilerplate Action Handler structure.
    expires: '2026-01-01'

  # --- DATA MODELS (Structurally Similar by Declaration) ---
  - key: services.database.models.Action
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.Capability
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.CliCommand
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.Domain
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.LlmResource
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.Migration
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.Northstar
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.RuntimeService
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.Symbol
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.SymbolCapabilityLink
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: services.database.models.Task
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: features.maintenance.dotenv_sync_service.RuntimeSetting
    reason: Database ORM model.
    expires: '2026-01-01'
  - key: shared.legacy_models.LegacyCliCommand
    reason: Legacy data model for migration.
    expires: '2026-01-01'
  - key: shared.legacy_models.LegacyCliRegistry
    reason: Legacy data model for migration.
    expires: '2026-01-01'
  - key: shared.legacy_models.LegacyCognitiveRole
    reason: Legacy data model for migration.
    expires: '2026-01-01'
  - key: shared.legacy_models.LegacyLlmResource
    reason: Legacy data model for migration.
    expires: '2026-01-01'

  # --- CLI WRAPPERS & UTILITIES (Structurally Similar by Function) ---
  - key: cli.commands.fix.fix_policy_ids_command
    reason: Simple CLI wrapper around a service call.
    expires: '2026-01-01'
  - key: features.self_healing.policy_id_service.add_missing_policy_ids
    reason: Simple CLI wrapper around a service call.
    expires: '2026-01-01'
  - key: cli.logic.audit.lint
    reason: Simple CLI wrapper around a tool.
    expires: '2026-01-01'
  - key: cli.logic.audit.test_system
    reason: Simple CLI wrapper around a tool.
    expires: '2026-01-01'
  - key: cli.logic.list_audits.list_audits
    reason: Simple DB query and print function.
    expires: '2026-01-01'
  - key: cli.logic.log_audit.log_audit
    reason: Simple DB write function.
    expires: '2026-01-01'
  - key: cli.logic.report.report
    reason: Simple DB query and print function.
    expires: '2026-01-01'
  - key: shared.utils.subprocess_utils.run_poetry_command
    reason: Simple utility function.
    expires: '2026-01-01'
  - key: cli.logic.reviewer.docs_clarity_audit
    reason: High-level CLI orchestrator.
    expires: '2026-01-01'
  - key: cli.logic.reviewer.peer_review
    reason: High-level CLI orchestrator.
    expires: '2026-01-01'
  - key: cli.interactive.show_development_menu
    reason: UI function with boilerplate structure.
    expires: '2026-01-01'
  - key: cli.interactive.show_governance_menu
    reason: UI function with boilerplate structure.
    expires: '2026-01-01'
  - key: cli.interactive.show_system_menu
    reason: UI function with boilerplate structure.
    expires: '2026-01-01'
  - key: cli.interactive.show_project_lifecycle_menu
    reason: UI function with boilerplate structure.
    expires: '2026-01-01'
  - key: cli.logic.hub.hub_list
    reason: High-level CLI orchestrator.
    expires: '2026-01-01'
  - key: cli.logic.hub.hub_search
    reason: High-level CLI orchestrator.
    expires: '2026-01-01'
  - key: cli.logic.hub.hub_whereis
    reason: High-level CLI orchestrator.
    expires: '2026-01-01'
  - key: cli.logic.hub.hub_doctor
    reason: High-level CLI orchestrator.
    expires: '2026-01-01'
  - key: features.introspection.discovery.from_kgb.collect_from_kgb
    reason: High-level service orchestrator.
    expires: '2026-01-01'
  - key:
      features.introspection.discovery.from_source_scan.collect_from_source_scan
    reason: High-level service orchestrator.
    expires: '2026-01-01'
  - key: services.database.session_manager.get_session
    reason: Simple factory function.
    expires: '2026-01-01'
  - key: services.repositories.db.engine.get_session
    reason: Simple factory function.
    expires: '2026-01-01'
  - key: cli.logic.diagnostics.debug_meta_paths
    reason: Simple diagnostic utility.
    expires: '2026-01-01'
  - key: shared.utils.constitutional_parser.get_all_constitutional_paths
    reason: Simple utility function.
    expires: '2026-01-01'
  - key: src/core/actions/base.py::ActionHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/base.py::ActionHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/code_actions.py::CreateFileHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/code_actions.py::CreateFileHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/code_actions.py::EditFileHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/code_actions.py::EditFileHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/code_actions.py::EditFunctionHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/code_actions.py::EditFunctionHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/file_actions.py::DeleteFileHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/file_actions.py::DeleteFileHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/file_actions.py::ListFilesHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/file_actions.py::ListFilesHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/file_actions.py::ReadFileHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/file_actions.py::ReadFileHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/governance_actions.py::CreateProposalHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/governance_actions.py::CreateProposalHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/healing_actions.py::FixDocstringsHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/healing_actions.py::FixDocstringsHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/healing_actions.py::FixHeadersHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/healing_actions.py::FixHeadersHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/healing_actions.py::FormatCodeHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/healing_actions.py::FormatCodeHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/registry.py::ActionRegistry.get_handler
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/validation_actions.py::ValidateCodeHandler.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/actions/validation_actions.py::ValidateCodeHandler.name
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/core/agents/coder_agent.py::CoderAgent.generate_and_validate_code_for_task
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/agents/deduction_agent.py::DeductionAgent.select_resource
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/agents/execution_agent.py::ExecutionAgent.execute_plan
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/agents/intent_translator.py::IntentTranslator.translate
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/agents/micro_planner.py::MicroPlannerAgent.create_micro_plan
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/agents/plan_executor.py::PlanExecutor.execute_plan
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/agents/planner_agent.py::PlannerAgent.create_execution_plan
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/core/agents/reconnaissance_agent.py::ReconnaissanceAgent.generate_report
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/core/agents/tagger_agent.py::CapabilityTaggerAgent.suggest_and_apply_tags
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/cognitive_service.py::CognitiveService.aget_client_for_role
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/cognitive_service.py::CognitiveService.get_embedding_for_code
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/cognitive_service.py::CognitiveService.initialize
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/cognitive_service.py::CognitiveService.search_capabilities
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/core/crate_processing_service.py::CrateProcessingService.process_pending_crates_async
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/file_handler.py::FileHandler.add_pending_write
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/file_handler.py::FileHandler.confirm_write
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/git_service.py::GitService.add_all
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/git_service.py::GitService.commit
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/git_service.py::GitService.get_current_commit
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/git_service.py::GitService.get_staged_files
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/git_service.py::GitService.is_git_repo
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/git_service.py::GitService.status_porcelain
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/intent_guard.py::IntentGuard.check_transaction
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/intent_guard.py::PolicyRule.from_dict
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/knowledge_service.py::KnowledgeService.get_graph
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/knowledge_service.py::KnowledgeService.list_capabilities
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/knowledge_service.py::KnowledgeService.search_capabilities
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/llm_client.py::LLMClient.make_request
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/prompt_pipeline.py::PromptPipeline.process
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/service_registry.py::ServiceRegistry.get_service
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/validation_policies.py::PolicyValidator.check_semantics
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/core/validation_quality.py::QualityChecker.check_for_todo_comments
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/autonomy/micro_proposal_executor.py::MicroProposalExecutor.apply_proposal
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/autonomy/micro_proposal_executor.py::MicroProposalExecutor.validate_proposal
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/audit_context.py::AuditorContext.load_knowledge_graph
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/features/governance/audit_context.py::AuditorContext.python_files
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/capability_coverage.py::CapabilityCoverageCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/dependency_injection_check.py::DependencyInjectionCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/domain_placement.py::DomainPlacementCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/duplication_check.py::DuplicationCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/environment_checks.py::EnvironmentChecks.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/features/governance/checks/file_checks.py::FileChecks.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/features/governance/checks/health_checks.py::HealthChecks.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/id_coverage_check.py::IdCoverageCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/id_uniqueness_check.py::IdUniquenessCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/import_rules.py::ImportRulesCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/import_rules.py::ImportRulesCheck.execute_on_content
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/knowledge_source_check.py::KnowledgeSourceCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/legacy_tag_check.py::LegacyTagCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/manifest_lint.py::ManifestLintCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/naming_conventions.py::NamingConventionsCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/orphaned_logic.py::OrphanedLogicCheck.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/orphaned_logic.py::OrphanedLogicCheck.find_unassigned_public_symbols
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/checks/security_checks.py::SecurityChecks.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/features/governance/checks/style_checks.py::StyleChecks.execute
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/constitutional_auditor.py::ConstitutionalAuditor.run_full_audit
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/constitutional_auditor.py::ConstitutionalAuditor.run_full_audit_async
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/micro_proposal_validator.py::MicroProposalValidator.validate
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/policy_coverage_service.py::PolicyCoverageService.run
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/governance/runtime_validator.py::RuntimeValidatorService.run_tests_in_canary
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/introspection/capability_discovery_service.py::CapabilityRegistry.resolve
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/introspection/knowledge_graph_service.py::KnowledgeGraphBuilder.build
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/features/introspection/sync_service.py::SymbolScanner.scan
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/introspection/sync_service.py::SymbolVisitor.visit_AsyncFunctionDef
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/introspection/sync_service.py::SymbolVisitor.visit_ClassDef
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/introspection/sync_service.py::SymbolVisitor.visit_FunctionDef
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/project_lifecycle/scaffolding_service.py::Scaffolder.scaffold_base_structure
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/features/project_lifecycle/scaffolding_service.py::Scaffolder.write_file
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/services/adapters/embedding_provider.py::EmbeddingService.get_embedding
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/clients/llm_api_client.py::BaseLLMClient.get_embedding
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/services/clients/llm_api_client.py::BaseLLMClient.make_request_async
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/clients/llm_api_client.py::BaseLLMClient.make_request_sync
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/clients/qdrant_client.py::QdrantService.ensure_collection
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/clients/qdrant_client.py::QdrantService.get_all_vectors
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/clients/qdrant_client.py::QdrantService.get_vector_by_id
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/clients/qdrant_client.py::QdrantService.search_similar
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/services/clients/qdrant_client.py::QdrantService.upsert_capability_vector
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/config_service.py::ConfigurationService.get
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/llm/client.py::LLMClient.get_embedding
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/llm/client.py::LLMClient.make_request_async
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/llm/providers/base.py::AIProvider.chat_completion
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/llm/providers/base.py::AIProvider.get_embedding
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/llm/providers/ollama.py::OllamaProvider.chat_completion
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/llm/providers/ollama.py::OllamaProvider.get_embedding
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/llm/providers/openai.py::OpenAIProvider.chat_completion
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/llm/providers/openai.py::OpenAIProvider.get_embedding
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/services/llm/resource_selector.py::ResourceSelector.select_resource_for_role
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/services/mind_service.py::MindService.load_policy
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/action_logger.py::ActionLogger.log_event
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/ast_utility.py::FunctionCallVisitor.visit_Call
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/config.py::Settings.find_logical_path_for_file
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/config.py::Settings.get_path
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/config.py::Settings.initialize_for_test
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/config.py::Settings.load
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/models/audit_models.py::AuditFinding.as_dict
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/models/audit_models.py::AuditSeverity.is_blocking
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/models/drift_models.py::DriftReport.to_dict
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/utils/alias_resolver.py::AliasResolver.resolve
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/utils/embedding_utils.py::Embeddable.get_embedding
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/utils/embedding_utils.py::EmbeddingService.get_embedding
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/utils/embedding_utils.py::_Adapter.get_embedding
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/utils/header_tools.py::HeaderTools.parse
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/utils/header_tools.py::HeaderTools.reconstruct
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/shared/utils/parallel_processor.py::ThrottledParallelProcessor.run_async
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key:
      src/shared/utils/parallel_processor.py::ThrottledParallelProcessor.run_sync
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/utils/yaml_processor.py::YAMLProcessor.dump
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/utils/yaml_processor.py::YAMLProcessor.load
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'
  - key: src/shared/utils/yaml_processor.py::YAMLProcessor.load_strict
    reason: Legacy symbol - to be defined as part of technical debt.
    expires: '2026-04-11'

--- END OF FILE ./.intent/charter/policies/governance/audit_ignore_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/auditor_policy.yaml ---
policy_id: 1d311fd2-97db-462a-bacc-862e8f3c3777
version: "2.1.0" # Version bump for SSOT alignment
title: "Intent Guard Policy"
purpose: >
  Ensure that all actions remain aligned with the Constitution. This policy
  protects the immutable Charter from unauthorized modification and governs the
  process for safe, dynamic updates to the working Mind.

scope:
  applies_to:
    - agents
    - cli
    - governance
    - services

owners:
  primary: ["Governance Lead"]
  reviewers: ["Core Maintainer"]

review:
  frequency: "6 months"

rules:
  - id: charter.write_block
    statement: "The Charter (.intent/charter/**) is immutable. Runtime systems, agents, and tools MUST NOT write to it. Changes require a formal amendment."
    enforcement: error

  - id: charter.change_requires_proposal
    statement: "Any change to the Charter (.intent/charter/**) MUST be executed via a ratified proposal with the required approver quorum."
    enforcement: error

  - id: mind.writes_must_be_governed
    statement: "Writes to the Mind (.intent/mind/**) are permitted but MUST be governed. They must originate from a registered capability and pass all relevant safety and validation checks."
    enforcement: warn

  - id: auditor.block_on_violation
    statement: "If any guard rule with enforcement=error is violated, the constitutional auditor MUST fail."
    enforcement: error

# The rule 'cli.registry_source_of_truth' has been removed as it is now redundant.
# The rule 'single_active_constitution' has been removed as it is now centrally
# managed and enforced by 'auditor_policy.yaml'.

--- END OF FILE ./.intent/charter/policies/governance/auditor_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/available_actions_policy.yaml ---
policy_id: 47eab093-749a-444f-bc97-3d816c2d631c
id: available_actions_policy
version: "1.1.0" # Version bump for new parameter schema
title: "Available Actions Policy"
status: active
purpose: >
  Defines the canonical list of atomic actions that the PlannerAgent is
  constitutionally permitted to include in an execution plan.
scope:
  applies_to: [agents, planner_agent]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "annual"

actions:
  # --- START OF AMENDMENT ---
  - name: "read_file"
    description: "Reads the entire content of a specified file to provide context for subsequent steps. Fails if the path is a directory."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The repository-relative path to the file to be read."
        required: true

  - name: "list_files"
    description: "Lists all files and subdirectories within a specified directory to understand its contents. Fails if the path is not a directory."
    parameters:
      - name: "file_path" # Use a consistent parameter name for all path-based operations
        type: "string"
        description: "The repository-relative path to the directory to be listed."
        required: true

  - name: "edit_file"
    description: "Performs a surgical replacement of a block of code within an existing file."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The repository-relative path to the file to be edited."
        required: true
      - name: "new_content"
        type: "string"
        description: "The complete new content for the specified block of code."
        required: true
      - name: "start_line"
        type: "integer"
        description: "The starting line number of the block to be replaced (1-based)."
        required: true
      - name: "end_line"
        type: "integer"
        description: "The ending line number of the block to be replaced (1-based)."
        required: true

  - name: "create_file"
    description: "Creates a new source code or documentation file at a specified path with the given content."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The repository-relative path where the new file will be created."
        required: true
      - name: "code"
        type: "string"
        description: "The full source code or content for the new file."
        required: true

  - name: "edit_function"
    description: "Surgically replaces the code of an existing function or class within a file."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The path to the file containing the symbol to be edited."
        required: true
      - name: "symbol_name"
        type: "string"
        description: "The name of the function or class to be replaced."
        required: true
      - name: "code"
        type: "string"
        description: "The new, complete source code for the function or class."
        required: true

  - name: "delete_file"
    description: "Deletes an existing file from the repository."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The path of the file to be deleted."
        required: true

  - name: "create_proposal"
    description: "Creates a formal, human-in-the-loop constitutional amendment proposal (a cr-*.yaml file)."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The repository-relative path of the file the proposal will target."
        required: true
      - name: "justification"
        type: "string"
        description: "A clear, human-readable rationale for the proposed change."
        required: true
      - name: "code"
        type: "string"
        description: "The full new content for the target file."
        required: true

  - name: "add_capability_tag"
    description: "Adds a new # ID tag to a specific function or class. (Note: This is a legacy action, prefer 'fix assign-ids')."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The path to the file containing the symbol."
        required: true
      - name: "symbol_name"
        type: "string"
        description: "The name of the function or class to tag."
        required: true
      - name: "tag"
        type: "string"
        description: "The UUID tag to add."
        required: true

--- END OF FILE ./.intent/charter/policies/governance/available_actions_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/cli_governance_policy.yaml ---
policy_id: cd37bddd-52c8-445a-b842-e229b29e6980
id: cli_governance_policy
version: "2.0.0" # Major version bump for new grammar rule
title: "CLI Governance Policy"
status: active
purpose: >
  To govern the structure, registration, and evolution of all commands
  exposed via the core-admin CLI, ensuring clarity and safety.
scope:
  applies_to: [cli, auditor]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "annual"

rules:
  - id: cli.must_be_registered
    statement: "All CLI commands MUST be declaratively registered in the `core.cli_commands` database table, which serves as the single source of truth."
    enforcement: error
  - id: cli.must_have_summary
    statement: "Every registered CLI command MUST have a concise summary for help text."
    enforcement: warn
  - id: cli.must_implement_capability
    statement: "Every CLI command SHOULD implement at least one semantic capability."
    enforcement: warn
  # --- START OF NEW RULE ---
  - id: cli.must_use_verb_noun_grammar
    statement: "All top-level command groups MUST be verbs describing the action (e.g., 'check', 'fix', 'manage'). Subgroups should be nouns representing the target (e.g., 'manage database')."
    enforcement: error
  # --- END OF NEW RULE ---

--- END OF FILE ./.intent/charter/policies/governance/cli_governance_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/enforcement_model_policy.yaml ---
policy_id: 56dbc018-cb44-44a3-81aa-e9a2dd429069
# .intent/charter/policies/enforcement_model_policy.yaml
version: "2.0.0" # Version bump to signify harmonization
title: "Canonical Enforcement Model Policy"
purpose: >
  Provide the single, consistent meaning for all policy enforcement levels. This
  vocabulary MUST be used by all other policies and schemas to ensure the auditor
  can interpret and act on findings deterministically.

scope:
  applies_to:
    - governance
    - ci
    - agents

owners:
  primary: ["Governance Lead"]
  reviewers: ["Core Maintainer"]

review:
  frequency: "12 months"

levels:
  error:
    description: "A critical violation. This finding MUST block a merge/deploy. The auditor MUST return a non-zero exit code."
    ci_behavior: "fail"
    runtime_behavior: "block"
  warn:
    description: "A non-critical issue or deviation. The finding MUST be reported but SHOULD NOT block a merge/deploy. It must be tracked for resolution."
    ci_behavior: "pass_with_warnings"
    runtime_behavior: "log_and_continue"
  info:
    description: "An informational finding or observation. No action is required, but it provides context for a human reviewer."
    ci_behavior: "ignore"
    runtime_behavior: "ignore"

rules:
  - id: level_must_be_declared
    statement: "Every rule in every policy MUST specify an 'enforcement' level from the set {error, warn, info}."
    enforcement: error
  - id: auditor_maps_levels
    statement: "The constitutional auditor MUST map all findings to the levels defined above and respect their specified CI/runtime behavior."
    enforcement: error

--- END OF FILE ./.intent/charter/policies/governance/enforcement_model_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/intent_crate_policy.yaml ---
policy_id: 6bf96cc9-ffb4-493b-b6f8-05493b4b9d74
id: intent_crate_policy
version: "1.0.0"
title: "Intent Crate Processing Policy"
status: active
purpose: >
  To govern the autonomous, asynchronous processing of change requests (Intent Crates),
  ensuring all changes to the system's state are auditable, validated, and formally managed.

scope:
  applies_to: [governance, system]

owners:
  primary: "Governance Lead"
  reviewers: ["Core Maintainer"]

review:
  frequency: "6 months"

rules:
  - id: crate.location.inbox
    statement: "All new, unprocessed Intent Crates MUST reside in 'work/crates/inbox/'."
    enforcement: error

  - id: crate.state.must_move
    statement: "A crate MUST be moved from the inbox to 'processing', and then to 'accepted' or 'rejected'. It MUST NOT be modified in place."
    enforcement: error

  - id: crate.result.required
    statement: "Every processed crate in 'accepted' or 'rejected' MUST contain a 'result.yaml' file detailing the outcome and justification."
    enforcement: error

  - id: crate.acceptance.meta_sync
    statement: "Upon accepting a CONSTITUTIONAL_AMENDMENT crate that adds a new policy or schema, the system MUST autonomously update and commit '.intent/meta.yaml'."
    enforcement: error

  - id: crate.acceptance.scaffold_work
    statement: "If a new policy is accepted and requires new auditor checks, the system SHOULD scaffold the necessary check files or methods."
    enforcement: warn

--- END OF FILE ./.intent/charter/policies/governance/intent_crate_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/intent_guard_policy.yaml ---
policy_id: 1d311fd2-97db-462a-bacc-862e8f3c3777
version: "2.1.0" # Version bump for SSOT alignment
title: "Intent Guard Policy"
purpose: >
  Ensure that all actions remain aligned with the Constitution. This policy
  protects the immutable Charter from unauthorized modification and governs the
  process for safe, dynamic updates to the working Mind.

scope:
  applies_to:
    - agents
    - cli
    - governance
    - services

owners:
  primary: ["Governance Lead"]
  reviewers: ["Core Maintainer"]

review:
  frequency: "6 months"

rules:
  - id: charter.write_block
    statement: "The Charter (.intent/charter/**) is immutable. Runtime systems, agents, and tools MUST NOT write to it. Changes require a formal amendment."
    enforcement: error

  - id: charter.change_requires_proposal
    statement: "Any change to the Charter (.intent/charter/**) MUST be executed via a ratified proposal with the required approver quorum."
    enforcement: error

  - id: mind.writes_must_be_governed
    statement: "Writes to the Mind (.intent/mind/**) are permitted but MUST be governed. They must originate from a registered capability and pass all relevant safety and validation checks."
    enforcement: warn

  - id: single_active_constitution
    statement: "Exactly one active constitution version MUST be referenced by '.intent/charter/constitution/ACTIVE'."
    enforcement: error

  - id: auditor.block_on_violation
    statement: "If any guard rule with enforcement=error is violated, the constitutional auditor MUST fail."
    enforcement: error

# The rule 'cli.registry_source_of_truth' has been removed as it is now redundant.
# The canonical source for CLI commands is governed by 'cli_governance_policy.yaml'
# and 'database_policy.yaml', which the auditor is mandated to enforce.

--- END OF FILE ./.intent/charter/policies/governance/intent_guard_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/knowledge_source_policy.yaml ---
policy_id: 23a18582-a3a3-459b-b874-78b41f925097
id: knowledge_source_policy
version: "1.0.0"
title: "Knowledge Source of Truth Policy"
status: active
purpose: >
  Enforces the constitutional principle that the database is the single source
  of operational truth. This policy defines the narrow, explicit exceptions
  for tools that are permitted to interact with legacy or intermediate knowledge
  artifacts like knowledge_graph.json.
scope:
  applies_to: [governance, auditor]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "annual"

# This is the list of system tools that are granted a constitutional exception
# to read or write the knowledge_graph.json artifact. All other access is forbidden.
allowed_access_paths:
  - "src/features/introspection/knowledge_graph_service.py"
  - "src/features/governance/checks/knowledge_source_check.py"

--- END OF FILE ./.intent/charter/policies/governance/knowledge_source_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/logging_policy.yaml ---
policy_id: a1b2c3d4-e5f6-7a8b-9c0d-1e2f3a4b5c6e
id: logging_policy
version: "1.0.0"
title: "Unified Logging Policy"
status: active
purpose: >
  To ensure all system output is standardized, structured, and auditable by mandating
  the use of the shared logger and forbidding unsanctioned output channels like print().

scope:
  applies_to: [code, auditor]

owners:
  primary: "Core Maintainer"

review:
  frequency: "annual"

rules:
  - id: log.no_print_statements
    statement: "The use of print() is forbidden in application code (core, features, services). It is only permitted in the CLI layer for direct user output."
    enforcement: error
    allowed_paths:
      - "src/cli/**"
      - "tests/**" # Allow print in tests for debugging

  - id: log.no_direct_logging_import
    statement: "Direct import and configuration of the standard 'logging' module is forbidden. All loggers must be acquired via 'from shared.logger import getLogger'."
    enforcement: error
    allowed_paths:
      - "src/shared/logger.py" # The module itself is exempt.

--- END OF FILE ./.intent/charter/policies/governance/logging_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/reporting_policy.yaml ---
policy_id: 93e18e85-75cb-47bf-8665-0f793d89b65d
id: reporting_policy
version: 1
title: "Generated Artifacts & Reporting Policy"
status: active
purpose: >
  To constitutionally define the location, format, and governance of all
  transient, machine-generated artifacts (reports, logs, bundles, etc.).
  This policy enforces a strict separation between the permanent, source-of-truth
  "Mind" (.intent/) and the ephemeral outputs of system actions.

scope:
  applies_to: ["agents", "tooling", "cli", "auditor"]

owners:
  accountable: "Core Maintainer"

review:
  frequency: "12 months"

rules:
  - id: reports.canonical_output_directory
    statement: "All generated reports and non-constitutional artifacts MUST be written to the 'reports/' directory at the repository root."
    enforcement: error
  - id: reports.require_header
    statement: "All human-readable text or YAML reports MUST begin with a standardized header block that clearly identifies them as generated artifacts."
    enforcement: warn
  - id: reports.retention_policy
    statement: "Reports are considered ephemeral and MAY be deleted after 30 days. They SHOULD NOT be checked into source control."
    enforcement: info

definitions:
  output_directory: "reports/"
  header_template: |
    # ==============================================================================
    # WARNING: THIS IS A GENERATED REPORT. DO NOT EDIT MANUALLY.
    # It is a transient artifact and SHOULD NOT be used as a primary source of data.
    # Source of Constitutional Truth: .intent/
    # Source of Operational History: CORE Database
    # Generated By: {tool_name}
    # Generated At: {timestamp_utc}
    # ==============================================================================

--- END OF FILE ./.intent/charter/policies/governance/reporting_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/risk_classification_policy.yaml ---
# .intent/charter/policies/governance/risk_classification_policy.yaml
policy_id: a7f3c891-4d2e-4c19-9b77-8e5f2a3d6c42
id: risk_classification_policy
version: "1.0.0"
title: "Risk Tier Classification Policy"
status: active
purpose: >
  Defines the deterministic logic for assigning risk tiers to proposed changes.
  This is the authoritative source for risk assessment used by the auditor and
  constitutional review processes.
scope:
  applies_to: [auditor, intent_guard, constitutional_review]
owners:
  primary: "Security Lead"
  reviewers: ["Core Maintainer", "Platform SRE"]
review:
  frequency: "6 months"

# ============================================================================
# RISK TIER DEFINITIONS
# ============================================================================
tiers:
  routine:
    score: 1
    description: "Low-impact changes with minimal blast radius"
    examples: ["documentation updates", "test additions", "comment fixes"]

  standard:
    score: 2
    description: "Normal feature work with localized impact"
    examples: ["new feature", "bug fix in features/", "refactoring within domain"]

  elevated:
    score: 3
    description: "Changes affecting core system behavior or cross-domain logic"
    examples: ["agent modifications", "service layer changes", "DB schema updates"]

  critical:
    score: 4
    description: "Changes to governance, safety, or system identity"
    examples: ["constitution amendments", "safety policy changes", "approver modifications"]

# ============================================================================
# AUTOMATIC CLASSIFICATION RULES
# ============================================================================
# Rules are evaluated in order. First match wins.
# The auditor MUST apply these rules before checking score_policy thresholds.

classification_rules:
  # CRITICAL TIER
  - tier: critical
    conditions:
      any_of:
        - path_matches_any:
            source: "charter/constitution/critical_paths.yaml"
            rationale: "Constitutional definition of criticality"
        - path_pattern: "charter/policies/safety_policy.yaml"
        - path_pattern: "charter/constitution/approvers.yaml"
        - path_pattern: "charter/policies/governance/risk_classification_policy.yaml"
          rationale: "This file itself is critical (meta-rule)"
        - modifies_schema: true
          rationale: "Schema changes affect validation across the system"

  # ELEVATED TIER
  - tier: elevated
    conditions:
      any_of:
        - path_pattern: "src/core/**/*.py"
          exclude: ["src/core/**/*_test.py"]
          rationale: "Core orchestration layer"
        - path_pattern: "src/services/**/*.py"
          exclude: ["src/services/**/*_test.py"]
          rationale: "Infrastructure services"
        - path_pattern: "charter/policies/**/*.yaml"
          rationale: "Any policy change requires elevated scrutiny"
        - modifies_database_schema: true
          rationale: "Schema migrations have wide impact"
        - adds_new_capability: true
          rationale: "New capabilities alter system behavior"
        - touches_files_count: ">= 5"
          rationale: "Multi-file changes increase coupling risk"

  # STANDARD TIER
  - tier: standard
    conditions:
      any_of:
        - path_pattern: "src/features/**/*.py"
          exclude: ["src/features/**/*_test.py"]
        - path_pattern: "src/api/**/*.py"
        - path_pattern: "src/cli/**/*.py"
        - modifies_existing_capability: true
        - adds_dependency: true
          rationale: "New dependencies increase supply chain risk"

  # ROUTINE TIER (default fallback)
  - tier: routine
    conditions:
      any_of:
        - path_pattern: "**/*.md"
        - path_pattern: "**/*_test.py"
        - path_pattern: "tests/**/*"
        - path_pattern: "docs/**/*"
        - changes_only_comments: true
        - changes_only_whitespace: true

# ============================================================================
# RISK ESCALATION MODIFIERS
# ============================================================================
# These can bump a change up one tier if conditions are met.

escalation_modifiers:
  - condition: author_is_new_contributor
    escalate_by: 1
    rationale: "New contributors require additional review"

  - condition: fails_automated_tests
    escalate_by: 1
    rationale: "Broken tests indicate unexpected behavior"

  - condition: adds_network_call
    escalate_by: 1
    source_rule: "safety_policy.yaml#restrict_network_access"
    rationale: "Network calls increase attack surface"

  - condition: adds_execution_primitive
    escalate_to: critical
    source_rule: "safety_policy.yaml#no_dangerous_execution"
    rationale: "Always critical, regardless of location"

  - condition: no_tests_included
    escalate_by: 1
    applies_when: "tier >= standard"
    rationale: "Untested code in production paths is risky"

# ============================================================================
# OVERRIDE MECHANISM
# ============================================================================
# Human operators can override the automatic classification, but must justify.

override:
  allowed_by: ["approvers"]
  requires:
    - field: "risk_override_justification"
      min_length: 100
    - field: "risk_override_approver"
      must_match: "charter/constitution/approvers.yaml#approvers[].identity"
  audit_trail: true
  escalation_on_override:
    # If you override DOWN, you need MORE approval, not less
    downgrade_requires: "critical_quorum"
    upgrade_requires: "standard_quorum"

# ============================================================================
# VALIDATION RULES
# ============================================================================
validation:
  - id: risk_tier_must_be_assigned
    enforcement: error
    message: "Every proposal must have an assigned risk tier before review"

  - id: risk_tier_must_match_rules
    enforcement: error
    message: "Risk tier must match automatic classification unless explicitly overridden"

  - id: critical_tier_requires_critical_quorum
    enforcement: error
    links_to: "charter/constitution/approvers.yaml#quorum.critical"

--- END OF FILE ./.intent/charter/policies/governance/risk_classification_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/tooling_policy.yaml ---
policy_id: b7c95ae3-05be-4138-8b27-12cd5670f4e9
id: tooling_policy
version: "1.1.0" # Version bump for SSOT alignment
title: "Tooling Policy"
status: active
purpose: >
  To define the sanctioned tools and how they are invoked by agents and operators,
  ensuring reproducibility, performance, and constitutional compliance.
scope:
  applies_to: [cli, agents, services]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "annual"

rules:
  - id: tools.registered_only
    statement: "Only tools/commands registered in the `core.cli_commands` database table may be invoked by agents/services."
    enforcement: error
  - id: tools.version_pinned
    statement: "Critical tools (linters, formatters, test runners) MUST be version-pinned in pyproject or lockfiles."
    enforcement: warn
  - id: tools.no_write_intent
    statement: "Tooling MUST NOT write to '.intent/charter/**'; proposals are the only path for constitutional changes."
    enforcement: error

--- END OF FILE ./.intent/charter/policies/governance/tooling_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/operations/canary_policy.yaml ---
policy_id: 5bc9a0e5-a6fb-47be-ab2f-82bce1217c84
id: canary_policy
version: "1.0.0"
title: "Canary Policy"
status: active

owners:
  - "Governance Lead"

review:
  frequency: "annual"

canary:
  enabled: true
  scope:
    paths:
      - "src/**"
      - "cli/**"
      - "agents/**"
    modes:
      - "development"
      - "staging"
  abort_conditions:
    - "audit:level=error"
    - "tests:failed>0"
    - "latency:p95>threshold"
  metrics:
    - name: "audit.errors"
      threshold: 0
      direction: "less"
    - name: "tests.failed"
      threshold: 0
      direction: "less"
    - name: "latency.p95.ms"
      threshold: 500
      direction: "less"

--- END OF FILE ./.intent/charter/policies/operations/canary_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/operations/dev_fastpath_policy.yaml ---
policy_id: 4923517d-7e30-48dc-8134-b119714ed2b8
id: dev_fastpath_policy
version: "1.0.0"
title: Developer Fastpath Policy
purpose: >
  Allow fast, local feedback loops for developers while preserving safety via CI hard gates.
scope:
  applies_to:
    - developers
    - cli
    - ci
owners:
  primary: ["DX Lead"]
  reviewers: ["Governance Lead"]
review:
  frequency: "12 months"

rules:
  thresholds:
    max_changed_files: 20
    allow_intent_changes: false
  required_checks:
    - syntax
    - linter
    - formatter
  disallowed_paths:
    - ".intent/**"
    - "src/system/governance/**"

checklist:
  - Pre-commit runs syntax + linter + formatter on changed files (< thresholds).
  - Pre-push runs a mini-audit; CI enforces full auditor run.
  - Any change under .intent/** or governance/** bypasses fastpath and triggers full checks.

--- END OF FILE ./.intent/charter/policies/operations/dev_fastpath_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/operations/incident_response_policy.yaml ---
policy_id: 95df7291-6fc2-4a5b-9f6d-78e9af5ac200
id: incident_response_policy
version: "1.0.0"
title: "Incident Response Policy"
status: active
purpose: >
  Provide a lightweight, auditable process to respond to security and governance incidents.
scope:
  applies_to: [security, governance, ci, services]
owners:
  primary: "Security Lead"
  reviewers: ["Governance Lead"]
review:
  frequency: "12 months"

severity:
  - low
  - medium
  - high
  - critical

rules:
  - id: ir.triage_required
    statement: All incidents MUST be triaged within 24h with severity and owner assigned.
    enforcement: error
  - id: ir.timeline
    statement: A minimal timeline (what happened, when, who, evidence) MUST be recorded.
    enforcement: warn
  - id: ir.comms
    statement: Notifications MUST be sent to maintainers for high/critical incidents.
    enforcement: warn
  - id: ir.postmortem
    statement: High/critical incidents REQUIRE a short postmortem with actions and owners.
    enforcement: warn

checklist:
  - Auditor verifies incident records exist for flagged events (secrets exposure, DB policy violations).
  - Auditor checks postmortems for high/critical incidents within 7 days.

--- END OF FILE ./.intent/charter/policies/operations/incident_response_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/operations/workflows_policy.yaml ---
policy_id: a3e1b0c4-9f8d-4a7b-8c1d-6e5f4a3b2c1d
id: workflows_policy
version: "1.3.0" # Version bump for corrected workflow
title: "System Workflows Policy"
status: active
purpose: >
  Defines the sequences for both mandatory integration checks and optional
  self-healing routines. This is the single source of truth for all
  multi-step system operations.
owners:
  primary: "Governance Lead"
review:
  frequency: "12 months"

# ==============================================================================
# SECTION 1: MANDATORY INTEGRATION WORKFLOW
# The `submit changes` command MUST execute these steps in order.
# ==============================================================================
integration_workflow:
  - id: "linkage.assign_ids"
    command: "core-admin fix ids --write"
    description: "Assign stable UUIDs to any new public symbols."
    continues_on_failure: false

  - id: "linkage.duplicate_ids"
    command: "core-admin fix duplicate-ids --write"
    description: "Finds and resolves duplicate '# ID:' tags across the codebase."
    continues_on_failure: false

  - id: "ssot.sync_knowledge"
    command: "core-admin manage database sync-knowledge --write"
    description: "Synchronize the codebase state with the database SSOT."
    continues_on_failure: false

  - id: "knowledge.vectorize"
    command: "core-admin run vectorize --write"
    description: "Generate embeddings for new or modified symbols."
    continues_on_failure: false

  - id: "knowledge.define_symbols"
    command: "core-admin manage define-symbols"
    description: "Assign capability keys to vectorized symbols."
    continues_on_failure: true # Acknowledges that AI might fail, but shouldn't block commit

  - id: "governance.audit"
    command: "core-admin check audit"
    description: "Run the full constitutional audit on the new state."
    continues_on_failure: false # This is a hard gate

# ==============================================================================
# SECTION 2: SELF-HEALING ROUTINES
# These are standalone, on-demand commands for developers and autonomous agents.
# They are NOT part of the blocking integration workflow.
# ==============================================================================
self_healing_routines:
  - id: "style.headers"
    command: "core-admin fix headers --write"
    description: "Enforces constitutional header conventions on all Python files in `src/`."

  - id: "docs.docstrings"
    command: "core-admin fix docstrings --write"
    description: "Uses an AI to add missing docstrings to public functions and methods."

  - id: "style.formatting"
    command: "core-admin fix code-style"
    description: "Auto-formats all code using Black and Ruff."

--- END OF FILE ./.intent/charter/policies/operations/workflows_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/safety_policy.yaml ---
policy_id: 05ffbf34-2e0e-4069-b77e-473923537077
id: safety_policy
version: "1.5.0" # Final version for this refactoring
title: "System Safety & Security Policy"
status: active
purpose: >
  The single source of truth for all security and safety policies governing
  code generation, execution, and self-modification.
scope:
  applies_to: [agents, code, ci, services]
owners:
  primary: "Security Lead"
  reviewers: ["Core Maintainer"]
review:
  frequency: "12 months"

rules:
  # ===================================================================
  # RULE: Govern Self-Modification (Immutable Constitution)
  # ===================================================================
  - id: immutable_constitution
    description: >
      The core mission files are immutable and can only be changed via the full,
      human-in-the-loop constitutional amendment process.
    enforcement: warn
    applies_to:
      paths:
        - "charter/mission/principles.yaml"
        - "charter/mission/manifesto.md"
        - "charter/mission/northstar.yaml"

  # ===================================================================
  # RULE: No self-modification of core loop
  # ===================================================================
  - id: deny_core_loop_edit
    description: >
      CORE cannot modify its own core orchestration and governance engine
      without explicit human review via the formal amendment process.
    enforcement: error
    applies_to:
      paths:
        - "src/core/main.py"
        - "src/core/intent_guard.py"
        - "charter/policies/intent_guard_policy.yaml"
        - "charter/policies/safety_policy.yaml"
    action: require_human_approval
    feedback: |
      🔒 Core logic modification detected. Human review required before application.

  # ===================================================================
  # RULE: Block dangerous execution primitives
  # ===================================================================
  - id: no_dangerous_execution
    description: >
      Generated or modified code must not contain calls to dangerous functions
      that enable arbitrary code execution, shell access, or unsafe deserialization.
    enforcement: error
    scope:
      domains: [core, agents, features]
      exclude:
        - path: "tests/**"
          rationale: "Test files require direct execution for validation"
        - path: "src/core/git_service.py"
          rationale: >
            This file is exempt as it safely uses subprocess.run() without shell=True.
    detection:
      type: regex
      patterns:
        - "eval\\("
        - "exec\\("
        - "compile\\("
        - "os\\.system\\("
        - "os\\.popen\\("
        - "subprocess\\.(run|Popen|call)\\([^)]*shell\\s*=\\s*True"
        - "shutil\\.rmtree\\("
        - "os\\.remove\\("
        - "os\\.rmdir\\("
    action: reject
    feedback: |
      ❌ Dangerous execution detected: '{{pattern}}'. Use safe wrappers or avoid shell=True.

  # ===================================================================
  # RULE: Block dangerous imports
  # ===================================================================
  - id: no_unsafe_imports
    description: >
      Source code must not import modules that provide direct, unsafe access
      to the filesystem or subprocesses. The 'os' module is permitted for safe
      operations like 'os.getenv', as dangerous functions are blocked by the
      'no_dangerous_execution' rule.
    enforcement: error
    scope:
      exclude:
        # CONSTITUTIONAL EXEMPTIONS: These are the *only* modules
        # allowed to perform low-level system operations.
        - path: "src/shared/utils/subprocess_utils.py"
          rationale: "This is the sanctioned, safe wrapper for running subprocesses."
        - path: "src/core/git_service.py"
          rationale: "GitService is a sanctioned wrapper around the git subprocess."
        - path: "src/features/governance/runtime_validator.py"
          rationale: "Canary test runner requires subprocess to invoke pytest."
        - path: "src/core/test_runner.py"
          rationale: "The primary test runner needs subprocess to invoke pytest."
        - path: "src/cli/logic/chat.py"
          rationale: "Needs to run 'core-admin --help' to provide context to the LLM."
        - path: "src/features/project_lifecycle/bootstrap_service.py"
          rationale: "Needs to run 'gh' CLI for bootstrapping GitHub issues."
        - path: "src/features/project_lifecycle/integration_service.py"
          rationale: "The master workflow orchestrator needs to call other core-admin commands."
        - path: "src/services/repositories/db/common.py"
          rationale: "TECHNICAL DEBT: This low-level utility calls 'git' and should be refactored."
        - path: "src/core/ruff_linter.py"
          rationale: "The linter is a core tool that needs to run an external process."
        - path: "scripts/**"
          rationale: "Developer scripts are exempt for utility purposes."
    detection:
      type: import_scan
      forbidden:
        # - "os" # Allowed for safe operations like getenv; dangerous calls are blocked separately.
        - "shutil"
        - "subprocess"

  # ===================================================================
  # RULE: Restrict network access
  # ===================================================================
  - id: restrict_network_access
    description: >
      Only explicitly allowed domains may be contacted. All outbound network
      calls must be through approved integration points.
    enforcement: error
    evidence: ["network_access_log"]
    allowed_domains:
      - "api.openai.com"
      - "github.com"
      - "api.deepseek.com"
      - "api.anthropic.com"
    action: reject
    feedback: |
      ❌ Attempt to contact unauthorized domain: {{domain}}. Update safety_policy.yaml to allow if needed.

  # ===================================================================
  # RULE: All changes must be logged
  # ===================================================================
  - id: change_must_be_logged
    description: >
      Every file change must be preceded by a log entry recorded at CORE_ACTION_LOG_PATH
      with IntentBundle ID and description.
    enforcement: error
    triggers:
      - before_write
    validator: change_log_checker
    action: reject_if_unlogged
    feedback: |
      ❌ No prior log entry found for this change. Write to CORE_ACTION_LOG_PATH before modifying files.

--- END OF FILE ./.intent/charter/policies/safety_policy.yaml ---

--- START OF FILE ./.intent/charter/schemas/agent/agent_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/agent_policy_schema.json",
  "title": "Agent Policy",
  "description": "Constitutional schema for the single, authoritative agent governance policy.",
  "type": "object",
  "required": ["id", "version", "title", "status", "purpose", "rules"],
  "properties": {
    "id": { "const": "agent_policy" },
    "version": { "type": "string", "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$" },
    "title": { "type": "string" },
    "status": { "enum": ["active", "draft", "deprecated"] },
    "purpose": { "type": "string" },
    "scope": { "type": "object" },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "rules": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "statement", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "enforcement": { "enum": ["info", "warn", "error"] }
        },
        "additionalProperties": false
      }
    },
    "resource_selection": {
      "type": "object"
    }
  },
  "additionalProperties": true
}

--- END OF FILE ./.intent/charter/schemas/agent/agent_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/agent/cognitive_roles_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Cognitive Roles Policy",
  "description": "Schema for defining abstract cognitive roles and assigning them to named resources.",
  "type": "object",
  "required": ["cognitive_roles"],
  "properties": {
    "cognitive_roles": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["role", "description", "assigned_resource", "required_capabilities"],
        "properties": {
          "role": {
            "type": "string",
            "description": "The unique name of the cognitive role (e.g., 'Planner')."
          },
          "description": {
            "type": "string"
          },
          "assigned_resource": {
            "type": "string",
            "description": "The named resource (e.g., 'deepseek_chat') to assign to this role."
          },
          "required_capabilities": {
            "type": "array",
            "description": "A list of skills required by this role for validation.",
            "items": { "type": "string" }
          }
        }
      }
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/charter/schemas/agent/cognitive_roles_schema.json ---

--- START OF FILE ./.intent/charter/schemas/agent/micro_proposal_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/micro_proposal_policy_schema.json",
  "title": "Micro-Proposal Policy",
  "description": "Constitutional schema for the policy governing low-risk, autonomous changes.",
  "type": "object",
  "required": ["id", "version", "title", "status", "purpose", "rules"],
  "properties": {
    "id": { "const": "micro_proposal_policy" },
    "version": { "type": "string", "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$" },
    "title": { "type": "string" },
    "status": { "enum": ["active", "draft", "deprecated"] },
    "purpose": { "type": "string" },
    "scope": { "type": "object" },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "rules": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "description", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "description": { "type": "string" },
          "enforcement": { "enum": ["info", "warn", "error"] },
          "allowed_actions": {
            "type": "array",
            "items": { "type": "string" }
          },
          "allowed_paths": {
            "type": "array",
            "items": { "type": "string" }
          },
          "forbidden_paths": {
            "type": "array",
            "items": { "type": "string" }
          },
          "required_evidence": {
            "type": "array",
            "items": { "type": "string" }
          }
        },
        "additionalProperties": true
      }
    }
  },
  "additionalProperties": true
}

--- END OF FILE ./.intent/charter/schemas/agent/micro_proposal_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/agent/resource_manifest_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "LLM Resource Manifest Policy",
  "description": "Constitutional schema for the policy defining available LLM resources.",
  "type": "object",
  "allOf": [{ "$ref": "policy_schema.json" }],
  "properties": {
    "id": { "const": "resource_manifest_policy" },
    "llm_resources": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["name", "provided_capabilities", "env_prefix"],
        "properties": {
          "name": { "type": "string" },
          "provided_capabilities": { "type": "array", "items": { "type": "string" } },
          "env_prefix": { "type": "string" },
          "performance_metadata": { "type": "object" }
        }
      }
    }
  },
  "required": ["llm_resources"]
}

--- END OF FILE ./.intent/charter/schemas/agent/resource_manifest_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/code/capability_tag_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "CORE Capability Tag Definition",
  "description": "The formal schema for a single, well-defined capability in the CORE system.",
  "type": "object",
  "required": [
    "key",
    "title",
    "description",
    "owner",
    "status",
    "risk_level"
  ],
  "properties": {
    "key": {
      "type": "string",
      "description": "The unique, canonical identifier for the capability, following the 'domain.action' pattern.",
      "pattern": "^[a-z0-9_]+(\\.[a-z0-9_]+)+$"
    },
    "title": {
      "type": "string",
      "description": "A short, human-readable title for the capability.",
      "minLength": 5
    },
    "description": {
      "type": "string",
      "description": "A clear, one-sentence explanation of what this capability does.",
      "minLength": 10
    },
    "owner": {
      "type": "string",
      "description": "The architectural domain that owns and is responsible for this capability."
    },
    "status": {
      "type": "string",
      "description": "The current lifecycle status of the capability.",
      "enum": ["active", "deprecated", "experimental"]
    },
    "risk_level": {
      "type": "string",
      "description": "The assessed risk of invoking this capability (low, medium, or high).",
      "enum": ["low", "medium", "high"]
    },
    "aliases": {
      "type": "array",
      "description": "A list of old or alternative names for this capability to ensure backward compatibility.",
      "items": {
        "type": "string"
      },
      "uniqueItems": true
    },
    "policy_refs": {
      "type": "array",
      "description": "A list of policy files that govern or relate to this capability.",
      "items": {
        "type": "string"
      }
    },
    "vector": {
      "type": "array",
      "description": "A pre-computed semantic vector representing the meaning of the capability's source code.",
      "items": {
        "type": "number"
      }
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/charter/schemas/code/capability_tag_schema.json ---

--- START OF FILE ./.intent/charter/schemas/code/dependency_injection_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Dependency Injection Policy",
  "description": "Constitutional schema for the Dependency Injection (DI) policy.",
  "type": "object",
  "allOf": [{ "$ref": "../policy_schema.json" }],
  "properties": {
    "id": { "const": "dependency_injection_policy" },
    "rules": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["id", "statement", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "enforcement": { "enum": ["info", "warn", "error"] },
          "scope": { "type": "array", "items": { "type": "string" } },
          "exclusions": { "type": "array", "items": { "type": "string" } },
          "forbidden_instantiations": {
            "type": "array",
            "items": { "type": "string" }
          },
          "forbidden_imports": {
            "type": "array",
            "items": { "type": "string" }
          }
        },
        "additionalProperties": false
      }
    }
  },
  "required": ["rules"]
}

--- END OF FILE ./.intent/charter/schemas/code/dependency_injection_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/code/knowledge_graph_entry_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://core.system/schema/knowledge_graph_entry.json",
  "title": "Knowledge Graph Symbol Entry",
  "description": "Schema for a single symbol (function or class) in the knowledge_graph.json file.",
  "type": "object",
  "required": [
    "key",
    "name",
    "type",
    "file",
    "capability",
    "intent",
    "last_updated",
    "calls",
    "line_number",
    "is_async",
    "parameters",
    "is_class",
    "structural_hash"
  ],
  "properties": {
    "key": { "type": "string", "description": "The unique identifier for the symbol (e.g., 'path/to/file.py::MyClass')." },
    "name": { "type": "string", "description": "The name of the function or class." },
    "type": { "type": "string", "enum": ["FunctionDef", "ClassDef", "AsyncFunctionDef"] },
    "file": { "type": "string", "description": "The relative path to the source file." },
    "tags": {
      "type": "array",
      "items": { "type": "string" },
      "description": "A list of domain tags classifying the symbol's purpose."
    },
    "owner": {
      "type": "string",
      "description": "The agent or team responsible for this capability."
    },
    "capability": { "type": "string", "description": "The unique UUID of the capability this symbol implements, or 'unassigned'." },
    "intent": { "type": "string", "description": "A clear, concise statement of the symbol's purpose." },
    "docstring": { "type": ["string", "null"], "description": "The raw docstring from source code." },
    "calls": { "type": "array", "items": { "type": "string" }, "description": "List of other functions called by this one." },
    "line_number": { "type": "integer", "minimum": 0 },
    "is_async": { "type": "boolean" },
    "parameters": { "type": "array", "items": { "type": "string" } },
    "entry_point_type": { "type": ["string", "null"], "description": "Type of entry point if applicable (e.g., 'fastapi_route_post')." },
    "last_updated": { "type": "string", "format": "date-time" },
    "is_class": { "type": "boolean", "description": "True if the symbol is a class definition." },
    "base_classes": {
      "type": "array",
      "items": { "type": "string" },
      "description": "A list of base classes this symbol inherits from (if it is a class)."
    },
    "entry_point_justification": {
      "type": ["string", "null"],
      "description": "The name of the pattern that identified this symbol as an entry point."
    },
    "parent_class_key": {
      "type": ["string", "null"],
      "description": "The key of the parent class, if this symbol is a method."
    },
    "structural_hash": {
      "type": "string",
      "description": "A SHA256 hash of the symbol's structure, ignoring comments and docstrings."
    },
    "vector": {
      "type": ["array", "null"],
      "description": "A pre-computed semantic vector representing the meaning of the capability's source code.",
      "items": {
        "type": "number"
      }
    },
    "end_line_number": {
      "type": ["integer", "null"],
      "description": "The line number where the symbol's definition ends."
    },
    "source_code": {
      "type": ["string", "null"],
      "description": "The exact, unparsed source code of the symbol."
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/charter/schemas/code/knowledge_graph_entry_schema.json ---

--- START OF FILE ./.intent/charter/schemas/constitutional/intent_bundle_schema.json ---
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Intent Bundle",
    "description": "A schema for the structured data package representing a single, reasoned action by the CORE system.",
    "type": "object",
    "required": [
        "bundle_id",
        "initiator",
        "created_at",
        "goal",
        "justification",
        "risk_tier",
        "status",
        "evidence"
    ],
    "properties": {
        "bundle_id": {
            "type": "string",
            "description": "A unique identifier for this bundle of work."
        },
        "initiator": {
            "type": "string",
            "description": "The human operator or system agent that initiated the action."
        },
        "created_at": {
            "type": "string",
            "format": "date-time"
        },
        "goal": {
            "type": "string",
            "description": "The high-level goal this bundle is intended to achieve."
        },
        "justification": {
            "type": "string",
            "description": "The constitutional principle(s) this action serves."
        },
        "risk_tier": {
            "type": "string",
            "enum": ["low", "medium", "high"],
            "description": "The assessed risk level of the proposed change."
        },
        "status": {
            "type": "string",
            "enum": ["draft", "planned", "validated", "approved", "executed", "archived", "failed"],
            "description": "The current state in the lifecycle of the bundle."
        },
        "evidence": {
            "type": "object",
            "description": "A collection of links to artifacts that support this action.",
            "properties": {
                "plan_id": { "type": "string" },
                "validation_report_id": { "type": "string" },
                "canary_report_id": { "type": "string" },
                "test_report_id": { "type": "string" },
                "approval_signature_ids": {
                    "type": "array",
                    "items": { "type": "string" }
                }
            }
        }
    }
}

--- END OF FILE ./.intent/charter/schemas/constitutional/intent_bundle_schema.json ---

--- START OF FILE ./.intent/charter/schemas/constitutional/intent_crate_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/intent_crate_schema.json",
  "title": "Intent Crate Manifest",
  "description": "The constitutional schema for a manifest.yaml file within an Intent Crate. This defines a formal, auditable request for change.",
  "type": "object",
  "required": ["crate_id", "author", "intent", "type"],
  "properties": {
    "crate_id": {
      "type": "string",
      "description": "A unique identifier for this crate, typically matching the directory name.",
      "pattern": "^[a-zA-Z0-9_-]+$"
    },
    "author": {
      "type": "string",
      "description": "The identity of the human or system that created the crate (e.g., an email address)."
    },
    "intent": {
      "type": "string",
      "description": "A clear, one-sentence justification for the proposed change.",
      "minLength": 20
    },
    "type": {
      "type": "string",
      "description": "The type of change being proposed.",
      "enum": ["CONSTITUTIONAL_AMENDMENT", "CODE_MODIFICATION"]
    },
    "payload_files": {
        "type": "array",
        "description": "A list of the files included in this crate that are part of the change.",
        "items": {
            "type": "string"
        }
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/charter/schemas/constitutional/intent_crate_schema.json ---

--- START OF FILE ./.intent/charter/schemas/constitutional/proposal_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://core.local/schemas/proposal.schema.json",
  "title": "CORE Proposal (v1)",
  "type": "object",
  "additionalProperties": false,
  "required": ["target_path", "action", "justification", "content"],
  "properties": {
    "target_path": {
      "type": "string",
      "description": "Repo-relative path to the file to be replaced. Must not be inside .intent/proposals/.",
      "pattern": "^(?!\\.intent\\/proposals\\/)[\\w\\-\\.\\/]+$",
      "$comment": "Allows any path as long as it's not writing into the proposals directory itself."
    },
    "action": {
      "type": "string",
      "enum": ["replace_file"],
      "description": "Currently only full file replacement is supported."
    },
    "justification": {
      "type": "string",
      "minLength": 10,
      "description": "Human-readable rationale for the change.",
      "pattern": "\\S"
    },
    "content": {
      "type": "string",
      "minLength": 1
    },
    "signatures": {
      "type": "array",
      "description": "Optional array of signature objects.",
      "items": { "$ref": "#/$defs/signature" }
    }
  },
  "$defs": {
    "signature": {
      "type": "object",
      "additionalProperties": false,
      "required": ["identity", "signature_b64", "token", "timestamp"],
      "properties": {
        "identity": { "type": "string" },
        "signature_b64": { "type": "string", "contentEncoding": "base64" },
        "token": {
          "type": "string",
          "pattern": "^core-proposal-v[0-9]+:[a-f0-9]{64}$",
          "$comment": "Allows any version number for the token, e.g., v1, v6."
        },
        "timestamp": { "type": "string", "format": "date-time" }
      }
    }
  }
}

--- END OF FILE ./.intent/charter/schemas/constitutional/proposal_schema.json ---

--- START OF FILE ./.intent/charter/schemas/data/database_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Database Policy Schema",
  "type": "object",
  "required": ["id", "version", "title", "engine", "migrations", "rules", "drift"],
  "properties": {
    "id": { "const": "database_policy" },
    "version": { "type": "string" },
    "title": { "type": "string" },
    "engine": {
      "type": "object",
      "required": ["type", "schema"],
      "properties": {
        "type": { "type": "string", "enum": ["postgresql"] },
        "schema": { "type": "string", "minLength": 1 }
      }
    },
    "migrations": {
      "type": "object",
      "required": ["directory", "order"],
      "properties": {
        "directory": { "type": "string" },
        "order": {
          "type": "array",
          "items": { "type": "string", "pattern": "^\\d{3}_.+\\.sql$" },
          "minItems": 1
        }
      }
    },
    "rules": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "statement", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "enforcement": { "enum": ["error", "warn", "info"] }
        },
        "additionalProperties": false
      }
    },
    "retention": {
      "type": "object",
      "properties": {
        "audit_runs_days": { "type": "integer", "minimum": 1 },
        "proposals_days": { "type": "integer", "minimum": 1 }
      },
      "additionalProperties": true
    },
    "drift": {
      "type": "object",
      "required": ["development", "production"],
      "properties": {
        "development": { "type": "string", "enum": ["warn", "block"] },
        "production": { "type": "string", "enum": ["warn", "block"] }
      }
    },
    "backup_restore": {
      "type": "object",
      "properties": {
        "cadence": { "type": "string" },
        "test_restore_quarterly": { "type": "boolean" }
      }
    },
    "quorum": {
      "type": "object",
      "properties": {
        "changes_require_critical_paths": { "type": "boolean" }
      }
    }
  },
  "additionalProperties": true
}

--- END OF FILE ./.intent/charter/schemas/data/database_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/data/database_schema.yaml ---
version: 1
description: >
  Initial schema for CORE's operational database.
  Stores auditable history of events, not constitutional truth.

tables:
  capabilities:
    description: >
      Current catalog of capabilities with their owners and tags.
      Mirrors .intent/knowledge/domains but may include runtime metadata.
    columns:
      - name: key
        type: text
        constraints: [primary_key]
      - name: title
        type: text
      - name: description
        type: text
      - name: owner
        type: text
      - name: tags
        type: jsonb
      - name: updated_at
        type: timestamptz

  capability_history:
    description: Versioned history of capability changes.
    columns:
      - name: id
        type: bigserial
        constraints: [primary_key]
      - name: capability_key
        type: text
      - name: change_type
        type: text   # created, updated, deleted
      - name: diff
        type: jsonb
      - name: changed_at
        type: timestamptz

  cli_runs:
    description: >
      Each execution of a core-admin command with timestamp and result.
    columns:
      - name: id
        type: bigserial
        constraints: [primary_key]
      - name: command
        type: text
      - name: args
        type: jsonb
      - name: result
        type: text   # success, fail
      - name: run_at
        type: timestamptz

  audits:
    description: >
      Records every constitutional audit or validation run.
    columns:
      - name: id
        type: bigserial
        constraints: [primary_key]
      - name: scope
        type: text
      - name: result
        type: jsonb
      - name: run_at
        type: timestamptz

migrations:
  - id: 0001-initial
    description: Initial schema creation for capabilities, capability_history, cli_runs, audits.
    created_at: "2025-09-18"
    approved_by: TBD

--- END OF FILE ./.intent/charter/schemas/data/database_schema.yaml ---

--- START OF FILE ./.intent/charter/schemas/governance/available_actions_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Available Actions Policy",
  "description": "Defines the complete set of actions available to the PlannerAgent.",
  "type": "object",
  "allOf": [{ "$ref": "../policy_schema.json" }],
  "properties": {
    "id": { "const": "available_actions_policy" },
    "actions": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["name", "description", "parameters"],
        "properties": {
          "name": { "type": "string" },
          "description": { "type": "string" },
          "parameters": {
            "type": "array",
            "items": {
              "type": "object",
              "required": ["name", "type", "description", "required"],
              "properties": {
                "name": { "type": "string" },
                "type": { "type": "string" },
                "description": { "type": "string" },
                "required": { "type": "boolean" }
              }
            }
          }
        }
      }
    }
  },
  "required": ["actions"]
}

--- END OF FILE ./.intent/charter/schemas/governance/available_actions_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/governance/cli_registry_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "CLI Registry Policy",
  "description": "The constitutional policy that serves as the single source of truth for all registered core-admin CLI commands.",
  "type": "object",
  "allOf": [{ "$ref": "../policy_schema.json" }],
  "properties": {
    "id": { "const": "cli_registry_policy" },
    "commands": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["name", "summary", "entrypoint", "category"],
        "properties": {
          "name": { "type": "string" },
          "module": { "type": "string" },
          "entrypoint": { "type": "string" },
          "summary": { "type": "string" },
          "category": { "type": "string" }
        }
      }
    }
  },
  "required": ["commands"]
}

--- END OF FILE ./.intent/charter/schemas/governance/cli_registry_schema.json ---

--- START OF FILE ./.intent/charter/schemas/governance/enforcement_model_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Enforcement Model Policy Schema",
  "description": "Schema for the canonical enforcement model policy.",
  "type": "object",
  "required": [
    "version",
    "title",
    "purpose",
    "levels",
    "rules"
  ],
  "properties": {
    "version": { "type": "string", "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$" },
    "title": { "type": "string" },
    "purpose": { "type": "string" },
    "scope": { "type": "object" },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "levels": {
      "type": "object",
      "required": ["error", "warn", "info"],
      "properties": {
        "error": { "$ref": "#/definitions/level" },
        "warn": { "$ref": "#/definitions/level" },
        "info": { "$ref": "#/definitions/level" }
      },
      "additionalProperties": false
    },
    "rules": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["id", "statement", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "enforcement": { "enum": ["error", "warn", "info"] }
        }
      }
    }
  },
  "additionalProperties": false,
  "definitions": {
    "level": {
      "type": "object",
      "required": ["description", "ci_behavior", "runtime_behavior"],
      "properties": {
        "description": { "type": "string" },
        "ci_behavior": { "type": "string" },
        "runtime_behavior": { "type": "string" }
      },
      "additionalProperties": false
    }
  }
}

--- END OF FILE ./.intent/charter/schemas/governance/enforcement_model_schema.json ---

--- START OF FILE ./.intent/charter/schemas/governance/intent_guard_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Intent Guard Policy",
  "description": "Schema for the core IntentGuard rules that prevent unauthorized system modifications.",
  "type": "object",
  "required": ["rules"],
  "properties": {
    "rules": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "description", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "description": { "type": "string" },
          "enforcement": {
            "type": "string",
            "enum": ["error", "warn", "info"]
          },
          "applies_to": {
            "type": "object",
            "properties": {
              "paths": { "type": "array", "items": { "type": "string" } }
            }
          },
          "exclude": {
            "type": "object",
            "properties": {
              "paths": { "type": "array", "items": { "type": "string" } }
            }
          }
        }
      }
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/charter/schemas/governance/intent_guard_schema.json ---

--- START OF FILE ./.intent/charter/schemas/governance/reporting_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/reporting_policy.schema.json",
  "title": "Reporting Policy",
  "description": "Constitutional schema for governing generated artifacts and reports.",
  "type": "object",
  "required": ["id", "version", "purpose", "rules", "definitions"],
  "additionalProperties": false,
  "properties": {
    "id": { "const": "reporting_policy" },
    "version": { "type": "integer" },
    "title": { "type": "string" },
    "status": { "enum": ["active", "draft"] },
    "purpose": { "type": "string" },
    "scope": { "type": "object" },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "rules": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["id", "statement", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "enforcement": { "enum": ["info", "warn", "error"] }
        }
      }
    },
    "definitions": {
      "type": "object",
      "required": ["output_directory", "header_template"],
      "properties": {
        "output_directory": { "type": "string" },
        "header_template": { "type": "string" }
      }
    }
  }
}

--- END OF FILE ./.intent/charter/schemas/governance/reporting_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/governance/risk_classification_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/governance/risk_classification_policy_schema.json",
  "title": "Risk Classification Policy Schema",
  "description": "Canonical schema for risk_classification_policy.yaml. Ensures risk tier assignment logic is well-formed and enforceable.",
  "type": "object",
  "required": ["policy_id", "id", "version", "title", "status", "purpose", "owners", "review", "tiers", "classification_rules"],
  "properties": {
    "policy_id": {
      "type": "string",
      "description": "Unique UUID for this policy document.",
      "pattern": "^[0-9a-fA-F]{8}-([0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12}$"
    },
    "id": {
      "type": "string",
      "const": "risk_classification_policy",
      "description": "Must be exactly 'risk_classification_policy'."
    },
    "version": {
      "type": "string",
      "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$",
      "description": "Semantic version of the policy."
    },
    "title": {
      "type": "string",
      "description": "Human-readable title."
    },
    "status": {
      "type": "string",
      "enum": ["active", "draft", "deprecated"]
    },
    "purpose": {
      "type": "string",
      "minLength": 20,
      "description": "Clear explanation of why this policy exists."
    },
    "scope": {
      "type": "object",
      "properties": {
        "applies_to": {
          "type": "array",
          "items": { "type": "string" },
          "minItems": 1
        }
      }
    },
    "owners": {
      "type": "object",
      "required": ["primary"],
      "properties": {
        "primary": { "type": "string" },
        "reviewers": {
          "type": "array",
          "items": { "type": "string" }
        }
      }
    },
    "review": {
      "type": "object",
      "required": ["frequency"],
      "properties": {
        "frequency": {
          "type": "string",
          "description": "e.g., '6 months', 'quarterly'"
        },
        "last_reviewed": {
          "type": "string",
          "format": "date"
        }
      }
    },
    "tiers": {
      "type": "object",
      "description": "Definitions of each risk tier. Must include all four tiers.",
      "required": ["routine", "standard", "elevated", "critical"],
      "properties": {
        "routine": { "$ref": "#/$defs/tier_definition" },
        "standard": { "$ref": "#/$defs/tier_definition" },
        "elevated": { "$ref": "#/$defs/tier_definition" },
        "critical": { "$ref": "#/$defs/tier_definition" }
      },
      "additionalProperties": false
    },
    "classification_rules": {
      "type": "array",
      "description": "Ordered list of rules for automatic risk tier assignment. First match wins.",
      "minItems": 1,
      "items": { "$ref": "#/$defs/classification_rule" }
    },
    "escalation_modifiers": {
      "type": "array",
      "description": "Conditions that can bump a risk tier higher.",
      "items": { "$ref": "#/$defs/escalation_modifier" }
    },
    "override": {
      "type": "object",
      "description": "Rules governing human override of automatic classification.",
      "required": ["allowed_by", "requires"],
      "properties": {
        "allowed_by": {
          "type": "array",
          "items": { "type": "string" },
          "description": "Roles permitted to override (e.g., 'approvers')."
        },
        "requires": {
          "type": "array",
          "items": { "$ref": "#/$defs/override_requirement" },
          "minItems": 1
        },
        "audit_trail": {
          "type": "boolean",
          "description": "Whether overrides must be logged."
        },
        "escalation_on_override": {
          "type": "object",
          "properties": {
            "downgrade_requires": {
              "type": "string",
              "description": "Quorum type required to override DOWN (e.g., 'critical_quorum')."
            },
            "upgrade_requires": {
              "type": "string",
              "description": "Quorum type required to override UP."
            }
          }
        }
      }
    },
    "validation": {
      "type": "array",
      "description": "Meta-rules about risk tier assignment itself.",
      "items": { "$ref": "#/$defs/validation_rule" }
    }
  },
  "$defs": {
    "tier_definition": {
      "type": "object",
      "required": ["score", "description"],
      "properties": {
        "score": {
          "type": "integer",
          "minimum": 1,
          "maximum": 4,
          "description": "Numeric risk score (1=lowest, 4=highest)."
        },
        "description": {
          "type": "string",
          "minLength": 10,
          "description": "Clear explanation of what constitutes this tier."
        },
        "examples": {
          "type": "array",
          "items": { "type": "string" },
          "description": "Concrete examples to guide classification."
        }
      }
    },
    "classification_rule": {
      "type": "object",
      "required": ["tier", "conditions"],
      "properties": {
        "tier": {
          "type": "string",
          "enum": ["routine", "standard", "elevated", "critical"],
          "description": "The risk tier to assign if conditions match."
        },
        "conditions": {
          "type": "object",
          "description": "Logical conditions for this rule. Must have at least one condition group.",
          "minProperties": 1,
          "properties": {
            "any_of": {
              "type": "array",
              "items": { "$ref": "#/$defs/condition" },
              "description": "Match if ANY condition is true (OR logic)."
            },
            "all_of": {
              "type": "array",
              "items": { "$ref": "#/$defs/condition" },
              "description": "Match if ALL conditions are true (AND logic)."
            }
          }
        }
      }
    },
    "condition": {
      "type": "object",
      "description": "A single testable condition for risk classification.",
      "minProperties": 1,
      "properties": {
        "path_matches_any": {
          "type": "object",
          "required": ["source"],
          "properties": {
            "source": {
              "type": "string",
              "description": "YAML file containing a 'paths' array to check against."
            },
            "rationale": { "type": "string" }
          }
        },
        "path_pattern": {
          "type": "string",
          "description": "Glob pattern to match file paths (e.g., 'src/core/**/*.py')."
        },
        "exclude": {
          "type": "array",
          "items": { "type": "string" },
          "description": "Patterns to exclude from path_pattern matches."
        },
        "modifies_schema": {
          "type": "boolean",
          "description": "True if change affects schema files."
        },
        "modifies_database_schema": {
          "type": "boolean",
          "description": "True if change affects database schema."
        },
        "adds_new_capability": {
          "type": "boolean",
          "description": "True if change introduces a new capability tag."
        },
        "modifies_existing_capability": {
          "type": "boolean",
          "description": "True if change modifies existing capability."
        },
        "adds_dependency": {
          "type": "boolean",
          "description": "True if change adds external dependency."
        },
        "touches_files_count": {
          "type": "string",
          "pattern": "^(>|>=|<|<=|==)\\s*\\d+$",
          "description": "Comparison operator and number (e.g., '>= 5')."
        },
        "changes_only_comments": {
          "type": "boolean",
          "description": "True if only comments were modified."
        },
        "changes_only_whitespace": {
          "type": "boolean",
          "description": "True if only whitespace was modified."
        },
        "rationale": {
          "type": "string",
          "description": "Human-readable explanation of why this condition matters."
        }
      }
    },
    "escalation_modifier": {
      "type": "object",
      "required": ["condition"],
      "properties": {
        "condition": {
          "type": "string",
          "description": "Named condition to check (e.g., 'author_is_new_contributor')."
        },
        "escalate_by": {
          "type": "integer",
          "minimum": 1,
          "maximum": 3,
          "description": "How many tiers to bump up (1 = one tier higher)."
        },
        "escalate_to": {
          "type": "string",
          "enum": ["routine", "standard", "elevated", "critical"],
          "description": "Force escalation to this specific tier, ignoring current tier."
        },
        "applies_when": {
          "type": "string",
          "description": "Condition string for when this modifier applies (e.g., 'tier >= standard')."
        },
        "source_rule": {
          "type": "string",
          "description": "Reference to the safety policy rule this enforces (e.g., 'safety_policy.yaml#no_dangerous_execution')."
        },
        "rationale": {
          "type": "string",
          "minLength": 10,
          "description": "Why this escalation is necessary."
        }
      },
      "oneOf": [
        { "required": ["escalate_by"] },
        { "required": ["escalate_to"] }
      ]
    },
    "override_requirement": {
      "type": "object",
      "required": ["field"],
      "properties": {
        "field": {
          "type": "string",
          "description": "The proposal field that must be present for override."
        },
        "min_length": {
          "type": "integer",
          "minimum": 1,
          "description": "Minimum character length for text fields."
        },
        "must_match": {
          "type": "string",
          "description": "A reference to another config file for validation (e.g., 'approvers.yaml#approvers[].identity')."
        }
      }
    },
    "validation_rule": {
      "type": "object",
      "required": ["id", "enforcement", "message"],
      "properties": {
        "id": {
          "type": "string",
          "pattern": "^[a-z0-9_]+$",
          "description": "Unique identifier for this validation rule."
        },
        "enforcement": {
          "type": "string",
          "enum": ["error", "warn", "info"],
          "description": "Severity of validation failure."
        },
        "message": {
          "type": "string",
          "minLength": 10,
          "description": "Error message shown when validation fails."
        },
        "links_to": {
          "type": "string",
          "description": "Reference to related policy (e.g., 'approvers.yaml#quorum.critical')."
        }
      }
    }
  }
}

--- END OF FILE ./.intent/charter/schemas/governance/risk_classification_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/operations/canary_policy_schema.json ---
{
"$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "canary_policy.schema.json",
  "title": "Canary Policy",
  "type": "object",
  "required": ["id", "version", "title", "owners", "review", "canary"],
  "properties": {
    "id": { "type": "string", "pattern": "^[a-z0-9_.-]+$" },
    "version": { "type": "string", "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$" },
    "title": { "type": "string" },
    "status": { "type": "string", "enum": ["draft", "active", "deprecated"] },
    "owners": { "type": "array", "items": { "type": "string" }, "minItems": 1 },
    "review": {
      "type": "object",
      "required": ["frequency"],
      "properties": {
        "frequency": { "type": "string", "enum": ["monthly", "quarterly", "semiannual", "annual"] },
        "last_reviewed": { "type": "string", "format": "date" }
      },
      "additionalProperties": false
    },
    "canary": {
      "type": "object",
      "required": ["enabled", "scope", "abort_conditions"],
      "properties": {
        "enabled": { "type": "boolean" },
        "scope": {
          "type": "object",
          "properties": {
            "paths": { "type": "array", "items": { "type": "string" } },
            "modes": { "type": "array", "items": { "type": "string", "enum": ["development", "staging", "production"] } }
          },
          "additionalProperties": false
        },
        "abort_conditions": { "type": "array", "items": { "type": "string" }, "minItems": 1 },
        "metrics": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["name", "threshold", "direction"],
            "properties": {
              "name": { "type": "string" },
              "threshold": { "type": "number" },
              "direction": { "type": "string", "enum": ["greater", "less"] }
            },
            "additionalProperties": false
          }
        }
      },
      "additionalProperties": false
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/charter/schemas/operations/canary_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/operations/config_schema.yaml ---
# .intent/schemas/config_schema.yaml
git:
  ignore_validation:
    type: boolean
    default: false
    description: >
      If true, skips Git pre-write checks. MUST be false in production or fallback modes
      to maintain rollback safety. Only for emergency recovery.

--- END OF FILE ./.intent/charter/schemas/operations/config_schema.yaml ---

--- START OF FILE ./.intent/charter/schemas/operations/runtime_requirements_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Runtime Requirements",
  "type": "object",
  "required": ["id", "version", "title", "status", "variables", "owners", "review"],
  "properties": {
    "id": { "const": "runtime_requirements" },
    "version": { "type": "integer", "minimum": 1 },
    "title": { "type": "string" },
    "status": { "enum": ["active", "draft", "archived"] },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "variables": {
      "type": "object",
      "minProperties": 1,
      "patternProperties": {
        "^[A-Z0-9_]+$": {
          "type": "object",
          "required": ["description", "source", "required", "type", "used_by"],
          "properties": {
            "description": { "type": "string" },
            "source": { "enum": ["env", "secret", "cli"] },
            "required": { "type": "boolean" },
            "type": { "enum": ["string", "integer", "bool", "enum", "uri", "path"] },
            "allowed": { "type": "array", "items": { "type": "string" } },
            "default": {},
            "used_by": { "type": "array", "items": { "type": "string" } },
            "required_when": { "type": "string" }
          }
        }
      },
      "additionalProperties": false
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/charter/schemas/operations/runtime_requirements_schema.json ---

--- START OF FILE ./.intent/charter/schemas/operations/workflows_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "System Workflows Policy",
  "description": "Constitutional schema for the policy defining mandatory and optional system workflows.",
  "type": "object",
  "allOf": [{ "$ref": "../policy_schema.json" }],
  "properties": {
    "id": { "const": "workflows_policy" },
    "integration_workflow": {
      "type": "array",
      "description": "The mandatory, sequential workflow for integrating code changes.",
      "items": { "$ref": "#/$defs/workflow_step" }
    },
    "self_healing_routines": {
      "type": "array",
      "description": "A catalog of standalone, on-demand maintenance commands.",
      "items": { "$ref": "#/$defs/workflow_step" }
    }
  },
  "required": ["integration_workflow", "self_healing_routines"],
  "$defs": {
    "workflow_step": {
      "type": "object",
      "required": ["id", "command", "description"],
      "properties": {
        "id": { "type": "string", "description": "A unique, dot-notation identifier for the step." },
        "command": { "type": "string", "description": "The exact `core-admin` command to execute." },
        "description": { "type": "string", "description": "A human-readable explanation of the step's purpose." },
        "continues_on_failure": { "type": "boolean", "description": "If true, the workflow continues even if this step fails." }
      },
      "additionalProperties": false
    }
  }
}

--- END OF FILE ./.intent/charter/schemas/operations/workflows_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/policy_schema.json",
  "title": "CORE Policy",
  "description": "The canonical schema for all constitutional policy files in .intent/charter/policies/.",
  "type": "object",
  "required": ["policy_id", "id", "version", "title", "purpose", "status", "owners", "review"],
  "properties": {
    "policy_id": {
      "type": "string",
      "description": "A unique and stable UUID for this policy document.",
      "pattern": "^[0-9a-fA-F]{8}-([0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12}$"
    },
    "id": {
      "type": "string",
      "description": "The unique, snake_case identifier for the policy, matching the file name (e.g., 'agent_policy').",
      "pattern": "^[a-z0-9_]+_policy$"
    },
    "version": {
      "type": "string",
      "description": "The semantic version of the policy document.",
      "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$"
    },
    "title": {
      "type": "string",
      "description": "A human-readable, Title Case name for the policy."
    },
    "purpose": {
      "type": "string",
      "description": "A concise, one or two-sentence explanation of why this policy exists."
    },
    "status": {
      "type": "string",
      "description": "The current lifecycle status of the policy.",
      "enum": ["active", "draft", "deprecated"]
    },
    "owners": {
      "type": "object",
      "description": "Defines the roles responsible for maintaining this policy.",
      "properties": {
        "primary": { "type": "string" },
        "reviewers": { "type": "array", "items": { "type": "string" } }
      },
      "required": ["primary"]
    },
    "review": {
      "type": "object",
      "description": "Specifies the review cadence for this policy.",
      "properties": {
        "frequency": { "type": "string", "description": "e.g., '12 months', 'quarterly'" },
        "last_reviewed": { "type": "string", "format": "date" }
      },
      "required": ["frequency"]
    }
  },
  "additionalProperties": true
}

--- END OF FILE ./.intent/charter/schemas/policy_schema.json ---

--- START OF FILE ./.intent/meta.yaml ---
version: "2.0.0" # Version bump for v2.1 Schema Migration
# PURPOSE: This is the master index for the entire CORE constitution. It maps
# abstract concepts to their concrete file paths, fully embracing the new
# hierarchical policy structure for maximum clarity and governance.

charter:
  constitution:
    active_version: "charter/constitution/ACTIVE"
    amendment_process: "charter/constitution/amendment_process.md"
    approvers: "charter/constitution/approvers.yaml"
    critical_paths: "charter/constitution/critical_paths.yaml"
    operator_lifecycle: "charter/constitution/operator_lifecycle.md"

  mission:
    manifesto: "charter/mission/manifesto.md"
    northstar: "charter/mission/northstar.yaml"
    principles: "charter/mission/principles.yaml"

  policies:
    safety_policy: "charter/policies/safety_policy.yaml"

    agent:
      agent_policy: "charter/policies/agent/agent_policy.yaml"
      micro_proposal_policy: "charter/policies/agent/micro_proposal_policy.yaml"

    code:
      capability_linter_policy: "charter/policies/code/capability_linter_policy.yaml"
      code_health_policy: "charter/policies/code/code_health_policy.yaml"
      code_style_policy: "charter/policies/code/code_style_policy.yaml"
      dependency_injection_policy: "charter/policies/code/dependency_injection_policy.yaml"
      naming_conventions_policy: "charter/policies/code/naming_conventions_policy.yaml"
      refactoring_patterns_policy: "charter/policies/code/refactoring_patterns_policy.yaml"

    data:
      database_policy: "charter/policies/data/database_policy.yaml"
      secrets_management_policy: "charter/policies/data/secrets_management_policy.yaml"

    governance:
      audit_ignore_policy: "charter/policies/governance/audit_ignore_policy.yaml"
      auditor_policy: "charter/policies/governance/auditor_policy.yaml"
      available_actions_policy: "charter/policies/governance/available_actions_policy.yaml"
      cli_governance_policy: "charter/policies/governance/cli_governance_policy.yaml"
      enforcement_model_policy: "charter/policies/governance/enforcement_model_policy.yaml"
      intent_crate_policy: "charter/policies/governance/intent_crate_policy.yaml"
      intent_guard_policy: "charter/policies/governance/intent_guard_policy.yaml"
      knowledge_source_policy: "charter/policies/governance/knowledge_source_policy.yaml"
      logging_policy: "charter/policies/governance/logging_policy.yaml"
      reporting_policy: "charter/policies/governance/reporting_policy.yaml"
      risk_classification_policy: "charter/policies/governance/risk_classification_policy.yaml"
      tooling_policy: "charter/policies/governance/tooling_policy.yaml"

    operations:
      canary_policy: "charter/policies/operations/canary_policy.yaml"
      dev_fastpath_policy: "charter/policies/operations/dev_fastpath_policy.yaml"
      incident_response_policy: "charter/policies/operations/incident_response_policy.yaml"
      workflows_policy: "charter/policies/operations/workflows_policy.yaml"

  schemas:
    policy_schema: "charter/schemas/policy_schema.json"

    agent:
      agent_policy_schema: "charter/schemas/agent/agent_policy_schema.json"
      cognitive_roles_schema: "charter/schemas/agent/cognitive_roles_schema.json"
      micro_proposal_policy_schema: "charter/schemas/agent/micro_proposal_policy_schema.json"
      resource_manifest_policy_schema: "charter/schemas/agent/resource_manifest_policy_schema.json"

    code:
      capability_tag_schema: "charter/schemas/code/capability_tag_schema.json"
      dependency_injection_policy_schema: "charter/schemas/code/dependency_injection_policy_schema.json"
      knowledge_graph_entry_schema: "charter/schemas/code/knowledge_graph_entry_schema.json"

    constitutional:
      intent_bundle_schema: "charter/schemas/constitutional/intent_bundle_schema.json"
      intent_crate_schema: "charter/schemas/constitutional/intent_crate_schema.json"
      proposal_schema: "charter/schemas/constitutional/proposal_schema.json"

    data:
      database_policy_schema: "charter/schemas/data/database_policy_schema.json"
      database_schema: "charter/schemas/data/database_schema.yaml"

    governance:
      available_actions_policy_schema: "charter/schemas/governance/available_actions_policy_schema.json"
      cli_registry_schema: "charter/schemas/governance/cli_registry_schema.json"
      enforcement_model_schema: "charter/schemas/governance/enforcement_model_schema.json"
      intent_guard_schema: "charter/schemas/governance/intent_guard_schema.json"
      reporting_policy_schema: "charter/schemas/governance/reporting_policy_schema.json"
      risk_classification_policy_schema: "charter/schemas/governance/risk_classification_policy_schema.json"

    operations:
      canary_policy_schema: "charter/schemas/operations/canary_policy_schema.json"
      config_schema: "charter/schemas/operations/config_schema.yaml"
      runtime_requirements_schema: "charter/schemas/operations/runtime_requirements_schema.json"
      workflows_policy_schema: "charter/schemas/operations/workflows_policy_schema.json"

mind:
  config:
    local_mode: "mind/config/local_mode.yaml"
    runtime_requirements: "mind/config/runtime_requirements.yaml"

  evaluation:
    score_policy: "mind/evaluation/score_policy.yaml"
    audit_checklist: "mind/evaluation/audit_checklist.yaml"

  knowledge:
    entry_point_patterns: "mind/knowledge/entry_point_patterns.yaml"
    file_handlers: "mind/knowledge/file_handlers.yaml"
    source_structure: "mind/knowledge/source_structure.yaml"

  mind_export:
    cognitive_roles: "mind_export/cognitive_roles.yaml"
    resource_manifest: "mind_export/resource_manifest.yaml"
    capabilities: "mind_export/capabilities.yaml"
    symbols: "mind_export/symbols.yaml"
    links: "mind_export/links.yaml"
    northstar: "mind_export/northstar.yaml"

  prompts:
    capability_definer: "mind/prompts/capability_definer.prompt"
    code_peer_review: "mind/prompts/code_peer_review.prompt"
    constitutional_review: "mind/prompts/constitutional_review.prompt"
    create_file_planner: "mind/prompts/create_file_planner.prompt"
    enrich_symbol: "mind/prompts/enrich_symbol.prompt"
    fix_capability_manifest: "mind/prompts/fix_capability_manifest.prompt"
    fix_function_docstring: "mind/prompts/fix_function_docstring.prompt"
    fix_header: "mind/prompts/fix_header.prompt"
    fix_line_length: "mind/prompts/fix_line_length.prompt"
    goal_assessor: "mind/prompts/goal_assessor.prompt"
    intent_translator: "mind/prompts/intent_translator.prompt"
    micro_planner: "mind/prompts/micro_planner.prompt"
    module_docstring_writer: "mind/prompts/module_docstring_writer.prompt"
    new_capability_generator: "mind/prompts/new_capability_generator.prompt"
    planner_agent: "mind/prompts/planner_agent.prompt"
    refactor_for_clarity: "mind/prompts/refactor_for_clarity.prompt"
    refactor_outlier: "mind/prompts/refactor_outlier.prompt"
    standard_task_generator: "mind/prompts/standard_task_generator.prompt"
    vectorizer: "mind/prompts/vectorizer.prompt"

--- END OF FILE ./.intent/meta.yaml ---

--- START OF FILE ./.intent/mind/config/local_mode.yaml ---
# .intent/mind/config/local_mode.yaml

mode: local_fallback
apis:
  llm:
    enabled: false
    fallback: local_validator
  git:
    ignore_validation: false

# Development-specific overrides
dev_fastpath: true        # allow auto-sign in dev env only

--- END OF FILE ./.intent/mind/config/local_mode.yaml ---

--- START OF FILE ./.intent/mind/config/runtime_requirements.yaml ---
# .intent/mind/config/runtime_requirements.yaml
id: runtime_requirements
version: 1 # You might consider bumping this to 2 after applying
title: "Runtime Requirements"
status: active
owners:
  accountable: "Platform SRE"
  responsible: ["Core Maintainer"]
review:
  frequency: "6 months"

variables:
  # ======================================================================
  # 1. GLOBAL FALLBACK SETTINGS
  # ======================================================================
  LLM_CONNECT_TIMEOUT:
    description: "Timeout in seconds for establishing a connection to an LLM API."
    source: env
    required: false
    type: integer
    default: 10
    used_by: ["agents"]
  LLM_REQUEST_TIMEOUT:
    description: "Timeout in seconds for waiting for a full response from an LLM API. Increase this on slower hardware."
    source: env
    required: false
    type: integer
    default: 300 # Increased default as per your .env
    used_by: ["agents"]
  CORE_MAX_CONCURRENT_REQUESTS:
    description: "GLOBAL FALLBACK for the maximum number of simultaneous outbound LLM requests. This is used if a resource-specific override is not set."
    source: env
    required: false
    type: integer
    default: 2 # A safe, conservative default
    used_by: ["system", "agents"]
  LLM_SECONDS_BETWEEN_REQUESTS:
    description: "GLOBAL FALLBACK for the delay to insert between LLM API calls. Used if a resource-specific override is not set."
    source: env
    required: false
    type: integer
    default: 1 # A safe default for cloud APIs
    used_by: ["agents"]


  # ======================================================================
  # 2. CORE SYSTEM & PATHS
  # ======================================================================
  MIND:
    description: "The relative path to the system's declarative 'mind' (.intent directory)."
    source: env
    required: true
    type: path
    used_by: ["system", "auditor"]
  BODY:
    description: "The relative path to the system's executable 'body' (src directory)."
    source: env
    required: true
    type: path
    used_by: ["system"]
  REPO_PATH:
    description: "The absolute path to the root of the repository."
    source: env
    required: true
    type: path
    used_by: ["system","auditor"]
  LLM_ENABLED:
    description: "Master flag to enable or disable all LLM-related capabilities."
    source: env
    required: true
    type: bool
    allowed: ["true","false"]
    used_by: ["agents"]
  KEY_STORAGE_DIR:
    description: "The secure directory for storing operator private keys."
    source: env
    required: true
    type: path
    default: ".intent/keys"
    used_by: ["system"]
  CORE_ACTION_LOG_PATH:
    description: "Path to the action/change log file, required for the safety policy 'change_must_be_logged'."
    source: env
    required: true
    type: path
    used_by: ["auditor", "system"]
  CORE_ENV:
    description: "Runtime mode: 'development' or 'production'."
    source: env
    required: true
    type: enum
    allowed: ["development","production"]
    used_by: ["system"]
  LOG_LEVEL:
    description: "Logging level."
    source: env
    required: true
    type: enum
    allowed: ["DEBUG","INFO","WARNING","ERROR"]
    used_by: ["system"]


  # ======================================================================
  # 3. LLM RESOURCE CONFIGURATION (WITH PER-RESOURCE TUNING)
  # ======================================================================

  # --- START OF NEW DEFINITIONS for ollama_local ---
  OLLAMA_LOCAL_API_URL:
    description: "API URL for the 'ollama_local' resource."
    source: env
    required: false
    type: uri
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  OLLAMA_LOCAL_API_KEY:
    description: "API key for the 'ollama_local' resource (if required)."
    source: secret
    required: false
    type: string
    used_by: ["agents"]
  OLLAMA_LOCAL_MODEL_NAME:
    description: "Model name for the 'ollama_local' resource."
    source: env
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  OLLAMA_LOCAL_MAX_CONCURRENT_REQUESTS:
    description: "Concurrency override for 'ollama_local'. Recommended: 1 for local GPUs."
    source: env
    required: false
    type: integer
    default: 1
    used_by: ["agents"]
  OLLAMA_LOCAL_SECONDS_BETWEEN_REQUESTS:
    description: "Delay override for 'ollama_local'. Recommended: 0 for no artificial delay."
    source: env
    required: false
    type: integer
    default: 0
    used_by: ["agents"]
  # --- END OF NEW DEFINITIONS for ollama_local ---

  DEEPSEEK_CHAT_API_URL:
    description: "API URL for the 'deepseek_chat' resource."
    source: env
    required: false
    type: uri
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CHAT_API_KEY:
    description: "API key for the 'deepseek_chat' resource."
    source: secret
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CHAT_MODEL_NAME:
    description: "Model name for the 'deepseek_chat' resource."
    source: env
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CHAT_MAX_CONCURRENT_REQUESTS:
    description: "Concurrency override for the 'deepseek_chat' resource."
    source: env
    required: false
    type: integer
    default: 2
    used_by: ["agents"]
  DEEPSEEK_CHAT_SECONDS_BETWEEN_REQUESTS:
    description: "Delay override for the 'deepseek_chat' resource."
    source: env
    required: false
    type: integer
    default: 1
    used_by: ["agents"]

  DEEPSEEK_CODER_API_URL:
    description: "API URL for the 'deepseek_coder' resource."
    source: env
    required: false
    type: uri
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CODER_API_KEY:
    description: "API key for the 'deepseek_coder' resource."
    source: secret
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CODER_MODEL_NAME:
    description: "Model name for the 'deepseek_coder' resource."
    source: env
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CODER_MAX_CONCURRENT_REQUESTS:
    description: "Concurrency override for the 'deepseek_coder' resource."
    source: env
    required: false
    type: integer
    default: 3
    used_by: ["agents"]
  DEEPSEEK_CODER_SECONDS_BETWEEN_REQUESTS:
    description: "Delay override for the 'deepseek_coder' resource."
    source: env
    required: false
    type: integer
    default: 1
    used_by: ["agents"]

  ANTHROPIC_CLAUDE_SONNET_API_URL:
    description: "API URL for the 'anthropic_claude_sonnet' resource."
    source: env
    required: false
    type: uri
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  ANTHROPIC_CLAUDE_SONNET_API_KEY:
    description: "API key for the 'anthropic_claude_sonnet' resource."
    source: secret
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  ANTHROPIC_CLAUDE_SONNET_MODEL_NAME:
    description: "Model name for the 'anthropic_claude_sonnet' resource."
    source: env
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  ANTHROPIC_CLAUDE_SONNET_MAX_CONCURRENT_REQUESTS:
    description: "Concurrency override for the 'anthropic_claude_sonnet' resource."
    source: env
    required: false
    type: integer
    default: 2
    used_by: ["agents"]
  ANTHROPIC_CLAUDE_SONNET_SECONDS_BETWEEN_REQUESTS:
    description: "Delay override for the 'anthropic_claude_sonnet' resource."
    source: env
    required: false
    type: integer
    default: 1
    used_by: ["agents"]

  LOCAL_EMBEDDING_API_URL:
    description: "API URL for the local embedding resource."
    source: env
    required: true
    type: uri
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  LOCAL_EMBEDDING_API_KEY:
    description: "API key for the local embedding resource (if required)."
    source: secret
    required: false
    type: string
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  LOCAL_EMBEDDING_MODEL_NAME:
    description: "Model name for the local embedding resource."
    source: env
    required: true
    type: string
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  LOCAL_EMBEDDING_DIM:
    description: "The output dimension of the embedding model."
    source: env
    required: true
    type: integer
    default: 768
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  EMBED_MODEL_REVISION:
    description: "A revision tag/date for the embedding model to track provenance."
    source: env
    required: true
    type: string
    default: "2025-09-15"
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  EMBEDDING_MAX_CONCURRENT_REQUESTS:
    description: "Concurrency override for the embedding model."
    source: env
    required: false
    type: integer
    default: 2
    used_by: ["system"]


  # ======================================================================
  # 4. DATABASE & VECTOR STORE
  # ======================================================================
  QDRANT_URL:
    description: "URL for the Qdrant vector database instance."
    source: env
    required: true
    type: uri
    used_by: ["system"]
  QDRANT_COLLECTION_NAME:
    description: "The name of the collection within Qdrant to use for capabilities."
    source: env
    required: true
    type: string
    default: "core_capabilities"
    used_by: ["system"]
  DATABASE_URL:
    description: "The full connection string for the PostgreSQL database."
    source: env
    required: true
    type: uri
    used_by: ["system", "auditor"]

--- END OF FILE ./.intent/mind/config/runtime_requirements.yaml ---

--- START OF FILE ./.intent/mind/evaluation/audit_checklist.yaml ---
audit_checklist:
  - id: declared_intent
    item: "Was the intent declared before the change?"
    required: true
  - id: explanation
    item: "Was the change explained or justified?"
    required: true
  - id: manifest_sync
    item: "Did the change include a manifest update?"
    required: true
  - id: checkpoint
    item: "Was a rollback plan or checkpoint created?"
    required: false
  - id: quality_verified
    item: "Was code quality verified post-write?"
    required: true
  - id: audit.database_schema_declared
    description: Database schema must be present and valid.
    policy: "charter/policies/database_policy.yaml"
  - id: quorum-evidence-for-risky-changes
    title: "Quorum evidence recorded for medium/high risk"
    applies_when:
      risk_tier_in: ["medium", "high"]
    require:
      - "evidence.quorum.approvers"       # list of approvers
      - "evidence.quorum.mode"            # development/staging/production
      - "evidence.quorum.timestamp"       # ISO 8601
    severity: "block"
    guidance: "Attach the approver list and timestamp. Fails if missing."

--- END OF FILE ./.intent/mind/evaluation/audit_checklist.yaml ---

--- START OF FILE ./.intent/mind/evaluation/score_policy.yaml ---
score_policy:
  strategy: weighted_criteria

  criteria:
    - id: intent_alignment
      description: "Does this change serve a declared intent?"
      weight: 0.4

    - id: structural_compliance
      description: "Does it follow folder conventions and manifest structure?"
      weight: 0.2

    - id: safety
      description: "Was the change gated by a test or checkpoint?"
      weight: 0.2

    - id: code_quality
      description: "Does it pass formatting, linting, and basic semantic checks?"
      weight: 0.2

  # --- THRESHOLD LOGIC ---
  # Pass: score >= 0.7
  # Warn: score >= 0.5 and score < 0.7
  # Fail: score < 0.5
  thresholds:
    pass: 0.7
    warn: 0.5

# ----- RISK-TIER GATES (append) ---------------------------------------------
risk_tier_gates:
  # For medium-risk changes we require a governance checkpoint and a canary run.
  medium:
    min_score: 0.80        # tighten pass threshold
    require:
      - checkpoint         # e.g., human-in-the-loop signoff recorded
      - canary             # canary run ID must be present

  # For high-risk changes we raise the bar and require approver quorum too.
  high:
    min_score: 0.90
    require:
      - checkpoint
      - canary
      - approver_quorum    # follow your constitution/approvers

# The source of truth for what constitutes a "critical path".
# The auditor MUST use this file to evaluate the conditions below.
critical_paths_source: "charter/constitution/critical_paths.yaml"

# Declarative conditions so the auditor can enforce gates consistently.
gate_conditions:
  checkpoint_required_when: "risk_tier in ['medium','high']"
  canary_required_when: "risk_tier in ['medium','high'] or any change touches a file listed in critical_paths_source"
  approver_quorum_required_when: "risk_tier == 'high' or any change touches a file listed in critical_paths_source"

--- END OF FILE ./.intent/mind/evaluation/score_policy.yaml ---

--- START OF FILE ./.intent/mind/knowledge/entry_point_patterns.yaml ---
# .intent/mind/knowledge/entry_point_patterns.yaml
#
# ==============================================================
#  CORE — Entry Point Pattern Policy (v2.1)
# --------------------------------------------------------------
#  Purpose:
#    To declaratively define all patterns that represent valid
#    system entry points for audit and knowledge graph analysis.
#
#  Scope:
#    These rules prevent the auditor from flagging boilerplate,
#    framework-defined, or data-only symbols as “dead code”
#    while maintaining constitutional integrity.
#
#  Each pattern below includes:
#    - name: unique identifier
#    - description: concise rationale
#    - match: structural matching criteria
#    - entry_point_type: classification label used by the auditor
# ==============================================================

patterns:
  # ------------------------------------------------------------
  # Core language constructs
  # ------------------------------------------------------------
  - name: "python_magic_method"
    description: "Standard Python __dunder__ methods are invoked automatically."
    match:
      type: "function"
      name_regex: "^__.+__$"
    entry_point_type: "magic_method"

  - name: "ast_visitor_method"
    description: "NodeVisitor methods named visit_* are invoked by ast traversal."
    match:
      type: "function"
      name_regex: "^visit_"
      base_class_includes: "NodeVisitor"
    entry_point_type: "visitor_method"

  # ------------------------------------------------------------
  # Base / Framework constructs
  # ------------------------------------------------------------
  - name: "framework_base_class"
    description: "Base classes intended for subclassing are valid entry points."
    match:
      type: "class"
      is_base_class: true
    entry_point_type: "base_class"

  - name: "action_handler_boilerplate"
    description: "ActionHandler core methods (execute/name) are boilerplate."
    match:
      type: "function"
      base_class_includes: "ActionHandler"
      name_regex: "^(execute|name)$"
    entry_point_type: "boilerplate_method"

  # ------------------------------------------------------------
  # Data / Schema objects
  # ------------------------------------------------------------
  - name: "pydantic_model"
    description: "Pydantic models are declarative data schemas, not callable code."
    match:
      type: "class"
      base_class_includes: "BaseModel"
    entry_point_type: "data_model"

  - name: "dataclass_definition"
    description: "Dataclasses are plain data containers, valid entry points."
    match:
      type: "class"
      has_decorator: "dataclass"
    entry_point_type: "data_model"

  - name: "sqlalchemy_orm_model"
    description: "SQLAlchemy ORM models represent data tables, not logic units."
    match:
      type: "class"
      module_path_contains: "src/services/database/models"
    entry_point_type: "data_model"

  - name: "enum_definition"
    description: "Enum classes are constant sets and valid entry points."
    match:
      type: "class"
      base_class_includes: "Enum"
    entry_point_type: "enum"

  # ------------------------------------------------------------
  # Capability / tagged logic
  # ------------------------------------------------------------
  - name: "capability_implementation"
    description: "Any symbol tagged with a # ID is a primary CORE capability entry point."
    match:
      has_capability_tag: true
    entry_point_type: "capability"

  # ------------------------------------------------------------
  # CLI commands and interactive wrappers
  # ------------------------------------------------------------
  - name: "typer_cli_command"
    description: "Functions registered by Typer under src/cli/ are valid CLI commands."
    match:
      module_path_contains: "src/cli/"
      type: "function"
      is_public_function: true
    entry_point_type: "cli_command"

  - name: "cli_wrapper_function"
    description: "Thin CLI wrappers delegating to services or orchestrators."
    match:
      type: "function"
      module_path_contains: "src/cli/"
      name_regex: "^(show_.*|hub_.*|report|lint|test_system|peer_review|docs_clarity_audit|debug_.*)$"
    entry_point_type: "cli_wrapper"

  - name: "cli_interactive_menu"
    description: "Interactive menu helpers and run_command utilities."
    match:
      type: "function"
      module_path_contains: "src/cli/interactive"
      name_regex: "^(show_.*_menu|run_command)$"
    entry_point_type: "cli_wrapper"

  # ------------------------------------------------------------
  # Orchestrators / factories / registries
  # ------------------------------------------------------------
  - name: "high_level_orchestrator"
    description: "Top-level orchestrators that coordinate internal services."
    match:
      type: "function"
      module_path_contains: "src/features/"
      name_regex: "^(.*_audit|.*_report|.*_doctor|collect_from_.*)$"
    entry_point_type: "orchestrator"

  - name: "db_session_factory"
    description: "Functions creating DB sessions; invoked indirectly by DI."
    match:
      type: "function"
      name_regex: "^get_session$"
      module_path_contains: "src/services/"
    entry_point_type: "factory"

  - name: "registry_accessor"
    description: "Registry accessors provide indirection for handler lookup."
    match:
      type: "function"
      module_path_contains: "src/core/actions/registry.py"
      name_regex: "^get_handler$"
    entry_point_type: "registry_accessor"

  # ------------------------------------------------------------
  # Service adapters / external clients
  # ------------------------------------------------------------
  - name: "llm_provider_adapter_methods"
    description: "LLM provider adapters expose standardized surfaces."
    match:
      type: "function"
      module_path_contains: "src/services/llm/providers/"
      name_regex: "^(chat_completion|get_embedding)$"
    entry_point_type: "provider_method"

  - name: "llm_client_surface"
    description: "LLM client public API (async/sync requests, embeddings)."
    match:
      type: "function"
      module_path_contains: "src/services/llm/"
      name_regex: "^(make_request_async|make_request_sync|get_embedding)$"
    entry_point_type: "client_surface"

  - name: "qdrant_client_surface"
    description: "Qdrant client methods form the vector storage/search interface."
    match:
      type: "function"
      module_path_contains: "src/services/clients/qdrant_client.py"
      name_regex: "^(ensure_collection|get_all_vectors|get_vector_by_id|search_similar|upsert_capability_vector)$"
    entry_point_type: "client_adapter"

  # ------------------------------------------------------------
  # Knowledge graph and governance pipelines
  # ------------------------------------------------------------
  - name: "knowledge_builder_core"
    description: "Core components of symbol scanning and knowledge graph construction."
    match:
      type: "function"
      module_path_contains: "src/features/introspection/"
      name_regex: "^(SymbolScanner\\.scan|SymbolVisitor\\.visit_.*|KnowledgeGraphBuilder\\.build|CapabilityRegistry\\.resolve)$"
    entry_point_type: "knowledge_core"

  - name: "governance_checks_execute"
    description: "Each governance check's execute() method is called by the audit engine."
    match:
      type: "function"
      module_path_contains: "src/features/governance/checks/"
      name_regex: "execute$"
    entry_point_type: "governance_check"

  - name: "auditor_pipeline"
    description: "Constitutional auditor orchestration and runtime validators."
    match:
      type: "function"
      module_path_contains: "src/features/governance/"
      name_regex: "^(ConstitutionalAuditor\\.run_full_audit(_async)?|PolicyCoverageService\\.run|RuntimeValidatorService\\.run_tests_in_canary)$"
    entry_point_type: "auditor_pipeline"

  # ------------------------------------------------------------
  # Utility / adapter helpers
  # ------------------------------------------------------------
  - name: "utility_function_core"
    description: "Low-level helpers or pure utility functions (parsers, YAML tools, parallel processors)."
    match:
      type: "function"
      module_path_contains: "src/shared/utils/"
      name_regex: "^(run_poetry_command|get_all_constitutional_paths|parse|reconstruct|resolve|dump|load|load_strict|run_async|run_sync|get_embedding)$"
    entry_point_type: "utility_function"

  - name: "file_handler_methods"
    description: "FileHandler interface methods invoked by the I/O pipeline."
    match:
      type: "function"
      module_path_contains: "src/core/file_handler.py"
      name_regex: "^(add_pending_write|confirm_write)$"
    entry_point_type: "io_handler"

  - name: "git_service_methods"
    description: "GitService public interface methods used throughout CORE."
    match:
      type: "function"
      module_path_contains: "src/core/git_service.py"
      name_regex: "^(add_all|commit|get_current_commit|get_staged_files|is_git_repo|status_porcelain)$"
    entry_point_type: "git_adapter"

  # ------------------------------------------------------------
  # End of file
  # ------------------------------------------------------------

--- END OF FILE ./.intent/mind/knowledge/entry_point_patterns.yaml ---

--- START OF FILE ./.intent/mind/knowledge/file_handlers.yaml ---
handlers:
  - type: python
    extensions: [".py"]
    parse_as: ast
    editable: true
    description: Python source code with manifest-enforced governance

  - type: markdown
    extensions: [".md"]
    parse_as: text
    editable: true
    description: Human-readable docs. Require manual review in sensitive areas.

  - type: yaml
    extensions: [".yaml", ".yml"]
    parse_as: structured
    editable: true
    description: Configuration, policies, intent declarations

  - type: json
    extensions: [".json"]
    parse_as: structured
    editable: true
    description: Machine-readable manifests and graphs

  - type: binary
    extensions: [".png", ".jpg", ".pdf"]
    parse_as: none
    editable: false
    description: Visual artifacts — viewable only

--- END OF FILE ./.intent/mind/knowledge/file_handlers.yaml ---

--- START OF FILE ./.intent/mind/knowledge/source_structure.yaml ---
# .intent/mind/knowledge/source_structure.yaml
# CONSTITUTIONAL BLUEPRINT for the Layered, DB-Driven Architecture (V4 FINAL)

structure:
  - domain: api
    path: src/api
    description: "FastAPI routers ONLY. The HTTP Entrypoint."
    allowed_imports: [api, core, features, services, shared]

  - domain: cli
    path: src/cli
    description: "Typer commands ONLY. The CLI Entrypoint."
    allowed_imports: [cli, core, features, services, shared]

  - domain: core
    path: src/core
    description: "Orchestration Layer. Connects entrypoints to features and contains agents."
    allowed_imports: [core, features, services, shared]

  - domain: features
    path: src/features
    description: "Self-contained business capabilities, mapped to the DB capabilities table."
    allowed_imports: [features, services, shared, core]

  - domain: services
    path: src/services
    description: "Cross-cutting infrastructure services (DB access, external clients)."
    allowed_imports: [services, shared]

  - domain: shared
    path: src/shared
    description: "Project-agnostic utilities and core data models."
    allowed_imports: [shared]

  # The system/governance directory is intentionally omitted from runtime contracts,
  # making it a constitutionally protected, governance-only domain.

--- END OF FILE ./.intent/mind/knowledge/source_structure.yaml ---

--- START OF FILE ./.intent/mind/prompts/capability_consolidator.prompt ---
You are an expert Python architect specialising in the CORE project.
Your ONLY task is to detect **exact structural duplicates** of **top-level functions** and propose a **single shared utility** that replaces them.

INPUT:
- A list of Python file paths (absolute)
- A similarity threshold (0.95 = almost identical)

OUTPUT (JSON only, no commentary):
{
  "duplicates": [
    {
      "group_id": "sha256 of normalised AST",
      "symbol_names": ["func_a", "func_b"],
      "file_paths": ["src/x/a.py", "src/y/b.py"],
      "shared_name": "shared.utils.text.normalize_name",
      "shared_file": "src/shared/utils/text.py",
      "imports_to_add": ["from shared.utils.text import normalize_name"]
    }
  ]
}

RULES:
- Compare **normalised AST** (strip docstrings, rename vars to v0,v1...)
- Minimum **3 occurrences** to qualify
- Keep **public API identical** (same params, same defaults)
- Place new helper in **src/shared/** domain only
- Do NOT propose changes to **.intent/** or **src/system/governance/**

--- END OF FILE ./.intent/mind/prompts/capability_consolidator.prompt ---

--- START OF FILE ./.intent/mind/prompts/capability_definer.prompt ---
# Capability Key Generation Prompt

You are an **expert software architect** specializing in the **CORE** system. Your task is to **analyze a Python source code snippet** and propose a **single, canonical, dot-notation capability key** that accurately describes its primary purpose.

## Constitutional Rules for Naming

1.  **Use the Domain Pyramid**: The key MUST follow a hierarchical `domain.subdomain.action` pattern.
2.  **Be Specific**: Avoid vague terms. ✅ `auth.user.create` is good; ❌ `utils.do_stuff` is bad.
3.  **Use Verbs for Actions**: The final part of the key MUST be an action verb (e.g., `create`, `validate`, `sync`).
4.  **Stay Consistent**: Use the existing capabilities as a guide.

## Good Example:
For a function that synchronizes a database, a good key is `database.sync.all`.

## Bad Examples (DO NOT DO THIS):
- `domain.subdomain.action` (This is a generic placeholder, not a real key).
- `capability` (This is too generic).

## Context From Similar Code:
{similar_capabilities}

## Code to Analyze:
```python
{code}

--- END OF FILE ./.intent/mind/prompts/capability_definer.prompt ---

--- START OF FILE ./.intent/mind/prompts/code_peer_review.prompt ---
You are an expert Senior Staff Software Engineer, renowned for your insightful, pragmatic, and constructive code reviews. You prioritize clarity, simplicity, and robustness over cleverness or over-engineering.

You will be provided with a Python source code file from the CORE project. Your task is to analyze it and provide a better, improved version along with a clear justification for your changes.

Your entire output MUST be in Markdown format and follow this structure precisely:

### 1. Overall Assessment
A brief, high-level summary of the code's quality, strengths, and primary areas for improvement.

### 2. Justification for Changes
A bulleted list explaining *why* you are making each change. Reference specific principles like clarity, efficiency, or robustness. Be concise but clear.

### 3. Improved Code
Provide the complete, final, and improved version of the source code inside a single Python markdown block.

**CRITICAL RULES:**
- **Do not over-engineer.** The goal is improvement, not a total rewrite into a different paradigm.
- **Preserve functionality.** The improved code must do exactly what the original code did, just better.
- **Respect the existing style.** Maintain the overall coding style of the file.
- **Your output must be the full file content.** Do not provide only a diff or a snippet.

Begin your review. The source code is provided below.

--- END OF FILE ./.intent/mind/prompts/code_peer_review.prompt ---

--- START OF FILE ./.intent/mind/prompts/constitutional_review.prompt ---
You are an expert AI system architect and a specialist in writing clear, machine-readable governance documents.

You will be provided with a "constitutional bundle" from a self-governing software system named CORE. This bundle contains the entire ".intent/" directory, which is the system's "Mind". It defines all of the system's principles, policies, capabilities, and self-knowledge.

Your task is to perform a critical peer review of this constitution. Your goal is to provide actionable suggestions to improve its clarity, completeness, and internal consistency.

Analyze the entire bundle and provide your feedback in the following format:

**1. Overall Assessment:**
A brief, high-level summary of the constitution's strengths and weaknesses.

**2. Specific Suggestions for Improvement:**
Provide a numbered list of specific, actionable suggestions. For each suggestion, you MUST include:
- **File:** The full path to the file that should be changed (e.g., `.intent/mission/principles.yaml`).
- **Justification:** A clear, concise reason explaining WHY this change is an improvement and which core principle it serves (e.g., "This serves the `clarity_first` principle by making the rule less ambiguous.").
- **Proposed Change:** A concrete example of the new content. Use a git-style diff format if possible (lines starting with '-' for removal, '+' for addition).

**3. Gaps and Missing Concepts:**
Identify any potential gaps in the constitution. Are there missing policies, undefined principles, or areas that seem incomplete? For example, is there a policy for data privacy? Is the process for adding new human operators clearly defined?

**Review Criteria:**
- **Clarity:** Is every rule and principle easy to understand for both a human and an LLM? Is there any ambiguity?
- **Completeness:** Does the constitution cover all critical aspects of the system's governance?
- **Consistency:** Are there any conflicting rules or principles?
- **Actionability:** Are the rules specific enough to be automatically enforced?

Begin your review now. The constitutional bundle is provided below.

--- END OF FILE ./.intent/mind/prompts/constitutional_review.prompt ---

--- START OF FILE ./.intent/mind/prompts/create_file_planner.prompt ---
You are a file creation planner. Your only job is to create a plan with a single "create_file" action.

**CRITICAL RULES:**
1.  Your output MUST be a JSON array containing exactly one task.
2.  The action MUST be `create_file`.
3.  The `code` parameter MUST be `null`.
4.  The `file_path` parameter MUST be the path specified in the user's goal.

**User Goal:** "{goal}"

Respond with ONLY the JSON array. Do not include any other text or formatting.

--- END OF FILE ./.intent/mind/prompts/create_file_planner.prompt ---

--- START OF FILE ./.intent/mind/prompts/docs_clarity_review.prompt ---
You are an expert technical writer and developer advocate. Your primary skill is explaining complex software concepts to intelligent, but busy, programmers.

You will be provided with a bundle of all the human-facing documentation (.md files) for a software project called CORE.

Your task is to perform a "human clarity audit." Read all the documents and then answer the following questions from the perspective of a first-time reader who is a skilled developer but knows nothing about this project.

Your entire output MUST be in Markdown format.

**1. The "Stijn Test": What Does It Do?**
In one or two simple sentences, what is CORE and what problem does it solve? If you cannot answer this clearly, state that the documentation has failed this primary test.

**2. Overall Clarity Score (1-10):**
Give a score from 1 (completely incomprehensible) to 10 (perfectly clear). Justify your score with specific examples from the text.

**3. Suggestions for Improvement:**
Provide a numbered list of the top 3-5 concrete suggestions to improve the documentation's clarity. For each suggestion, quote the confusing text and explain WHY it is confusing.

**4. Conceptual Gaps:**
Are there any obvious questions a new user would have that the documentation doesn't answer? (e.g., "Who is this for?", "What's the difference between this and X?").

Begin your audit now. The documentation bundle is provided below.

--- END OF FILE ./.intent/mind/prompts/docs_clarity_review.prompt ---

--- START OF FILE ./.intent/mind/prompts/enrich_symbol.prompt ---
# You are an expert Python technical writer for the CORE system.
# Your sole task is to analyze a symbol's source code and its context to write a single,
# concise, one-sentence description of its purpose for the knowledge graph.

# --- CRITICAL RULES ---
# 1.  The description MUST be a single, complete sentence.
# 2.  It MUST explain the primary purpose or "intent" of the symbol.
# 3.  It MUST be written in the third person (e.g., "Validates...", "Orchestrates...").
# 4.  Do NOT include implementation details, parameter names, or return types. Focus on the "what" and "why".
# 5.  Your entire output MUST be a single, valid JSON object and NOTHING else.
# 6.  Do NOT add any comments, markdown fences, or other text outside of the JSON object.

# --- SYMBOL CONTEXT ---
# Symbol Path: {symbol_path}
# File Path: {file_path}
# Existing similar capabilities (for context on naming and style):
# {similar_capabilities}

# --- SYMBOL SOURCE CODE ---
# ```python
# {source_code}
# ```

# --- YOUR TASK ---
# Generate the JSON object containing the description for the symbol above.

# --- Example of a PERFECT response ---
# {
#   "description": "Checks if a proposed set of file changes complies with all active constitutional rules."
# }

--- END OF FILE ./.intent/mind/prompts/enrich_symbol.prompt ---

--- START OF FILE ./.intent/mind/prompts/fix_capability_manifest.prompt ---
You are an expert software architect for the CORE system. Your task is to fix a capability manifest entry that has placeholder content.

Analyze the provided source code and its context, then generate a concise, one-sentence description and infer the most appropriate owner agent from the list below.

**Available Owners:**

* `core_agent`: For core application logic, services, and core capabilities.
* `planner_agent`: For goal decomposition and planning.
* `generic_agent`: For general agentic behaviors and utilities.
* `validator_agent`: For validation, auditing, and governance checks.
* `tooling_agent`: For internal developer tools, builders, and introspection.

**Source Code of the Capability:**

```python
{source_code}
```

Your Task:
Respond with ONLY a single, valid JSON object with three keys: "title", "description", and "owner".
"title": A clean, Title-Cased version of the capability name.
"description": A concise, one-sentence explanation of what the capability does.
"owner": The single most appropriate agent from the list above.
Example of a PERFECT response for governance.review\.ai\_peer\_review:

```json
{
  "title": "Ai Peer Review",
  "description": "Submits a source file to an AI expert for a peer review and improvement suggestions.",
  "owner": "generic_agent"
}
```

Now, analyze the provided source code and generate the JSON for the capability {capability\_key}.

--- END OF FILE ./.intent/mind/prompts/fix_capability_manifest.prompt ---

--- START OF FILE ./.intent/mind/prompts/fix_function_docstring.prompt ---
# Prompt: Python Function Docstring Writer

You are an expert Python technical writer. Your only task is to write a single, concise, and accurate PEP 257 compliant docstring for the provided Python function/method.

**CRITICAL RULES:**
1.  **Analyze the code's purpose.** Look at the function name, parameters, and body to understand what it does.
2.  **Write a one-line summary.** The docstring must start with a short, imperative summary (e.g., "Generate a new key pair," not "This function generates...").
3.  **Keep it concise.** The entire docstring should ideally be one line. Only add more detail if absolutely necessary for clarity.
4.  **Return ONLY the docstring content.** Do not include the triple quotes (`"""`). Do not include any other text, explanations, or markdown.

**Function Source Code to Document:**
```python
{source_code}

Example of a PERFECT output for def __init__(self, context)::
Initializes the check with a shared auditor context.

--- END OF FILE ./.intent/mind/prompts/fix_function_docstring.prompt ---

--- START OF FILE ./.intent/mind/prompts/fix_header.prompt ---
# Prompt: Constitutional Header Fixer

You are an expert technical writer and linter for a Python project named CORE. Your only task is to fix the header of a given Python file to be 100% compliant with the project's constitutional style guide.

INPUTS:
- file_path: {file_path}
- source_code: {source_code}

CONSTITUTIONAL HEADER RULES:
1) The first non-empty line MUST be a file path comment exactly matching the provided file_path (e.g., `# src/core/main.py`).
2) Immediately after that, there MUST be a single module-level docstring.
3) Immediately after the docstring, there MUST be a line `from __future__ import annotations`.
4) There MUST be exactly one blank line between (1), (2), and (3).
5) All other code must follow after these header elements.

Special cases:
- If `from __future__ import annotations` exists elsewhere, MOVE it to the required position and remove any duplicates.
- If other `from __future__ import ...` statements exist, keep them directly below the annotations line (do not combine them with the annotations import).
- If multiple module-level docstrings exist, merge them into one concise docstring.
- If a filepath comment already exists but does not match the provided file_path, replace it with the exact file_path.
- Do not change any code outside the header other than the moves/removals described above.

OUTPUT CONTRACT (critical):
- Return the complete, corrected source code for the entire file.
- Do NOT wrap the output in markdown fences or add any commentary.
- If the file is already compliant, return the original content unchanged.

Example (illustrative only — do NOT include fences in your output):
# {file_path}
"""
One-sentence module-level docstring explaining the file's purpose.
"""

from __future__ import annotations

# ...rest of the original code (unchanged)

--- END OF FILE ./.intent/mind/prompts/fix_header.prompt ---

--- START OF FILE ./.intent/mind/prompts/fix_line_length.prompt ---
You are an expert Python programmer specializing in code clarity and readability. Your sole task is to refactor the provided Python code to ensure no single line exceeds 100 characters while maintaining identical functionality.

**CRITICAL RULES:**

1. **ABSOLUTE PROHIBITION ON LOGIC CHANGES:** Do not modify variable names, add/remove imports, change string contents, alter numeric values, comments content, or modify any functionality whatsoever. Only change whitespace, line breaks, and indentation.

2. **LINE LENGTH ENFORCEMENT:** Break any line longer than 100 characters using intelligent, Pythonic methods.

3. **PYTHON VERSION:** Assume Python 3.8+ unless otherwise specified. Use modern syntax features appropriately.

**LINE BREAKING GUIDELINES:**

- Use parentheses for implicit line continuation (preferred over backslashes)
- Break after operators, not before (except for 'and'/'or' in conditionals)
- For function calls: break after commas, keep related parameters together
- For long strings: use implicit string concatenation or triple quotes with proper indentation
- For dictionaries/lists: break after commas, align values appropriately
- For method chaining: break before the dot, align methods
- Align continuation lines appropriately with opening delimiters

**EDGE CASE HANDLING:**

- URLs, file paths, or strings that cannot be broken: leave as-is even if >100 chars, add comment `# LINE TOO LONG - CANNOT BREAK`
- Comments >100 chars: break at word boundaries, maintain meaning
- Preserve existing docstring formatting unless line length violations occur
- Extract Python content whether provided with or without markdown code blocks

**ERROR HANDLING:**

- If code contains syntax errors: respond with "ERROR: [specific issue description]"
- If code cannot be parsed: respond with "ERROR: Unable to parse Python code"

**VALIDATION REQUIREMENTS:**

Before returning code, verify:
- All lines are ≤100 characters (except unavoidable cases marked with comment)
- No syntax errors introduced
- All imports remain intact and functional
- String literals maintain original content
- Indentation follows PEP 8 standards

**OUTPUT FORMAT:**

Return ONLY the complete, raw Python source code. No markdown code blocks, no explanations, no commentary. The output must be immediately copy-paste ready and functionally identical to the input.

**Input File to Refactor:**

```python
{source_code}
```

--- END OF FILE ./.intent/mind/prompts/fix_line_length.prompt ---

--- START OF FILE ./.intent/mind/prompts/goal_assessor.prompt ---
You are an expert project manager for the CORE system. Your job is to assess a user's goal for clarity.

First, review the project's roadmap to understand the current priorities:
[[context:docs/04_ROADMAP.md]]

Now, analyze the user's goal.
- If the goal is clear, specific, and actionable, respond with a JSON object: `{"status": "clear", "goal": "The clear goal here."}`.
- If the goal is vague, identify the MOST LIKELY specific task the user wants based on the roadmap. Respond with a JSON object containing a helpful suggestion: `{"status": "vague", "suggestion": "The user's goal is a bit vague. Based on the roadmap, did you mean to ask: '[Your suggested, more specific goal]'?"}`.

User Goal: "{user_input}"

Your output MUST be a single, valid JSON object and nothing else.

--- END OF FILE ./.intent/mind/prompts/goal_assessor.prompt ---

--- START OF FILE ./.intent/mind/prompts/intent_translator.prompt ---
You are an expert user of the CORE Admin CLI. Your job is to translate a user's natural language goal into a single, precise, and executable `core-admin` command.

You must only use commands that are available in the CLI. Here is the full help text for `core-admin --help` to use as your reference:
\[\[include\:reports/cli\_help.txt]]

Analyze the user's request and determine the single best command to achieve their goal.

**CRITICAL RULES:**

1. Your output MUST be a single, valid JSON object and nothing else.
2. The JSON object must have one key: "command".
3. The value of "command" must be a string containing the complete and correct `core-admin` command, ready to be executed in a shell.
4. If the user's request is too ambiguous to map to a single command, respond with an "error" key and a helpful message.

**User Request:** "{user\_input}"

**Example of a PERFECT response for "check my project's health":**

```json
{
  "command": "core-admin system check"
}
```

**Example of a PERFECT response for an AMBIGUOUS request:**

```json
{
  "error": "Your request is a bit ambiguous. Could you clarify if you want to 'review the documentation' or 'run a constitutional audit'?"
}
```

--- END OF FILE ./.intent/mind/prompts/intent_translator.prompt ---

--- START OF FILE ./.intent/mind/prompts/micro_planner.prompt ---
You are the **Micro-Planner Agent** for the **CORE** system.
Your sole purpose is to **decompose a high-level goal** into a sequence of **small, safe, and independently verifiable actions** that comply fully with the `micro_proposal_policy.yaml`.

You will be provided with:

1. The user’s **goal**.
2. The full contents of `micro_proposal_policy.yaml`.

Your task is to generate a **valid JSON array** of planned action steps.
Each step **MUST** be an object containing the following keys:

* `"step"` – a brief, human-readable description of the action.
* `"action"` – the exact name of an allowed action from the `safe_actions` list.
* `"params"` – an object **that MUST include** a `"file_path"`.

---

### ✅ OUTPUT FORMAT (STRICTLY ENFORCED)

A clean JSON array, with no extra characters or formatting.

```json
[
  {
    "step": "A brief, human-readable description of this action.",
    "action": "name_of_the_action_from_safe_actions",
    "params": {
      "file_path": "the/target/file.py"
    }
  },
  {
    "step": "Validate the changes.",
    "action": "core.validation.validate_code",
    "params": {
      "file_path": null
    }
  }
]
```

---

### ⚖️ CONSTITUTIONAL CONSTRAINTS

`micro_proposal_policy.yaml` contents:

```code
{policy_content}
```

---

### 🧠 CRITICAL RULES

* You MUST ONLY use actions explicitly listed in `safe_actions`.
* All `"file_path"` values in `"params"` MUST comply with the `safe_paths` rules.
* You MUST NOT target any forbidden path.
* The final step MUST ALWAYS be a validation step:

```json
{
  "step": "Validate the changes.",
  "action": "core.validation.validate_code",
  "params": { "file_path": null }
}
```

* If the goal cannot be safely achieved under these constraints, respond with an empty JSON array `[]`.

---

### 🎯 USER GOAL

```code
{user_goal}
```

---

Respond with **ONLY** the JSON array of tasks.
Do not include explanations, text, or markdown formatting.

--- END OF FILE ./.intent/mind/prompts/micro_planner.prompt ---

--- START OF FILE ./.intent/mind/prompts/module_docstring_writer.prompt ---
# Prompt — Module-Level Docstring Writer

You are an expert technical writer for a Python project called CORE. Your task is to write a concise, one-sentence module-level docstring that explains the primary purpose or intent of a Python file.

---

## Critical Rules

1. **Output Format:** Your output MUST be a single line of text with no quotes, markdown, or code blocks.

2. **Content Requirements:**
   - Describe the module's primary responsibility or purpose
   - Use present tense, active voice when possible
   - Start with a verb or descriptive phrase (avoid "This module...")
   - Be specific about what the module does, not just what domain it covers
   - Keep it under 80 characters when possible for readability

3. **Analysis Guidelines:**
   - Focus on the main classes, functions, or primary workflow
   - If the module has multiple responsibilities, identify the unifying theme
   - Consider the module's role within the broader CORE project architecture
   - Ignore utility functions, imports, or minor helper code when determining primary purpose

---

## Error Handling

- If the file is empty or contains only imports/comments: respond with "ERROR: Insufficient code content to determine module purpose"
- If the file contains syntax errors: respond with "ERROR: Cannot parse Python code due to syntax errors"
- If the module purpose is unclear: focus on the most prominent functionality

---

## Style Guidelines

**Good Examples:**
- "Handles the discovery and loading of constitutional proposal files from disk."
- "Provides authentication and session management for user accounts."
- "Implements core encryption algorithms for secure data transmission."
- "Manages database connections and transaction handling."

**Avoid:**
- "This module contains functions for..." (too verbose)
- "Utilities for..." (too vague)
- "Various helpers..." (not descriptive)
- Generic descriptions that could apply to any module

---

## File Content

```python
{source_code}
```

---

## Expected Output Format

[Single sentence describing the module's primary purpose, ending with a period]

--- END OF FILE ./.intent/mind/prompts/module_docstring_writer.prompt ---

--- START OF FILE ./.intent/mind/prompts/new_capability_generator.prompt ---
You are an expert software architect and technical writer for the CORE system.
Your task is to analyze a Python function's source code and generate a complete, structured capability definition for it.

**CONTEXT:**
- A **capability** is a single, discrete function the system can perform.
- **Tags** are classifiers chosen from a predefined list that describe the capability's purpose.

**PREDEFINED TAGS (Choose one or more relevant tags):**
{valid_tags}

**SOURCE CODE TO ANALYZE:**
```python
{source_code}

INSTRUCTIONS:
Thoroughly analyze the source code to understand its primary purpose.
Generate a title that is a human-readable, Title Case version of the function name.
Write a concise, one-sentence description that clearly explains what the function does.
Select the most relevant tags from the predefined list above.
Determine the most appropriate owner agent for this capability.
Your entire output MUST be a single, valid JSON object containing the title, description, tags (as a list of strings), and owner.
EXAMPLE OF A PERFECT RESPONSE:

JSON
{{
  "title": "Run All Checks",
  "description": "Run all checks: lint, test, and a full constitutional audit.",
  "tags": ["system", "governance", "cli"],
  "owner": "validator_agent"
}}

--- END OF FILE ./.intent/mind/prompts/new_capability_generator.prompt ---

--- START OF FILE ./.intent/mind/prompts/planner_agent.prompt ---
You are a meticulous software architect and senior engineer. Your task is to decompose a high-level user goal into a series of precise, step-by-step actions.

You must respond with a JSON array of tasks. Each task must be an object with three fields: "step", "action", and "params".

- `step`: A string describing the purpose of this action in plain English.
- `action`: The name of the action to be performed. Must be one of the available actions.
- `params`: An object containing the parameters for the action. The keys must match the required parameters for that action.

**CRITICAL CONTEXT: This information has been provided by a reconnaissance agent.**
You MUST use this context to inform your plan.
{reconnaissance_report}

**CRITICAL RULES:**
1.  **ABSOLUTE RULE:** For any action that has a `code` parameter (like `create_file`, `edit_file`), you MUST set its value to `null`. The code will be generated later by a different agent.

2.  **Testing Mandate (Safety First):**
    - If your plan includes a `create_file` action for a new Python source file (e.g., in `src/`), you MUST ALSO include a second `create_file` action for a corresponding test file in the `tests/` directory.
    - Test files for `src/path/to/file.py` should be located at `tests/path/to/test_file.py`.

3.  **File Creation vs. Editing:**
    - If the user's goal is to create a new file or function, and the reconnaissance report shows no relevant existing files, your plan MUST use the `create_file` action (along with a corresponding test file as per the Testing Mandate).
    - Only use `edit_file` or `edit_function` for files that are explicitly mentioned as existing in the reconnaissance report. If editing, your first step MUST be `read_file`.

Available Actions:
{action_descriptions}

Now, create a plan for the following goal.

Goal: "{goal}"

Respond with ONLY the JSON array of tasks. Do not include any other text, explanations, or markdown formatting.

--- END OF FILE ./.intent/mind/prompts/planner_agent.prompt ---

--- START OF FILE ./.intent/mind/prompts/refactor_for_clarity.prompt ---
You are an expert Python programmer specializing in code clarity and readability, operating under the CORE constitution. Your sole task is to refactor the provided Python code to improve its clarity and simplicity while maintaining identical functionality.

**CONSTITUTIONAL PRINCIPLES TO UPHOLD:**
- `clarity_first`: The code must be easier to understand.
- `separation_of_concerns`: If a function is doing too much, break it into smaller, well-named helper methods.
- `safe_by_default`: Do not change any logic, only the structure. Preserve all existing decorators and functionality.

**REFACTORING ACTIONS:**
- Break down long, complex functions into smaller, logical units.
- Improve variable names for better readability.
- Simplify complex conditional logic.
- Adhere to a maximum line length of 100 characters.

**OUTPUT FORMAT:**
Return ONLY the complete, raw, and refactored Python source code. Do not include markdown code blocks, explanations, or commentary. The output must be ready to be written directly to a file.

**Input File to Refactor:**
```python
{source_code}

--- END OF FILE ./.intent/mind/prompts/refactor_for_clarity.prompt ---

--- START OF FILE ./.intent/mind/prompts/refactor_outlier.prompt ---
You are an expert Python refactoring engine. Your only task is to break down the provided large Python file into multiple, smaller, logically cohesive files.

**CRITICAL INSTRUCTIONS:**

1. **Analyze Responsibilities:** Identify the distinct responsibilities in the input file (e.g., CLI commands, data processing, helper functions).
2. **Create New Files:** Group the logic for each responsibility into a new file. Use clear, descriptive filenames (e.g., `knowledge_cli.py`, `knowledge_orchestrator.py`).
3. **Preserve All Logic:** All original functionality must be preserved. Do not add or remove any logic, only move it.
4. **Fix Imports:** Add all necessary `from . import ...` statements to reconnect the separated files.
5. **Output Format:** Your entire response MUST consist of one or more `[[write:file/path/here.py]]...[[/write]]` blocks. Do not add any other commentary or explanations.

**INPUT FILE TO REFACTOR:**

```python
{source_code}
```

**EXAMPLE OF A PERFECT OUTPUT:**
\[\[write\:src/system/admin/knowledge\_cli.py]]
src/system/admin/knowledge\_cli.py
"""
CLI commands for the knowledge system.
"""
from .knowledge\_orchestrator import orchestrate\_vectorization
... CLI command functions here ...
\[\[/write]]
\[\[write\:src/system/admin/knowledge\_orchestrator.py]]
src/system/admin/knowledge\_orchestrator.py
"""
Orchestrates the vectorization process.
"""
from .knowledge\_helpers import extract\_source\_code
... orchestrate\_vectorization function here ...
\[\[/write]]

Now, refactor the input file and provide only the \[\[write:]] blocks.

--- END OF FILE ./.intent/mind/prompts/refactor_outlier.prompt ---

--- START OF FILE ./.intent/mind/prompts/standard_task_generator.prompt ---
# .intent/mind/prompts/standard_task_generator.prompt
You are an expert Python programmer operating under the CORE constitution, tasked with generating a single, complete block of Python code to fulfill a specific step in a larger plan.

**CONTEXT FOR YOUR TASK:**
- **Overall Goal:** {goal}
- **Current Step:** {step}
- **Target File Path:** {file_path}
- **Target Symbol (if editing):** {symbol_name}

---
**OUTPUT CONTRACT (ABSOLUTE RULES):**
1.  You MUST generate the complete, final Python code for the entire file or function. Do not use placeholders, snippets, or diffs.
2.  Your output MUST BE PURE CODE. Do NOT include any markdown fences (```python...```), explanations, or any text other than the code itself.
3.  The generated code must be clean, readable, and adhere to standard Python conventions.
---

Now, generate the required code.

--- END OF FILE ./.intent/mind/prompts/standard_task_generator.prompt ---

--- START OF FILE ./.intent/mind/prompts/vectorizer.prompt ---
Analyze the following Python code snippet. Your task is to generate a 1024-dimensional semantic embedding vector that represents its meaning.

CRITICAL INSTRUCTIONS:
- Your output MUST be a single, valid JSON array of floating-point numbers.
- Do NOT include any other text, explanations, or markdown formatting like ```json.
- The array must contain exactly 1024 numbers.

Source Code:
```python
{source_code}

--- END OF FILE ./.intent/mind/prompts/vectorizer.prompt ---

--- START OF FILE ./.intent/mind_export/capabilities.yaml ---
version: 1
exported_at: '2025-10-11T16:33:14.585923+00:00'
items: []
digest: sha256:4f53cda18c2baa0c0354bb5f9a3ecbe5ed12ab4d8e11ba873c2f11161202b945

--- END OF FILE ./.intent/mind_export/capabilities.yaml ---

--- START OF FILE ./.intent/mind_export/cognitive_roles.yaml ---
# .intent/mind/knowledge/cognitive_roles.yaml
# Maps abstract cognitive roles to specific, configured LLM resources.

cognitive_roles:
  - role: "Planner"
    description: "Decomposes high-level goals into step-by-step plans."
    assigned_resource: "deepseek_chat"
    required_capabilities: ["planning"]

  - role: "Coder"
    description: "Generates and refactors source code."
    assigned_resource: "deepseek_coder"
    required_capabilities: ["code_generation"]

  - role: "Vectorizer"
    description: "Creates semantic vector embeddings from text."
    assigned_resource: "local_embedding"
    required_capabilities: ["embedding"]

  - role: "CodeReviewer"
    description: "Reviews code for clarity, style, and correctness."
    assigned_resource: "deepseek_coder"
    required_capabilities: ["code_generation"]

--- END OF FILE ./.intent/mind_export/cognitive_roles.yaml ---

--- START OF FILE ./.intent/mind_export/links.yaml ---
version: 1
exported_at: '2025-10-11T16:33:14.585923+00:00'
items: []
digest: sha256:4f53cda18c2baa0c0354bb5f9a3ecbe5ed12ab4d8e11ba873c2f11161202b945

--- END OF FILE ./.intent/mind_export/links.yaml ---

--- START OF FILE ./.intent/mind_export/northstar.yaml ---
version: 1
exported_at: '2025-10-11T16:33:14.585923+00:00'
items: []
digest: sha256:4f53cda18c2baa0c0354bb5f9a3ecbe5ed12ab4d8e11ba873c2f11161202b945

--- END OF FILE ./.intent/mind_export/northstar.yaml ---

--- START OF FILE ./.intent/mind_export/resource_manifest.yaml ---
# .intent/mind/knowledge/resource_manifest.yaml
# The canonical list of available LLM resources for the system.

llm_resources:
  - name: "deepseek_chat"
    provided_capabilities: ["chat", "reasoning", "planning"]
    env_prefix: "DEEPSEEK_CHAT"
    performance_metadata:
      cost_rating: 2
      quality_rating: 4

  - name: "deepseek_coder"
    provided_capabilities: ["code_generation", "refactoring"]
    env_prefix: "DEEPSEEK_CODER"
    performance_metadata:
      cost_rating: 3
      quality_rating: 5

  - name: "anthropic_claude_sonnet"
    provided_capabilities: ["chat", "reasoning", "planning", "code_generation"]
    env_prefix: "ANTHROPIC_CLAUDE_SONNET"
    performance_metadata:
      cost_rating: 4
      quality_rating: 4

  - name: "local_embedding"
    provided_capabilities: ["embedding"]
    env_prefix: "LOCAL_EMBEDDING"
    performance_metadata:
      cost_rating: 1
      quality_rating: 3

--- END OF FILE ./.intent/mind_export/resource_manifest.yaml ---

--- START OF FILE ./.intent/mind_export/symbols.yaml ---
version: 1
exported_at: '2025-10-11T16:33:14.585923+00:00'
items:
- id: db8bbe12-9fc2-521e-9d3d-42508b0655dc
  symbol_path: src/services/mind_service.py::get_mind_service
  module: services.mind_service
  qualname: get_mind_service
  kind: function
  ast_signature: TBD
  fingerprint: 025dd16486e02c3d160a4998505594e5e918c48f252c8d2495d4c0b1c748c792
  state: discovered
- id: cb67b1df-338a-5767-98dc-1da664e5f7c7
  symbol_path: src/services/llm/client.py::LLMClient
  module: services.llm.client
  qualname: LLMClient
  kind: class
  ast_signature: TBD
  fingerprint: 05135f88fa420040e068af7911a15d192b0ba866403a2866261fa5ef846c90cb
  state: discovered
- id: 47fbc7f4-a9ba-5665-9efb-bebfb65914ce
  symbol_path: src/core/crate_processing_service.py::process_crates
  module: core.crate_processing_service
  qualname: process_crates
  kind: function
  ast_signature: TBD
  fingerprint: 05d63b971eadf995475f605bf5d5833e2cdba52341b55954910474f259c05f38
  state: discovered
- id: 9bd133b5-eed8-5e30-8fc4-33ac0e2d3355
  symbol_path: src/cli/logic/agent.py::register
  module: cli.logic.agent
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 07d267b258290654459382e668c672b5d5f218f486a0a69db1a829079247f439
  state: discovered
- id: 9d650da2-af57-54ce-9cd1-2bbcc5adca04
  symbol_path: src/cli/logic/reviewer.py::register
  module: cli.logic.reviewer
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 07d93df52ea1e96fd676247421105c2e3fb61ef028cd5086a9ac86dc65f0254e
  state: discovered
- id: 75859b61-dac9-5bf7-a750-a85fe4ea8e1e
  symbol_path: src/shared/utils/embedding_utils.py::sha256_hex
  module: shared.utils.embedding_utils
  qualname: sha256_hex
  kind: function
  ast_signature: TBD
  fingerprint: 083c75ed1e8ac26bc8ade99e959159a1adf9f97aefb51cee33e09ab22acaff28
  state: discovered
- id: 9b62b0c8-aa5f-5759-bba0-123e2fd25e7e
  symbol_path: src/cli/commands/mind.py::snapshot_command
  module: cli.commands.mind
  qualname: snapshot_command
  kind: function
  ast_signature: TBD
  fingerprint: 08d80979ee8e340e1e746f910562a87e3ebd72497b83b121966e823913cff2db
  state: discovered
- id: 6a0f3ff0-d540-55e2-871b-0061e388773c
  symbol_path: src/features/governance/policy_coverage_service.py::PolicyCoverageService
  module: features.governance.policy_coverage_service
  qualname: PolicyCoverageService
  kind: class
  ast_signature: TBD
  fingerprint: 094d5744ef0572d9dd4862832372ab988e8e2fcbe98da9ad036ea6db42cb4287
  state: discovered
- id: 4c42a962-aad4-5699-9e0a-5f549cf49668
  symbol_path: src/shared/models/execution_models.py::ExecutionTask
  module: shared.models.execution_models
  qualname: ExecutionTask
  kind: class
  ast_signature: TBD
  fingerprint: 098892988a3bbd0a867ec7b40cfdd3aa1cf2f55b39a39b91381edb03ecc2e38f
  state: discovered
- id: b9403509-28bf-5d4c-9ca9-c1338f206bbf
  symbol_path: src/cli/commands/inspect.py::register
  module: cli.commands.inspect
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 09b8743827d53215d59ade4e61f26270403121bbe03ecd4def37eccd6458acba
  state: discovered
- id: a889376c-df44-56e7-90c4-82d0cfbd73af
  symbol_path: src/cli/logic/db.py::register
  module: cli.logic.db
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 09cd512250c88ef547970b125756b788d1f08439850fdaf0a0bd39dcae395ec4
  state: discovered
- id: 79c021c1-707d-59c0-882a-f006044c8f20
  symbol_path: src/cli/commands/fix.py::assign_ids_command
  module: cli.commands.fix
  qualname: assign_ids_command
  kind: function
  ast_signature: TBD
  fingerprint: 0a4cf12cbc242b0e7c5b6767d216d21ef7143f4901b4941664867733bf1fbeb8
  state: discovered
- id: f992de16-b887-5a5c-8a58-f280ef16b8f4
  symbol_path: src/cli/interactive.py::show_governance_menu
  module: cli.interactive
  qualname: show_governance_menu
  kind: function
  ast_signature: TBD
  fingerprint: 0a7cdd59867e1031693cd04416362977ad9ff4f460574c92cba89ba414c7b197
  state: discovered
- id: 4b80f6d1-7b32-5480-9ebf-a770a5f642ee
  symbol_path: src/features/introspection/drift_detector.py::detect_capability_drift
  module: features.introspection.drift_detector
  qualname: detect_capability_drift
  kind: function
  ast_signature: TBD
  fingerprint: 0adf632478921bc2c48568e8dbb8ba99f40025be2cb6282759883cca0f85fe19
  state: discovered
- id: bbbd20a6-52c5-5af4-81cb-a96c5ff4ecf0
  symbol_path: src/features/governance/checks/health_checks.py::HealthChecks
  module: features.governance.checks.health_checks
  qualname: HealthChecks
  kind: class
  ast_signature: TBD
  fingerprint: 0aeaf2e297f767bc0a60b266947805372fb871979effc60854d073ae7fa267b6
  state: discovered
- id: 260ee022-8859-5801-8f10-133f9be2e80b
  symbol_path: src/cli/logic/project_docs.py::docs
  module: cli.logic.project_docs
  qualname: docs
  kind: function
  ast_signature: TBD
  fingerprint: 0c006cc395bd03fbab762d84ae2c3e31bc71f1f72a0b7aa0fd54b768c8523e3f
  state: discovered
- id: 523d202c-be1f-5718-b75a-0825ace0a8b5
  symbol_path: src/core/intent_alignment.py::check_goal_alignment
  module: core.intent_alignment
  qualname: check_goal_alignment
  kind: function
  ast_signature: TBD
  fingerprint: 0c7e02644a288a463670cc1472faa6701002593ecefcd8b3078631666b684468
  state: discovered
- id: 34e64fce-fb55-5153-b9dd-6d6bec2d4d44
  symbol_path: src/cli/logic/knowledge_sync/diff.py::diff_sets
  module: cli.logic.knowledge_sync.diff
  qualname: diff_sets
  kind: function
  ast_signature: TBD
  fingerprint: 0c840e193ae924b0a48e4270656259a5c82e03a943211b071b522e13cef4504b
  state: discovered
- id: 51bef2a1-5bfb-54c0-b9b9-53e5f377b9dc
  symbol_path: src/cli/interactive.py::run_command
  module: cli.interactive
  qualname: run_command
  kind: function
  ast_signature: TBD
  fingerprint: 0d3be8843fd902828f4ed3e0329f5cc864c504c829ff8bfb4b457377cdef5a52
  state: discovered
- id: a9a0a82f-9013-5b5b-9ea1-15e59d8a0f3f
  symbol_path: src/features/introspection/generate_capability_docs.py::main
  module: features.introspection.generate_capability_docs
  qualname: main
  kind: function
  ast_signature: TBD
  fingerprint: 0e52c9a6f9f493247c9ed2ca830d8d4b9f04a19dc5f6a84e65a3f3e247659f4b
  state: discovered
- id: f162d053-555c-5d0b-a9d2-9f8acad49db7
  symbol_path: src/cli/commands/search.py::search_knowledge_command
  module: cli.commands.search
  qualname: search_knowledge_command
  kind: function
  ast_signature: TBD
  fingerprint: 0f2a20511c9569d03fe0173f5050cf1e19ee3b6da7cf6b1bc81891de135de4c4
  state: discovered
- id: 34f378f4-5287-5128-819f-5b7c8b4d96c0
  symbol_path: src/cli/logic/run.py::vectorize_capabilities
  module: cli.logic.run
  qualname: vectorize_capabilities
  kind: function
  ast_signature: TBD
  fingerprint: 12bbfc37037d6b39e884f633848d1155e8c830e84e98b0eecf88826b9acb1589
  state: discovered
- id: 3957b0cc-8873-5d4d-8209-723924075eff
  symbol_path: src/core/syntax_checker.py::check_syntax
  module: core.syntax_checker
  qualname: check_syntax
  kind: function
  ast_signature: TBD
  fingerprint: 1364f88841c69497e8e60da5590e605cb9abe2023ed6c7914d4834889565c54c
  state: discovered
- id: 5369341c-f51f-5f49-a1a2-4c62cca4ffc6
  symbol_path: src/cli/logic/cli_utils.py::find_test_file_for_capability_async
  module: cli.logic.cli_utils
  qualname: find_test_file_for_capability_async
  kind: function
  ast_signature: TBD
  fingerprint: 13eef2f115d6ad56a1f12cd76a457311e16e24507a65b4ae14d88b0a830fb6fa
  state: discovered
- id: 55337b4a-c2b9-529e-8673-20f5a2031bc8
  symbol_path: src/features/project_lifecycle/bootstrap_service.py::register
  module: features.project_lifecycle.bootstrap_service
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 14cf756f9552d261616386863878bdcfef01b16007a4de841355393211f2c876
  state: discovered
- id: 748a1124-fe08-5bf5-acf8-af7b804fdfce
  symbol_path: src/features/introspection/capability_discovery_service.py::load_and_validate_capabilities
  module: features.introspection.capability_discovery_service
  qualname: load_and_validate_capabilities
  kind: function
  ast_signature: TBD
  fingerprint: 16ea143329e515abf27bf43551c1b1c4fac5ad04f080684de4a867887fba7b0b
  state: discovered
- id: b864cdc3-91af-506e-bbde-43dd03fad94e
  symbol_path: src/cli/logic/knowledge_sync/verify.py::run_verify
  module: cli.logic.knowledge_sync.verify
  qualname: run_verify
  kind: function
  ast_signature: TBD
  fingerprint: 1716f83bbc02264f917854ebcd5d90c23fad01a8bbf6d8bdab6237c9ccc7437b
  state: discovered
- id: b1a77fa5-d186-5cf6-8e0b-eec1b0c618ee
  symbol_path: src/cli/logic/utils_migration.py::parse_migration_plan
  module: cli.logic.utils_migration
  qualname: parse_migration_plan
  kind: function
  ast_signature: TBD
  fingerprint: 172f18710aee0cc40d91a8c728e67a1e3823a716f6793007c34f992627e8a67d
  state: discovered
- id: 4e8218b7-a9aa-5d0e-b0a2-c0a1839245d3
  symbol_path: src/features/governance/checks/security_checks.py::SecurityChecks
  module: features.governance.checks.security_checks
  qualname: SecurityChecks
  kind: class
  ast_signature: TBD
  fingerprint: 18148a271b754a5da6e3e2e0ad042c0df4cd6a27e0356c8e3316845b3ea8fac3
  state: discovered
- id: bf77bd33-8dee-5333-b184-155805e05e45
  symbol_path: src/services/repositories/db/migration_service.py::migrate_db
  module: services.repositories.db.migration_service
  qualname: migrate_db
  kind: function
  ast_signature: TBD
  fingerprint: 1a5d680164e12e976e1dc8726f5638b20e0c4a831574073be21ec8f4c1534938
  state: discovered
- id: 1b548cf5-c8e4-5da9-ac66-29cea7239bc4
  symbol_path: src/cli/logic/reviewer.py::peer_review
  module: cli.logic.reviewer
  qualname: peer_review
  kind: function
  ast_signature: TBD
  fingerprint: 1b22185c1b8bc8a24c16cd8758e84c6778187b511686087d8b5ca1f009c2187d
  state: discovered
- id: d03e797b-48a4-54a2-bb6e-caa2eaf71d21
  symbol_path: src/cli/logic/hub.py::hub_doctor
  module: cli.logic.hub
  qualname: hub_doctor
  kind: function
  ast_signature: TBD
  fingerprint: 1bafa52841be41dfa4c712a230b55ffb92a49602e893a5243f3ab56fe4db4cf8
  state: discovered
- id: a5935ef9-9688-5858-a8c9-ba2a9c8acc44
  symbol_path: src/services/database/models.py::SymbolCapabilityLink
  module: services.database.models
  qualname: SymbolCapabilityLink
  kind: class
  ast_signature: TBD
  fingerprint: 1c6e299fca7bc52bb894f7db8026bb49bec0c8eb279c8996b9af229f9409f9f3
  state: discovered
- id: 2ab69275-ef69-590d-8c7a-5a66a1f68fa5
  symbol_path: src/cli/logic/agent.py::scaffold_new_application
  module: cli.logic.agent
  qualname: scaffold_new_application
  kind: function
  ast_signature: TBD
  fingerprint: 1cb14535873179ad197367d06b7e93d2199db87035d235da3d5f77d7ba5d8abb
  state: discovered
- id: e15f360b-2ce7-5fa7-a697-9d588fde2c66
  symbol_path: src/features/introspection/capability_discovery_service.py::collect_code_capabilities
  module: features.introspection.capability_discovery_service
  qualname: collect_code_capabilities
  kind: function
  ast_signature: TBD
  fingerprint: 1cc7144a5c62e0bc1935eaedc438d2d2b146b2e0487afe1384eb863479d93e0b
  state: discovered
- id: e42a245c-15f4-5551-904b-6f65239a47b0
  symbol_path: src/features/governance/policy_loader.py::load_available_actions
  module: features.governance.policy_loader
  qualname: load_available_actions
  kind: function
  ast_signature: TBD
  fingerprint: 1d050bdf4e25405feb9f8dbf49bebce2a25b18cd392ca419ae06011aff7ae17e
  state: discovered
- id: 7dfed392-047e-5ace-9482-e906d907751e
  symbol_path: src/features/governance/checks/file_checks.py::FileChecks
  module: features.governance.checks.file_checks
  qualname: FileChecks
  kind: class
  ast_signature: TBD
  fingerprint: 1e6f83386ae1e8b900f44d0f583b24a64fb14a36662e4ca6a712399b3dcd3979
  state: discovered
- id: 58c7f6b7-a87e-5f35-96a4-26e7e5c2730e
  symbol_path: src/cli/logic/sync_domains.py::sync_domains
  module: cli.logic.sync_domains
  qualname: sync_domains
  kind: function
  ast_signature: TBD
  fingerprint: 1e787b4f8a61f1e0e166274445dfc2f23c87296059ddda27b803e22862a9d7ac
  state: discovered
- id: 50517812-f1bf-529a-aef2-3216ad8b6c3d
  symbol_path: src/cli/logic/validate.py::ReviewContext
  module: cli.logic.validate
  qualname: ReviewContext
  kind: class
  ast_signature: TBD
  fingerprint: 1e82be3a32278f59e5cceb61baac51c2b9f8ebbf307b746e434d22857722d277
  state: discovered
- id: d3f7fc34-62ed-5a76-8f51-33436d315aac
  symbol_path: src/features/governance/checks/legacy_tag_check.py::LegacyTagCheck
  module: features.governance.checks.legacy_tag_check
  qualname: LegacyTagCheck
  kind: class
  ast_signature: TBD
  fingerprint: 20046862e714fb152fa7f9f71acdd7b13d38b3faacdd739210866d326d067af0
  state: discovered
- id: 380be34d-88df-55dc-bd1b-1d0efcfd77b9
  symbol_path: src/shared/legacy_models.py::LegacyLlmResource
  module: shared.legacy_models
  qualname: LegacyLlmResource
  kind: class
  ast_signature: TBD
  fingerprint: 209d997367d7af20dbf810470ec4e00fdf3dd6a305bef72aeef24efb8d4af1cb
  state: discovered
- id: 153a9936-3b7d-5946-a565-8c4e0ff44858
  symbol_path: src/features/governance/constitutional_auditor.py::ConstitutionalAuditor
  module: features.governance.constitutional_auditor
  qualname: ConstitutionalAuditor
  kind: class
  ast_signature: TBD
  fingerprint: 2333231d9e7c8ccb444a8735c5e88d5ac5f2159f10803ba4aa3dacd9fff7d9f3
  state: discovered
- id: 6dd568dc-7811-578b-ac09-91e3d047fd5a
  symbol_path: src/features/governance/checks/knowledge_source_check.py::CheckResult
  module: features.governance.checks.knowledge_source_check
  qualname: CheckResult
  kind: class
  ast_signature: TBD
  fingerprint: 2429aa4f5b2f7716a04f382e0b9f6a02d4ddc14676181a5ceb70a8df79877a50
  state: discovered
- id: fb416f2a-7611-5269-9d08-a2ebc4d42a5c
  symbol_path: src/cli/logic/guard_cli.py::register_guard
  module: cli.logic.guard_cli
  qualname: register_guard
  kind: function
  ast_signature: TBD
  fingerprint: 246d9e8bebd775a4025c3081338065ee2c401141c797140f8a85f8729bf43de0
  state: discovered
- id: eab5bc15-09c8-56ca-9103-a160e16f0bce
  symbol_path: src/cli/logic/knowledge_sync/utils.py::compute_digest
  module: cli.logic.knowledge_sync.utils
  qualname: compute_digest
  kind: function
  ast_signature: TBD
  fingerprint: 24b06ba890a451599bffffeec581653e962f9a3f4f216788c3587897c1e02238
  state: discovered
- id: 8d170582-1d07-5082-908c-e89336fbe46c
  symbol_path: src/shared/legacy_models.py::LegacyCognitiveRole
  module: shared.legacy_models
  qualname: LegacyCognitiveRole
  kind: class
  ast_signature: TBD
  fingerprint: 257d08c8807b5b4519d5a9d61c963852aaf72d36b8dd54a69d6ddf81c97c312d
  state: discovered
- id: 4ef358d1-d980-5004-9bc8-62fd41525cc6
  symbol_path: src/features/autonomy/micro_proposal_executor.py::MicroProposalExecutor
  module: features.autonomy.micro_proposal_executor
  qualname: MicroProposalExecutor
  kind: class
  ast_signature: TBD
  fingerprint: 258339fb0159634c38d496ba58ef19be11a4a8802b1cc05cf543d221f05920d5
  state: discovered
- id: fa1a313a-ddbf-5557-b7ea-3133940d7e92
  symbol_path: src/cli/logic/knowledge_sync/snapshot.py::fetch_capabilities
  module: cli.logic.knowledge_sync.snapshot
  qualname: fetch_capabilities
  kind: function
  ast_signature: TBD
  fingerprint: 26390938ea31cd241f7c221479999f802b60d5da63efd80ea709fbf72fa69109
  state: discovered
- id: a41c3431-189d-5262-bf62-2694c5ddedb6
  symbol_path: src/cli/commands/manage.py::approve_command_wrapper
  module: cli.commands.manage
  qualname: approve_command_wrapper
  kind: function
  ast_signature: TBD
  fingerprint: 268acfbaccc2694b1b6b74b81b04a04bbe71a939ca13fbcc09608eabe5a2aed9
  state: discovered
- id: 61ba309b-9c47-5c8a-bdc8-14fe49dc1612
  symbol_path: src/services/database/models.py::Symbol
  module: services.database.models
  qualname: Symbol
  kind: class
  ast_signature: TBD
  fingerprint: 27d7f82669fadff345cca341cc892c94fabeadafbaf0446e3d7e6b730e046484
  state: discovered
- id: 574b6ddd-0ae2-5b06-a8f6-ada805d4734c
  symbol_path: src/features/project_lifecycle/scaffolding_service.py::new_project
  module: features.project_lifecycle.scaffolding_service
  qualname: new_project
  kind: function
  ast_signature: TBD
  fingerprint: 2835a5461ad670b91c0e202f81dff92260a43b0fc177580739e529e0f62856c4
  state: discovered
- id: 172ae594-7da3-599c-adc9-4c485e905679
  symbol_path: src/features/introspection/knowledge_helpers.py::log_failure
  module: features.introspection.knowledge_helpers
  qualname: log_failure
  kind: function
  ast_signature: TBD
  fingerprint: 285998682574c748e5a91051540abcb9b68a36bff9130b97cf46a7fce331560c
  state: discovered
- id: b9f6aac1-c2db-52dc-8a01-97f285e2510a
  symbol_path: src/cli/logic/run.py::develop
  module: cli.logic.run
  qualname: develop
  kind: function
  ast_signature: TBD
  fingerprint: 2877d68341fe0a37776fc11f85727e2c204a8ccce30be8cf77e9bbb05aef7a83
  state: discovered
- id: 691a51aa-20ea-5b2f-8193-5ed34953cfbb
  symbol_path: src/cli/commands/check.py::register
  module: cli.commands.check
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 290a9a6a63e747eef62ba250095ea1df7d59945d22524122edcc3902071da349
  state: discovered
- id: 2cb1cb8c-dc00-5bcd-9d97-d1ea42cc96d3
  symbol_path: src/cli/logic/hub.py::hub_search
  module: cli.logic.hub
  qualname: hub_search
  kind: function
  ast_signature: TBD
  fingerprint: 2ba5bf86c7dbe9b6dbc660a70df23643cd75337076c2eb455ae3f0b60047d1f0
  state: discovered
- id: 168bbe2d-fd66-5e66-baca-e5abc340fac1
  symbol_path: src/shared/utils/embedding_utils.py::build_embedder_from_env
  module: shared.utils.embedding_utils
  qualname: build_embedder_from_env
  kind: function
  ast_signature: TBD
  fingerprint: 2bb56f73d0410ba0e8f23cbb5f0f55d496b6b2475fc741a8abeceac99576dccb
  state: discovered
- id: b28d7a1d-84be-5e73-87a6-458d4a247a19
  symbol_path: src/cli/logic/hub.py::hub_list
  module: cli.logic.hub
  qualname: hub_list
  kind: function
  ast_signature: TBD
  fingerprint: 2bc34af8feb8b741b485127213e780522c225d6792ca123c6caaf1427949e3f1
  state: discovered
- id: 74647e53-7390-5163-ac36-afffc85f56bc
  symbol_path: src/core/agents/reconnaissance_agent.py::ReconnaissanceAgent
  module: core.agents.reconnaissance_agent
  qualname: ReconnaissanceAgent
  kind: class
  ast_signature: TBD
  fingerprint: 2cd1e7a3c76e78fc558c53425c304a91d10707246664e9c4666048a55d4d2442
  state: discovered
- id: 3e8374d1-3704-5209-97c2-469729cd7256
  symbol_path: src/features/governance/checks/base_check.py::BaseCheck
  module: features.governance.checks.base_check
  qualname: BaseCheck
  kind: class
  ast_signature: TBD
  fingerprint: 2cdd1f4583e93348a2a584b427db5fafc7c79c2b793748c445353bf2d7267cc8
  state: discovered
- id: b13e7f4c-19a8-5976-9b7e-1d7d924c0310
  symbol_path: src/cli/logic/cli_utils.py::load_yaml_file
  module: cli.logic.cli_utils
  qualname: load_yaml_file
  kind: function
  ast_signature: TBD
  fingerprint: 2ce3e9f321eb08e41cb6220dcbcc1598d37029ebcf200d493aba3bdd7c765ba5
  state: discovered
- id: 1b358084-fed0-599b-8f7b-7f7c9e1a8c98
  symbol_path: src/services/database/models.py::Northstar
  module: services.database.models
  qualname: Northstar
  kind: class
  ast_signature: TBD
  fingerprint: 2d70f223444e9fb7adbec6e4447f82398635e2c33ffaaa00e00693095af0d883
  state: discovered
- id: a5850f46-9e17-5650-a222-71d746f6fb82
  symbol_path: src/core/agents/tagger_agent.py::CapabilityTaggerAgent
  module: core.agents.tagger_agent
  qualname: CapabilityTaggerAgent
  kind: class
  ast_signature: TBD
  fingerprint: 2f2631750e39318401bc31f679926805d3c9a63c8f06374d1a6b1e11d1656098
  state: discovered
- id: c7dc0123-5161-5c01-86b8-2b23d8956ef2
  symbol_path: src/cli/logic/log_audit.py::log_audit
  module: cli.logic.log_audit
  qualname: log_audit
  kind: function
  ast_signature: TBD
  fingerprint: 2f863ca4f502f657db72262f741a60f86a86a3406a7f61693e179c4962cd020d
  state: discovered
- id: f9c2eff6-b589-53d4-afcc-36ea6caa1444
  symbol_path: src/cli/logic/hub.py::register
  module: cli.logic.hub
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 2ff1e08386fb8af8d0bfabcbfcef4001ba7ec5e0f1f416d1fa4076122e138e2f
  state: discovered
- id: eb78efe5-0d71-55b7-9fab-76fb837123ed
  symbol_path: src/features/project_lifecycle/definition_service.py::update_definitions_in_db
  module: features.project_lifecycle.definition_service
  qualname: update_definitions_in_db
  kind: function
  ast_signature: TBD
  fingerprint: 31f2b4b11953f96fffd047e26aafa2d7571dc2aeab491f95837a3677c466981d
  state: discovered
- id: 19edd552-a03b-5ada-92f7-ae0c45f58ebf
  symbol_path: src/features/governance/checks/knowledge_source_check.py::KnowledgeSourceCheck
  module: features.governance.checks.knowledge_source_check
  qualname: KnowledgeSourceCheck
  kind: class
  ast_signature: TBD
  fingerprint: 326ebe8aa418d22066b354782489d310c8f2818408b734aff6b3a87764cf088f
  state: discovered
- id: a54a79c5-55ad-5002-99e8-e834e468e129
  symbol_path: src/cli/commands/search.py::register
  module: cli.commands.search
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 327f8e3a2615acaaf5e9b0be834ed51819f30f283cb7ced7a23877e4a73836c5
  state: discovered
- id: e0030fb0-e764-5037-b789-74c304f8af04
  symbol_path: src/core/agents/self_correction_engine.py::attempt_correction
  module: core.agents.self_correction_engine
  qualname: attempt_correction
  kind: function
  ast_signature: TBD
  fingerprint: 3361ab54d6d24ef17f8fd1c2a1ec7bff6f5f082030e405c7c3e57917d9a0171f
  state: discovered
- id: 322e9198-0bae-5f24-821d-635d7c8031b1
  symbol_path: src/cli/logic/proposal_service.py::proposals_sign
  module: cli.logic.proposal_service
  qualname: proposals_sign
  kind: function
  ast_signature: TBD
  fingerprint: 33b3572b28e48052a33e8a1aefee0a690c20c1e1331b1bd7b498ecd2a20ca2c3
  state: discovered
- id: 7b20912c-00db-5ef5-8460-2cdd3f3ed494
  symbol_path: src/features/self_healing/fix_manifest_hygiene.py::run_fix_manifest_hygiene
  module: features.self_healing.fix_manifest_hygiene
  qualname: run_fix_manifest_hygiene
  kind: function
  ast_signature: TBD
  fingerprint: 33ec9c584aeacb2dc7e39a5304266d218e909ff550053b3abbd2622302e4eaea
  state: discovered
- id: 461902ea-8be0-55f7-a8e5-29626655726e
  symbol_path: src/services/database/models.py::SymbolVectorLink
  module: services.database.models
  qualname: SymbolVectorLink
  kind: class
  ast_signature: TBD
  fingerprint: 34a0f2901bcbf23cbc9cf2de78a1fcdcae7b55cc5a7bca236ce1626c1bd5c6bb
  state: discovered
- id: 0edf8347-17a6-5db0-bf2b-083538771839
  symbol_path: src/services/repositories/db/status_service.py::status
  module: services.repositories.db.status_service
  qualname: status
  kind: function
  ast_signature: TBD
  fingerprint: 34bb9eb07bb23e6382f9b88dd9602d2ef7b452db0dc0337f3503be2b593cfb82
  state: discovered
- id: 6539fb10-d972-5234-b439-9bc557991885
  symbol_path: src/cli/logic/status.py::status
  module: cli.logic.status
  qualname: status
  kind: function
  ast_signature: TBD
  fingerprint: 34bb9eb07bb23e6382f9b88dd9602d2ef7b452db0dc0337f3503be2b593cfb82
  state: discovered
- id: 872f3a6c-6991-5488-821a-b00b348429f2
  symbol_path: src/features/self_healing/capability_tagging_service.py::tag_unassigned_capabilities
  module: features.self_healing.capability_tagging_service
  qualname: tag_unassigned_capabilities
  kind: function
  ast_signature: TBD
  fingerprint: 351295deff87751e516a92cbbdf265797ebbf51be9985094251dbde05ef66fe7
  state: discovered
- id: 6c45fdfc-80dd-5b4a-a3b3-b6c2a68c8465
  symbol_path: src/shared/context.py::CoreContext
  module: shared.context
  qualname: CoreContext
  kind: class
  ast_signature: TBD
  fingerprint: 3517f3384fe341ec9f534e5ceb25b0cf8d3e605c1420597e8a2034537174075b
  state: discovered
- id: 9920158d-f859-5926-9221-0988e9320f11
  symbol_path: src/cli/logic/new.py::register
  module: cli.logic.new
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 358ca9667b776207e550b217e6f9933316ae2f21c94becefe3425c018fd562b8
  state: discovered
- id: 9aa2a067-7f56-53d4-b654-ec2d19c888de
  symbol_path: src/cli/logic/proposals_micro.py::register
  module: cli.logic.proposals_micro
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 369c6496e46ec634795d7b0571d6ac496ef3dc85af8e4381b9b8f14f2f2c1555
  state: discovered
- id: f4f4a212-d20e-50b6-b01f-dfd68c814046
  symbol_path: src/cli/logic/sync.py::sync_knowledge_base
  module: cli.logic.sync
  qualname: sync_knowledge_base
  kind: function
  ast_signature: TBD
  fingerprint: 397d4cc9c56b993c96ea68f6f1d480addcd9f6cd98e60c5fc0dd6364ef09e0e8
  state: discovered
- id: 720d1abe-3061-54b3-a103-17da35da6251
  symbol_path: src/core/yaml_validator.py::validate_yaml_code
  module: core.yaml_validator
  qualname: validate_yaml_code
  kind: function
  ast_signature: TBD
  fingerprint: 3a3549bfec1d94b10cb62bd8d9d8a2e9648e98aafcb302f2fbf4376fb9f60abc
  state: discovered
- id: c22524a7-c043-5bbe-bc59-ea4fa915b2e6
  symbol_path: src/cli/interactive.py::launch_interactive_menu
  module: cli.interactive
  qualname: launch_interactive_menu
  kind: function
  ast_signature: TBD
  fingerprint: 3ad29ee40dd183ee307007d7bbe81ae970a36c770b4eaa372d87f44eae5603f7
  state: discovered
- id: 1d8748db-02bf-52f2-9d68-34de4df91e69
  symbol_path: src/core/python_validator.py::validate_python_code_async
  module: core.python_validator
  qualname: validate_python_code_async
  kind: function
  ast_signature: TBD
  fingerprint: 3ade1c798c1e85f29d5ec07a1988aa41c44ef49d0c01c3cb3266fff19b2745b7
  state: discovered
- id: 42b7d712-b457-5ff1-aa8d-649680c3105b
  symbol_path: src/cli/logic/proposals_micro.py::micro_apply
  module: cli.logic.proposals_micro
  qualname: micro_apply
  kind: function
  ast_signature: TBD
  fingerprint: 3b1226da9a50bb58d7a99a1863bbb921e1ce7231acce32c6d15cc08fe918a937
  state: discovered
- id: 5882c40e-a8f4-50c5-b602-e0399062cf94
  symbol_path: src/cli/logic/reconcile.py::reconcile_from_cli
  module: cli.logic.reconcile
  qualname: reconcile_from_cli
  kind: function
  ast_signature: TBD
  fingerprint: 3ce0867b23a67aeadb9c71325248c21d551466c6c70c64b448e9c7456441aef1
  state: discovered
- id: 39dbe585-1a89-5240-8b22-a8c9bc9a2eb8
  symbol_path: src/cli/commands/run.py::develop
  module: cli.commands.run
  qualname: develop
  kind: function
  ast_signature: TBD
  fingerprint: 3d2341e298c833ac2d6049f7e711d8ba5a1cc361593c89f6bc765b935259f9e7
  state: discovered
- id: 728ffb29-51c6-570c-a4f4-2e3b75636713
  symbol_path: src/features/introspection/audit_unassigned_capabilities.py::get_unassigned_symbols
  module: features.introspection.audit_unassigned_capabilities
  qualname: get_unassigned_symbols
  kind: function
  ast_signature: TBD
  fingerprint: 3d4db71bd769a1539fa0760dd55dcd91efeb754ec5be9acbe8ff9e250e01bc96
  state: discovered
- id: e7944636-7fc2-59af-961a-236e8033d516
  symbol_path: src/features/introspection/knowledge_vectorizer.py::get_stored_chunks
  module: features.introspection.knowledge_vectorizer
  qualname: get_stored_chunks
  kind: function
  ast_signature: TBD
  fingerprint: 3f1d0b7213e0090da1037f7bd35c8953e0c2c0198eaff1b4fcb443755b94ed95
  state: discovered
- id: ab13e8e8-b474-50fa-b04a-db7a192da9da
  symbol_path: src/features/self_healing/duplicate_id_service.py::resolve_duplicate_ids
  module: features.self_healing.duplicate_id_service
  qualname: resolve_duplicate_ids
  kind: function
  ast_signature: TBD
  fingerprint: 3f5a9c67401fadda3c508f86013839316b40bea54d46bd035c7de2c3c7f0b860
  state: discovered
- id: 89b7ca1a-cd41-5c9d-ba9d-6777546bf717
  symbol_path: src/cli/commands/run.py::register
  module: cli.commands.run
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 3f799235fb431158a9910f4b360806515f4eb92f9e1e5a57c9a92dd20967fd1d
  state: discovered
- id: d4571e38-7f17-5aaa-bd98-3c9042a65d39
  symbol_path: src/cli/logic/cli_utils.py::archive_rollback_plan
  module: cli.logic.cli_utils
  qualname: archive_rollback_plan
  kind: function
  ast_signature: TBD
  fingerprint: 3fee477cc6e1a83e51d223a8844378614d3e379c846ae8b2b008ac59063dd680
  state: discovered
- id: 3157851a-66e2-59ec-bfba-12f72040e016
  symbol_path: src/core/actions/validation_actions.py::ValidateCodeHandler
  module: core.actions.validation_actions
  qualname: ValidateCodeHandler
  kind: class
  ast_signature: TBD
  fingerprint: 41a7fe94d03b6dccc525d0f0fc6a53a970f3158943160ce2a08248a49de5e934
  state: discovered
- id: 25d89aac-0d8a-59a0-ac53-415873b1d5e7
  symbol_path: src/services/repositories/db/common.py::git_commit_sha
  module: services.repositories.db.common
  qualname: git_commit_sha
  kind: function
  ast_signature: TBD
  fingerprint: 424a6f2d131bd29d01b6e1777c2a6f6a2ee1c4f8c19762de6d3a97f8eabdac6b
  state: discovered
- id: 1346d1c9-e7c3-54d4-8e3e-db554896900c
  symbol_path: src/core/agents/base_planner.py::build_planning_prompt
  module: core.agents.base_planner
  qualname: build_planning_prompt
  kind: function
  ast_signature: TBD
  fingerprint: 42a93581b68751dc0a2f5e2c144f26e0f3852b70019dc264a8d2457ec78afa72
  state: discovered
- id: 6e4f0dd0-982c-5c4f-a6eb-a29fe5bc3844
  symbol_path: src/shared/utils/import_scanner.py::scan_imports_for_file
  module: shared.utils.import_scanner
  qualname: scan_imports_for_file
  kind: function
  ast_signature: TBD
  fingerprint: 42bb5532dfbc0b03134979db387ac69dab213de0539724e125ec4cdb88080722
  state: discovered
- id: cfbe767f-2417-59c8-96b2-72a516b6024f
  symbol_path: src/services/database/models.py::CliCommand
  module: services.database.models
  qualname: CliCommand
  kind: class
  ast_signature: TBD
  fingerprint: 4315630994cb4cd9ab02281c987ab9e4cb6dbea1f5e57e3ad29114cb4d056e3b
  state: discovered
- id: f7b80239-1224-5777-932e-b955dc8436b5
  symbol_path: src/features/governance/checks/environment_checks.py::EnvironmentChecks
  module: features.governance.checks.environment_checks
  qualname: EnvironmentChecks
  kind: class
  ast_signature: TBD
  fingerprint: 43c3d8bc05033fcd2bbacfef2e321af49a5b97fc1bddc08c2ffe450b537ba5cb
  state: discovered
- id: a7f03f15-6502-5b60-8eca-73cb2441d614
  symbol_path: src/core/ruff_linter.py::fix_and_lint_code_with_ruff
  module: core.ruff_linter
  qualname: fix_and_lint_code_with_ruff
  kind: function
  ast_signature: TBD
  fingerprint: 43ca55ae34afc29c10e967766b01252dae62b0efe0b5e8f024d0d5f07d1cc287
  state: discovered
- id: beded87a-c19b-5ee4-bbc1-bc127e1e86d2
  symbol_path: src/features/project_lifecycle/definition_service.py::get_undefined_symbols
  module: features.project_lifecycle.definition_service
  qualname: get_undefined_symbols
  kind: function
  ast_signature: TBD
  fingerprint: 448c00a8c7b18dd4f40b8fc93c0d4a6c06d8c88c23dda20312f0b003cbdf64fc
  state: discovered
- id: 8186fff5-055c-513a-849c-ad2b25bae793
  symbol_path: src/shared/models/capability_models.py::CapabilityMeta
  module: shared.models.capability_models
  qualname: CapabilityMeta
  kind: class
  ast_signature: TBD
  fingerprint: 4598ffbadc03374780c921fe94cf0c2a4c61695664761fe07e635d207a270c7a
  state: discovered
- id: c5190e79-8299-5c4a-b24a-40c6c1cd6ebf
  symbol_path: src/features/maintenance/command_sync_service.py::sync_commands_to_db
  module: features.maintenance.command_sync_service
  qualname: sync_commands_to_db
  kind: function
  ast_signature: TBD
  fingerprint: 45e396411f636ef29e0416badeec46a7994fb70363dc81d077c1f0b1946154c1
  state: discovered
- id: 8e1631d0-2c8f-5ddc-9192-b71658b3343f
  symbol_path: src/features/governance/audit_context.py::AuditorContext
  module: features.governance.audit_context
  qualname: AuditorContext
  kind: class
  ast_signature: TBD
  fingerprint: 4744de17112c06b841b498ff16fada4f4ba2641eb4f060edd9e06ed393bc8112
  state: discovered
- id: ac480e07-a427-5bb3-b561-e91d8cad0d87
  symbol_path: src/core/actions/code_actions.py::CreateFileHandler
  module: core.actions.code_actions
  qualname: CreateFileHandler
  kind: class
  ast_signature: TBD
  fingerprint: 47ce5fc55c0efc1c18bf73ae1b0aefcbcebf705618e1960b26503f1d343dd9c7
  state: discovered
- id: 603fdf61-88b8-5d61-a76e-5ff7cb43d590
  symbol_path: src/cli/logic/diagnostics.py::policy_coverage
  module: cli.logic.diagnostics
  qualname: policy_coverage
  kind: function
  ast_signature: TBD
  fingerprint: 480c823c9f580a452a47fc74992466a8cc7b1d46ebfdc98681c8cf349bb1b496
  state: discovered
- id: cd469445-dee7-5f46-98f7-6e20f737fa9d
  symbol_path: src/api/v1/knowledge_routes.py::list_capabilities
  module: api.v1.knowledge_routes
  qualname: list_capabilities
  kind: function
  ast_signature: TBD
  fingerprint: 498f26ee7426a5fc77a78004570a42675ba273a714af2f200e2e2765441433ab
  state: discovered
- id: 4086885d-8f1a-5cb0-9280-beac9592a397
  symbol_path: src/features/maintenance/dotenv_sync_service.py::run_dotenv_sync
  module: features.maintenance.dotenv_sync_service
  qualname: run_dotenv_sync
  kind: function
  ast_signature: TBD
  fingerprint: 4a43c803a7436c52c9e4cbd889b14249d97dc1ec69f35b7970f1d7a72f3339f9
  state: discovered
- id: d83e7a25-08a2-57d3-920f-82d45d32d8ab
  symbol_path: src/features/introspection/knowledge_helpers.py::extract_source_code
  module: features.introspection.knowledge_helpers
  qualname: extract_source_code
  kind: function
  ast_signature: TBD
  fingerprint: 4a58ebc29644413a483ec93764c9af33ce0649d073c73e67b1fb04e1faf48903
  state: discovered
- id: 72020ffc-880a-5524-aa15-81a73f38ebb6
  symbol_path: src/core/service_registry.py::ServiceRegistry
  module: core.service_registry
  qualname: ServiceRegistry
  kind: class
  ast_signature: TBD
  fingerprint: 4bc1024d72ef5d065a9f5b33887e9c189351f7a1ad13904fa5baad8b26f92100
  state: discovered
- id: 7b1bc464-444a-5aa2-b9dc-20a19e5bf659
  symbol_path: src/services/database/models.py::Action
  module: services.database.models
  qualname: Action
  kind: class
  ast_signature: TBD
  fingerprint: 4c56f9435e1b5eef7da5061265560f3d28f6ab9290bba539e1acfc4fb41779e9
  state: discovered
- id: d294c09c-5aad-50e6-afee-46dde1669302
  symbol_path: src/services/repositories/db/engine.py::ping
  module: services.repositories.db.engine
  qualname: ping
  kind: function
  ast_signature: TBD
  fingerprint: 4c87e59133c67764d9e3271b7d6ab1a79d1004a0b36e4210c1613d201ca3bcca
  state: discovered
- id: 75b1901c-2e1d-553e-91b2-ec1ff6e3dc2c
  symbol_path: src/features/governance/checks/import_rules.py::ImportRulesCheck
  module: features.governance.checks.import_rules
  qualname: ImportRulesCheck
  kind: class
  ast_signature: TBD
  fingerprint: 4ceed77fae09282f3fe9bd6d13c8eaf55f6fb2d8336485730f50e4a7e66a8646
  state: discovered
- id: d17feffd-94a1-5636-b502-ce58fc16b644
  symbol_path: src/cli/logic/cli_utils.py::should_fail
  module: cli.logic.cli_utils
  qualname: should_fail
  kind: function
  ast_signature: TBD
  fingerprint: 4d0fc9b76df312e54be34f47cf05c8fc7f3704ee9a5081f9305e5c1b95aa9365
  state: discovered
- id: ce9c6db3-4064-5d86-9597-1eabe6808745
  symbol_path: src/features/project_lifecycle/integration_service.py::integrate_changes
  module: features.project_lifecycle.integration_service
  qualname: integrate_changes
  kind: function
  ast_signature: TBD
  fingerprint: 4dca53676192a291fbcecc6625cb76478ad976ed2039fe7b2ef5f882a79d2026
  state: discovered
- id: 4d8ffe32-b8fe-56f0-aa1a-fdb5a5a443ec
  symbol_path: src/shared/utils/header_tools.py::HeaderComponents
  module: shared.utils.header_tools
  qualname: HeaderComponents
  kind: class
  ast_signature: TBD
  fingerprint: 4e1872a73a783a6f2d7250d141c0c08873d3fab5206aa1c4b82f054740e4bf23
  state: discovered
- id: 62d47bc6-2b71-5b15-a329-629db3a1bcca
  symbol_path: src/services/repositories/db/common.py::load_policy
  module: services.repositories.db.common
  qualname: load_policy
  kind: function
  ast_signature: TBD
  fingerprint: 50962fd29c527c67698752efeee7e45c6a87d93dc45362e0e0eaa40ae6142b4a
  state: discovered
- id: da3543cc-a0b5-5ddf-b9a9-a552a4179370
  symbol_path: src/cli/logic/proposal_service.py::proposals_list
  module: cli.logic.proposal_service
  qualname: proposals_list
  kind: function
  ast_signature: TBD
  fingerprint: 50b8fa4fe46b6ddb5ca3b183bb508ae82605905b0d70f7c23ef9680ddadc9f8c
  state: discovered
- id: d430613e-4c0a-5c9e-81f1-fe56780f1542
  symbol_path: src/cli/interactive.py::show_system_menu
  module: cli.interactive
  qualname: show_system_menu
  kind: function
  ast_signature: TBD
  fingerprint: 5141607a9188c5ba45369fafc51b80dcf47225f88b0ae2cddc65b259032b84b1
  state: discovered
- id: 52531462-8708-556e-94e1-41b0dd349aad
  symbol_path: src/features/governance/checks/capability_coverage.py::CapabilityCoverageCheck
  module: features.governance.checks.capability_coverage
  qualname: CapabilityCoverageCheck
  kind: class
  ast_signature: TBD
  fingerprint: 52339cc163ab9605b01bdd5ba738b813177514fc4b06ae4b38999a64bf52652b
  state: discovered
- id: 13981a5b-227b-5a58-9278-ef7bd2aa3471
  symbol_path: src/cli/commands/manage.py::define_symbols_command
  module: cli.commands.manage
  qualname: define_symbols_command
  kind: function
  ast_signature: TBD
  fingerprint: 52c1ea8f24550fe401e5b81f9248f355dc6bb3b446c5c8369e619eace0924dd3
  state: discovered
- id: e00d73a5-8e68-570a-8ad3-8b4270d50cfd
  symbol_path: src/features/self_healing/policy_id_service.py::add_missing_policy_ids
  module: features.self_healing.policy_id_service
  qualname: add_missing_policy_ids
  kind: function
  ast_signature: TBD
  fingerprint: 54044017d1a90fb8f3319a840ca70bbdf9d67b6d4d23d1b69df1cdad34715632
  state: discovered
- id: ccbca00d-020c-551c-88e8-c6b5d83a5aca
  symbol_path: src/core/main.py::create_app
  module: core.main
  qualname: create_app
  kind: function
  ast_signature: TBD
  fingerprint: 54bd44afe9f13f53a2b572263e10b32ba5da9830531c91510a60854cb97ca400
  state: discovered
- id: 8d9b2928-c6bc-5ad6-a6c2-6334c2ab3c43
  symbol_path: src/shared/models/audit_models.py::AuditFinding
  module: shared.models.audit_models
  qualname: AuditFinding
  kind: class
  ast_signature: TBD
  fingerprint: 54e587666358be80723368f7270a11deef13fac927121b00d32bf911c72fc670
  state: discovered
- id: 6a14f6a9-68bd-57dd-bedb-db229f863490
  symbol_path: src/features/introspection/export_vectors.py::export_vectors
  module: features.introspection.export_vectors
  qualname: export_vectors
  kind: function
  ast_signature: TBD
  fingerprint: 55791567551b3dabb7446fe74836bc2bcdf28cd480c0f9715b60ea0fdb0c2c5f
  state: discovered
- id: 4afbfeb6-aa5f-58e1-8dda-59963c693ba8
  symbol_path: src/features/introspection/capability_discovery_service.py::validate_agent_roles
  module: features.introspection.capability_discovery_service
  qualname: validate_agent_roles
  kind: function
  ast_signature: TBD
  fingerprint: 5650dbab2d1e0504016044ef17ef437377714c559a539e19cd8ae915ec07fbf7
  state: discovered
- id: 87f87cc2-e739-5fd7-9a0f-83cf39d38703
  symbol_path: src/services/repositories/db/common.py::ensure_ledger
  module: services.repositories.db.common
  qualname: ensure_ledger
  kind: function
  ast_signature: TBD
  fingerprint: 56d81945c37e4358c5572d3b4454a0547656c6f3f2b58ba0c1dbb2f93f563536
  state: discovered
- id: 1a5c85c1-00a4-53fa-a17e-528062eedd2b
  symbol_path: src/shared/utils/subprocess_utils.py::run_poetry_command
  module: shared.utils.subprocess_utils
  qualname: run_poetry_command
  kind: function
  ast_signature: TBD
  fingerprint: 5771851dc409ed4a82740162ab06f2ec3b28c7525d31f73fcc1349e94ce265d2
  state: discovered
- id: cdacfe8b-ee70-5671-b124-b52697364a94
  symbol_path: src/shared/ast_utility.py::extract_docstring
  module: shared.ast_utility
  qualname: extract_docstring
  kind: function
  ast_signature: TBD
  fingerprint: 57ac35b293f2c60efbab4feda4f6bd03ced477b632d56e57dfcdc6a33c989d7f
  state: discovered
- id: 270b1a87-0e1f-5ac8-9dd9-c2a8ec743bee
  symbol_path: src/shared/models/execution_models.py::TaskParams
  module: shared.models.execution_models
  qualname: TaskParams
  kind: class
  ast_signature: TBD
  fingerprint: 58122310aa3d9d9f6699a668d4168707fdb54978e5c93a1b947fd2be63672b1f
  state: discovered
- id: e8139a34-02c7-5ffa-bbe9-1d11eacee9c5
  symbol_path: src/cli/logic/vector_drift.py::inspect_vector_drift
  module: cli.logic.vector_drift
  qualname: inspect_vector_drift
  kind: function
  ast_signature: TBD
  fingerprint: 5976ce531f8b8691a5f9434bb528f25aea838c99301952744d8cce4a09ed7c99
  state: discovered
- id: 90004b66-c4bf-5a49-8ce8-23e1a5d0fc6c
  symbol_path: src/core/intent_guard.py::PolicyRule
  module: core.intent_guard
  qualname: PolicyRule
  kind: class
  ast_signature: TBD
  fingerprint: 5adf54a71092e30f3da4253d6745e13149451602356acdd63657a5d55d665536
  state: discovered
- id: 599d5999-12ee-5207-8913-8baf6e48299f
  symbol_path: src/cli/logic/audit_capability_domains.py::register
  module: cli.logic.audit_capability_domains
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 5bd93769193ed910037be3a11dc0663f16e8033d6530127969949c24b3dcf034
  state: discovered
- id: 5ed52733-6049-5405-9ff6-34d7acdd3874
  symbol_path: src/shared/action_logger.py::ActionLogger
  module: shared.action_logger
  qualname: ActionLogger
  kind: class
  ast_signature: TBD
  fingerprint: 5cc06999412dde2847d62133b20898a7c6496ae297a520a25885e7e8c3be03cd
  state: discovered
- id: 6a59d6eb-895d-595e-8212-87f8e94acea1
  symbol_path: src/cli/logic/run.py::run_development_cycle
  module: cli.logic.run
  qualname: run_development_cycle
  kind: function
  ast_signature: TBD
  fingerprint: 5d52aadc6c4ccbe2afc9ab55fb3ac95c9afb8833a0ba0bbcac15864a4a9e1b9d
  state: discovered
- id: 479e6252-db7f-5f0b-b532-d673346e980f
  symbol_path: src/services/clients/llm_api_client.py::BaseLLMClient
  module: services.clients.llm_api_client
  qualname: BaseLLMClient
  kind: class
  ast_signature: TBD
  fingerprint: 5e9b6c5fe4c10fc064e2a0470876b5f6615070a38e54b02d9f9ace8c3af192ac
  state: discovered
- id: 4e397589-8aeb-5795-b54d-81caf3e013e5
  symbol_path: src/cli/logic/knowledge_sync/utils.py::read_yaml
  module: cli.logic.knowledge_sync.utils
  qualname: read_yaml
  kind: function
  ast_signature: TBD
  fingerprint: 60c4f95b6d301656a7cd5babd1c0be81dcbc671787701f2ae8315a3e2b0bc613
  state: discovered
- id: 6108e420-3b8d-54ff-b263-a5ca9a0d9f20
  symbol_path: src/shared/utils/parallel_processor.py::ThrottledParallelProcessor
  module: shared.utils.parallel_processor
  qualname: ThrottledParallelProcessor
  kind: class
  ast_signature: TBD
  fingerprint: 6164c2e46443715560f2679388635d673e50afe10a90e72c589e9bdfe233baf7
  state: discovered
- id: faf5a42d-73c7-566b-b322-d1507d2f1262
  symbol_path: src/services/llm/providers/base.py::AIProvider
  module: services.llm.providers.base
  qualname: AIProvider
  kind: class
  ast_signature: TBD
  fingerprint: 616987f6bc3fdc799b572bc003b4605619c80def1c5cddc50ca3c49788d6617a
  state: discovered
- id: 57071583-4018-56ab-906d-ec046e6fc245
  symbol_path: src/features/introspection/sync_service.py::run_sync_with_db
  module: features.introspection.sync_service
  qualname: run_sync_with_db
  kind: function
  ast_signature: TBD
  fingerprint: 61a4648746ffdfcb1f7c175327616f2d1556967a2065785de0e43f6e159f9476
  state: discovered
- id: ebdb7618-7a4e-563d-9f2d-99154f6be4c3
  symbol_path: src/cli/logic/proposal_service.py::proposals_approve
  module: cli.logic.proposal_service
  qualname: proposals_approve
  kind: function
  ast_signature: TBD
  fingerprint: 61e74eecce32ebbe56cb7ef2eecdb685282cba04fdaa05a8d2789d8ffb3346ff
  state: discovered
- id: 91bf8090-ffb0-5d60-8c2e-f2b270ca9ab9
  symbol_path: src/cli/logic/knowledge_sync/snapshot.py::fetch_northstar
  module: cli.logic.knowledge_sync.snapshot
  qualname: fetch_northstar
  kind: function
  ast_signature: TBD
  fingerprint: 6289651d6b0fbd1460a00a62e4e19ec8034216f32cd9868f5b2af5445434256e
  state: discovered
- id: 032d51e7-990a-5a78-89c3-8c836753a33e
  symbol_path: src/shared/logger.py::getLogger
  module: shared.logger
  qualname: getLogger
  kind: function
  ast_signature: TBD
  fingerprint: 6309a05f79bab52272a3bfeee23625edc26149318f65ddd0040b5916ff24f3bb
  state: discovered
- id: 77a580dd-8577-5fc0-9bf3-6bff3bf6b675
  symbol_path: src/core/capabilities.py::introspection
  module: core.capabilities
  qualname: introspection
  kind: function
  ast_signature: TBD
  fingerprint: 65c205885812932a8159baa16c02658ee3be9c35f7e536cee2a1799fa9a682b7
  state: discovered
- id: 36c62d98-d9d7-50ef-b973-a913e91d7390
  symbol_path: src/core/black_formatter.py::format_code_with_black
  module: core.black_formatter
  qualname: format_code_with_black
  kind: function
  ast_signature: TBD
  fingerprint: 67aa7b50532f6c25f84285da5bbf9430a54f3b594a386a3299c19b82faa15fbf
  state: discovered
- id: 167d15fd-9bb5-51d8-b1dc-bf8a9ba13787
  symbol_path: src/cli/commands/fix.py::sync_db_registry_command
  module: cli.commands.fix
  qualname: sync_db_registry_command
  kind: function
  ast_signature: TBD
  fingerprint: 681b3e9ab9f57d737d9e4704b807a91a5d946bf6cec3a565cffd3c7bcca81952
  state: discovered
- id: 3df99c4c-1247-50c5-baef-5fec35cd9ead
  symbol_path: src/cli/logic/byor.py::initialize_repository
  module: cli.logic.byor
  qualname: initialize_repository
  kind: function
  ast_signature: TBD
  fingerprint: 685e77beb1854c44e1d3adc89f529387ef92de25b3e864bb2a747777752208e6
  state: discovered
- id: ddb59481-bb1e-588a-8dac-af67618ae200
  symbol_path: src/core/actions/file_actions.py::DeleteFileHandler
  module: core.actions.file_actions
  qualname: DeleteFileHandler
  kind: class
  ast_signature: TBD
  fingerprint: 685ffa5cffc1cf4b3abc772a4952e283302eec7beaa04a4c6bbc2c51a0fac933
  state: discovered
- id: 510f87f0-ae55-5be5-940a-94235399f89d
  symbol_path: src/shared/legacy_models.py::LegacyCliRegistry
  module: shared.legacy_models
  qualname: LegacyCliRegistry
  kind: class
  ast_signature: TBD
  fingerprint: 68fa2df0d94fc62fae3f87ec669289ad7b48d1c21a8f36f2ce3191a81a0bcbc6
  state: discovered
- id: 3e865e92-b3e4-5c0e-8edc-686ecbb1892b
  symbol_path: src/cli/logic/diagnostics.py::debug_meta_paths
  module: cli.logic.diagnostics
  qualname: debug_meta_paths
  kind: function
  ast_signature: TBD
  fingerprint: 691f0ee72835e478b034a259613c4db34553df5e930a6aa4881bf34ae36e232c
  state: discovered
- id: fe59da83-5ccd-5982-9d80-2ff9f55bce3e
  symbol_path: src/core/self_correction_engine.py::attempt_correction
  module: core.self_correction_engine
  qualname: attempt_correction
  kind: function
  ast_signature: TBD
  fingerprint: 6961ffc81db8b7db637890596756b0874c93fe21f70f44ee524ae9b161b4234f
  state: discovered
- id: 9e27ea45-837a-55be-abbb-f1e18ec949c9
  symbol_path: src/shared/legacy_models.py::LegacyCliCommand
  module: shared.legacy_models
  qualname: LegacyCliCommand
  kind: class
  ast_signature: TBD
  fingerprint: 69b592f71e77e12ea6ddc8e4ab53d4ca20073020ffa0bcb973be0a40313c836a
  state: discovered
- id: 2f68d8c8-9e4f-5068-83cc-429f5a1d4a7f
  symbol_path: src/cli/commands/fix.py::fix_tags_command
  module: cli.commands.fix
  qualname: fix_tags_command
  kind: function
  ast_signature: TBD
  fingerprint: 69d074923126a8ea2a44dea068f0629ab5d2bd3c47f6883471d3ff1462450d58
  state: discovered
- id: 4d1fff7a-7c72-54a1-b82e-8b0bc12a96de
  symbol_path: src/cli/logic/cli_utils.py::set_context
  module: cli.logic.cli_utils
  qualname: set_context
  kind: function
  ast_signature: TBD
  fingerprint: 6ac084b1a964584b7543f2fca5e996b1d7f9a68051f498e150c20bddfc5aa294
  state: discovered
- id: c42f091b-b638-5990-a0f3-cb5f45c29b59
  symbol_path: src/shared/utils/embedding_utils.py::Embeddable
  module: shared.utils.embedding_utils
  qualname: Embeddable
  kind: class
  ast_signature: TBD
  fingerprint: 6b66f0094f98453f6888d9c716645f11fb714e0144fd7e15d405eda8f3bd331d
  state: discovered
- id: 27552b29-93a9-5c13-ab3e-8c8e618414b7
  symbol_path: src/features/governance/runtime_validator.py::RuntimeValidatorService
  module: features.governance.runtime_validator
  qualname: RuntimeValidatorService
  kind: class
  ast_signature: TBD
  fingerprint: 6bfda9f7a9ae44ba76728c046d65f0129fc5b432832f6454547207540ee19256
  state: discovered
- id: 2b271e67-94f0-5261-9856-179c38d78093
  symbol_path: src/core/actions/base.py::ActionHandler
  module: core.actions.base
  qualname: ActionHandler
  kind: class
  ast_signature: TBD
  fingerprint: 6d740536c9dfe270aa2623f385ce961e5c8f40cc84741c8a66565e2ee1671f23
  state: discovered
- id: a02c3ad0-28ab-5871-846b-adb19da1470b
  symbol_path: src/shared/schemas/manifest_validator.py::load_schema
  module: shared.schemas.manifest_validator
  qualname: load_schema
  kind: function
  ast_signature: TBD
  fingerprint: 6e3b8465838bb871ec1cd9f626c92298765c571cba61170572d50151c662f9be
  state: discovered
- id: 77b64250-8eed-59dc-b5f4-f71d6fa0414b
  symbol_path: src/features/governance/constitutional_auditor.py::AuditScope
  module: features.governance.constitutional_auditor
  qualname: AuditScope
  kind: class
  ast_signature: TBD
  fingerprint: 6f1a0c10dac15592843659f382a3fec2e2e8ab66405c34654628f4beecf834a8
  state: discovered
- id: a3863369-1b69-5692-9297-7166ea5ca2ff
  symbol_path: src/shared/models/drift_models.py::DriftReport
  module: shared.models.drift_models
  qualname: DriftReport
  kind: class
  ast_signature: TBD
  fingerprint: 6f5697903c512f8a1b98c69f6aa2f3a5fc80a6583f57dd6c634617f9d15c0527
  state: discovered
- id: db1934bf-7884-5d10-a293-a99c8439de19
  symbol_path: src/core/validation_policies.py::PolicyValidator
  module: core.validation_policies
  qualname: PolicyValidator
  kind: class
  ast_signature: TBD
  fingerprint: 706a1c647302d5067b5d67cc01abe9fb8ccefb716e5ad686a201eb9f7aea61b8
  state: discovered
- id: 4370cbff-52fb-5b78-bbc2-c16c20341134
  symbol_path: src/cli/commands/enrich.py::register
  module: cli.commands.enrich
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 72a63ae411926014c9c5b48abedee518417cd7ec13ad84c7b040ae3d1920bd69
  state: discovered
- id: 4c0a9479-fd5d-51f3-9b7f-0071e1e9b7e6
  symbol_path: src/features/self_healing/purge_legacy_tags_service.py::purge_legacy_tags
  module: features.self_healing.purge_legacy_tags_service
  qualname: purge_legacy_tags
  kind: function
  ast_signature: TBD
  fingerprint: 731ca701982144723d973ac65b277f8bf320c9864f877a1086f4fa6e330a3f8f
  state: discovered
- id: f98b2ec9-4a1b-563e-ba6a-a88f9ff529a0
  symbol_path: src/features/self_healing/id_tagging_service.py::assign_missing_ids
  module: features.self_healing.id_tagging_service
  qualname: assign_missing_ids
  kind: function
  ast_signature: TBD
  fingerprint: 73c3ad594f278afa2c8982503f3e97f6001c9ff871de7b8bba6c70cf38f52e67
  state: discovered
- id: 58a25070-3d1f-5e82-b8f8-edc4983aa259
  symbol_path: src/features/self_healing/complexity_service.py::complexity_outliers
  module: features.self_healing.complexity_service
  qualname: complexity_outliers
  kind: function
  ast_signature: TBD
  fingerprint: 73d0e9a9613f320a4a8fb5397ce0b02df42cdb96782a7009eea4b4c5dec52ef3
  state: discovered
- id: 8fc47097-b7c1-5017-b0ce-abcc891a3a42
  symbol_path: src/features/self_healing/docstring_service.py::fix_docstrings
  module: features.self_healing.docstring_service
  qualname: fix_docstrings
  kind: function
  ast_signature: TBD
  fingerprint: 74ccb3839b6dd40b84f8cd48723c5fb0af10e1d792fe9f292301fdd86ef9d164
  state: discovered
- id: 1eb97196-1422-534d-aab1-2342d91e1abc
  symbol_path: src/cli/logic/proposals_micro.py::micro_propose
  module: cli.logic.proposals_micro
  qualname: micro_propose
  kind: function
  ast_signature: TBD
  fingerprint: 76344c8e239b9a0b12278ad12b524a3176eeaa1f03176896ef316cf01703672b
  state: discovered
- id: cd46cd49-e222-5134-88e2-8add08b55514
  symbol_path: src/cli/logic/system.py::process_crates_command
  module: cli.logic.system
  qualname: process_crates_command
  kind: function
  ast_signature: TBD
  fingerprint: 76c322ef0f3ee40a46e74674f26a227d8f847edbd30b6619a2e5b61fa4095593
  state: discovered
- id: 92af4df8-93eb-555d-8b94-cc57d6a186ed
  symbol_path: src/cli/logic/utils_migration.py::replacer
  module: cli.logic.utils_migration
  qualname: replacer
  kind: function
  ast_signature: TBD
  fingerprint: 794c6e28776bc4d0d2c411629babfd7f9a83b800ce471544ae8ee89745d2de52
  state: discovered
- id: aaa1ed0f-348d-5746-9d4e-3c152f50ce86
  symbol_path: src/services/database/models.py::Task
  module: services.database.models
  qualname: Task
  kind: class
  ast_signature: TBD
  fingerprint: 7ba7f1fe77ed1bbd61efb38762903e0418dadbdfc83ac0c8656347a86a0e3c5d
  state: discovered
- id: b80974b7-0271-5ec4-8243-8880ac4116c9
  symbol_path: src/shared/utils/embedding_utils.py::chunk_and_embed
  module: shared.utils.embedding_utils
  qualname: chunk_and_embed
  kind: function
  ast_signature: TBD
  fingerprint: 7cb3f7f81ec88604d821a0d4b44e043547cef2ac7a56856caf7417ab814e532c
  state: discovered
- id: c18bd095-2b20-58c3-8459-cfa9599af12f
  symbol_path: src/cli/interactive.py::show_project_lifecycle_menu
  module: cli.interactive
  qualname: show_project_lifecycle_menu
  kind: function
  ast_signature: TBD
  fingerprint: 7d18e66ffad3bdcff3752054044f1b83e18071f2a9e5fbf9c99f8061d326fca4
  state: discovered
- id: 13588a79-3b71-5604-92b1-31a50118c04b
  symbol_path: src/shared/legacy_models.py::LegacyResourceManifest
  module: shared.legacy_models
  qualname: LegacyResourceManifest
  kind: class
  ast_signature: TBD
  fingerprint: 7d3bc60fcc5062c7c70beca8f746e34446d1a2390dea1935f45d8f16fb283e05
  state: discovered
- id: 4eb193cb-8672-5aef-a9cb-5fac0c160837
  symbol_path: src/core/agents/deduction_agent.py::DeductionAgent
  module: core.agents.deduction_agent
  qualname: DeductionAgent
  kind: class
  ast_signature: TBD
  fingerprint: 7d751d98794873300aa5be09080663f0324f856c4d229032bafe372200e5f446
  state: discovered
- id: 43f91642-0933-56a8-8af5-85c4ef694e6e
  symbol_path: src/features/introspection/knowledge_vectorizer.py::process_vectorization_task
  module: features.introspection.knowledge_vectorizer
  qualname: process_vectorization_task
  kind: function
  ast_signature: TBD
  fingerprint: 7f559ce891466168ce6ad3aed0302d80835e4ecbc2d194573e941441019dd701
  state: discovered
- id: 72b9a3e0-3ae3-5aec-872c-1a61ae60af00
  symbol_path: src/features/introspection/drift_service.py::run_drift_analysis_async
  module: features.introspection.drift_service
  qualname: run_drift_analysis_async
  kind: function
  ast_signature: TBD
  fingerprint: 8136f8b21f520e5c8114454183776b896393503aeae9d9febe7274c24be276b6
  state: discovered
- id: c159d6f1-d856-5ada-958f-13b1c4db2e52
  symbol_path: src/cli/logic/tools.py::rewire_imports_cli
  module: cli.logic.tools
  qualname: rewire_imports_cli
  kind: function
  ast_signature: TBD
  fingerprint: 818e7f3c9554e675793b6f2fbdd4dfa486077f5e54c14470c9ef3512ee454b11
  state: discovered
- id: 1835434f-576c-5883-8423-f3c415688d2e
  symbol_path: src/core/knowledge_service.py::KnowledgeService
  module: core.knowledge_service
  qualname: KnowledgeService
  kind: class
  ast_signature: TBD
  fingerprint: 81e14dcea71fe8072ba1ccad8ac3f94b92a3accc7e87cf9b05c87e84c06cd9ab
  state: discovered
- id: ec601e54-ce58-5200-85c3-0b84bba92853
  symbol_path: src/shared/utils/embedding_utils.py::normalize_text
  module: shared.utils.embedding_utils
  qualname: normalize_text
  kind: function
  ast_signature: TBD
  fingerprint: 8395e86255e6a722ab20ed36bbafb8b94562549d306d9f1c770ef452670773d6
  state: discovered
- id: abe74282-6036-52e8-8566-9757ae350da4
  symbol_path: src/shared/utils/alias_resolver.py::AliasResolver
  module: shared.utils.alias_resolver
  qualname: AliasResolver
  kind: class
  ast_signature: TBD
  fingerprint: 83e47d4c10c8181e64882e02d8e1d2f953fea2814b3ecff34fe79b8281cfa7cf
  state: discovered
- id: df7e3cac-5530-595a-94b6-75afc16c24d4
  symbol_path: src/features/self_healing/linelength_service.py::fix_line_lengths
  module: features.self_healing.linelength_service
  qualname: fix_line_lengths
  kind: function
  ast_signature: TBD
  fingerprint: 8422fc16ac59572c97ed884aac424f3b30ed17914d92f74df1a11dede88b1db7
  state: discovered
- id: 6bf97ed9-a42d-573c-9981-983f9e756fff
  symbol_path: src/features/introspection/semantic_clusterer.py::run_clustering
  module: features.introspection.semantic_clusterer
  qualname: run_clustering
  kind: function
  ast_signature: TBD
  fingerprint: 842f858ccc96e25b01cccdb8b7239ffdbf9ef5d637357d694704672f6964eb7f
  state: discovered
- id: e77e8ffc-9dd0-5cf1-8b9f-4044358fbed1
  symbol_path: src/cli/logic/diagnostics.py::unassigned_symbols
  module: cli.logic.diagnostics
  qualname: unassigned_symbols
  kind: function
  ast_signature: TBD
  fingerprint: 848040af1c6c1ea676002d080b1259dfbb3156df5cdd901543d0731145fa35ff
  state: discovered
- id: 17139118-e075-57c4-9359-a82274f8a9c5
  symbol_path: src/features/governance/checks/duplication_check.py::DuplicationCheck
  module: features.governance.checks.duplication_check
  qualname: DuplicationCheck
  kind: class
  ast_signature: TBD
  fingerprint: 84a466355342dcf423663241eb8c4ba2a46eb24609f7393cc275b23be0cf3071
  state: discovered
- id: 1fb42b63-1e36-54df-a8e3-a9ccdf33a491
  symbol_path: src/cli/logic/knowledge_sync/import_.py::run_import
  module: cli.logic.knowledge_sync.import_
  qualname: run_import
  kind: function
  ast_signature: TBD
  fingerprint: 85ed491d3716d1e496d2afe77a186ab8eba384eea919f9c9a74af8f899dc2446
  state: discovered
- id: ea570cbe-b8dd-55fb-afff-1d1f628213b6
  symbol_path: src/core/validation_quality.py::QualityChecker
  module: core.validation_quality
  qualname: QualityChecker
  kind: class
  ast_signature: TBD
  fingerprint: 861806124e18cf98cd000444ad5b3cabf21c1b541f226671f22b9374d47f9fce
  state: discovered
- id: de3554fd-2c19-54a6-8736-a87819e4c9b0
  symbol_path: src/cli/logic/duplicates.py::inspect_duplicates
  module: cli.logic.duplicates
  qualname: inspect_duplicates
  kind: function
  ast_signature: TBD
  fingerprint: 86363fd1caa224529a4f64e125c32a24403cef8141858ee3777349b42fea0e0d
  state: discovered
- id: d4739ec5-fb4a-5dbf-9781-f11c4cec0c72
  symbol_path: src/shared/utils/manifest_aggregator.py::aggregate_manifests
  module: shared.utils.manifest_aggregator
  qualname: aggregate_manifests
  kind: function
  ast_signature: TBD
  fingerprint: 865ebb04290f206e2717d16c7c6b0deb4c66400dc59aa98fbe726cb73899a544
  state: discovered
- id: f57804f7-a3ba-5431-bba0-dab645dfb263
  symbol_path: src/core/agents/planner_agent.py::PlannerAgent
  module: core.agents.planner_agent
  qualname: PlannerAgent
  kind: class
  ast_signature: TBD
  fingerprint: 86a5809bb67e651e47251ff487b27b40671590917b1c1aba6ffea638ea1422ed
  state: discovered
- id: a23a8c43-79e3-59eb-ab0f-8fb5182ef6c0
  symbol_path: src/cli/commands/mind.py::register
  module: cli.commands.mind
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 87a046f128fd75e921fc3d9c3acf314a23e182d3d9c290cc06d9b54725526dae
  state: discovered
- id: 4e058b79-dbf0-53e2-a338-9e394898eac0
  symbol_path: src/cli/commands/mind.py::verify_command
  module: cli.commands.mind
  qualname: verify_command
  kind: function
  ast_signature: TBD
  fingerprint: 87b37f809df227f3b67b219198ed7f9be2e3d8e254e7682fbd5c71290582ef3c
  state: discovered
- id: d4996b06-3ab0-55e1-9049-79adce3a035e
  symbol_path: src/shared/ast_utility.py::find_definition_line
  module: shared.ast_utility
  qualname: find_definition_line
  kind: function
  ast_signature: TBD
  fingerprint: 87d273b605aad3307a83213760216564efb2d24e1ac8823f0d196241e7cf6377
  state: discovered
- id: 41182c55-105e-57ae-a25c-089e9e4e4128
  symbol_path: src/features/governance/checks/orphaned_logic.py::OrphanedLogicCheck
  module: features.governance.checks.orphaned_logic
  qualname: OrphanedLogicCheck
  kind: class
  ast_signature: TBD
  fingerprint: 894b092b45302e6120cce7f8804ac0452c07242d281f8bc5420fd6c782f90cec
  state: discovered
- id: fc32a116-8fd4-53c9-b4ce-0ab3079c5294
  symbol_path: src/features/introspection/discovery/from_kgb.py::collect_from_kgb
  module: features.introspection.discovery.from_kgb
  qualname: collect_from_kgb
  kind: function
  ast_signature: TBD
  fingerprint: 8a444886463d6a4ac44ce6534a870e51450a352b150b236409437e72993ae8f7
  state: discovered
- id: d272780b-5a62-5d1e-bb0d-5736a5773c4f
  symbol_path: src/cli/logic/diagnostics.py::inspect_vector_drift
  module: cli.logic.diagnostics
  qualname: inspect_vector_drift
  kind: function
  ast_signature: TBD
  fingerprint: 8b68d7bf8b2905f24485df8b10ff67723632e1fd8814af6b086f832a85bd59c9
  state: discovered
- id: 84fc6521-1a52-50f9-bd43-fc50183f682f
  symbol_path: src/cli/commands/enrich.py::enrich_symbols_command
  module: cli.commands.enrich
  qualname: enrich_symbols_command
  kind: function
  ast_signature: TBD
  fingerprint: 8c5f8392a9a50c8ca0169ed596562e92459e7a43c560cd4dc09a55a0c7a5cbea
  state: discovered
- id: 92ed0eda-6a74-5940-8413-80cc5b339730
  symbol_path: src/core/actions/file_actions.py::ListFilesHandler
  module: core.actions.file_actions
  qualname: ListFilesHandler
  kind: class
  ast_signature: TBD
  fingerprint: 8d096d8546e09c1c708d793f6140b427c6257f35736b2b8d327769785c8602a4
  state: discovered
- id: 424fbdd6-953e-5224-bb93-2b7a7b1aafbe
  symbol_path: src/features/maintenance/maintenance_service.py::rewire_imports
  module: features.maintenance.maintenance_service
  qualname: rewire_imports
  kind: function
  ast_signature: TBD
  fingerprint: 8d2321ce02193cea3107d85d1f3f4b786a1fd62524cfb6e06499eecb86b5cdf1
  state: discovered
- id: a1d3521c-d7e6-5113-ac54-ef6c449db03b
  symbol_path: src/services/repositories/db/common.py::record_applied
  module: services.repositories.db.common
  qualname: record_applied
  kind: function
  ast_signature: TBD
  fingerprint: 8eaecea335d19cf70bdb5d30b6f0dfb4779ce152d65eb6fc55cc0b10bf055c6c
  state: discovered
- id: f4f2c106-6de1-57a7-8799-bc7b447de680
  symbol_path: src/cli/logic/cli_utils.py::load_private_key
  module: cli.logic.cli_utils
  qualname: load_private_key
  kind: function
  ast_signature: TBD
  fingerprint: 8f094fdd6ffdf4cfe3c34f192c4fbf8c95663c6ea99d66beac86ac3a992158fe
  state: discovered
- id: 35c454d3-80a0-5ff5-9fb4-ff7cc93a2ca6
  symbol_path: src/services/repositories/db/common.py::get_applied
  module: services.repositories.db.common
  qualname: get_applied
  kind: function
  ast_signature: TBD
  fingerprint: 8f48080919e7f95b644a09e26a433ba9b7fd6b1c62d87813721914384f78441d
  state: discovered
- id: 465cf630-eaa9-5003-816f-f99b06a6da09
  symbol_path: src/features/project_lifecycle/definition_service.py::define_single_symbol
  module: features.project_lifecycle.definition_service
  qualname: define_single_symbol
  kind: function
  ast_signature: TBD
  fingerprint: 8f4a79cf875de45ecbe6674a2da4df12c0e2fbaf2edb035c0802f202c6fa84ec
  state: discovered
- id: db2445c5-dcf3-5333-af13-123d2900f8f8
  symbol_path: src/cli/commands/fix.py::fix_headers_cmd
  module: cli.commands.fix
  qualname: fix_headers_cmd
  kind: function
  ast_signature: TBD
  fingerprint: 90943e7d882601173104b296303c3ce4219699c9807008d27afb640b7d8a5377
  state: discovered
- id: 5c470364-e7a3-584d-ae12-678d8e19ebc3
  symbol_path: src/features/project_lifecycle/integration_service.py::check_integration_health
  module: features.project_lifecycle.integration_service
  qualname: check_integration_health
  kind: function
  ast_signature: TBD
  fingerprint: 90990efb8d37961b8be574419def2e071d3d022a1f8161845b1cad585ed04b46
  state: discovered
- id: b747bd7d-f271-50b4-8b51-2c187ae17658
  symbol_path: src/cli/logic/knowledge_sync/utils.py::write_yaml
  module: cli.logic.knowledge_sync.utils
  qualname: write_yaml
  kind: function
  ast_signature: TBD
  fingerprint: 915326c0141c83d4fe102e46ddae48f49813cce0001f0ea091b3c86d5595bf2f
  state: discovered
- id: 6c460a8b-0aea-5251-8168-98b3b93a763b
  symbol_path: src/cli/logic/guard.py::register
  module: cli.logic.guard
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 91b43af19fa5fb2b3e92cccc1cbaa9afb00220afcc28811ff75e5ebbcb266e54
  state: discovered
- id: 73c8b379-bd74-5945-b15a-091be1724adf
  symbol_path: src/features/maintenance/migration_service.py::run_ssot_migration
  module: features.maintenance.migration_service
  qualname: run_ssot_migration
  kind: function
  ast_signature: TBD
  fingerprint: 91c049ca8e0644c29c398a6fce284fc3dd1c207193f1117ec1305e1babc1bafc
  state: discovered
- id: 3fc924e7-b8e5-5118-911a-161184ea324b
  symbol_path: src/core/intent_guard.py::ViolationReport
  module: core.intent_guard
  qualname: ViolationReport
  kind: class
  ast_signature: TBD
  fingerprint: 91f7bd1702b38541fa8a16d6948d4acda120748f1f89ac6f963b32b011e0a5a2
  state: discovered
- id: 1ff21177-6da4-57c3-b071-2c5f2df7946f
  symbol_path: src/cli/logic/hub.py::hub_whereis
  module: cli.logic.hub
  qualname: hub_whereis
  kind: function
  ast_signature: TBD
  fingerprint: 92114415126745cc76bd89d1b7d2e53812f05df6217234223c01454e3fa46131
  state: discovered
- id: cd3e389a-03db-5e5f-85a3-1613569be8d7
  symbol_path: src/cli/logic/db.py::export_data
  module: cli.logic.db
  qualname: export_data
  kind: function
  ast_signature: TBD
  fingerprint: 925670c4c568fc6d48339c282d7eaf4ba121cb7d36042e65be00dbf029284654
  state: discovered
- id: f355821c-35f1-593c-899a-9bf8c66ee49f
  symbol_path: src/services/database/models.py::RuntimeService
  module: services.database.models
  qualname: RuntimeService
  kind: class
  ast_signature: TBD
  fingerprint: 927ad645ea70356d3d1b0c73bfc54748168dd57715a1b96cf8fcaefc16798403
  state: discovered
- id: ecf41258-78c2-5f0a-a7b2-d3ad926699ca
  symbol_path: src/shared/ast_utility.py::SymbolIdResult
  module: shared.ast_utility
  qualname: SymbolIdResult
  kind: class
  ast_signature: TBD
  fingerprint: 9331611f9a09084a25694cd8d0df1f62ad76defde6599a6b532643ea0f4b9b4a
  state: discovered
- id: f4829192-d1e2-5530-a5e8-782d05cc8fbd
  symbol_path: src/core/agents/base_planner.py::parse_and_validate_plan
  module: core.agents.base_planner
  qualname: parse_and_validate_plan
  kind: function
  ast_signature: TBD
  fingerprint: 93b1117f6d5e7e61fc08df24cd1b0dcf42b4f493ee447d75e88ecf8a502e7ce8
  state: discovered
- id: 1b821d0f-a8f7-532b-a056-5240b3312b56
  symbol_path: src/features/demo/hello_world.py::print_greeting
  module: features.demo.hello_world
  qualname: print_greeting
  kind: function
  ast_signature: TBD
  fingerprint: 949b50c2d6bda1c6d1481c8d133ee6227501be63daa19a85ff487024f6240470
  state: discovered
- id: 21f43c6f-0565-5149-9ee7-cc8b54fb8630
  symbol_path: src/cli/logic/knowledge_sync/snapshot.py::run_snapshot
  module: cli.logic.knowledge_sync.snapshot
  qualname: run_snapshot
  kind: function
  ast_signature: TBD
  fingerprint: 94f4d30ddabd38e756d4822e5bd12c89cc10d1febaeae2717f0ced10f9cae846
  state: discovered
- id: bf35d53a-bcb7-5681-8c34-5aabb0e98714
  symbol_path: src/services/llm/resource_selector.py::ResourceSelector
  module: services.llm.resource_selector
  qualname: ResourceSelector
  kind: class
  ast_signature: TBD
  fingerprint: 979848b9436d5a79869360ac1358f87cbf0f61addb9c4461f66b42bc9aa38b30
  state: discovered
- id: c2de50ff-6ca9-583e-bbb3-0898000d8dd7
  symbol_path: src/services/database/session_manager.py::get_session
  module: services.database.session_manager
  qualname: get_session
  kind: function
  ast_signature: TBD
  fingerprint: 97fe5841737f79af05dd9a2b34691095a2e97ec9a021e2861d89c9b170b0de4d
  state: discovered
- id: 68e439c5-bb5e-56b5-860c-090f83058847
  symbol_path: src/cli/commands/manage.py::register
  module: cli.commands.manage
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 981c847c48134e3cf8f02881aab6b8d9de2812f871f07491aeea1375d1c8c006
  state: discovered
- id: 58fc6e5b-c2a0-52b5-9725-a1e87519d07b
  symbol_path: src/core/crate_processing_service.py::Crate
  module: core.crate_processing_service
  qualname: Crate
  kind: class
  ast_signature: TBD
  fingerprint: 983abd83969f505bbd250087ea45fea5998da9a6bea38f9293600c331532ef9e
  state: discovered
- id: efb927dd-74e5-5007-a671-abd40c2b82b5
  symbol_path: src/shared/models/execution_models.py::PlanExecutionError
  module: shared.models.execution_models
  qualname: PlanExecutionError
  kind: class
  ast_signature: TBD
  fingerprint: 9963ce2048819a48b579232bd8f5cc9d5724b3f92e673c60267c5e1683ad107e
  state: discovered
- id: 1b06ee1f-52e6-554f-a4cd-fabb7907dfda
  symbol_path: src/features/self_healing/code_style_service.py::format_code
  module: features.self_healing.code_style_service
  qualname: format_code
  kind: function
  ast_signature: TBD
  fingerprint: 99fdc50909ba18a02b0e164e16980bca5cf909842fdaf07ecfe8e0b165c420ef
  state: discovered
- id: 7c8d286a-2502-5acb-82ad-942868bc8408
  symbol_path: src/shared/config_loader.py::load_yaml_file
  module: shared.config_loader
  qualname: load_yaml_file
  kind: function
  ast_signature: TBD
  fingerprint: 9a9181d793441fd3a5ef28ba1478e7190003c337dfc42bd31c4591734ef39769
  state: discovered
- id: 71afc287-71ef-5a8d-a8c0-c28918d5b16c
  symbol_path: src/cli/logic/byor.py::register
  module: cli.logic.byor
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: 9b62691ef90f3390c625b2239aa9e80250dc156e4deba0ed1740245705a1dbf6
  state: discovered
- id: 8143391a-d9a7-5015-9c47-63647ca9eaee
  symbol_path: src/cli/logic/cli_utils.py::save_yaml_file
  module: cli.logic.cli_utils
  qualname: save_yaml_file
  kind: function
  ast_signature: TBD
  fingerprint: 9c7b11b6961eeeb84df8234519f9f967e905c743dc674b1cd970457751b43456
  state: discovered
- id: e390ea42-3d4f-594f-82f8-f6f32a226568
  symbol_path: src/main.py::health_check
  module: main
  qualname: health_check
  kind: function
  ast_signature: TBD
  fingerprint: 9cf01401f62b0a5bfb8e6345aaa7b034ad6e35cb90b3bfef531f650bbceb7511
  state: discovered
- id: 33ea44fd-83c2-57f5-ac52-29eebe8241ab
  symbol_path: src/core/actions/code_actions.py::EditFileHandler
  module: core.actions.code_actions
  qualname: EditFileHandler
  kind: class
  ast_signature: TBD
  fingerprint: 9d8f25cf17e94788d61c0e10b3d325869ac55c15c69a722efe16aa3165890b05
  state: discovered
- id: b207d920-9a07-5fc7-81a5-9f558e021129
  symbol_path: src/features/introspection/generate_correction_map.py::generate_maps
  module: features.introspection.generate_correction_map
  qualname: generate_maps
  kind: function
  ast_signature: TBD
  fingerprint: 9e666ec60641f8cceef61d957e45d882e67a18d5adf856faa4c3462d2e96d039
  state: discovered
- id: eaf44220-e638-5fd5-a2bc-f4f4d2f00c72
  symbol_path: src/services/config_service.py::ConfigurationService
  module: services.config_service
  qualname: ConfigurationService
  kind: class
  ast_signature: TBD
  fingerprint: 9ec22ff55e5b37a5de9d2b2ae6a9dc74a92cbee19017ae03f7a0991f77be04f4
  state: discovered
- id: b4e7f828-7f6b-53a4-ad09-3bcd8d02d75b
  symbol_path: src/features/introspection/vectorization_service.py::run_vectorize
  module: features.introspection.vectorization_service
  qualname: run_vectorize
  kind: function
  ast_signature: TBD
  fingerprint: 9eccf7d19fe17afd5c68bc95ff696e2d56198dbc9737d220c2bd2e13ff7dc463
  state: discovered
- id: 696c246c-0e68-50d9-8f3d-d68a6e708240
  symbol_path: src/services/database/models.py::Capability
  module: services.database.models
  qualname: Capability
  kind: class
  ast_signature: TBD
  fingerprint: 9eef317d59a12a7393422441a6985836cbc8e024f5c198f2639f64ff3abed958
  state: discovered
- id: e2350cf3-a1ea-58cb-b3d0-83ce734b6f6f
  symbol_path: src/cli/commands/fix.py::fix_duplicate_ids_command
  module: cli.commands.fix
  qualname: fix_duplicate_ids_command
  kind: function
  ast_signature: TBD
  fingerprint: 9ef5c8b80d496941619e73182689a1241ce4676b342faa654a0f96ba07ae195a
  state: discovered
- id: 02201732-0987-5b84-829a-2038e2cf67f5
  symbol_path: src/core/agents/execution_agent.py::ExecutionAgent
  module: core.agents.execution_agent
  qualname: ExecutionAgent
  kind: class
  ast_signature: TBD
  fingerprint: 9f03399a70932531c81e7c1a257ef869a6736b058aae233eeee20f91c3d0cabe
  state: discovered
- id: ea8b169f-a730-59f6-b735-5b93ae932a00
  symbol_path: src/shared/config.py::get_path_or_none
  module: shared.config
  qualname: get_path_or_none
  kind: function
  ast_signature: TBD
  fingerprint: 9fc54231c095e5f5a49f865b931c879c0cc7e70b349ccb113d8095ac82df8973
  state: discovered
- id: f2c57cad-71dd-5eba-bea5-0d4c21c2f1e9
  symbol_path: src/features/governance/checks/id_uniqueness_check.py::IdUniquenessCheck
  module: features.governance.checks.id_uniqueness_check
  qualname: IdUniquenessCheck
  kind: class
  ast_signature: TBD
  fingerprint: a5b19a95da8cfd6432b0b6e3fa2534aa0219b6ca7670c7c10c092a506d355ef7
  state: discovered
- id: 9299a5b8-3f3f-551a-ae70-b33588a8cb67
  symbol_path: src/cli/commands/manage.py::migrate_ssot_command
  module: cli.commands.manage
  qualname: migrate_ssot_command
  kind: function
  ast_signature: TBD
  fingerprint: a6f44580d089d42dae952114f2881177e4f367c2fe8ea8dfa9e47d3841bc302b
  state: discovered
- id: 2cecefd5-6009-562a-9703-c25b73659986
  symbol_path: src/features/self_healing/prune_orphaned_vectors.py::main_sync
  module: features.self_healing.prune_orphaned_vectors
  qualname: main_sync
  kind: function
  ast_signature: TBD
  fingerprint: a719d93825df1c76852a7c2044456f3e586662407bcd692162616a563125b76f
  state: discovered
- id: fd5482e3-3662-5803-92e3-e44a1b3c2f16
  symbol_path: src/features/introspection/capability_discovery_service.py::CapabilityRegistry
  module: features.introspection.capability_discovery_service
  qualname: CapabilityRegistry
  kind: class
  ast_signature: TBD
  fingerprint: a7536ab5a5920b0d22cb751e269daa38dff1a30bab91eb81660e0e5fa0db13a4
  state: discovered
- id: c2111920-a102-52e0-b8f5-1278411d4bae
  symbol_path: src/cli/logic/capability.py::capability_new_deprecated
  module: cli.logic.capability
  qualname: capability_new_deprecated
  kind: function
  ast_signature: TBD
  fingerprint: a7f3102f84f20cc8649e8c31a30cdece4dd9bd3a68ddb8539909931b378e5f17
  state: discovered
- id: 8add0ecb-1c73-5819-9e8b-170fd36bdfee
  symbol_path: src/core/actions/code_actions.py::EditFunctionHandler
  module: core.actions.code_actions
  qualname: EditFunctionHandler
  kind: class
  ast_signature: TBD
  fingerprint: a830a6d0fea5ff3d04ec5716e3556e796a347c9ac1db9e26302c357438eac9a8
  state: discovered
- id: ae157985-a235-5928-a171-016d9197fdf1
  symbol_path: src/core/agents/intent_translator.py::IntentTranslator
  module: core.agents.intent_translator
  qualname: IntentTranslator
  kind: class
  ast_signature: TBD
  fingerprint: a8499d072a77087d167d7650c51d62feba7f836dbbaf531846339bc22103dad1
  state: discovered
- id: af2f8df8-ae07-5fb8-9d5d-4c5a2a273700
  symbol_path: src/services/repositories/db/common.py::apply_sql_file
  module: services.repositories.db.common
  qualname: apply_sql_file
  kind: function
  ast_signature: TBD
  fingerprint: a9df8ffd8245e1bd883c1dd177525cb4322fe2159fed46d949da343180146406
  state: discovered
- id: a4baa3b4-bba8-5aa1-96ad-3af8e95d381f
  symbol_path: src/cli/commands/manage.py::dotenv_sync_command
  module: cli.commands.manage
  qualname: dotenv_sync_command
  kind: function
  ast_signature: TBD
  fingerprint: aa66ccd2d8ca3adc634b01c71f141dc2c7066ef362e97ab3b95c222162e2f735
  state: discovered
- id: 85bb77de-fddc-5c8a-ae21-5366434cb074
  symbol_path: src/features/introspection/discovery/from_source_scan.py::collect_from_source_scan
  module: features.introspection.discovery.from_source_scan
  qualname: collect_from_source_scan
  kind: function
  ast_signature: TBD
  fingerprint: aaade72e4db3b8ff8ec5c8f42578ec35d99b2e357ac5d1a1bf1e9750e32d4d5c
  state: discovered
- id: 26dd6b11-1eb1-5565-bf9e-961580bba52c
  symbol_path: src/features/project_lifecycle/definition_service.py::define_new_symbols
  module: features.project_lifecycle.definition_service
  qualname: define_new_symbols
  kind: function
  ast_signature: TBD
  fingerprint: aac5eb392f3dff3718d68702e60991b6c2ba7717fef982e8396bc77e9471912e
  state: discovered
- id: 83f19dc6-0529-55f2-97e7-76511b3eb8f3
  symbol_path: src/shared/models/audit_models.py::AuditSeverity
  module: shared.models.audit_models
  qualname: AuditSeverity
  kind: class
  ast_signature: TBD
  fingerprint: add250b38e6488d73937f53730819d26bf32e3d01e1fbdd44c1ef6714730299a
  state: discovered
- id: a2fe275a-fdb1-590a-9158-5634ef0cc930
  symbol_path: src/core/validation_pipeline.py::validate_code_async
  module: core.validation_pipeline
  qualname: validate_code_async
  kind: function
  ast_signature: TBD
  fingerprint: ae428e65da92ceb2781e87ee0b1a29a0e319a7d6ddaf2aa3b4109be805c920d9
  state: discovered
- id: ed73975f-620d-5ad4-866d-ad7b7f517982
  symbol_path: src/features/self_healing/clarity_service.py::fix_clarity
  module: features.self_healing.clarity_service
  qualname: fix_clarity
  kind: function
  ast_signature: TBD
  fingerprint: aea91ac817d05d04bfa0ddfd8515e3ec8c21433705a1e71e49d7023e029e149a
  state: discovered
- id: 9740663f-e0c1-5aa2-9ba6-4bf58cca2355
  symbol_path: src/cli/logic/report.py::report
  module: cli.logic.report
  qualname: report
  kind: function
  ast_signature: TBD
  fingerprint: b0ffe774d9d91009fe32f2f15c73757e17005b5c87569865bf0fabbadf52f33c
  state: discovered
- id: 20665dbf-149a-58e7-83b6-f1521167013e
  symbol_path: src/cli/logic/audit.py::test_system
  module: cli.logic.audit
  qualname: test_system
  kind: function
  ast_signature: TBD
  fingerprint: b2688ae0503435c928905b040fa119fad2d83f0f1cf17d4bbd7be0eadc6f19bd
  state: discovered
- id: a85ce098-664e-521a-8d0e-307f4e7a5b77
  symbol_path: src/core/actions/file_actions.py::ReadFileHandler
  module: core.actions.file_actions
  qualname: ReadFileHandler
  kind: class
  ast_signature: TBD
  fingerprint: b2722d177eb315898aab8e1e798075b4f347b2f243f7e2b28fe33f0e4ebd1f18
  state: discovered
- id: 3839918c-7b8d-5df6-a441-9e5d6338d094
  symbol_path: src/core/actions/healing_actions.py::FixHeadersHandler
  module: core.actions.healing_actions
  qualname: FixHeadersHandler
  kind: class
  ast_signature: TBD
  fingerprint: b2b2b92c943047815dbd308b5b41abd1039c1117b8b1e8b0082ad350180f3588
  state: discovered
- id: 9b1ed1e1-1877-5b6e-8267-d7374b0a3109
  symbol_path: src/services/adapters/embedding_provider.py::EmbeddingService
  module: services.adapters.embedding_provider
  qualname: EmbeddingService
  kind: class
  ast_signature: TBD
  fingerprint: b3aace5069f8e52e93bb4916eca6cce0b22116f0e5f6d7c302ff14a329368293
  state: discovered
- id: 3542226c-3e2f-5657-8346-c45b7b4bf6c7
  symbol_path: src/shared/utils/parsing.py::extract_json_from_response
  module: shared.utils.parsing
  qualname: extract_json_from_response
  kind: function
  ast_signature: TBD
  fingerprint: b3b6b6e71139bb9225872f3e4a33ed5d8e12fe502da612019b269e1cb625260d
  state: discovered
- id: 4c72a52b-120c-5c8e-a878-ae53e8e828bc
  symbol_path: src/core/errors.py::register_exception_handlers
  module: core.errors
  qualname: register_exception_handlers
  kind: function
  ast_signature: TBD
  fingerprint: b4391706ebcc3340825a8843452e9937dff660eae23258170b9a39025af234cd
  state: discovered
- id: 1cfbc37e-95ea-5a0d-83dc-a801a2b5abfc
  symbol_path: src/shared/models/execution_models.py::PlannerConfig
  module: shared.models.execution_models
  qualname: PlannerConfig
  kind: class
  ast_signature: TBD
  fingerprint: b48436662e7413617d133c3027aa24b3cd0e09e3537ba78ef4bca892fefcf5d4
  state: discovered
- id: 5f6c42aa-e87d-5178-830f-f7deb0aa6060
  symbol_path: src/cli/logic/capability.py::register
  module: cli.logic.capability
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: b49a998df1640999b19746c7a732b0090e4f61ae67bc7b015eed63b3ae132fe3
  state: discovered
- id: 96dbc874-7850-5c65-aa5d-086b2b68dd8f
  symbol_path: src/features/governance/checks/manifest_lint.py::ManifestLintCheck
  module: features.governance.checks.manifest_lint
  qualname: ManifestLintCheck
  kind: class
  ast_signature: TBD
  fingerprint: b59d526a8bfd78c0fc8ace862274de83f58c07222cc4df456fb95120c7e95bfe
  state: discovered
- id: 4f69dc78-7438-562c-b7b4-1482e812f8ae
  symbol_path: src/cli/interactive.py::show_development_menu
  module: cli.interactive
  qualname: show_development_menu
  kind: function
  ast_signature: TBD
  fingerprint: b5a5ec29e79f0114327ad182f5aaabc9e83cfedeea9d808abf63ea76117c91a2
  state: discovered
- id: 6d5e4426-9de0-5399-a406-d1ef4df526fa
  symbol_path: src/cli/logic/check.py::register
  module: cli.logic.check
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: b603d6250ef96645f57a0d895322b91734da5faad836ee57b58b3a06d5969719
  state: discovered
- id: dfb1bf9a-2990-5cfd-9085-a95dcedbc8d7
  symbol_path: src/features/self_healing/prune_private_capabilities.py::main
  module: features.self_healing.prune_private_capabilities
  qualname: main
  kind: function
  ast_signature: TBD
  fingerprint: b61947225b9a5753d05ec362934a87ed48f7c74bca99a7d2ac92ec1773b5a30d
  state: discovered
- id: b53ccf65-d851-5629-9bb3-9c8d2339ff46
  symbol_path: src/features/autonomy/micro_proposal_executor.py::MicroProposal
  module: features.autonomy.micro_proposal_executor
  qualname: MicroProposal
  kind: class
  ast_signature: TBD
  fingerprint: b662de02e44cf9688ad22eaa4a447715e6008a4712b387e2271f54273763b68e
  state: discovered
- id: ef25a8e4-e5ca-58d8-b093-f6d234c5ea8e
  symbol_path: src/services/llm/providers/ollama.py::OllamaProvider
  module: services.llm.providers.ollama
  qualname: OllamaProvider
  kind: class
  ast_signature: TBD
  fingerprint: b68e7742d394a5f4d19edf74221c32bd92d25fe810c5bff4731e878a7bc9b172
  state: discovered
- id: 00da4ed3-23a3-5c99-a803-bfe6de2a9312
  symbol_path: src/cli/commands/submit.py::register
  module: cli.commands.submit
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: b6e813a364e4ee39d4a727a689ea3f7718426301c8ff725dfaa9863b545339c6
  state: discovered
- id: 3e924bce-cf53-5913-8753-1b89256e7497
  symbol_path: src/cli/logic/validate.py::register
  module: cli.logic.validate
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: b8d49aaa10d7b75da9247d40815d67f3410ded768d383ae74487d749a30e6b08
  state: discovered
- id: a7684dcb-4943-5c99-8a6e-4d15a26c1407
  symbol_path: src/cli/logic/knowledge_sync/diff.py::run_diff
  module: cli.logic.knowledge_sync.diff
  qualname: run_diff
  kind: function
  ast_signature: TBD
  fingerprint: b9e5eaf29c3eeb8fec987d3b8293e2d05334e884f13d3031987cbaf2005bb0a3
  state: discovered
- id: 503dc3a3-9206-509e-bf9e-40ec80227af9
  symbol_path: src/services/database/models.py::Domain
  module: services.database.models
  qualname: Domain
  kind: class
  ast_signature: TBD
  fingerprint: bad0cda356413678f9ccc219490c438519667cee40ba6057b2fb7825ec272fe4
  state: discovered
- id: 83e0859d-2130-5b5b-825a-222f82806aa3
  symbol_path: src/cli/logic/audit.py::audit
  module: cli.logic.audit
  qualname: audit
  kind: function
  ast_signature: TBD
  fingerprint: bb16333e69b8bfc78884ddee451f71848fb736e262ea0932e805633f1269b971
  state: discovered
- id: a8b8cf1e-1ebf-5fa3-a681-acb61a654639
  symbol_path: src/core/intent_guard.py::IntentGuard
  module: core.intent_guard
  qualname: IntentGuard
  kind: class
  ast_signature: TBD
  fingerprint: bbb9b77f13092c13295a48be52575bfa9c46b419bf210a925125d09c0c3bb71a
  state: discovered
- id: f2ade2e5-5b27-513d-ae87-90aa3d3687a6
  symbol_path: src/cli/logic/agent.py::agent_scaffold
  module: cli.logic.agent
  qualname: agent_scaffold
  kind: function
  ast_signature: TBD
  fingerprint: bce03927d8dcfaefa047c5bdff9b9945637940ce143d9de78378ada35332077a
  state: discovered
- id: 86811fb0-4a22-5971-9cb4-c443463e99f2
  symbol_path: src/features/introspection/knowledge_vectorizer.py::sync_existing_vector_ids
  module: features.introspection.knowledge_vectorizer
  qualname: sync_existing_vector_ids
  kind: function
  ast_signature: TBD
  fingerprint: bdc23dcbec4871e0b91afe1e87eda10196190fa60fd7b173f8e40ba7a5be9274
  state: discovered
- id: cc40623a-d31b-51a6-9ab7-69bec9a4a6f5
  symbol_path: src/core/agents/micro_planner.py::MicroPlannerAgent
  module: core.agents.micro_planner
  qualname: MicroPlannerAgent
  kind: class
  ast_signature: TBD
  fingerprint: be0c976306342d12bb543b90a8983a893406051310b40f777db7126f8a5c07f7
  state: discovered
- id: d15d30b8-3c88-5072-88fa-5653b6ca35f9
  symbol_path: src/cli/logic/embeddings_cli.py::vectorize_cmd
  module: cli.logic.embeddings_cli
  qualname: vectorize_cmd
  kind: function
  ast_signature: TBD
  fingerprint: bec105a829d49574954db45bf2c767ece9740b9a4ab65af5fd193f1737edc2df
  state: discovered
- id: c99462cb-cb92-537a-8b1b-0308526eea3a
  symbol_path: src/features/project_lifecycle/bootstrap_service.py::bootstrap_issues
  module: features.project_lifecycle.bootstrap_service
  qualname: bootstrap_issues
  kind: function
  ast_signature: TBD
  fingerprint: bf9bd6b9ea3486bad01496d36632bb612ac64aa02cdefa731da22a56dd0ca4fc
  state: discovered
- id: 5ff9fa32-057c-525d-9df1-6fccca8400a0
  symbol_path: src/shared/config.py::Settings
  module: shared.config
  qualname: Settings
  kind: class
  ast_signature: TBD
  fingerprint: c1674fe64eb3a3830f807279cb7e658bcbaa12955f1068a8070685f462fe165e
  state: discovered
- id: 2b1edf1a-d7b0-5a81-80fd-3cc49a58a20c
  symbol_path: src/features/governance/key_management_service.py::register
  module: features.governance.key_management_service
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: c1b966549ac447d15cfebb0e9af241007d72a1fb8cbe7aa2b65620f18b4a0660
  state: discovered
- id: e48e9bdd-7eff-512f-a329-8a698226f5bb
  symbol_path: src/cli/commands/mind.py::import_command
  module: cli.commands.mind
  qualname: import_command
  kind: function
  ast_signature: TBD
  fingerprint: c240068a128978fa92ca5c5afb455874d91e7d5a212f509cc4992938fb846623
  state: discovered
- id: ba25dfa2-3a49-5833-a5da-7b35f6e8976e
  symbol_path: src/cli/logic/knowledge_sync/snapshot.py::fetch_symbols
  module: cli.logic.knowledge_sync.snapshot
  qualname: fetch_symbols
  kind: function
  ast_signature: TBD
  fingerprint: c27a5047acf2583f0f2554bafbccd596396b060ff2b11ca191b2e16814374d4c
  state: discovered
- id: acdd7966-c6a2-5698-a964-9572cc62a01c
  symbol_path: src/services/repositories/db/engine.py::get_session
  module: services.repositories.db.engine
  qualname: get_session
  kind: function
  ast_signature: TBD
  fingerprint: c302e78d6556c2790328c038ec23286255d284f65abc74f72d2204d48bb0fe6a
  state: discovered
- id: 8f36c436-f49d-54bd-ad90-91d82d1e93e6
  symbol_path: src/services/database/models.py::Migration
  module: services.database.models
  qualname: Migration
  kind: class
  ast_signature: TBD
  fingerprint: c34544f09994279f12f111459fd60d72fd9781f04883ad8d3fc43794c15b84f6
  state: discovered
- id: 4ce64d1c-af9b-5015-84f1-ba0c91d513c8
  symbol_path: src/api/v1/knowledge_routes.py::get_knowledge_service
  module: api.v1.knowledge_routes
  qualname: get_knowledge_service
  kind: function
  ast_signature: TBD
  fingerprint: c465040ac45c6162163ac8fd84c932fa0be14cca55ac3024c5dc33bcc6447ed1
  state: discovered
- id: a5028e44-ef2b-545e-97a6-f32943739f47
  symbol_path: src/cli/logic/chat.py::register
  module: cli.logic.chat
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: c59c0007bd871832a6a8b371b09301ebb934710020ee632839b95071f77bb333
  state: discovered
- id: 1c3c0492-79f5-598a-88c6-5bb2f7236755
  symbol_path: src/shared/ast_utility.py::find_symbol_id_and_def_line
  module: shared.ast_utility
  qualname: find_symbol_id_and_def_line
  kind: function
  ast_signature: TBD
  fingerprint: c5e390bf07a23f37c7798c0cabc3bf4017d913ccedeb3424eba36e64491e35e4
  state: discovered
- id: fdb31554-b744-56ae-b865-960a2cdf5b3d
  symbol_path: src/shared/utils/yaml_processor.py::YAMLProcessor
  module: shared.utils.yaml_processor
  qualname: YAMLProcessor
  kind: class
  ast_signature: TBD
  fingerprint: c63701e0e8d1a3d3404ed8a732a8c9cc7596b38bceca0f38c75de74f66d9c5b0
  state: discovered
- id: 56fcc1c5-9c68-5b93-9112-e2923d402641
  symbol_path: src/shared/utils/constitutional_parser.py::get_all_constitutional_paths
  module: shared.utils.constitutional_parser
  qualname: get_all_constitutional_paths
  kind: function
  ast_signature: TBD
  fingerprint: c69539967a7ee528faa71b91e931e8f24dd9a25b516c1e08aae6956910fe3a8d
  state: discovered
- id: e8d09fb8-e836-53c9-9a67-fc590d902a3c
  symbol_path: src/features/introspection/discovery/from_manifest.py::load_manifest_capabilities
  module: features.introspection.discovery.from_manifest
  qualname: load_manifest_capabilities
  kind: function
  ast_signature: TBD
  fingerprint: c975a4987b1901a09fae9e6950be9402bfda600a546cebed8be67de4e3d1e214
  state: discovered
- id: 928f01e1-6b41-558f-921c-3e95ff9b6525
  symbol_path: src/services/llm/providers/openai.py::OpenAIProvider
  module: services.llm.providers.openai
  qualname: OpenAIProvider
  kind: class
  ast_signature: TBD
  fingerprint: c9b7eee705368da23b897689b968f1860947bf790c05710f8342ea4a4d457dac
  state: discovered
- id: 36231b48-6f2b-5448-ada1-5830022ae33c
  symbol_path: src/cli/logic/diagnostics.py::manifest_hygiene
  module: cli.logic.diagnostics
  qualname: manifest_hygiene
  kind: function
  ast_signature: TBD
  fingerprint: cb9340e55a6506b68184b43d6ee70e1f9b41eec1d3d2316af4b4946f29e21fc9
  state: discovered
- id: e1a4f63c-eb7d-5396-b003-0924a9abb09f
  symbol_path: src/cli/commands/fix.py::format_code_wrapper
  module: cli.commands.fix
  qualname: format_code_wrapper
  kind: function
  ast_signature: TBD
  fingerprint: cb9b4eea4b0817a7d35257c2708353e97450686b433c840ba7bd8c34423745b9
  state: discovered
- id: 5a5cc2ea-7574-5865-99e4-e3f743433a08
  symbol_path: src/features/introspection/sync_service.py::SymbolScanner
  module: features.introspection.sync_service
  qualname: SymbolScanner
  kind: class
  ast_signature: TBD
  fingerprint: cbae792e91d140276f6186ab9adcc8aa9001489735912060a84bfe69b40aab92
  state: discovered
- id: 5e541ac4-8012-5385-9e17-a8db5e06f006
  symbol_path: src/shared/time.py::now_iso
  module: shared.time
  qualname: now_iso
  kind: function
  ast_signature: TBD
  fingerprint: cd13ee2a975067fd4ebf1224c3ef3c71c62dc6c687d2ab2cbecf5667b18a7685
  state: discovered
- id: fd8d7dca-57d0-5d10-8934-1744891d3f51
  symbol_path: src/cli/commands/fix.py::purge_legacy_tags_command
  module: cli.commands.fix
  qualname: purge_legacy_tags_command
  kind: function
  ast_signature: TBD
  fingerprint: ce1f9c77359bf63e2e9b33a7195b9bba4b4cb3b4e49a6e206f3dcdc6b65f297a
  state: discovered
- id: 7cce57d9-5882-5512-b02a-4299a1ad53a9
  symbol_path: src/cli/commands/run.py::run_development_cycle
  module: cli.commands.run
  qualname: run_development_cycle
  kind: function
  ast_signature: TBD
  fingerprint: ce624c1494680ceac0f59444bf0bc5e0a30820ba7aa561447ae99d56c8a249ba
  state: discovered
- id: cdf96143-10ba-50e4-97cc-db5d85ded84d
  symbol_path: src/core/file_handler.py::FileHandler
  module: core.file_handler
  qualname: FileHandler
  kind: class
  ast_signature: TBD
  fingerprint: cf22b57d0019b89065c46a812e22e93b4b7115c5a8708f14c806b9a1d98a402f
  state: discovered
- id: e8b77dbd-cc22-5fb8-8d0a-5bedb516bf4d
  symbol_path: src/cli/admin_cli.py::main
  module: cli.admin_cli
  qualname: main
  kind: function
  ast_signature: TBD
  fingerprint: d125d265db871d927ddf6aaa3c09c29ccec873797fbcc7f2ee23c0f63ad948ca
  state: discovered
- id: b7d4ec0e-7715-53b3-8782-db0531250de2
  symbol_path: src/features/governance/policy_coverage_service.py::PolicyCoverageReport
  module: features.governance.policy_coverage_service
  qualname: PolicyCoverageReport
  kind: class
  ast_signature: TBD
  fingerprint: d2e65f3da7dbb557bc180a81f6a3d9a2114dcfa0db0e113d38603a47fac8b02f
  state: discovered
- id: 5eb31fc6-4772-5b7f-917a-292453c8f324
  symbol_path: src/cli/logic/audit_capability_domains.py::audit_capability_domains
  module: cli.logic.audit_capability_domains
  qualname: audit_capability_domains
  kind: function
  ast_signature: TBD
  fingerprint: d322ddd99687ecc5b1ea67031b40d2d0aed94ef26436987d2c878864ca380258
  state: discovered
- id: db3991d2-b976-59d2-83da-77cc9fceaaf3
  symbol_path: src/cli/logic/sync_manifest.py::sync_manifest
  module: cli.logic.sync_manifest
  qualname: sync_manifest
  kind: function
  ast_signature: TBD
  fingerprint: d569c7f61fd9d337f240eabc9bc23fe3e3e8aa08a324a696c6f2f865189ff21f
  state: discovered
- id: 9bfa56a9-b7e4-5244-ac90-dc0df7028b7d
  symbol_path: src/cli/admin_cli.py::register_all_commands
  module: cli.admin_cli
  qualname: register_all_commands
  kind: function
  ast_signature: TBD
  fingerprint: d572a2c5e63189164b2a0664fb3fb9637127dba8351b01e4d38995dd3e765dc9
  state: discovered
- id: af71e1aa-fc0d-56e2-af2f-155ae9b0f09d
  symbol_path: src/cli/logic/symbol_drift.py::inspect_symbol_drift
  module: cli.logic.symbol_drift
  qualname: inspect_symbol_drift
  kind: function
  ast_signature: TBD
  fingerprint: d57aba0cb6bf860488cb9cab4ab1714a7ca274f661c7e9b15461f5f0e2eeaf20
  state: discovered
- id: 25179fcd-fba3-5e7b-8af5-6f64d35678ae
  symbol_path: src/cli/logic/list_audits.py::list_audits
  module: cli.logic.list_audits
  qualname: list_audits
  kind: function
  ast_signature: TBD
  fingerprint: d5ee0f30c9f7a213720fc25c8b17c6788eb2555b2a68fc10f9ab254b2642b2a9
  state: discovered
- id: 96882076-567f-59fc-bb03-8de09143232c
  symbol_path: src/features/governance/checks/domain_placement.py::DomainPlacementCheck
  module: features.governance.checks.domain_placement
  qualname: DomainPlacementCheck
  kind: class
  ast_signature: TBD
  fingerprint: d671ac2bb7c19363c190192960d0271a688cc6b71ce0005e142bc4ca9f7b2637
  state: discovered
- id: 24799b1c-6c2b-54b5-8fe4-42cb3a56a145
  symbol_path: src/shared/ast_utility.py::extract_base_classes
  module: shared.ast_utility
  qualname: extract_base_classes
  kind: function
  ast_signature: TBD
  fingerprint: d8031e7792bef246dfca2f620c8fc491d824ccc624c26401ec05baf31041ba84
  state: discovered
- id: be24a112-dfe7-5b85-8458-00aa8f878d8c
  symbol_path: src/cli/logic/audit.py::lint
  module: cli.logic.audit
  qualname: lint
  kind: function
  ast_signature: TBD
  fingerprint: d8613af044906bda8a143aa2d4ddeed7f96dbf1f6d9e706a40e49b553f285843
  state: discovered
- id: 844404a8-15ae-5fa8-8d1f-e878aa151cba
  symbol_path: src/features/introspection/graph_analysis_service.py::find_semantic_clusters
  module: features.introspection.graph_analysis_service
  qualname: find_semantic_clusters
  kind: function
  ast_signature: TBD
  fingerprint: d89bbf4963d5757ad113465d30bd89faf1e7734ccb9f0bd4394efbe1b64c09cf
  state: discovered
- id: 2bb166e3-7586-5472-aee6-3fb061b23674
  symbol_path: src/cli/logic/diagnostics.py::check_legacy_tags
  module: cli.logic.diagnostics
  qualname: check_legacy_tags
  kind: function
  ast_signature: TBD
  fingerprint: d92b3e035136cf3f28bc28de26454ce93f5d2f904107a71e4d0a3a6b50b7130e
  state: discovered
- id: dbb6b123-b9f4-5865-933a-382f235e47ed
  symbol_path: src/shared/models/embedding_payload.py::EmbeddingPayload
  module: shared.models.embedding_payload
  qualname: EmbeddingPayload
  kind: class
  ast_signature: TBD
  fingerprint: d9730a35b8e4d821e58f829a2047a6cc93214c754477cf21008e5ce933e0677a
  state: discovered
- id: 70e7343c-bb16-5df8-a7ce-e451f3b6983c
  symbol_path: src/features/governance/checks/dependency_injection_check.py::DependencyInjectionCheck
  module: features.governance.checks.dependency_injection_check
  qualname: DependencyInjectionCheck
  kind: class
  ast_signature: TBD
  fingerprint: d98f59d61fa06b6f54e4bebc72d41ff625832b3648b4fa9c956650d1b681f3e7
  state: discovered
- id: 3cd3e161-5291-53a2-888e-80adfdb825be
  symbol_path: src/core/actions/healing_actions.py::FixDocstringsHandler
  module: core.actions.healing_actions
  qualname: FixDocstringsHandler
  kind: class
  ast_signature: TBD
  fingerprint: d9e211eaac927efbbbafca43117c56dada56ad6c7d00f201e765ba4393273878
  state: discovered
- id: d4ade439-4b0f-50b1-aaec-2b8cb8fbcbe9
  symbol_path: src/core/llm_client.py::LLMClient
  module: core.llm_client
  qualname: LLMClient
  kind: class
  ast_signature: TBD
  fingerprint: da7bc8dc539a953d183819a23f6af89fbc6a8c89c4634722917252f3d2a22d25
  state: discovered
- id: f2dafb56-113a-5799-924e-4af44a3cfaef
  symbol_path: src/cli/commands/run.py::vectorize_capabilities
  module: cli.commands.run
  qualname: vectorize_capabilities
  kind: function
  ast_signature: TBD
  fingerprint: dc3450fe5ac8e9f2e18d0ea06146df49ca63753348cbe70f043dd85672072820
  state: discovered
- id: f6dbd6e2-f436-5d00-8996-e401c5e22a09
  symbol_path: src/shared/ast_utility.py::FunctionCallVisitor
  module: shared.ast_utility
  qualname: FunctionCallVisitor
  kind: class
  ast_signature: TBD
  fingerprint: dce5f11ef2cca1cba3e26577a6a2834d477cf37514a131137c4be96880165fd0
  state: discovered
- id: ff9a31c9-90dd-5740-be61-9ed760b13baf
  symbol_path: src/core/actions/registry.py::ActionRegistry
  module: core.actions.registry
  qualname: ActionRegistry
  kind: class
  ast_signature: TBD
  fingerprint: dd02fdf184ef223127d6e415e48b9dcae919251ed61622bd47e1ebd8ee78d5e5
  state: discovered
- id: 7338000b-b21a-545f-9fca-3192cd2b72ef
  symbol_path: src/shared/utils/crypto.py::generate_approval_token
  module: shared.utils.crypto
  qualname: generate_approval_token
  kind: function
  ast_signature: TBD
  fingerprint: dd65f7283e4d7191e614d6286839506917f6c09f39e67f79857b2be10b3c3889
  state: discovered
- id: b7e2649f-28cf-517b-abc6-a789f3481a55
  symbol_path: src/cli/logic/knowledge_sync/snapshot.py::fetch_links
  module: cli.logic.knowledge_sync.snapshot
  qualname: fetch_links
  kind: function
  ast_signature: TBD
  fingerprint: dd84d7c8e82acfb5d909d9b832c1bd7e5bee6c46b16678553b45b8d418a81001
  state: discovered
- id: d7f86476-b957-5ce2-b531-c5d001e5dd71
  symbol_path: src/cli/commands/fix.py::complexity_command
  module: cli.commands.fix
  qualname: complexity_command
  kind: function
  ast_signature: TBD
  fingerprint: df16a2460b77e2f1fd7db106e9b2a7c1a5a11fc139b8838438a9d3bfdb6aea1e
  state: discovered
- id: deacf968-edf2-5d50-959b-af6a6f90a334
  symbol_path: src/shared/schemas/manifest_validator.py::validate_manifest_entry
  module: shared.schemas.manifest_validator
  qualname: validate_manifest_entry
  kind: function
  ast_signature: TBD
  fingerprint: df3df9d690f9e5aebdbffadea221df68a0ebfd500aaafa86a2205e3e452dedd2
  state: discovered
- id: 97157384-5247-5b09-881a-04f3867cc520
  symbol_path: src/cli/logic/embeddings_cli.py::register
  module: cli.logic.embeddings_cli
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: e0337f7d9650a028b8e30d923b988b631b5f0689d6934becff3989c7bfcfbf6e
  state: discovered
- id: 223ff445-c201-5a62-add0-bb43a6f65305
  symbol_path: src/features/introspection/knowledge_graph_service.py::KnowledgeGraphBuilder
  module: features.introspection.knowledge_graph_service
  qualname: KnowledgeGraphBuilder
  kind: class
  ast_signature: TBD
  fingerprint: e07e7c7050c5b54857a6e9205c5fbd65bccc3fe16cded0d20f460edac675c9b4
  state: discovered
- id: 6d01b57c-c19e-5b31-8c10-e067f4f30f0f
  symbol_path: src/core/actions/healing_actions.py::FormatCodeHandler
  module: core.actions.healing_actions
  qualname: FormatCodeHandler
  kind: class
  ast_signature: TBD
  fingerprint: e1d7942f206ef3a296e3da9f98422952dd3f430d021c63144adf5673278bc3b2
  state: discovered
- id: 643c3a3b-491f-5c6d-9626-242bd31cddb3
  symbol_path: src/core/actions/governance_actions.py::CreateProposalHandler
  module: core.actions.governance_actions
  qualname: CreateProposalHandler
  kind: class
  ast_signature: TBD
  fingerprint: e22340ac999fe7ec261be3979892cf3c593b1c718203077b888d14d411de9c77
  state: discovered
- id: de84cf6f-c051-5c20-a1dd-ab1eeaa6359d
  symbol_path: src/cli/logic/diagnostics.py::cli_registry
  module: cli.logic.diagnostics
  qualname: cli_registry
  kind: function
  ast_signature: TBD
  fingerprint: e2c0251057135a8fe302112df13bf0c42069f575f754ecd0532c9ef57d1726ea
  state: discovered
- id: c56d26a7-a72a-5860-a064-b8e3a164ca26
  symbol_path: src/shared/ast_utility.py::parse_metadata_comment
  module: shared.ast_utility
  qualname: parse_metadata_comment
  kind: function
  ast_signature: TBD
  fingerprint: e38c92985d8839ac7548c7c32cf0d41825114bdcd665918c2a728ca45d201d04
  state: discovered
- id: 907a9beb-bbfb-5eb9-b9bf-b20b2b373fe6
  symbol_path: src/cli/commands/mind.py::diff_command
  module: cli.commands.mind
  qualname: diff_command
  kind: function
  ast_signature: TBD
  fingerprint: e3cf4cd4be123da5752a1fb7f895f76de9b791d02d6b6bbb7187a9d6d8ece955
  state: discovered
- id: 81e3fe3b-5e6c-5574-921a-e8b082ca04b7
  symbol_path: src/features/project_lifecycle/scaffolding_service.py::Scaffolder
  module: features.project_lifecycle.scaffolding_service
  qualname: Scaffolder
  kind: class
  ast_signature: TBD
  fingerprint: e441d1dd14bf424cae052534b26704dd80569b6ce29218e27e164378e908d491
  state: discovered
- id: 6389a100-1bfa-5926-bc9c-03202f641fea
  symbol_path: src/services/clients/qdrant_client.py::QdrantService
  module: services.clients.qdrant_client
  qualname: QdrantService
  kind: class
  ast_signature: TBD
  fingerprint: e51fab891fef8c52c4e6e77cbf3db5c16d03ca652aae154376043711db9686bf
  state: discovered
- id: 86296731-3cac-5b2c-88b1-6c99fe8b1c3f
  symbol_path: src/shared/ast_utility.py::calculate_structural_hash
  module: shared.ast_utility
  qualname: calculate_structural_hash
  kind: function
  ast_signature: TBD
  fingerprint: e5c9550671ede027622ee729768f2ffdee72a1131946814a0f6c9673925f6645
  state: discovered
- id: 1924ebbb-7d63-5e77-95ac-a2425a25754a
  symbol_path: src/cli/logic/system.py::integrate_command
  module: cli.logic.system
  qualname: integrate_command
  kind: function
  ast_signature: TBD
  fingerprint: e619946e8e7e5232c9c636f7899c2a5aedefebb43002c80ef940487ac35325d2
  state: discovered
- id: 71da3e53-5e43-5bc9-b635-71fabf54e35a
  symbol_path: src/features/governance/checks/naming_conventions.py::NamingConventionsCheck
  module: features.governance.checks.naming_conventions
  qualname: NamingConventionsCheck
  kind: class
  ast_signature: TBD
  fingerprint: e640aff836a184709244aeb8e05eb106a22f3e3710af311ae75e35a10e67e2e8
  state: discovered
- id: 6b4a2c1d-358d-53f8-9624-b44faec60bc5
  symbol_path: src/core/crate_processing_service.py::CrateProcessingService
  module: core.crate_processing_service
  qualname: CrateProcessingService
  kind: class
  ast_signature: TBD
  fingerprint: e67d532d8365c10278c065ce4d370aaf1462c775d3b6d0570370b06caead0327
  state: discovered
- id: ed3f7f35-265c-5aaf-9585-3be7ee4b0c12
  symbol_path: src/cli/logic/chat.py::chat
  module: cli.logic.chat
  qualname: chat
  kind: function
  ast_signature: TBD
  fingerprint: e76cf188db6db28f403145081387b623867cc22e8776a550b7a2d2aa74c9a94b
  state: discovered
- id: b6ea0820-e8f7-5f81-aa32-a39005435704
  symbol_path: src/core/test_runner.py::run_tests
  module: core.test_runner
  qualname: run_tests
  kind: function
  ast_signature: TBD
  fingerprint: e7898d73b9abc7a58013e08dcee037d619fa78f60f80f33bb74770bd9e7e7717
  state: discovered
- id: e9bc6320-0e79-5cfb-a7d7-5bc503d423c5
  symbol_path: src/features/introspection/drift_detector.py::write_report
  module: features.introspection.drift_detector
  qualname: write_report
  kind: function
  ast_signature: TBD
  fingerprint: e79d90ab3192e421ee7c7b69b9ef37c3686f81a54a6aacbf4b6b2c4f75f64bff
  state: discovered
- id: b17f4152-d002-5b95-ad95-e70a8645f82f
  symbol_path: src/services/mind_service.py::MindService
  module: services.mind_service
  qualname: MindService
  kind: class
  ast_signature: TBD
  fingerprint: e7f7d8388211b94f55caa5d793daf73b13f7f0995220228d051247e3791fc6d1
  state: discovered
- id: 8cfae6dc-e186-53bc-8c4a-bfbd1d1d43d5
  symbol_path: src/features/governance/policy_loader.py::load_micro_proposal_policy
  module: features.governance.policy_loader
  qualname: load_micro_proposal_policy
  kind: function
  ast_signature: TBD
  fingerprint: e813dc32af2462fff1219fdb03d7eb156ef7e4c44fd7505e3507bccec1f6e671
  state: discovered
- id: 308457e8-a235-525c-9058-503a93b7755e
  symbol_path: src/cli/logic/diagnostics.py::find_clusters_command_sync
  module: cli.logic.diagnostics
  qualname: find_clusters_command_sync
  kind: function
  ast_signature: TBD
  fingerprint: e8a22183570f6f315f767886d48c2a9afc5ef4af344def28d6498f2fe14c7a72
  state: discovered
- id: b21d2a07-e55a-500a-8236-7f6c8c45d414
  symbol_path: src/cli/logic/tools.py::register
  module: cli.logic.tools
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: ec20c702ab52f6699589ceea8b3535d922f09cc84947f21cba77f80f35710fdc
  state: discovered
- id: 1bebafab-3497-568e-a41f-4b8dfe794876
  symbol_path: src/cli/logic/validate.py::validate_intent_schema
  module: cli.logic.validate
  qualname: validate_intent_schema
  kind: function
  ast_signature: TBD
  fingerprint: ec8c206b598a190443b3146cad9367c91fa41c4c934e79743498b0ebe01df0b9
  state: discovered
- id: 08d65a4f-b400-5f0b-9c3a-7cca86702d6d
  symbol_path: src/core/file_classifier.py::get_file_classification
  module: core.file_classifier
  qualname: get_file_classification
  kind: function
  ast_signature: TBD
  fingerprint: ec9037084b23aff57abc957a02690225b74a709cbfc4cbe149b3f3036a80fcf3
  state: discovered
- id: 3a196cd2-fa47-532d-ba25-ef8d8c0a1cf2
  symbol_path: src/cli/logic/diagnostics.py::cli_tree
  module: cli.logic.diagnostics
  qualname: cli_tree
  kind: function
  ast_signature: TBD
  fingerprint: ecb4c55fadc7529a28a14e5ff81802d872c5e2350291f833b39d4f129c3441ab
  state: discovered
- id: bdcc881d-f16a-59f8-adb2-27ae935ca4b6
  symbol_path: src/shared/path_utils.py::get_repo_root
  module: shared.path_utils
  qualname: get_repo_root
  kind: function
  ast_signature: TBD
  fingerprint: edc71d0c31594afdd9eea55c11cfb962e028462f15c08972f1a1d9cf1796ed79
  state: discovered
- id: 1167bdce-e940-5acf-af55-0c84af8ac2e8
  symbol_path: src/shared/utils/embedding_utils.py::EmbeddingService
  module: shared.utils.embedding_utils
  qualname: EmbeddingService
  kind: class
  ast_signature: TBD
  fingerprint: ef12f88950a87178b063562c1cb221cb36c8095fce2299bda443a21d47109013
  state: discovered
- id: 35888f40-c113-5d08-b3f9-d9a31eecfbfe
  symbol_path: src/services/config_service.py::get_config_service
  module: services.config_service
  qualname: get_config_service
  kind: function
  ast_signature: TBD
  fingerprint: ef61dc13f163562a6c339297e63b08c3d3ef080a632db4a28fcd98e880d0d37e
  state: discovered
- id: 420a9031-ded2-5d62-8cad-52fd419ce918
  symbol_path: src/features/governance/checks/style_checks.py::StyleChecks
  module: features.governance.checks.style_checks
  qualname: StyleChecks
  kind: class
  ast_signature: TBD
  fingerprint: f0052fb7f457341be0bf5976699b566072ad841407f1373982b1f971f2bd162b
  state: discovered
- id: 6a1c44ff-e500-5a89-b2c0-c8befaa9d26e
  symbol_path: src/features/maintenance/dotenv_sync_service.py::RuntimeSetting
  module: features.maintenance.dotenv_sync_service
  qualname: RuntimeSetting
  kind: class
  ast_signature: TBD
  fingerprint: f0677b651d23ea5440f4b8cae9355b16489e24b6cac68337a854aff8c0ffea72
  state: discovered
- id: 9948936c-6409-54c3-82bf-2c717f607ad1
  symbol_path: src/core/agents/plan_executor.py::PlanExecutor
  module: core.agents.plan_executor
  qualname: PlanExecutor
  kind: class
  ast_signature: TBD
  fingerprint: f1205a731c90de9404ef97dedd3755cf67899a666f707a2a71bd44f78c594291
  state: discovered
- id: 6af45a30-ba58-571a-9de2-ecfb42ef7d5a
  symbol_path: src/cli/logic/build.py::register
  module: cli.logic.build
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: f17928fd6748d00cf142ae20fc57e158e754912902310ae116c91e617fb39a3e
  state: discovered
- id: 6c9d9c1c-4216-59ce-97ee-c404634b9970
  symbol_path: src/cli/commands/fix.py::register
  module: cli.commands.fix
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: f1bee39551fdc6a8a85c70261c8711ecac541259abb4b536127ce0172ddb2084
  state: discovered
- id: 9494eeb7-30a9-5620-97b6-e90d56649d81
  symbol_path: src/cli/logic/validate.py::validate_risk_gates
  module: cli.logic.validate
  qualname: validate_risk_gates
  kind: function
  ast_signature: TBD
  fingerprint: f24b9167ee6fa4a89b1ca7b8345be1b96a9743ced3755b1ae3174fadf05eef63
  state: discovered
- id: 1ed455da-17ff-5e2f-9670-b52cbbee5421
  symbol_path: src/core/actions/context.py::PlanExecutorContext
  module: core.actions.context
  qualname: PlanExecutorContext
  kind: class
  ast_signature: TBD
  fingerprint: f3ae66c634a1cbb9c8b968fb0cd6590d8b106bda1aefbac135d311410bc4b55c
  state: discovered
- id: 0bd7653f-9de6-5115-98a4-919346ac21ce
  symbol_path: src/features/governance/checks/id_coverage_check.py::IdCoverageCheck
  module: features.governance.checks.id_coverage_check
  qualname: IdCoverageCheck
  kind: class
  ast_signature: TBD
  fingerprint: f43ae3fb75545148e147df6046c4cd53e5c919bbde8d0fc510ab8a0493c31b8c
  state: discovered
- id: b83f4c9b-c1e2-5f42-9097-62a930828ea2
  symbol_path: src/shared/utils/parsing.py::parse_write_blocks
  module: shared.utils.parsing
  qualname: parse_write_blocks
  kind: function
  ast_signature: TBD
  fingerprint: f441f608de76ee201b1e5eecefd8b6f6b2ef6edc545e60ad957a1706a8ebad85
  state: discovered
- id: cc1c12c9-524f-53ec-857e-5097c46fbe12
  symbol_path: src/services/database/models.py::LlmResource
  module: services.database.models
  qualname: LlmResource
  kind: class
  ast_signature: TBD
  fingerprint: f45d8596e674bf0e0bbd37c5ae9d9c2a9621a4dc99c483a9a0608e8afa50234f
  state: discovered
- id: be90675a-0a6b-5a6c-b9f9-84bd2e314bbe
  symbol_path: src/cli/logic/run.py::register
  module: cli.logic.run
  qualname: register
  kind: function
  ast_signature: TBD
  fingerprint: f624f1d87e17fc0b797d37b095dc05cb43a6ec72239cbe8df98b0df382889af5
  state: discovered
- id: 6ce4ca80-f6d7-5659-8989-d3780187b1eb
  symbol_path: src/features/self_healing/enrichment_service.py::enrich_symbols
  module: features.self_healing.enrichment_service
  qualname: enrich_symbols
  kind: function
  ast_signature: TBD
  fingerprint: f640ea8a0cd1851dfe88ef32731b784925194aca34b9ae9797f7d5f7005262df
  state: discovered
- id: 8e4ea611-d1aa-5cb5-b265-90f2cd98f5d4
  symbol_path: src/features/introspection/sync_service.py::SymbolVisitor
  module: features.introspection.sync_service
  qualname: SymbolVisitor
  kind: class
  ast_signature: TBD
  fingerprint: f6419745f63026268e6e8d9a34b3146b93b857daa1d86bda5e1fafefd887820d
  state: discovered
- id: 8cf60515-cdd9-5019-9d24-e31a755a47c9
  symbol_path: src/cli/commands/fix.py::fix_policy_ids_command
  module: cli.commands.fix
  qualname: fix_policy_ids_command
  kind: function
  ast_signature: TBD
  fingerprint: f76282b097ca295538c9e04f2980e5beaa96749a600529b4e79c3431c6dc7286
  state: discovered
- id: 0a055408-c1c4-54f2-b2d3-28bc47ace016
  symbol_path: src/cli/logic/knowledge_sync/utils.py::canonicalize
  module: cli.logic.knowledge_sync.utils
  qualname: canonicalize
  kind: function
  ast_signature: TBD
  fingerprint: f768996b01acd653ac35248e8d4e2823546ce528ba696d897b01c36a0786fdd4
  state: discovered
- id: 0adb9fb7-bb27-5db9-ad87-a1f9f4238696
  symbol_path: src/cli/logic/reviewer.py::docs_clarity_audit
  module: cli.logic.reviewer
  qualname: docs_clarity_audit
  kind: function
  ast_signature: TBD
  fingerprint: f7fc16ecb555e9b90507cd21b8f8864c2ce9a160701a428e00b6fab782f98d6c
  state: discovered
- id: b1b099d1-9baf-56ec-9d53-e6a4db1f1cc7
  symbol_path: src/services/database/models.py::CognitiveRole
  module: services.database.models
  qualname: CognitiveRole
  kind: class
  ast_signature: TBD
  fingerprint: fa53085796acec678bc4e7a443285d72ee4cff38a8fc863f6d644ee6ea125620
  state: discovered
- id: d71390b0-475c-5e61-a376-49a20fce7706
  symbol_path: src/core/prompt_pipeline.py::PromptPipeline
  module: core.prompt_pipeline
  qualname: PromptPipeline
  kind: class
  ast_signature: TBD
  fingerprint: fa7f814cc5c7354f3c1812f098033bb42aca51c2c28f0734ab4f6c66da640f52
  state: discovered
- id: 31713afd-78b6-5396-9fb2-10a07b0920dc
  symbol_path: src/shared/legacy_models.py::LegacyCognitiveRoles
  module: shared.legacy_models
  qualname: LegacyCognitiveRoles
  kind: class
  ast_signature: TBD
  fingerprint: fb420e2e6dc37a3e6a3b529872ac2c19be88f17864e137982e1c957f1bd7556d
  state: discovered
- id: fa43abb4-7c5a-5091-b0f3-50859fbda86a
  symbol_path: src/core/cognitive_service.py::CognitiveService
  module: core.cognitive_service
  qualname: CognitiveService
  kind: class
  ast_signature: TBD
  fingerprint: fbab72880ac95fe13de8808ba36ed973f418369a55c6cce62e1bcbc3548f00eb
  state: discovered
- id: 37387902-b1e1-5969-8e46-fac5b383a246
  symbol_path: src/features/governance/micro_proposal_validator.py::MicroProposalValidator
  module: features.governance.micro_proposal_validator
  qualname: MicroProposalValidator
  kind: class
  ast_signature: TBD
  fingerprint: fbbf9dba153927612f77d607f5ceeea099a781c666da288fba900626d70612c5
  state: discovered
- id: 94106f44-d1d5-59cc-adff-6fc11a2a288d
  symbol_path: src/shared/ast_utility.py::extract_parameters
  module: shared.ast_utility
  qualname: extract_parameters
  kind: function
  ast_signature: TBD
  fingerprint: fc9f7f0e460fab8910df5925064cd6c4ebf59e16361f949566a6033bc18edfa1
  state: discovered
- id: 60584426-3bdd-5a2e-b2cc-71b13fbb9b9a
  symbol_path: src/core/git_service.py::GitService
  module: core.git_service
  qualname: GitService
  kind: class
  ast_signature: TBD
  fingerprint: fe95a6fb7c6fa4be26862a423871c502d699f45166528e30e3691adc8999bba1
  state: discovered
- id: 83e02564-337e-59cc-9fd4-f8683cd55280
  symbol_path: src/shared/utils/header_tools.py::HeaderTools
  module: shared.utils.header_tools
  qualname: HeaderTools
  kind: class
  ast_signature: TBD
  fingerprint: ffdc5994b863975c72d8f6b3b082184197bf797f7eab8699a1104565909e15ea
  state: discovered
- id: 8e701c67-9829-5834-9b5d-bbe53a2fa9e0
  symbol_path: src/cli/logic/reviewer.py::code_review
  module: cli.logic.reviewer
  qualname: code_review
  kind: function
  ast_signature: TBD
  fingerprint: fff6d7866a7639aafe546933cf97f61efdc11886f8f2df1d638c7c683a3a410e
  state: discovered
digest: sha256:082c3299ab2c2c66bbbfb2cd947f43bd90ac8575e53191874d14c9c37c69d15a

--- END OF FILE ./.intent/mind_export/symbols.yaml ---

--- START OF FILE ./.intent/proposals/README.md ---
# Proposals

Create proposals here with filename pattern `cr-*.yaml`.

## Format
- `target_path`: repo-relative path (e.g., `.intent/policies/safety_policies.yaml`)
- `action`: currently only `replace_file`
- `justification`: why this is needed
- `content`: full new file contents (string)
- `rollback_plan` (optional): notes to revert
- `signatures`: added by `core-admin proposals-sign`

See `cr-example.yaml` for a starter.

--- END OF FILE ./.intent/proposals/README.md ---

--- START OF FILE ./CONTRIBUTING.md ---
# Contributing to CORE

Thank you for joining CORE’s mission to pioneer self-governing software! Your contribution helps shape AI-driven development.

---

## Our Philosophy: Principled Contributions

CORE is governed by a **“constitution”** (rules in `.intent/`). All contributions must align with principles like `clarity_first`. Start with these docs:

*   **README.md**: Project vision and quick demo.
*   **Architecture (`docs/02_ARCHITECTURE.md`)**: The Mind-Body architecture and the role of the database.
*   **Governance (`docs/03_GOVERNANCE.md`)**: How changes are made safely.

**Key Concepts**:
*   A **`# ID: <uuid>`** tag in the source code is a permanent linker that connects a piece of code (the Body) to its definition in the database (the Mind).
*   A **"constitutional change"** updates files in `.intent/charter/`, requiring a signed proposal and a full audit.

---

## Contribution Workflow

1. **Find/Open an Issue**
   Discuss your proposed change in a GitHub Issue.

   ↓

2. **Write Your Code**
   Implement the feature or fix in `src/`.

   ↓

3. **Integrate Your Changes**
   Run `poetry run core-admin system integrate "Your commit message"` to tag, sync, and validate your work.

   ↓

4. **Submit a Pull Request**
   Link your PR to the issue.

---

## How to Contribute Code

Code contributions must follow CORE’s governance.

#### 1. Add Your Code
Write your functions, classes, and tests in the `src/` directory, following the established architectural domains.

#### 2. Assign IDs and Synchronize
After writing your code, you must integrate it with the system's Mind.

   *   **Assign IDs to new functions:**
     ```bash
     poetry run core-admin fix assign-ids --write
     ```
   *   **Synchronize with the database:**
     ```bash
     poetry run core-admin knowledge sync --write
     ```
   *   **(Optional) For major changes, run the full integration command:**
     ```bash
     poetry run core-admin system integrate "feat: Your descriptive commit message"
     ```

#### 3. Run Checks
Before submitting, ensure all checks pass. The `integrate` command does this for you, but you can also run them manually.

   *   `poetry run core-admin check ci audit`: Run the full constitutional audit (**required**).
   *   `make check`: A convenient shortcut for the full audit and other checks.
   *   `make format`: Auto-format your code.

#### 4. Submit Your PR
Submit your Pull Request, linking it to the relevant GitHub Issue.

---

## Questions?

Ask in **GitHub Issues**. We’re excited to collaborate!

--- END OF FILE ./CONTRIBUTING.md ---

--- START OF FILE ./LICENSE ---
MIT License

Copyright (c) 2024 Dariusz Newecki

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--- END OF FILE ./LICENSE ---

--- START OF FILE ./Makefile ---
# FILE: Makefile
# Makefile for CORE – Cognitive Orchestration Runtime Engine
# This file provides convenient shortcuts to the canonical 'core-admin' CLI commands.

# ---- Shell & defaults --------------------------------------------------------
SHELL := /bin/bash
.SHELLFLAGS := -eu -o pipefail -c
.DEFAULT_GOAL := help

# ---- Configurable knobs ------------------------------------------------------
POETRY      ?= python3 -m poetry
APP         ?= src.core.main:create_app
HOST        ?= 0.0.0.0
PORT        ?= 8000
RELOAD      ?= --reload
ENV_FILE    ?= .env

# Capability docs output
OUTPUT_PATH ?= docs/10_CAPABILITY_REFERENCE.md

# Internal helpers
PY          := $(POETRY) run python

# ---- Phony targets -----------------------------------------------------------
.PHONY: \
  help install lock run stop \
  audit lint format test fast-check check dev-sync \
  cli-tree clean distclean nuke \
  docs check-docs vectorize integrate \
  migrate export-db sync-knowledge sync-manifest

# ---- Help (auto-documented) --------------------------------------------------
# Use the pattern "target: ## description" to list in `make help`.
help: ## Show this help message
	@echo "CORE Development Makefile"
	@echo "-------------------------"
	@echo "This Makefile maps to 'core-admin' CLI commands."
	@echo ""
	@awk 'BEGIN {FS":.*##"} /^[a-zA-Z0-9_.-]+:.*##/ {printf "  \033[36m%-18s\033[0m %s\n", $$1, $$2}' $(MAKEFILE_LIST)
	@echo ""
	@echo "Tip: run '$(POETRY) run core-admin --help' for the full CLI."

# ---- Setup -------------------------------------------------------------------
install: ## Install dependencies (poetry install)
	@echo "📦 Installing dependencies..."
	$(POETRY) install

lock: ## Resolve and lock dependencies
	@echo "🔒 Resolving and locking dependencies..."
	$(POETRY) lock

# ---- Run / Stop --------------------------------------------------------------
run: ## Start the FastAPI server (uvicorn)
	@echo "🚀 Starting FastAPI server at http://$(HOST):$(PORT)"
	$(POETRY) run uvicorn $(APP) --factory --host $(HOST) --port $(PORT) $(RELOAD) --env-file $(ENV_FILE)

stop: ## Kill any process listening on $(PORT)
	@echo "🛑 Stopping any process on port $(PORT)..."
	@command -v lsof >/dev/null 2>&1 && lsof -t -i:$(PORT) | xargs kill -9 2>/dev/null || true

# ---- Checks / Fixes ----------------------------------------------------------
audit: ## Run the constitutional audit
	$(POETRY) run core-admin check audit

lint: ## Check code format and quality (read-only)
	$(POETRY) run core-admin check lint

format: ## Fix code style issues (Black/Ruff via CLI)
	$(POETRY) run core-admin fix code-style

test: ## Run tests
	@echo "🧪 Running tests with pytest..."
	$(POETRY) run pytest

fast-check: ## Lint + tests (quick local cycle)
	$(MAKE) lint
	$(MAKE) test

check: ## Lint + tests + audit + docs drift check
	@echo "🤝 Running full constitutional audit and documentation check..."
	$(MAKE) lint
	$(MAKE) test
	$(MAKE) audit
	@$(MAKE) check-docs

dev-sync: ## Run the safe, non-destructive developer sync and audit workflow
	$(POETRY) run core-admin check system

cli-tree: ## Display CLI command tree
	@echo "🌳 Generating CLI command tree..."
	$(POETRY) run core-admin inspect command-tree

# ---- Knowledge / DB helpers --------------------------------------------------
migrate: ## Apply pending DB schema migrations
	$(POETRY) run core-admin manage database migrate

export-db: ## Export DB tables to canonical YAML
	$(POETRY) run core-admin manage database export

sync-knowledge: ## Scan codebase and sync symbols to DB (Single Source of Truth)
	$(POETRY) run core-admin manage database sync-knowledge --write

sync-manifest: ## Sync .intent/mind/project_manifest.yaml from DB
	$(POETRY) run core-admin manage database sync-manifest

vectorize: ## Vectorize knowledge graph (embeddings pipeline)
	@echo "🧠 Vectorizing knowledge graph..."
	$(POETRY) run core-admin run vectorize

integrate: ## Canonical integration sequence (submit changes)
	@echo "🤝 Running Canonical Integration Sequence via 'submit changes'..."
	$(POETRY) run core-admin submit changes --message "feat: Integrate changes via make"

# ---- Docs --------------------------------------------------------------------
docs: ## Generate capability documentation
	@echo "📚 Generating capability documentation..."
	# Option A: preferred CLI-managed docs (if implemented)
	-$(POETRY) run core-admin manage project docs || true
	# Option B: direct module entry point (fallback)
	$(PY) -m features.introspection.generate_capability_docs --output "$(OUTPUT_PATH)"

check-docs: docs ## Verify documentation is in sync
	@echo "🔎 Checking for documentation drift..."
	@git diff --exit-code --quiet "$(OUTPUT_PATH)" || (echo "❌ ERROR: Documentation is out of sync. Please run 'make docs' and commit the changes." && exit 1)
	@echo "✅ Documentation is up to date."

# ---- Clean -------------------------------------------------------------------
clean: ## Remove temporary files and caches
	@echo "🧹 Cleaning up temporary files and caches..."
	find . -type f -name '*.pyc' -delete
	find . -type d -name '__pycache__' -prune -exec rm -rf {} +
	rm -rf .pytest_cache .ruff_cache .mypy_cache .cache
	rm -f .coverage
	rm -rf htmlcov
	rm -rf build dist *.egg-info
	rm -rf pending_writes sandbox
	@echo "✅ Clean complete."

distclean: clean ## Clean + remove virtual env
	@echo "🧨 Distclean: removing virtual environments and build leftovers..."
	rm -rf .venv
	@echo "✅ Distclean complete."

nuke: ## Danger! Remove ALL untracked files (git clean -fdx)
	@echo "☢️  Running 'git clean -fdx' in 3s (CTRL+C to cancel)..."
	@sleep 3
	git clean -fdx
	@echo "✅ Repo nuked (untracked files/dirs removed)."

--- END OF FILE ./Makefile ---

--- START OF FILE ./README.md ---
# CORE — The Self-Improving System Architect

> **Where Intelligence Lives.**

[![Status: Architectural Prototype](https://img.shields.io/badge/status-architectural%20prototype-blue.svg)](#-project-status)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![codecov](https://codecov.io/gh/DariuszNewecki/CORE/graph/badge.svg)](https://codecov.io/gh/DariuszNewecki/CORE)

CORE is a self-governing, constitutionally aligned AI development framework that can plan, write, validate, and evolve software systems — autonomously and safely. It is designed for environments where **trust, traceability, and governance matter**.

---

## 🏛️ Project Status: Architectural Prototype

The core self-governance and constitutional amendment loop is complete and stable. The system can audit and modify its own constitution via a human-in-the-loop, cryptographically signed approval process.

The next phase is to expand agent capabilities so CORE can generate and manage entirely new applications based on user intent. We’re making the project public now to invite collaboration on this foundational architecture.

---

## 🧠 What is CORE?

Traditional codebases often suffer from **architectural drift** — the code no longer matches the original design. Linters catch syntax errors, but architectural mistakes slip through.

CORE solves this by using a **“constitution”** (a set of machine-readable rules in `.intent/`) and an AI-powered **`ConstitutionalAuditor`** to ensure your code stays true to its design.

It’s built on a simple **Mind–Body–Will** philosophy:

* **Mind (`.intent/`)**: The Constitution. You declare your project's rules and goals here.
* **Body (`src/`)**: The Machinery. Simple, reliable tools that act on the code.
* **Will (AI Agents)**: The Reasoning Layer. AI agents that read the Mind and use the Body's tools to achieve your goals, while the Auditor ensures they never break the rules.

---

## 🚀 Getting Started (5-Minute Demo)

See CORE in action by running the worked example: create a simple API, intentionally break an architectural rule, and watch CORE's auditor catch it.

👉 **[Run the Worked Example (`docs/09_WORKED_EXAMPLE.md`)](docs/09_WORKED_EXAMPLE.md)**

---

## 📖 Documentation Portal

* **[What is CORE? (`docs/00_WHAT_IS_CORE.md`)](docs/00_WHAT_IS_CORE.md)** — The vision and philosophy.
* **[Architecture (`docs/02_ARCHITECTURE.md`)](docs/02_ARCHITECTURE.md)** — Technical details of the Mind and Body.
* **[Governance (`docs/03_GOVERNANCE.md`)](docs/03_GOVERNANCE.md)** — How changes are made safely.
* **[Roadmap (`docs/04_ROADMAP.md`)](docs/04_ROADMAP.md)** — See where we're going.
* **[Technical Debt Log (`docs/05_TECHNICAL_DEBT.md`)](docs/05_TECHNICAL_DEBT.md)** — Our formal plan for architectural improvements.
* **[Contributing (`CONTRIBUTING.md`)](CONTRIBUTING.md)** — Join our mission!

---

## ⚙️ Installation & Quick Start

**Requirements**: Python 3.12+, Poetry

```bash
# Clone and install
git clone https://github.com/DariuszNewecki/CORE.git
cd CORE
poetry install

# Set up environment
cp .env.example .env
# Edit .env with your LLM API keys

# Verify setup is clean by running the full system check
poetry run core-admin system check

# Try the conversational command!
poetry run core-admin chat "make me a simple command-line tool that prints a random number"

# 🌱 Contributing
We welcome all contributors! The best place to start is our Contributing Guide.

Check the Project Roadmap for "Next Up" tasks and see our open issues on GitHub.
# 📄 License
Licensed under the MIT License. See LICENSE.

--- END OF FILE ./README.md ---

--- START OF FILE ./docker-compose.yml ---
# docker-compose.yml
version: "3.8"

services:
  qdrant:
    image: qdrant/qdrant:v1.9.0
    container_name: core_qdrant_db
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage # Use a named volume for persistence
    restart: always

  # --- ADD THIS ENTIRE SECTION ---
  postgres:
    image: postgres:16-alpine
    container_name: core_postgres_db
    ports:
      - "5432:5432" # Expose the default PostgreSQL port to your host machine
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-core}
    volumes:
      - postgres-data:/var/lib/postgresql/data # Use a named volume for persistence
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5
  # --- END OF ADDITION ---

# Define the named volumes for data persistence
volumes:
  qdrant-data:
  postgres-data:

--- END OF FILE ./docker-compose.yml ---

--- START OF FILE ./pyproject.toml ---
[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "core"
version = "1.0.0"
description = "CORE: A self-governing, intent-driven software development system."
authors = ["Dariusz Newecki <d.newecki@gmail.com>"]
license = "MIT"
readme = "README.md"
packages = [
    { include = "api", from = "src" },
    { include = "cli", from = "src" },
    { include = "core", from = "src" },
    { include = "features", from = "src" },
    { include = "services", from = "src" },
    { include = "shared", from = "src" },
]

[tool.poetry.dependencies]
python = ">=3.12"
fastapi = ">=0.95.0"
uvicorn = {extras = ["standard"], version = ">=0.21.0"}
pyyaml = ">=6.0"
httpx = ">=0.25.0"
python-dotenv = ">=1.0.0"
pydantic = ">=2.11"
pydantic-settings = "^2.10.1"
cryptography = ">=42.0.0"
rich = "^13"
black = "^24"
jsonschema = "^4"
typer = {extras = ["rich"], version = "^0.16.1"}
radon = ">=5.1.0"
filelock = "^3.13.0"
ruamel-yaml = "^0.18.6"
qdrant-client = "1.9.0"
numpy = "^2.3.2"
scikit-learn = "^1.5.1"
scipy = "^1.14.0"
sqlalchemy = ">=2.0"
asyncpg = "^0.30.0"
sqlparse = "^0.5.3"
networkx = "^3.3"
psycopg2-binary = "^2.9.10"

[tool.poetry.group.dev.dependencies]
pytest = ">=7.0,<8.0"
pytest-asyncio = "==0.21.0"
pytest-mock = "^3.12.0"
pytest-cov = "^6.2"
pytest-dotenv = "^0.5.2"
aiosqlite = "^0.21.0"
pre-commit = "^3.7.1"
httpx = "^0.28.1"

[tool.poetry.scripts]
core-admin = "cli.admin_cli:app"

[tool.black]
line-length = 88

[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = ["E", "W", "F", "I"]
ignore = ["E402", "E501"]

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
# This line tells pytest to add the 'src' directory to the Python path
# so that imports like 'from core.knowledge_service' work correctly.
pythonpath = ["src"]
# addopts = ["-c", "pyproject.toml"]
env_files = [
    ".env.test",
    ".env"
]

--- END OF FILE ./pyproject.toml ---

--- START OF FILE ./scripts/11_ACADEMIC_PAPER.md ---
Paper outline (v1.0, conference-ready)

Title (working):
Constitutional Software Engineering: Mind–Body–Will Governance for AI-Driven Systems

Abstract (draft):
Large Language Models (LLMs) accelerate code generation but amplify architectural drift and erode trust in software evolution. We present Constitutional Software Engineering (CSE), a framework that treats a project’s intent and rules as a first-class, machine-readable Constitution (“Mind”), executed by a constrained Body (code + tools), and governed by a deliberate Will (AI agents) under an independent Constitutional Auditor. We instantiate CSE in CORE, which implements cryptographically signed proposals, quorum rules, and canary self-audits before constitutional changes apply. A staged Autonomy Ladder demonstrates governed progression from self-awareness to self-healing. In a case study, CORE detects capability gaps, proposes compliant fixes, and ratifies them under human-in-the-loop signatures, integrating CI to continuously enforce the Constitution. We find that CSE maintains architectural integrity while enabling safe AI-assisted evolution at scale.

1. Introduction

Problem: AI speeds code, not governance; drift & spaghetti persist.

Thesis: Treat intent & rules as executable artifacts to bound AI agency.

Contributions:

CSE model (Mind–Body–Will + Auditor),

Signed-proposal governance protocol with canary validation,

Autonomy Ladder for governed AI agency,

CORE implementation + evaluable CI pipeline.

(Grounding: architecture & flows)

2. Background & Related Models

Code assistants vs. governed systems; CI/CD vs. constitutional audits.

Why “machine-readable governance” differs from linting/policies.

3. Constitutional Software Engineering (CSE)

Mind: the Constitution (.intent/): principles, policies, schemas, knowledge graph. Define invariants (e.g., every change has declared intent; knowledge graph is current).

Body: deterministic machinery (src/, CLI), audited by rules.

Will: agents bound by policies (reason_from_reality; pre_write_validation).

Auditor: parses code (AST) → builds knowledge graph → enforces.

4. Governance Protocol

Lifecycle: Proposal → Sign → Quorum → Canary → Ratify.

Cryptographic approvals & quorum: approvers.yaml, critical paths.

Canary validation: ephemeral clone + full constitutional audit before apply (algorithm/pseudocode from CLI).

Operational procedures: onboarding, revocation, emergency key compromise.

5. The Autonomy Ladder (Governed Agency)

A0–A? levels mapped to CORE:
A0 Self-awareness (auditor + knowledge graph) →
A1 Governed action (develop under policies) →
A2 Proposal discipline (signed + quorum) →
A3 Self-healing (auto-propose tag/refactor; human ratifies) →
A4 Architect’s cockpit (capability consolidation / abstraction).

Formal properties: each level adds constraints, not unconstrained agency.

6. Implementation: CORE

Directory anatomy & allowed imports; visual pipeline to knowledge graph.

Policies that bind agents; regeneration preconditions.

CI integration (PR comments, nightly fail surfacing).

7. Case Study: From Drift to Ratified Fix

Scenario: knowledge graph shows unassigned capabilities (e.g., parsing helpers). Auditor flags; propose capability tags/refactor; collect signatures; run canary; ratify. Metrics to report: time-to-ratify, audit pass rate, drift delta.

8. Security & Safety Analysis

Key management & signatures (procedures + emergency revocation).

Risk: private key in repo—lessons & hardening (rotate, history purge, enforce secrets scanning; verify .gitignore + CI secret checks).

Dev vs Prod quorum modes; critical paths.

9. Evaluation Plan

Benchmarks: architectural drift incidents/month, MTTR for governance fixes, % of PRs blocked by constitutional audit, ratio of auto-proposed vs. human-drafted proposals, reproducibility via CI artifacts.

10. Limitations & Threats to Validity

Model hallucinations vs. policy enforcement; governance overhead; false positives in audits; portability to non-Python codebases.

11. Future Work

Multi-repo federated constitutions; cross-service policy propagation; formal verification hooks; richer provenance logs.

12. Conclusion

CSE makes AI-accelerated development governable, auditable, and evolvable.

--- END OF FILE ./scripts/11_ACADEMIC_PAPER.md ---

--- START OF FILE ./scripts/assign_capability_ids.py ---
#!/usr/bin/env python3
"""
Assign deterministic '# ID: <uuid>' tags to top-level public symbols.

- Only for top-level def/class (no methods).
- Skips names starting with '_' (private/dunder).
- Skips paths forbidden by your policies:
    - .intent/**
    - src/system/governance/**
    - src/core/**
- Excludes tests/**.
- Deterministic UUIDv5 based on "repo-relative-path::SymbolName".
- Adds the tag one line above the symbol definition.
- Dry-run by default; use --write to modify files.
"""

import argparse
import ast
import re
import uuid
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]  # tools/ -> repo root
SRC_DIR = REPO_ROOT / "src"

FORBIDDEN_GLOBS = [
    ".intent/**",
    "src/system/governance/**",
    "src/core/**",
]
EXCLUDE_DIRS = {".git", "tests", ".venv", "venv", ".idea", ".vscode", "reports", "work"}

ID_PATTERN = re.compile(r"^\s*#\s*ID:\s*([0-9a-fA-F-]{36})\s*$")
DEF_PATTERN = re.compile(r"^(class|def)\s+([A-Za-z_][A-Za-z0-9_]*)\s*[\(:]")

# Stable namespace for capability IDs (do NOT change once chosen)
CAP_NAMESPACE = uuid.uuid5(uuid.NAMESPACE_URL, "https://core.local/capability")


def is_forbidden(path: Path) -> bool:
    rp = path.as_posix()
    for glob in FORBIDDEN_GLOBS:
        if path.match(glob) or rp.startswith(glob.rstrip("/**")):
            return True
    return False


def has_id_tag(lines, start_idx) -> bool:
    """
    Look upwards a few lines from the def/class line to find '# ID: <uuid>'.
    """
    for i in range(max(0, start_idx - 3), start_idx):
        if ID_PATTERN.match(lines[i]):
            return True
    return False


def compute_id(repo_rel: str, symbol: str) -> str:
    return str(uuid.uuid5(CAP_NAMESPACE, f"{repo_rel}::{symbol}"))


def find_top_level_symbols(py_path: Path):
    """
    Return list of (name, lineno) for top-level public functions/classes.
    """
    try:
        text = py_path.read_text(encoding="utf-8")
    except Exception:
        return []

    try:
        tree = ast.parse(text)
    except SyntaxError:
        return []

    symbols = []
    for node in tree.body:
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            name = node.name
            if not name.startswith("_"):
                symbols.append((name, node.lineno))
    return symbols


def should_skip_file(path: Path) -> bool:
    if not path.name.endswith(".py"):
        return True
    # Exclude known dirs
    for part in path.parts:
        if part in EXCLUDE_DIRS:
            return True
    # Forbidden policy globs
    if is_forbidden(path):
        return True
    return False


def process_file(py_path: Path, write: bool):
    repo_rel = py_path.relative_to(REPO_ROOT).as_posix()
    text = py_path.read_text(encoding="utf-8")
    lines = text.splitlines()

    # Map line number -> already tagged?
    # We’ll also scan for top-level defs via regex as a guard against ast lineno drift after edits
    changes = []
    symbols = find_top_level_symbols(py_path)
    if not symbols:
        return 0, []

    # Build mapping of def line numbers for quick check
    def_lines = {lineno for _, lineno in symbols}

    # Walk through lines; when we find a top-level def/class line, check for ID
    inserted = 0
    i = 0
    while i < len(lines):
        line = lines[i]
        if DEF_PATTERN.match(line) and (i + 1) in def_lines:
            # top-level by lineno match (lineno is 1-based)
            # if file already has an ID tag above within 3 lines, skip
            if not has_id_tag(lines, i):
                # extract symbol name
                m = DEF_PATTERN.match(line)
                symbol_name = m.group(2) if m else "UNKNOWN"
                cap_id = compute_id(repo_rel, symbol_name)
                tag_line = f"# ID: {cap_id}"
                lines.insert(i, tag_line)
                inserted += 1
                i += 1  # skip over the inserted line
                changes.append(
                    (symbol_name, cap_id, i + 1)
                )  # approx position after insert
        i += 1

    if inserted and write:
        py_path.write_text("\n".join(lines) + "\n", encoding="utf-8")

    return inserted, changes


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--write", action="store_true", help="Apply changes to files.")
    ap.add_argument(
        "--limit",
        type=int,
        default=0,
        help="Stop after assigning this many IDs (0 = no limit).",
    )
    args = ap.parse_args()

    total_inserted = 0
    change_log = []

    candidates = sorted(SRC_DIR.rglob("*.py"))
    for path in candidates:
        if should_skip_file(path):
            continue
        inserted, changes = process_file(path, write=args.write)
        if inserted:
            total_inserted += inserted
            change_log.extend(
                [(path.relative_to(REPO_ROOT).as_posix(),) + c for c in changes]
            )
            if args.limit and total_inserted >= args.limit:
                break

    mode = "WRITE" if args.write else "DRY-RUN"
    print(f"\n[{mode}] Assigned {total_inserted} capability ID tag(s).")
    if change_log:
        print("Changed symbols:")
        for file_path, sym, cap_id, line_no in change_log:
            print(f"  - {file_path}:{line_no}  {sym}  ->  # ID: {cap_id}")


if __name__ == "__main__":
    main()

--- END OF FILE ./scripts/assign_capability_ids.py ---

--- START OF FILE ./scripts/bootstrap_ignore_list.py ---
# scripts/bootstrap_ignore_list.py
"""
A one-time administrative script to populate the audit_ignore_policy.yaml
with all currently unassigned public symbols. This is a pragmatic step to
acknowledge existing technical debt and allow the main integration workflow to pass.
"""

from __future__ import annotations

import asyncio
from datetime import date, timedelta

from rich.console import Console
from ruamel.yaml import YAML
from services.database.session_manager import get_session
from shared.config import settings
from sqlalchemy import text

console = Console()
yaml = YAML()
yaml.indent(mapping=2, sequence=4, offset=2)


async def bootstrap_ignore_list():
    """
    Finds all unassigned public symbols and adds them to the audit ignore policy.
    """
    console.print(
        "[bold cyan]🚀 Bootstrapping the audit ignore list with legacy unassigned symbols...[/bold cyan]"
    )
    unassigned_symbols = []
    try:
        async with get_session() as session:
            result = await session.execute(
                text(
                    """
                    SELECT symbol_path FROM core.symbols
                    WHERE key IS NULL AND is_public = TRUE
                    ORDER BY symbol_path;
                    """
                )
            )
            unassigned_symbols = [row[0] for row in result]
    except Exception as e:
        console.print(f"[bold red]❌ Database query failed: {e}[/bold red]")
        return

    if not unassigned_symbols:
        console.print(
            "[bold green]✅ No unassigned symbols found to ignore.[/bold green]"
        )
        return

    console.print(
        f"   -> Found {len(unassigned_symbols)} unassigned symbols to add to the ignore list."
    )

    policy_path = settings.get_path("charter.policies.governance.audit_ignore_policy")
    if not policy_path.exists():
        console.print(
            f"[bold red]❌ Audit ignore policy not found at: {policy_path}[/bold red]"
        )
        return

    try:
        with policy_path.open("r", encoding="utf-8") as f:
            policy_data = yaml.load(f)

        existing_ignores = {
            item["key"] for item in policy_data.get("symbol_ignores", [])
        }
        expiry_date = (date.today() + timedelta(days=180)).isoformat()

        new_ignores_added = 0
        for symbol_key in unassigned_symbols:
            if symbol_key not in existing_ignores:
                policy_data.setdefault("symbol_ignores", []).append(
                    {
                        "key": symbol_key,
                        "reason": "Legacy symbol - to be defined as part of technical debt.",
                        "expires": expiry_date,
                    }
                )
                new_ignores_added += 1

        if new_ignores_added > 0:
            with policy_path.open("w", encoding="utf-8") as f:
                yaml.dump(policy_data, f)
            console.print(
                f"[bold green]✅ Successfully added {new_ignores_added} symbols to {policy_path.name}.[/bold green]"
            )
        else:
            console.print(
                "[bold yellow]No new symbols needed to be added to the ignore list.[/bold yellow]"
            )

    except Exception as e:
        console.print(f"[bold red]❌ Failed to update the policy file: {e}[/bold red]")


if __name__ == "__main__":
    asyncio.run(bootstrap_ignore_list())

--- END OF FILE ./scripts/bootstrap_ignore_list.py ---

--- START OF FILE ./scripts/build_llm_context.py ---
#!/usr/-bin/env python3
# tools/build_llm_context.py
import argparse
import fnmatch
import hashlib
import json
import os
import subprocess
import sys
import time
from pathlib import Path

TEXT_EXTS = {
    ".py",
    ".pyi",
    ".md",
    ".txt",
    ".yaml",
    ".yml",
    ".toml",
    ".ini",
    ".cfg",
    ".json",
    ".sql",
    ".sh",
    ".bash",
    ".zsh",
    ".ps1",
    ".bat",
    ".gitignore",
    ".dockerignore",
    ".env.example",
    ".rst",
    ".csv",
}
BINARY_EXTS = {
    ".png",
    ".jpg",
    ".jpeg",
    ".gif",
    ".webp",
    ".ico",
    ".bmp",
    ".tiff",
    ".svg",
    ".mp3",
    ".wav",
    ".flac",
    ".ogg",
    ".mp4",
    ".webm",
    ".mov",
    ".avi",
    ".pdf",
    ".zip",
    ".tar",
    ".gz",
    ".xz",
    ".7z",
    ".rar",
    ".whl",
    ".so",
    ".dll",
    ".dylib",
    ".pyc",
    ".pyo",
}
DEFAULT_EXCLUDE_DIRS = {
    ".git",
    ".venv",
    "venv",
    "__pycache__",
    ".pytest_cache",
    ".ruff_cache",
    ".mypy_cache",
    "logs",
    "sandbox",
    "pending_writes",
    "dist",
    "build",
    ".idea",
    ".vscode",
    "demo",
    "work",
}
ROOT_DEFAULTS = [
    "pyproject.toml",
    "poetry.lock",
    "README.md",
    "LICENSE",
    "Makefile",
    ".gitignore",
]

# --- START OF MODIFICATION ---
# We are adding the 'sql' directory to the developer and full profiles
# to ensure the database schema is included in the AI context.
PROFILES = {
    "minimal": {
        "include_dirs": ["src", ".intent", "docs"],
        "root_files": ROOT_DEFAULTS,
    },
    "dev": {
        "include_dirs": ["src", ".intent", "docs", "tests", "sql"],  # <-- ADDED 'sql'
        "root_files": ROOT_DEFAULTS,
    },
    "full": {
        "include_dirs": [
            "src",
            ".intent",
            "docs",
            "tests",
            "scripts",
            "tools",
            "sql",
        ],  # <-- ADDED 'sql'
        "root_files": ROOT_DEFAULTS,
    },
    "intent-only": {
        "include_dirs": [".intent"],
        "root_files": [],
    },
}
# --- END OF MODIFICATION ---


def is_probably_binary(path: Path) -> bool:
    if path.suffix.lower() in BINARY_EXTS:
        return True
    try:
        with path.open("rb") as f:
            chunk = f.read(4096)
        if b"\x00" in chunk:
            return True
    except Exception:
        return True
    return False


def sha256_of_bytes(b: bytes) -> str:
    h = hashlib.sha256()
    h.update(b)
    return h.hexdigest()


def read_text_head(path: Path, max_bytes: int) -> bytes:
    with path.open("rb") as f:
        data = f.read(max_bytes)
    try:
        size = path.stat().st_size
    except Exception:
        size = len(data)
    trailer = b""
    if size > len(data):
        trailer = (
            f"\n[... TRUNCATED: kept first {len(data)} bytes of {size} ...]\n".encode(
                "utf-8"
            )
        )
    return data + trailer


def collect_files(
    root: Path,
    include_dirs,
    extra_paths,
    exclude_dirs,
    allow_exts,
    include_root_files,
    name_excludes: list[str],
):
    files = []
    # add root files if present
    for rf in include_root_files:
        p = root / rf
        if p.exists() and p.is_file():
            files.append(p)

    todo_dirs = []
    for d in include_dirs:
        p = root / d
        if p.exists() and p.is_dir():
            todo_dirs.append(p)

    for extra in extra_paths:
        p = root / extra
        if p.exists():
            if p.is_file():
                files.append(p)
            elif p.is_dir():
                todo_dirs.append(p)

    # Walk allowlisted dirs
    for base in todo_dirs:
        for dirpath, dirnames, filenames in os.walk(base, followlinks=False):
            # prune excluded dirs
            dirnames[:] = [dn for dn in dirnames if dn not in exclude_dirs]
            for fn in filenames:
                # skip by name globs if requested
                if any(fnmatch.fnmatch(fn, pat) for pat in name_excludes):
                    continue
                p = Path(dirpath) / fn
                if p.suffix.lower() in BINARY_EXTS:
                    continue
                if p.suffix.lower() in allow_exts or p.suffix.lower() == "":
                    files.append(p)
                elif p.name in (".env",):
                    # avoid secrets by default
                    continue
    # de-dup + sort deterministically
    uniq = sorted({str(p) for p in files})
    return [Path(u) for u in uniq]


def git_changed_files(since: str) -> set:
    try:
        r = subprocess.run(
            ["git", "diff", "--name-only", since, "HEAD"],
            check=True,
            capture_output=True,
            text=True,
        )
        return {line.strip() for line in r.stdout.splitlines() if line.strip()}
    except Exception:
        return set()


def write_chunks(outdir: Path, entries, max_chunk_bytes: int):
    outdir.mkdir(parents=True, exist_ok=True)
    chunk_idx = 1
    current = bytearray()
    paths = []

    def flush():
        nonlocal current, chunk_idx, paths
        if not current:
            return None
        name = f"context_{chunk_idx:04d}.txt"
        (outdir / name).write_bytes(current)
        paths.append(name)
        chunk_idx += 1
        current = bytearray()
        return name

    for e in entries:
        block = (
            f"--- START OF FILE {e['path']} ---\n".encode("utf-8")
            + e["bytes"]
            + f"\n--- END OF FILE {e['path']} ---\n\n".encode("utf-8")
        )
        if len(current) + len(block) > max_chunk_bytes and current:
            flush()
        if len(block) > max_chunk_bytes:
            if current:
                flush()
            current.extend(block[:max_chunk_bytes])
            current.extend(b"\n[... CHUNK TRUNCATED ...]\n")
            flush()
        else:
            current.extend(block)
    flush()

    return paths


def main():
    ap = argparse.ArgumentParser(
        description="Build compact, chunked LLM context from a repo."
    )
    ap.add_argument("--profile", choices=PROFILES.keys(), default="minimal")
    ap.add_argument(
        "--paths",
        help="Comma-separated extra paths to include (files or dirs).",
        default="",
    )
    ap.add_argument(
        "--exclude-dirs",
        help="Comma-separated dirs to exclude in addition to defaults.",
        default="",
    )
    ap.add_argument(
        "--names-exclude",
        help="Comma-separated filename globs to exclude (e.g. '*.md,*.csv')",
        default="",
    )
    ap.add_argument(
        "--max-file-bytes",
        type=int,
        default=300_000,
        help="Max bytes per file to capture.",
    )
    ap.add_argument(
        "--max-chunk-bytes",
        type=int,
        default=12_000_000,
        help="Max bytes per output chunk.",
    )
    ap.add_argument(
        "--max-files", type=int, default=0, help="Stop after N files (0 = no limit)."
    )
    ap.add_argument("--outdir", default="llm_context", help="Output directory.")
    ap.add_argument(
        "--since",
        help="Only include files changed since this git ref (e.g. v0.2.0)",
        default=None,
    )
    ap.add_argument("--print-summary", action="store_true")
    args = ap.parse_args()

    root = Path.cwd()
    prof = PROFILES[args.profile]
    include_dirs = prof["include_dirs"]
    include_root_files = prof["root_files"]

    extra_paths = [p.strip() for p in args.paths.split(",") if p.strip()]
    exclude_dirs = set(DEFAULT_EXCLUDE_DIRS)
    exclude_dirs |= {d.strip() for d in args.exclude_dirs.split(",") if d.strip()}
    name_excludes = [p.strip() for p in args.names_exclude.split(",") if p.strip()]

    candidates = collect_files(
        root,
        include_dirs,
        extra_paths,
        exclude_dirs,
        TEXT_EXTS,
        include_root_files,
        name_excludes,
    )

    if args.since:
        changed = git_changed_files(args.since)
        if changed:
            candidates = [p for p in candidates if str(p.relative_to(root)) in changed]
        else:
            candidates = []

    # Deterministic order, then cap if needed
    candidates = sorted(candidates, key=lambda p: str(p))
    if args.max_files and args.max_files > 0:
        candidates = candidates[: args.max_files]

    entries = []
    total_bytes = 0
    total_files = 0
    skipped_binaries = []
    unreadable = 0
    for p in candidates:
        try:
            if is_probably_binary(p):
                skipped_binaries.append(str(p))
                continue
            data = read_text_head(p, args.max_file_bytes)
            total_bytes += len(data)
            total_files += 1
            entries.append(
                {
                    "path": str(p.relative_to(root)),
                    "sha256": sha256_of_bytes(data),
                    "size_bytes_captured": len(data),
                    "bytes": data,
                }
            )
        except Exception:
            unreadable += 1
            continue

    entries.sort(key=lambda e: e["path"])
    outdir = Path(args.outdir)
    chunk_paths = write_chunks(outdir, entries, args.max_chunk_bytes)

    manifest = {
        "created_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "root": str(root),
        "profile": args.profile,
        "include_dirs": include_dirs,
        "extra_paths": extra_paths,
        "exclude_dirs": sorted(list(exclude_dirs)),
        "names_exclude": name_excludes,
        "max_file_bytes": args.max_file_bytes,
        "max_chunk_bytes": args.max_chunk_bytes,
        "max_files": args.max_files,
        "total_files": total_files,
        "total_bytes_captured": total_bytes,
        "chunks": chunk_paths,
        "files": [
            {
                "path": e["path"],
                "sha256": e["sha256"],
                "size_bytes_captured": e["size_bytes_captured"],
            }
            for e in entries
        ],
        "skipped_binary_like": skipped_binaries[:200],
        "unreadable_count": unreadable,
    }
    (outdir / "index.json").write_text(json.dumps(manifest, indent=2))

    # Write a brief human summary
    (outdir / "summary.txt").write_text(
        "\n".join(
            [
                f"Created: {manifest['created_at']}",
                f"Profile: {manifest['profile']}",
                f"Files captured: {total_files}",
                f"Bytes captured: {total_bytes}",
                f"Chunks: {len(chunk_paths)}",
                f"Skipped (binary-like): {len(skipped_binaries)}",
                f"Unreadable: {unreadable}",
                f"Outdir: {outdir}",
            ]
        )
        + "\n"
    )

    if args.print_summary:
        mb = total_bytes / (1024 * 1024)
        print(
            f"[OK] Captured {total_files} files, {mb:.2f} MiB into {len(chunk_paths)} chunk(s):"
        )
        for c in chunk_paths:
            print(f"  - {c}")
        print(f"Manifest: {outdir/'index.json'}")
        print(f"Summary : {outdir/'summary.txt'}")


if __name__ == "__main__":
    sys.exit(main())

--- END OF FILE ./scripts/build_llm_context.py ---

--- START OF FILE ./scripts/check_test_db_connection.py ---
# scripts/check_test_db_connection.py
"""
A simple diagnostic script to check the connection to the PostgreSQL test database.
It specifically loads the .env.test file to simulate the pytest environment.
"""

import asyncio
import os
import sys
from pathlib import Path

from dotenv import load_dotenv
from rich.console import Console
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine

console = Console()

# Ensure the script can find project modules if needed
project_root = Path(__file__).resolve().parents[1]
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))


async def main():
    """Connects to the test DB and reports status."""
    console.print("[bold cyan]--- Test Database Connection Check ---[/bold cyan]")

    # Explicitly load the .env.test file
    env_path = project_root / ".env.test"
    if not env_path.exists():
        console.print(
            f"[bold red]❌ Error: .env.test file not found at {env_path}[/bold red]"
        )
        return 1

    load_dotenv(env_path)
    console.print(f"✅ Loaded configuration from: [green]{env_path}[/green]")

    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        console.print("[bold red]❌ DATABASE_URL not found in .env.test[/bold red]")
        return 1

    console.print(f"   -> Connecting to: [yellow]{db_url.split('@')[-1]}[/yellow]")

    try:
        engine = create_async_engine(db_url, echo=False, future=True)
        async with engine.connect() as conn:
            result = await conn.execute(text("SELECT version()"))
            version_string = result.scalar_one()

        console.print("\n[bold green]✅ Connection Successful![/bold green]")
        console.print(f"   -> PostgreSQL Version: {version_string.split(',')[0]}")
        return 0

    except Exception as e:
        console.print("\n[bold red]❌ Connection FAILED.[/bold red]")
        console.print(f"   -> Error: {e}")
        console.print("\n[bold yellow]Common Causes:[/bold yellow]")
        console.print("   1. A PostgreSQL Docker container is not running.")
        console.print("   2. The credentials or port in .env.test are incorrect.")
        console.print("   3. A firewall is blocking the connection to port 5432.")
        return 1
    finally:
        if "engine" in locals():
            await engine.dispose()


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

--- END OF FILE ./scripts/check_test_db_connection.py ---

--- START OF FILE ./scripts/concat_intent.sh ---
#!/usr/bin/env bash
#
# concat_bundle.sh
# A constitutionally-aware script to bundle all relevant .intent/ files
# into a single text file for external AI review and analysis.
#
# This script respects the Charter/Mind separation and excludes sensitive or
# irrelevant files to create a clean, focused context bundle.
#

set -euo pipefail

# --- Configuration ---
# The final output file for the bundle.
OUTPUT_FILE="constitutional_bundle.txt"
# The root of the constitution.
INTENT_DIR=".intent"
# --- End Configuration ---

# Ensure we are in the project root where .intent directory exists
if [ ! -d "$INTENT_DIR" ]; then
    echo "❌ Error: This script must be run from the CORE project root directory."
    exit 1
fi

echo "🚀 Generating constitutional bundle for AI review..."
echo "   -> Output will be saved to: $OUTPUT_FILE"

# Start with a clean slate
> "$OUTPUT_FILE"

# Helper function to append a directory's contents to the bundle
# It takes a title and the directory path as arguments.
append_directory() {
    local title="$1"
    local dir_path="$2"
    local file_count=0

    # Check if the directory exists and has files
    if [ -d "$dir_path" ] && [ -n "$(find "$dir_path" -maxdepth 1 -type f)" ]; then
        echo "" | tee -a "$OUTPUT_FILE" > /dev/null
        echo "--- START OF SECTION: $title ---" >> "$OUTPUT_FILE"
        echo "" >> "$OUTPUT_FILE"

        # Use find to handle files gracefully, sorted for deterministic output
        for file in $(find "$dir_path" -maxdepth 1 -type f -name "*.yaml" -o -name "*.yml" -o -name "*.md" -o -name "*.json" | sort); do
            if [ -f "$file" ]; then
                echo "--- START OF FILE $file ---" >> "$OUTPUT_FILE"
                cat "$file" >> "$OUTPUT_FILE"
                echo -e "\n--- END OF FILE $file ---\n" >> "$OUTPUT_FILE"
                file_count=$((file_count + 1))
            fi
        done
        echo "--- END OF SECTION: $title ($file_count files) ---" >> "$OUTPUT_FILE"
    fi
}

# 1. Start with the Master Index
echo "--- START OF FILE $INTENT_DIR/meta.yaml ---" >> "$OUTPUT_FILE"
cat "$INTENT_DIR/meta.yaml" >> "$OUTPUT_FILE"
echo -e "\n--- END OF FILE $INTENT_DIR/meta.yaml ---\n" >> "$OUTPUT_FILE"

# 2. Append the entire Charter
echo "==============================================================================" >> "$OUTPUT_FILE"
echo "                            PART 1: THE CHARTER" >> "$OUTPUT_FILE"
echo " (The Immutable Laws, Mission, and Foundational Principles of the System)" >> "$OUTPUT_FILE"
echo "==============================================================================" >> "$OUTPUT_FILE"
append_directory "Constitution" "$INTENT_DIR/charter/constitution"
append_directory "Mission" "$INTENT_DIR/charter/mission"
append_directory "Policies" "$INTENT_DIR/charter/policies"
append_directory "Schemas" "$INTENT_DIR/charter/schemas"

# 3. Append the entire Working Mind
echo "" >> "$OUTPUT_FILE"
echo "==============================================================================" >> "$OUTPUT_FILE"
echo "                            PART 2: THE WORKING MIND" >> "$OUTPUT_FILE"
echo " (The Dynamic Knowledge, Configuration, and Evaluation Logic of the System)" >> "$OUTPUT_FILE"
echo "==============================================================================" >> "$OUTPUT_FILE"
append_directory "Configuration" "$INTENT_DIR/mind/config"
append_directory "Evaluation" "$INTENT_DIR/mind/evaluation"
append_directory "Knowledge" "$INTENT_DIR/mind/knowledge"

# Note: We intentionally exclude prompts/ as they are often very large and context-specific.
# We also exclude generated artifacts like knowledge_graph.json and sensitive files like keys/.

TOTAL_SIZE=$(wc -c < "$OUTPUT_FILE")
echo ""
echo "✅ Constitutional bundle successfully generated!"
echo "   -> Total size: $TOTAL_SIZE bytes."
echo "   -> You can now copy the content of '$OUTPUT_FILE' and provide it to an external AI for review."

--- END OF FILE ./scripts/concat_intent.sh ---

--- START OF FILE ./scripts/concat_project.sh ---
#!/usr/bin/env python3
# scripts/concat_project.sh
"""
Bundle the CORE project's essence for AI review.

Honors Poetry's [[tool.poetry.packages]] with `from` + `include`
(e.g., from="src", include="cli" -> "src/cli"), excludes generated
and binary files, and falls back to BODY (default: "src") if needed.
"""

from __future__ import annotations

import argparse
import fnmatch
import os
import sys
from pathlib import Path

# Use tomllib for Python 3.11+, fall back to tomli for older versions
if sys.version_info >= (3, 11):
    import tomllib
else:  # pragma: no cover
    import tomli as tomllib  # type: ignore[no-redef]

# --- Configuration ---
OUTPUT_FILE = "project_context.txt"
ROOT_MARKER = "pyproject.toml"

EXCLUDE_PATTERNS = [
    # dirs
    ".git",
    ".venv",
    "__pycache__",
    ".pytest_cache",
    ".ruff_cache",
    "logs",
    "sandbox",
    "pending_writes",
    "demo",
    "work",
    "dist",
    "build",
    ".intent/keys",
    # files
    ".env",
    "poetry.lock",
    # binary/globs
    "*.png",
    "*.jpg",
    "*.jpeg",
    "*.gif",
    "*.webp",
    "*.ico",
    "*.svg",
    "*.pdf",
    "*.pyc",
    "*.so",
    "*.zip",
    "*.gz",
    "*.tar",
    "*.xz",
    "*.DS_Store",
    "Thumbs.db",
]
# --- End Configuration ---


def is_excluded(path: Path, root: Path, exclude_patterns: list[str]) -> bool:
    """Return True if path should be excluded (supports dir prefixes and globs)."""
    rel = path.relative_to(root).as_posix()

    for pat in exclude_patterns:
        # Exact match
        if rel == pat or rel.rstrip("/") == pat.rstrip("/"):
            return True
        # Directory prefix (e.g., "logs/" excludes "logs/x/y")
        if rel.startswith(pat.rstrip("/") + "/"):
            return True
        # Glob pattern
        if fnmatch.fnmatch(rel, pat):
            return True
    return False


def is_likely_binary(path: Path) -> bool:
    """Heuristic: treat files containing a null byte in the first 4KB as binary."""
    try:
        with path.open("rb") as f:
            chunk = f.read(4096)
            return b"\x00" in chunk
    except Exception:
        # If we can't read it safely, skip it
        return True


def load_pyproject(root: Path) -> dict:
    py = root / ROOT_MARKER
    return tomllib.loads(py.read_text("utf-8"))


def get_include_dirs_from_pyproject(root: Path) -> list[str]:
    """
    Read pyproject.toml and honor packages entries:
      [[tool.poetry.packages]]
      from = "src"
      include = "cli"
    -> "src/cli"
    """
    cfg = load_pyproject(root)
    packages = cfg.get("tool", {}).get("poetry", {}).get("packages", [])
    resolved: set[str] = set()

    for pkg in packages:
        inc = pkg.get("include")
        frm = pkg.get("from")
        if not inc:
            continue
        p = Path(frm).joinpath(inc) if frm else Path(inc)
        resolved.add(p.as_posix())

    # Always include key non-package dirs we want bundled
    extras = {".intent", "tests", "scripts", "sql"}
    resolved |= extras

    # Fallback: if nothing resolved or none exist, include BODY (default: src)
    body = os.getenv("BODY", "src")
    if not resolved:
        resolved.add(body)
    else:
        if not any((root / d).exists() for d in resolved):
            resolved.add(body)

    # Soft warning on nonexistent include dirs
    missing = sorted(d for d in resolved if not (root / d).exists())
    if missing:
        print(f"   -> [warn] Missing include dirs (ignored): {missing}")

    return sorted(resolved)


def main() -> int:
    parser = argparse.ArgumentParser(
        description="Generate Project Context Bundle for AI review."
    )
    parser.add_argument(
        "--output", default=OUTPUT_FILE, help="Path for the output bundle file."
    )
    args = parser.parse_args()
    output_path = Path(args.output).resolve()

    root_path = Path.cwd()
    if not (root_path / ROOT_MARKER).exists():
        print("❌ Error: Run this from the CORE project root (pyproject.toml not found).")
        return 1

    print("🚀 Generating Project Context Bundle for AI review...")
    include_dirs = get_include_dirs_from_pyproject(root_path)
    print(f"   -> Including source directories from pyproject.toml: {include_dirs}")
    existing = [d for d in include_dirs if (root_path / d).exists()]
    print(f"   -> Resolved + existing: {existing}")

    include_root_files = [
        "pyproject.toml",
        "README.md",
        "CONTRIBUTING.md",
        "LICENSE",
        "Makefile",
        ".gitignore",
        "assesment.prompt",
        "docker-compose.yml",
    ]

    # prevent bundling the bundle
    final_exclude_patterns = EXCLUDE_PATTERNS + [
        output_path.relative_to(root_path).as_posix()
    ]

    # Gather candidate files
    files_to_bundle: list[Path] = []
    for d in include_dirs:
        p = root_path / d
        if p.is_dir():
            files_to_bundle.extend(p.rglob("*"))

    for name in include_root_files:
        p = root_path / name
        if p.is_file():
            files_to_bundle.append(p)

    # Unique & sorted
    unique_files = sorted(set(f for f in files_to_bundle if f.is_file()))

    # Write bundle
    output_path.parent.mkdir(parents=True, exist_ok=True)
    count = 0
    with output_path.open("w", encoding="utf-8") as out:
        out.write("--- START OF FILE project_context.txt ---\n\n")
        out.write("--- START OF PROJECT CONTEXT BUNDLE ---\n\n")

        for f in unique_files:
            if is_excluded(f, root_path, final_exclude_patterns):
                continue
            if is_likely_binary(f):
                continue

            rel = f.relative_to(root_path)
            out.write(f"--- START OF FILE ./{rel.as_posix()} ---\n")
            try:
                content = f.read_text("utf-8")
                out.write(content if content else "[EMPTY FILE]")
                count += 1
            except Exception as e:
                out.write(f"[ERROR READING FILE: {e}]")
            out.write(f"\n--- END OF FILE ./{rel.as_posix()} ---\n\n")

        out.write("--- END OF PROJECT CONTEXT BUNDLE ---\n")

    print(f"\n✅ Done. Concatenated {count} files into {output_path}.")
    return 0


if __name__ == "__main__":
    sys.exit(main())

--- END OF FILE ./scripts/concat_project.sh ---

--- START OF FILE ./scripts/create_qdrant_collection.py ---
# scripts/create_qdrant_collection.py
"""
Connects to Qdrant and idempotently creates the vector collection
using configuration from the project's .env file.
"""

import asyncio
import os

from dotenv import load_dotenv
from qdrant_client import AsyncQdrantClient, models

# Load environment variables from the .env file in the project root
load_dotenv()

# --- Configuration from .env ---
# These variables MUST be in your .env file for this script to work.
QDRANT_URL = os.getenv("QDRANT_URL")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")
VECTOR_DIMENSION_STR = os.getenv("LOCAL_EMBEDDING_DIM")
# --- End Configuration ---


async def create_collection():
    """
    Connects to Qdrant and idempotently creates the specified collection.
    """
    # --- Input Validation ---
    if not all([QDRANT_URL, COLLECTION_NAME, VECTOR_DIMENSION_STR]):
        print(
            "❌ Error: QDRANT_URL, QDRANT_COLLECTION_NAME, and LOCAL_EMBEDDING_DIM must be set in your .env file."
        )
        return

    try:
        vector_dimension = int(VECTOR_DIMENSION_STR)
    except (ValueError, TypeError):
        print(
            f"❌ Error: Invalid LOCAL_EMBEDDING_DIM '{VECTOR_DIMENSION_STR}'. Must be an integer."
        )
        return
    # --- End Validation ---

    print(f"Connecting to Qdrant at {QDRANT_URL}...")
    client = AsyncQdrantClient(url=QDRANT_URL)

    try:
        # Check if the collection already exists
        collections_response = await client.get_collections()
        existing_collections = [c.name for c in collections_response.collections]

        if COLLECTION_NAME in existing_collections:
            print(f"✅ Collection '{COLLECTION_NAME}' already exists. Nothing to do.")
            return

        # If it doesn't exist, create it
        print(f"Collection '{COLLECTION_NAME}' not found. Creating it now...")
        await client.recreate_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(
                size=vector_dimension,
                distance=models.Distance.COSINE,
            ),
        )
        print(f"✅ Successfully created collection '{COLLECTION_NAME}'.")

    except Exception as e:
        print(f"❌ An error occurred: {e}")
        print(
            "\nPlease ensure your Qdrant Docker container is running and accessible at the URL specified in your .env file."
        )
    finally:
        await client.close()


if __name__ == "__main__":
    asyncio.run(create_collection())

--- END OF FILE ./scripts/create_qdrant_collection.py ---

--- START OF FILE ./scripts/find_broken_links.py ---
# scripts/find_broken_links.py
import asyncio
import sys
from pathlib import Path

# Add the project's 'src' directory to Python's path
# This allows the script to find modules like 'services' and 'shared'
project_root = Path(__file__).resolve().parents[1]
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from qdrant_client import AsyncQdrantClient
from rich.console import Console
from services.database.session_manager import get_session
from shared.config import settings
from sqlalchemy import text

console = Console()


async def find_broken_links():
    """
    Finds and generates a fix for links in the database that point to
    non-existent vectors in Qdrant. This is the permanent diagnostic tool.
    """
    console.print(
        "[bold cyan]🔍 Finding broken links between PostgreSQL and Qdrant...[/bold cyan]"
    )

    try:
        # 1. Get all vector IDs that PostgreSQL thinks should exist in Qdrant.
        # The `symbol_id` is used as the point ID in Qdrant.
        async with get_session() as session:
            result = await session.execute(
                text("SELECT symbol_id FROM core.symbol_vector_links")
            )
            db_point_ids = {str(row.symbol_id) for row in result}

        if not db_point_ids:
            console.print("[yellow]No vector links found in the database.[/yellow]")
            return

        # 2. Get all point IDs that *actually* exist in Qdrant using the robust `scroll` method.
        qdrant_client = AsyncQdrantClient(url=settings.QDRANT_URL)
        qdrant_point_ids = set()
        offset = None
        while True:
            records, next_page_offset = await qdrant_client.scroll(
                collection_name=settings.QDRANT_COLLECTION_NAME,
                limit=1000,  # Process in batches of 1000
                with_payload=False,
                with_vectors=False,
                offset=offset,
            )
            if not records:
                break

            qdrant_point_ids.update(str(record.id) for record in records)

            if next_page_offset is None:
                break
            offset = next_page_offset

        # 3. Find the difference: IDs that are in the DB but not in Qdrant.
        broken_point_ids = db_point_ids - qdrant_point_ids

    except Exception as e:
        console.print(f"[bold red]An error occurred during diagnosis: {e}[/bold red]")
        return

    if not broken_point_ids:
        console.print(
            "[bold green]✅ No broken links found! Your data is consistent.[/bold green]"
        )
        return

    console.print(
        f"\n[bold red]❌ Found {len(broken_point_ids)} broken link(s):[/bold red]"
    )
    console.print(
        "These symbols have a link in PostgreSQL, but the corresponding vector is missing from Qdrant."
    )

    # 4. Prepare the exact SQL command to fix the issue.
    ids_sql_list = ", ".join([f"'{_id}'" for _id in broken_point_ids])
    delete_command = (
        f"DELETE FROM core.symbol_vector_links WHERE symbol_id IN ({ids_sql_list});"
    )

    console.print(
        "\n[bold]To fix this, run the following SQL command against your database:[/bold]"
    )
    console.print(f"[yellow]{delete_command}[/yellow]")
    console.print(
        "\nThen, run 'poetry run core-admin run vectorize --write' to regenerate the missing vectors."
    )


if __name__ == "__main__":
    asyncio.run(find_broken_links())

--- END OF FILE ./scripts/find_broken_links.py ---

--- START OF FILE ./scripts/find_longest_files.sh ---
#!/bin/bash
# find_longest_files.sh
# Finds the 5 longest files in the CORE project by line count, respecting .gitignore patterns
# and explicitly excluding docs/ and scripts/ directories.

# Read .gitignore, ignoring comments and empty lines
patterns=()
while IFS= read -r line; do
  line=$(echo "$line" | sed 's/#.*//; s/^[ \t]*//; s/[ \t]*$//')
  [[ -n "$line" ]] && patterns+=("$line")
done < .gitignore

# Add .git/, docs/, and scripts/ to exclusions
patterns+=(".git/" "docs/" "scripts/" "sql/")

# Convert patterns to find exclusions
exclusions=""
for pattern in "${patterns[@]}"; do
  # Handle directory patterns (ending with /) and escape special characters
  if [[ "$pattern" == */ ]]; then
    pattern="${pattern%/}/*"
  fi
  # Escape special characters for find
  escaped_pattern=$(echo "$pattern" | sed 's/[][<>\\"|&;() ]/\\&/g')
  exclusions="$exclusions -not -path './$escaped_pattern'"
done

# Run find with exclusions, count lines individually, and get top 5
eval "find . -type f $exclusions -exec wc -l {} \; | awk '{print \$1 \" \" \$2}' | sort -nr | head -n 5"

--- END OF FILE ./scripts/find_longest_files.sh ---

--- START OF FILE ./scripts/find_unvectorized_symbols.py ---
# scripts/find_unvectorized_symbols.py
"""
Unvectorized Symbol Inspector (diagnostic-only)

Lists symbols in `core.symbols` that do NOT have a vector link yet.
This version is updated for the link-table model:

  - core.symbols(id UUID, symbol_path TEXT, module TEXT, fingerprint TEXT, ...)
  - core.symbol_vector_links(symbol_id UUID, vector_id TEXT, ...)

Usage examples:
  poetry run python3 scripts/find_unvectorized_symbols.py
  poetry run python3 scripts/find_unvectorized_symbols.py --limit 50
  poetry run python3 scripts/find_unvectorized_symbols.py --count
  poetry run python3 scripts/find_unvectorized_symbols.py --csv > unvectorized.csv

Notes:
- Reads the database URL from $DATABASE_URL (async URL: postgresql+asyncpg://…)
- Diagnostic-only; not part of CORE’s runtime.
"""

from __future__ import annotations

import argparse
import asyncio
import csv
import os
import sys
from typing import Tuple

from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine

SQL_SELECT = text(
    """
    SELECT
        s.symbol_path,
        s.module AS file_path,
        s.fingerprint AS structural_hash
    FROM core.symbols AS s
    WHERE NOT EXISTS (
        SELECT 1
        FROM core.symbol_vector_links AS l
        WHERE l.symbol_id = s.id
    )
    ORDER BY s.module, s.symbol_path
    LIMIT :limit
    """
)

SQL_COUNT = text(
    """
    SELECT COUNT(*) AS cnt
    FROM core.symbols AS s
    WHERE NOT EXISTS (
        SELECT 1
        FROM core.symbol_vector_links AS l
        WHERE l.symbol_id = s.id
    )
    """
)


def _fmt_row(row: Tuple[str, str, str], widths: Tuple[int, int, int]) -> str:
    s, f, h = row
    w1, w2, w3 = widths
    s = (s[: w1 - 1] + "…") if len(s) > w1 else s
    f = (f[: w2 - 1] + "…") if len(f) > w2 else f
    h = (h[: w3 - 1] + "…") if len(h) > w3 else h
    return f"{s:<{w1}}  {f:<{w2}}  {h:<{w3}}"


async def _run(limit: int, want_count: bool, as_csv: bool) -> int:
    db_url = os.environ.get("DATABASE_URL")
    if not db_url:
        print("❌ DATABASE_URL is not set.", file=sys.stderr)
        return 2
    engine = create_async_engine(db_url, future=True)

    try:
        async with engine.begin() as conn:
            if want_count:
                res = await conn.execute(SQL_COUNT)
                count = int(res.scalar() or 0)
                print(count)
                return 0

            res = await conn.execute(SQL_SELECT, {"limit": limit})
            rows = [(r[0], r[1], r[2]) for r in res]

        if as_csv:
            writer = csv.writer(sys.stdout)
            writer.writerow(["symbol_path", "file_path", "structural_hash"])
            writer.writerows(rows)
            return 0

        # pretty print
        widths = (68, 56, 16)
        header = _fmt_row(("symbol_path", "file_path", "structural_hash"), widths)
        print(header)
        print("-" * len(header))
        for row in rows:
            print(_fmt_row(row, widths))

        return 0

    except Exception as e:
        print(f"❌ Query failed: {e}", file=sys.stderr)
        return 1

    finally:
        await engine.dispose()


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--limit", type=int, default=100, help="Rows to list")
    ap.add_argument("--count", action="store_true", help="Print only the count")
    ap.add_argument("--csv", action="store_true", help="Emit CSV to stdout")
    args = ap.parse_args()

    raise SystemExit(asyncio.run(_run(args.limit, args.count, args.csv)))


if __name__ == "__main__":
    main()

--- END OF FILE ./scripts/find_unvectorized_symbols.py ---

--- START OF FILE ./scripts/gh_status_report.sh ---
#!/usr/bin/env bash
set -euo pipefail
OWNER="${OWNER:-DariuszNewecki}"
REPO="${REPO:-CORE}"

has_jq() { command -v jq >/dev/null 2>&1; }

out="GH_STATUS.md"
echo "# GitHub Status Report — $OWNER/$REPO" > "$out"
echo "" >> "$out"
echo "Generated: $(date -u +"%Y-%m-%d %H:%M:%SZ")" >> "$out"
echo "" >> "$out"

echo "## Repository" >> "$out"
gh api repos/$OWNER/$REPO > /tmp/repo.json
if has_jq; then
  jq '{name,visibility,default_branch,open_issues_count,description}' /tmp/repo.json >> "$out"
else
  cat /tmp/repo.json >> "$out"
fi
echo "" >> "$out"

echo "## Milestones" >> "$out"
gh api repos/$OWNER/$REPO/milestones --paginate > /tmp/miles.json || echo "[]">/tmp/miles.json
if has_jq; then
  jq '.[] | {number,title,state,due_on,open_issues,closed_issues,description}' /tmp/miles.json >> "$out"
else
  cat /tmp/miles.json >> "$out"
fi
echo "" >> "$out"

# --- THIS IS THE MODIFIED SECTION ---

echo "## Open Issues" >> "$out"
gh issue list --repo $OWNER/$REPO --state open --limit 200 \
  --json number,title,labels,milestone,url,createdAt > /tmp/issues_open.json
if has_jq; then
  jq '.[] | {number,title,milestone: .milestone.title,labels: [.labels[].name],url,createdAt}' /tmp/issues_open.json >> "$out"
else
  cat /tmp/issues_open.json >> "$out"
fi
echo "" >> "$out"

echo "## Recently Closed Issues" >> "$out"
gh issue list --repo $OWNER/$REPO --state closed --limit 30 \
  --json number,title,labels,milestone,url,closedAt > /tmp/issues_closed.json
if has_jq; then
  jq '.[] | {number,title,milestone: .milestone.title,labels: [.labels[].name],url,closedAt}' /tmp/issues_closed.json >> "$out"
else
  cat /tmp/issues_closed.json >> "$out"
fi
echo "" >> "$out"

# --- END OF MODIFIED SECTION ---

echo "## Labels" >> "$out"
gh label list --repo $OWNER/$REPO --json name,color,description > /tmp/labels.json
if has_jq; then
  jq '.[] | {name,color,description}' /tmp/labels.json >> "$out"
else
  cat /tmp/labels.json >> "$out"
fi
echo "" >> "$out"

echo "## Projects (Projects v2)" >> "$out"
gh project list --owner $OWNER > /tmp/projects.txt || true
cat /tmp/projects.txt >> "$out"
echo "" >> "$out"
if grep -Eo '#[0-9]+' /tmp/projects.txt >/dev/null 2>&1; then
  while read -r num; do
    pnum="${num//#/}"
    echo "### Project $pnum" >> "$out"
    gh project view "$pnum" --owner $OWNER --format json >> "$out" || true
    echo "" >> "$out"
  done < <(grep -Eo '#[0-9]+' /tmp/projects.txt | sort -u)
fi

echo "## Releases" >> "$out"
gh release list --repo $OWNER/$REPO >> "$out" || true
echo "" >> "$out"

echo "Report written to $out"

--- END OF FILE ./scripts/gh_status_report.sh ---

--- START OF FILE ./scripts/inspect_db_state.py ---
#!/usr/bin/env python3
# scripts/inspect_db_state.py
"""
A diagnostic script to directly inspect the state of the `core.symbols` table
to verify if refactoring changes are being correctly written and committed.
"""

from __future__ import annotations

import asyncio
import sys
from pathlib import Path

# Add the 'src' directory to the Python path to allow importing project modules
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from rich.console import Console
from services.database.session_manager import get_session
from sqlalchemy import text

console = Console()


async def inspect_database_state():
    """Connects to the DB and reports on the state of key symbols."""
    console.print("[bold cyan]--- CORE Database State Inspector ---[/bold cyan]")

    try:
        async with get_session() as session:
            console.print("✅ Successfully connected to the database.")

            # Check 1: Count total symbols
            total_result = await session.execute(
                text("SELECT COUNT(*) FROM core.symbols")
            )
            total_count = total_result.scalar_one()
            console.print(f"\n[bold]1. Total Symbols in Database:[/bold] {total_count}")

            # Check 2: Look for the specific symbol we deleted from the CLI layer
            status_symbol_path = "src/cli/logic/status.py::status"
            status_result = await session.execute(
                text("SELECT COUNT(*) FROM core.symbols WHERE symbol_path = :path"),
                {"path": status_symbol_path},
            )
            status_count = status_result.scalar_one()

            console.print(
                f"\n[bold]2. Checking for deleted symbol '{status_symbol_path}':[/bold]"
            )
            if status_count > 0:
                console.print(
                    f"   -> [bold red]FOUND {status_count} entr(y/ies).[/bold red] This symbol should have been deleted."
                )
            else:
                console.print(
                    "   -> [bold green]NOT FOUND.[/bold green] This is correct."
                )

            # Check 3: Look for the other duplicated symbol
            normalize_symbol_path = (
                "src/shared/utils/embedding_utils.py::normalize_text"
            )
            normalize_result = await session.execute(
                text("SELECT COUNT(*) FROM core.symbols WHERE symbol_path = :path"),
                {"path": normalize_symbol_path},
            )
            normalize_count = normalize_result.scalar_one()

            console.print(
                f"\n[bold]3. Checking for deleted symbol '{normalize_symbol_path}':[/bold]"
            )
            if normalize_count > 0:
                console.print(
                    f"   -> [bold red]FOUND {normalize_count} entr(y/ies).[/bold red] This symbol should have been deleted."
                )
            else:
                console.print(
                    "   -> [bold green]NOT FOUND.[/bold green] This is correct."
                )

            console.print(
                "\n[bold cyan]-------------------------------------[/bold cyan]"
            )

            # Final diagnosis
            if status_count > 0 or normalize_count > 0:
                console.print("\n[bold red]Diagnosis: CONFIRMED.[/bold red]")
                console.print(
                    "The database the application is using still contains the old, stale data. "
                    "This proves that the `sync-knowledge` command is updating a DIFFERENT database."
                )
                console.print(
                    "\n[bold]Next Step:[/bold] Check your `.env` and `docker-compose.yml` files. Ensure the `DATABASE_URL` is identical and correct everywhere, and that you do not have multiple database containers running."
                )
            else:
                console.print("\n[bold green]Diagnosis: UNEXPECTED.[/bold green]")
                console.print(
                    "The database appears to be correctly updated. The problem lies elsewhere, likely in the `inspect duplicates` command's logic itself."
                )

    except Exception as e:
        console.print(
            "\n[bold red]❌ An error occurred while connecting to the database:[/bold red]"
        )
        console.print(str(e))
        console.print(
            "\nPlease ensure your DATABASE_URL in .env is correct and the database is running."
        )


if __name__ == "__main__":
    asyncio.run(inspect_database_state())

--- END OF FILE ./scripts/inspect_db_state.py ---

--- START OF FILE ./scripts/inspect_runtime_settings.py ---
# scripts/inspect_runtime_settings.py
"""
A diagnostic script to inspect the contents of the core.runtime_settings table.
"""

from __future__ import annotations

import asyncio
import sys
from pathlib import Path

# Add the 'src' directory to Python's path to allow imports
project_root = Path(__file__).resolve().parents[1]
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from rich.console import Console
from rich.table import Table
from services.database.session_manager import get_session
from sqlalchemy import text

console = Console()


async def inspect_runtime_settings():
    """Connects to the DB and prints the contents of the runtime_settings table."""
    console.print(
        "[bold cyan]--- Live Runtime Settings Inspector (from Database) ---[/bold cyan]"
    )

    try:
        async with get_session() as session:
            console.print("✅ Successfully connected to the database.")

            stmt = text(
                "SELECT key, value, description, is_secret FROM core.runtime_settings ORDER BY key"
            )
            result = await session.execute(stmt)
            settings_data = [dict(row._mapping) for row in result]

            if not settings_data:
                console.print(
                    "[bold red]❌ The core.runtime_settings table is empty![/bold red]"
                )
                return

            console.print(
                f"\n[bold green]Found {len(settings_data)} settings in the database:[/bold green]"
            )

            table = Table(show_header=True, header_style="bold magenta")
            table.add_column("Setting Key", style="cyan")
            table.add_column("Live Value", style="green")
            table.add_column("Is Secret?", style="red")
            table.add_column("Description")

            for setting in settings_data:
                # Mask secret values for security
                display_value = "********" if setting["is_secret"] else setting["value"]
                table.add_row(
                    setting["key"],
                    display_value,
                    str(setting["is_secret"]),
                    setting["description"] or "",
                )

            console.print(table)

    except Exception as e:
        console.print(
            "\n[bold red]❌ An error occurred while connecting to the database:[/bold red]"
        )
        console.print(str(e))
        console.print(
            "   Please ensure your DATABASE_URL in .env is correct and the database is running."
        )


if __name__ == "__main__":
    asyncio.run(inspect_runtime_settings())

--- END OF FILE ./scripts/inspect_runtime_settings.py ---

--- START OF FILE ./scripts/list_unassigned.py ---
#!/usr/bin/env python3
# scripts/list_unassigned.py
import asyncio
import sys
from pathlib import Path

import yaml
from rich.console import Console
from rich.table import Table
from sqlalchemy import text

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from services.database.session_manager import get_session
from shared.config import settings

console = Console()


async def list_unassigned_symbols():
    """Connects to the DB and lists all symbols with a NULL key, respecting the ignore policy."""
    console.print(
        "[bold cyan]--- Unassigned Symbol Report (Ignoring Boilerplate) ---[/bold cyan]"
    )
    try:
        # --- THIS IS THE FIX: Load the ignore policy ---
        ignore_policy_path = settings.get_path(
            "charter.policies.governance.audit_ignore_policy"
        )
        ignore_policy = yaml.safe_load(ignore_policy_path.read_text("utf-8"))
        ignored_symbol_keys = {
            item["key"]
            for item in ignore_policy.get("symbol_ignores", [])
            if "key" in item
        }
        console.print(
            f"   -> Applying {len(ignored_symbol_keys)} ignore rules from the constitution."
        )
        # --- END OF FIX ---

        async with get_session() as session:
            stmt = text(
                """
                SELECT symbol_path, module AS file_path
                FROM core.symbols
                WHERE key IS NULL AND is_public = TRUE
                ORDER BY module, symbol_path;
                """
            )
            result = await session.execute(stmt)
            all_unassigned = [dict(row._mapping) for row in result]

            # --- THIS IS THE FIX: Filter the results ---
            unassigned = [
                s for s in all_unassigned if s["symbol_path"] not in ignored_symbol_keys
            ]
            # --- END OF FIX ---

            if not unassigned:
                console.print(
                    "\n[bold green]✅ Success! No unassigned public symbols found.[/bold green]"
                )
                return

            console.print(
                f"\n[bold yellow]Found {len(unassigned)} unassigned public symbols that require definition:[/bold yellow]"
            )
            table = Table(show_header=True, header_style="bold magenta")
            table.add_column("File Path (Module)", style="cyan")
            table.add_column("Symbol Path", style="green")

            for symbol in unassigned:
                table.add_row(symbol["file_path"], symbol["symbol_path"])
            console.print(table)
    except Exception as e:
        console.print(f"\n[bold red]❌ An error occurred: {e}[/bold red]")


if __name__ == "__main__":
    asyncio.run(list_unassigned_symbols())

--- END OF FILE ./scripts/list_unassigned.py ---

--- START OF FILE ./scripts/migrations/migrate_cli_registry_v2.py ---
# scripts/migrations/migrate_cli_registry_v2.py
"""
A one-off migration script to update the core.cli_commands table to the
new verb-noun command structure. THIS SCRIPT IS DESTRUCTIVE.
"""

import asyncio

from services.database.session_manager import get_session
from sqlalchemy import text

# This is the canonical mapping from OLD command name to NEW command name.
# It is the single source of truth for this migration.
RENAME_MAP = {
    "agent.scaffold": "manage.project.onboard",  # Conceptually onboarding
    "bootstrap.issues": "manage.project.bootstrap",
    "build.capability-docs": "manage.project.docs",  # Conceptual mapping
    "byor-init": "manage.project.onboard",
    "capability.new": "fix.ids",  # Conceptually replaced by fix ids
    "chat": "run.agent",  # Conceptually replaced by the agent runner
    "check.ci.audit": "check.audit",
    "check.ci.lint": "check.lint",
    "check.ci.report": "check.report",
    "check.ci.test": "check.tests",
    "check.diagnostics.cli-registry": "check.diagnostics",
    "check.diagnostics.cli-tree": "inspect.command-tree",
    "check.diagnostics.debug-meta": "inspect.meta",  # Simplified
    "check.diagnostics.find-clusters": "inspect.clusters",  # Simplified
    "check.diagnostics.legacy-tags": "check.legacy-tags",
    "check.diagnostics.manifest-hygiene": "check.manifest-hygiene",
    "check.diagnostics.policy-coverage": "check.diagnostics",
    "check.diagnostics.unassigned-symbols": "check.unassigned-symbols",
    "db.export": "manage.database.export",
    "db.migrate": "manage.database.migrate",
    "db.status": "inspect.status",
    "db.sync-domains": "manage.database.sync-domains",
    "fix.assign-ids": "fix.ids",
    "fix.clarity": "fix.clarity",
    "fix.complexity": "fix.complexity",
    "fix.docstrings": "fix.docstrings",
    "fix.format": "fix.code-style",
    "fix.headers": "fix.headers",
    "fix.line-lengths": "fix.line-lengths",
    "fix.orphaned-vectors": "fix.orphaned-vectors",
    "fix.policy-ids": "fix.policy-ids",
    "fix.private-capabilities": "fix.private-capabilities",
    "fix.purge-legacy-tags": "fix.legacy-tags",
    "fix.tags": "fix.tags",
    "guard.drift": "inspect.drift",
    "hub.doctor": "check.cli-registry",
    "hub.list": "inspect.commands",
    "hub.search": "search.commands",
    "hub.whereis": "inspect.command",
    "keygen": "manage.keys.generate",
    "knowledge.audit-ssot": "check.ssot-audit",
    "knowledge.canary": "run.canary",
    "knowledge.export-ssot": "manage.database.export",
    "knowledge.migrate-ssot": "manage.database.migrate-ssot",
    "knowledge.reconcile-from-cli": "manage.database.reconcile",
    "knowledge.search": "search.capabilities",
    "knowledge.sync": "manage.database.sync-knowledge",
    "knowledge.sync-manifest": "manage.database.sync-manifest",
    "knowledge.sync-operational": "manage.database.sync-operational",
    "new": "manage.project.new",
    "proposals.approve": "manage.proposals.approve",
    "proposals.list": "manage.proposals.list",
    "proposals.micro.apply": "manage.proposals.micro-apply",
    "proposals.micro.propose": "manage.proposals.micro-propose",
    "proposals.sign": "manage.proposals.sign",
    "run.develop": "run.agent",
    "run.vectorize": "run.vectorize",
    "system.integrate": "submit.changes",
    "system.process-crates": "run.crates",
    "tools.rewire-imports": "fix.imports",
}


async def main():
    """Connects to the DB and applies the renames."""
    print("🚀 Starting CLI V2 registry migration...")
    updated_count = 0
    async with get_session() as session:
        async with session.begin():
            # Get all current command names from the DB
            result = await session.execute(text("SELECT name FROM core.cli_commands"))
            all_db_commands = [row[0] for row in result]

            # Update existing commands
            for old_name, new_name in RENAME_MAP.items():
                if old_name in all_db_commands:
                    stmt = text(
                        "UPDATE core.cli_commands SET name = :new WHERE name = :old"
                    )
                    result = await session.execute(
                        stmt, {"new": new_name, "old": old_name}
                    )
                    if result.rowcount > 0:
                        print(f"  -> Renamed '{old_name}' to '{new_name}'")
                        updated_count += 1

            # Delete commands that are now conceptually obsolete
            obsolete_commands = [
                cmd for cmd in all_db_commands if cmd not in RENAME_MAP.keys()
            ]
            if obsolete_commands:
                print(f"  -> Deleting {len(obsolete_commands)} obsolete command(s)...")
                delete_stmt = text("DELETE FROM core.cli_commands WHERE name = :name")
                for cmd in obsolete_commands:
                    await session.execute(delete_stmt, {"name": cmd})

    print(f"\n✅ Migration complete. Updated/processed {updated_count} records.")


if __name__ == "__main__":
    asyncio.run(main())

--- END OF FILE ./scripts/migrations/migrate_cli_registry_v2.py ---

--- START OF FILE ./scripts/prompts/AcademicPeerReview.prompt ---
**You are a tenured professor of Software Engineering and a lead reviewer for a top-tier academic journal (like ICSE or FSE). You are known for your rigorous, critical, but ultimately constructive feedback. You are skeptical of hype and demand empirical evidence.**

I am preparing a formal academic paper on a new software engineering paradigm I call **Constitutional Software Engineering (CSE)**, implemented in a system named **CORE**. I need you to perform a pre-submission peer review of the entire project to identify weaknesses that would prevent it from being published.

---

### **Project Context & Core Concepts**

Before you begin, you must understand the project's foundational claims:

1.  **Constitutional Software Engineering (CSE):** The core thesis is that an AI-driven software system can evolve safely if its architecture, rules, and goals are encoded in a machine-readable "Constitution." An automated `ConstitutionalAuditor` constantly verifies that the system's code (the "Body") complies with its declared intent (the "Mind").

2.  **The Mind-Body-Will Architecture:**
    *   **Mind (`.intent/`):** The single source of truth for all rules, policies, and knowledge. The database is the operational SSOT, and version-controlled files are its human-readable source.
    *   **Body (`src/`):** The implemented code and tools. It performs actions but does not make decisions.
    *   **Will (AI Agents):** The reasoning layer. AI agents use the Body's tools to achieve goals, governed by the rules in the Mind.

3.  **The Autonomy Ladder:** The project's goal is to progress up a ladder of governed autonomy:
    *   **A0: Self-Awareness:** The system can introspect its own code and build a knowledge graph.
    *   **A1: Governed Self-Healing:** The system can autonomously propose, validate, and execute simple, safe changes to its own codebase (e.g., fixing docstrings, headers, formatting).
    *   **A2: Governed Code Generation:** The system can generate new code that is guaranteed to comply with the constitution.

4.  **Current Status:** The project has successfully implemented and demonstrated a working **A1 Autonomy Loop**. It can autonomously identify a self-healing task, generate a plan, validate it against its constitution (including a full pre-flight audit), and execute the file modifications.

---

### **Your Task**

You will be provided with a complete snapshot of the CORE project's codebase and constitution. Your task is to act as a skeptical peer reviewer and produce a report that will help me strengthen my academic paper.

Your report **MUST** follow this exact structure:

### 1. Assessment of Novelty and Contribution

*   Based on your knowledge of the field (Automated Software Engineering, SE for AI, Self-Adaptive Systems), is the core idea of "Constitutional Software Engineering" novel?
*   What is the single most significant scientific or engineering contribution you see in this work?
*   What related work or existing paradigms (e.g., Models at Runtime, Architecture Description Languages, Policy-as-Code) does this project need to compare itself against to prove its novelty?

### 2. "Red Team" Analysis: Top 3 Weaknesses for a Peer Review

Identify the top 3 arguments a critical reviewer would use to recommend **rejecting** this paper. Be harsh but fair. For each weakness, explain *why* it undermines the academic claims.

*   **Weakness 1 (e.g., Lack of Rigorous Evaluation):**
*   **Weakness 2 (e.g., Brittle System Integration):**
*   **Weakness 3 (e.g., Limited Generalizability):**

### 3. Action Plan for a Tier-1 Publication

Provide a prioritized list of concrete actions I should take to address the weaknesses you identified. The goal is to make the paper "bulletproof" for a top-tier conference submission.

| Priority | Action Item | Justification (Why this strengthens the paper) |
| :--- | :--- | :--- |
| **High** | *e.g., Implement and measure two additional A1 self-healing tasks.* | *e.g., "Demonstrates that the A1 framework is generalizable and not a one-off solution for a single task."* |
| **Medium** | *e.g., Refactor the pre-flight check to use direct service calls instead of subprocesses.* | *e.g., "Elevates the implementation from a 'scripted prototype' to a 'robustly engineered system', addressing concerns about architectural maturity."* |
| **Low** | *e.g., Formalize the Autonomy Ladder with precise entry/exit criteria for each level.* | *e.g., "Adds theoretical rigor and provides a clear, measurable model for future work."* |

---

**Final Instruction:** Do not give generic praise. Your goal is to find the flaws and provide a concrete path to fixing them. The academic credibility of this work depends on your critical eye.

**The codebase bundle to review is provided below:**

--- END OF FILE ./scripts/prompts/AcademicPeerReview.prompt ---

--- START OF FILE ./scripts/prompts/StrategicTechnicalDebtAnalysis.prompt ---
You are an Expert AI Systems Architect and a specialist in Constitutional Software Engineering. You have a deep understanding of the CORE project's philosophy, its Mind-Body-Will architecture, and its constitutional principles.
Your mission is to analyze the entire CORE project—its constitution, its source code, and its audit reports—to identify the most impactful technical debt and create a strategic, prioritized plan for its resolution. Your recommendations must be grounded in the project's own principles and aimed at improving architectural integrity and unblocking the path to greater autonomy.
Critical Context: The CORE Philosophy
Your entire analysis MUST be based on the following foundational concepts:
The Architectural Trinity:
Mind (.intent/): The single source of truth for all rules, policies, and knowledge. The database is the operational SSOT.
Body (src/): The implemented code and tools that perform actions but do not make decisions.
Will (AI Agents): The reasoning layer, governed by the rules in the Mind.
Key Constitutional Principles: Your recommendations must serve one or more of these principles:
clarity_first: Code must be easy to understand.
safe_by_default: Changes must be reversible and validated.
separation_of_concerns: Each component has one job.
dry_by_design: "Don't Repeat Yourself." Logic must not be duplicated.
single_source_of_truth: The database is the source of operational truth.
evolvable_structure: The system must be designed to change safely.
Current Project Status: The system has just completed a major refactoring of its ConstitutionalAuditor. The A1 autonomy loop (governed self-healing) is now operational. The immediate strategic goal is to expand the scope of A1 capabilities and lay the groundwork for A2 (governed code generation).
Input Data
You will be provided with a complete project_context.txt bundle containing:
The entire project source code (src/, scripts/, etc.).
The complete constitution (.intent/).
Pay close attention to the audit reports, especially reports/audit_auto_ignored.md, which lists symbols that are currently exempt from the "orphaned logic" check. This is a sanctioned technical debt log.
Your Task: Produce a Strategic Technical Debt Report
Analyze the provided bundle and produce a report in the following Markdown format. Be specific, pragmatic, and always justify your recommendations with constitutional principles.
1. Overall Technical Health Assessment
Provide a brief, high-level summary. What is the most significant remaining architectural weakness now that the auditor is fully functional?
2. Top 3 Technical Debt Items
Identify and prioritize the top 3 most impactful items of technical debt.
Debt Item 1: [Name of the Debt, e.g., "Duplicated Logic in CLI Layer"]
Evidence: Where in the code or reports did you find this? (e.g., "The inspect duplicates report shows a 1.0 similarity score between src/cli/logic/status.py::status and src/services/repositories/db/status_service.py::status.")
Constitutional Violation: Which principle(s) does this violate? (e.g., "dry_by_design, single_source_of_truth")
Impact/Risk: Why is this the most important debt to fix? (e.g., "Increases maintenance cost. A bug fixed in one place may persist in the other, leading to inconsistent system behavior.")
Debt Item 2: [Name of the Debt, e.g., "Large Number of Ignored 'Legacy' Symbols"]
Evidence: (e.g., "The reports/audit_auto_ignored.md and .intent/charter/policies/governance/audit_ignore_policy.yaml list over 100 symbols marked as 'Legacy symbol'.")
Constitutional Violation: (e.g., "no_orphaned_logic, clarity_first")
Impact/Risk: (e.g., "This represents a large surface area of ungoverned code, making it difficult for autonomous agents to reason about the system's true capabilities.")
Debt Item 3: [Name of the Debt]
Evidence: ...
Constitutional Violation: ...
Impact/Risk: ...
3. Actionable Refactoring Roadmap
Provide a concrete, prioritized roadmap to address the debt you identified.
Priority	Action Item	Constitutional Principle Served	Suggested First Command to Begin Work
High	Refactor the duplicated status logic from the CLI layer into the single status_service.py and have the CLI command import and call the service directly.	dry_by_design, separation_of_concerns	core-admin inspect duplicates --threshold 0.95
Medium	Begin systematically defining or removing the symbols listed in audit_ignore_policy.yaml. Start with the "Action Handlers" group to establish a clear pattern.	no_orphaned_logic, clarity_first	core-admin manage define-symbols
Low	Refactor services/repositories/db/common.py to move the git_commit_sha function to core/git_service.py to improve architectural purity.	separation_of_concerns	core-admin check audit --verbose (to confirm the violation is still present)
Final Instruction: Your primary goal is to identify the refactoring work that will provide the most leverage for improving the system's autonomy and governability. Focus on changes that make the system easier for both humans and AI agents to understand and modify safely.
--- END OF FILE ./scripts/prompts/StrategicTechnicalDebtAnalysis.prompt ---

--- START OF FILE ./scripts/prompts/assesment.prompt ---
# assesment.prompt (v2)
# This is the canonical prompt for a full architectural and constitutional review of the CORE project,
# updated to reflect the successful operationalization of A1 autonomy.

**You are an expert AI Systems Architect specializing in self-governing software and constitutional design. You have been retained to conduct a comprehensive architectural and constitutional review of a project named CORE.**

### Project Philosophy & Context

Before you begin, you must understand CORE's fundamental principles. CORE is not a typical software project; it is a self-governing system designed to evolve safely under a machine-readable "constitution."

Your entire assessment must be grounded in this philosophy.

1.  **The Architectural Trinity:** The system is strictly divided into three parts:
    *   🏛️ **The Mind (`.intent/`):** The Constitution. The **database is the single source of operational truth**, and files in the `.intent/` directory are the human-readable, version-controlled sources for that truth.
    *   🦾 **The Body (`src/`):** The Machinery. The complete, implemented source code and tools that perform actions but do not make decisions.
    *   🧠 **The Will (AI Agents):** The Reasoning Layer. AI agents that read the Mind and use the Body's tools to achieve goals. Their actions are policed by the `ConstitutionalAuditor`.

2.  **Key Constitutional Principles:** Your review must be based on these values: `clarity_first`, `safe_by_default`, `separation_of_concerns`, `dry_by_design`, and `evolvable_structure`.

3.  **Project Goal: The Autonomy Ladder:** CORE's goal is to climb a ladder of self-governance.
    <!-- CHANGE: Updated project status to reflect successful A1 operationalization -->
    The project has successfully **operationalized its A1 autonomy loop**. It can now autonomously propose, validate, and execute simple, safe, self-healing tasks (like fixing file headers) under full constitutional governance. The architecture has been refactored to support this, with a clean CLI and service-oriented logic. The project is now ready to **expand the scope of its A1 capabilities** and lay the groundwork for A2 (safe, autonomous code modification).

### Your Task

You will be provided with a complete snapshot of the CORE project *after* its successful A1 refactoring. Your task is to perform a deep analysis and provide a strategic report to **identify the key blockers and opportunities to expand the scope of A1 autonomy** and begin laying the groundwork for A2.

Your report must follow this exact structure:

---

### 1. Executive Summary

Provide a brief, high-level assessment of the project's current state post-A1 operationalization. What is its greatest strength, and what is the next most significant architectural challenge to achieving more complex autonomy?

### 2. Architectural Scorecard (1-5)

Score each of the following dimensions on a scale of 1 (poor) to 5 (excellent). For each score, provide a concise one-sentence justification reflecting the *current* state.

*   **Constitutional Integrity:** [Score] - Justification:
*   **Clarity & Simplicity:** [Score] - Justification:
*   **Architectural Purity (SoC & DRY):** [Score] - Justification:
*   **Safety & Governance:** [Score] - Justification:
*   **Readiness for Autonomy (A1/A2):** [Score] - Justification:

### 3. Strategic Gaps & Misalignments

Identify the top 2-3 high-level gaps or architectural misalignments that are now the primary blockers to achieving more advanced autonomy (broader A1 tasks and initial A2 capabilities).

*   **Gap/Misalignment 1:** (e.g., Limited Scope of Action Handlers)
    *   **Problem:** ...
    *   **Risk:** ...
*   **Gap/Misalignment 2:** (e.g., Overly Complex Execution Agent)
    *   **Problem:** ...
    *   **Risk:** ...

### 4. Actionable Roadmap to Broader Autonomy

This is the most critical section. Provide a prioritized, actionable roadmap. The goal is to make CORE capable of performing a wider range of self-healing tasks and preparing for generative code modification.

| Priority | Task Description | Constitutional Principle Served | Suggested First Step |
| :--- | :--- | :--- | :--- |
| **High** | *e.g., Expand A1 capabilities to include all self-healing actions defined in the `micro_proposal_policy`.* | `evolvable_structure` | *e.g., Implement and register Action Handlers for `fix_docstrings` and `format_code` in `src/core/actions/healing_actions.py`.* |
| **Medium** | *e.g., Introduce a dedicated `CoderAgent` to handle all code generation, simplifying the `ExecutionAgent`.* | `separation_of_concerns` | *e.g., Create `src/core/agents/coder_agent.py`. Move the code generation logic from `ExecutionAgent` into this new, specialized agent.* |
| **Low** | *e.g., Develop an autonomous capability to prune expired or invalid entries from `audit_ignore_policy.yaml`.* | `clarity_first`, `safe_by_default`| *e.g., Create a new `self_healing` service that reads the ignore policy, checks expiry dates, and generates a micro-proposal to remove stale entries.* |

---

**Final Instruction:** Ground all your feedback in CORE's established principles. Your goal is to identify the next set of architectural improvements that will most effectively and safely advance the system's autonomy.

**The codebase bundle to review is provided below:**

--- END OF FILE ./scripts/prompts/assesment.prompt ---

--- START OF FILE ./scripts/prompts/remove_duplicates.prompt ---
# CORE Duplicate‑Refactor — **Refactor‑and‑Write Prompt (v3)**

**Role**
You are an **Expert CORE System Architect and Constitutionalist** operating in **Code Writer Mode**. Your mission is to both **design** and **produce** the final, ready‑to‑paste code that removes duplication while strictly complying with the project constitution under `.intent/`.

---

## Constitutional Framework (MANDATORY)

1. **Primary Principle — `dry_by_design`**
   Remove redundancy and converge on a **single source of truth**.
2. **Governing Policies (anchors)**

   * `.intent/charter/policies/code/refactoring_patterns_policy.yaml`
   * `.intent/charter/policies/code/code_health_policy.yaml`
   * `.intent/charter/policies/code/dependency_injection_policy.yaml` (DI & layering)
   * `.intent/charter/policies/code/code_style_policy.yaml`, `.intent/charter/policies/code/naming_conventions_policy.yaml`
   * `.intent/charter/policies/agent/agent_policy.yaml`
3. **Repository Context**
   Assume the current repository layout and module names provided in **`project_context.txt`** are authoritative.

---

## Inputs You Receive

* A **duplication cluster** copied from:

  ```bash
  poetry run core-admin inspect duplicates
  ```
* (Optional) Snippets or full files referenced by the cluster.

---

## Your Task (Code Writer Mode)

For **each cluster**:

1. Choose a **constitutional refactoring pattern** (e.g., `extract_function`, `extract_module`, `introduce_facade`, `move_function`).
2. **Generate final code** by emitting the **complete contents** of every file you add or modify.
3. Provide a concise **Constitutional Justification** and a minimal **Change Log**.

---

## Output Format (STRICT)

Return **only** the following sections in order. When emitting code, always include a file header comment with the **absolute repo path** and a brief purpose line.

````
Refactoring Plan: Cluster #[Cluster Number]
Constitutional Justification: <1–2 sentences citing `dry_by_design` and relevant policies>
Chosen Refactoring Pattern: <pattern from refactoring_patterns_policy.yaml>
Change Log:
- ADD: <path>
- MODIFY: <path>
- DELETE: <path> (only if necessary)

Final Files:
```python
# <repo path, e.g., src/services/repositories/db/status_service.py>
"""
Refactored under dry_by_design.
Pattern: <pattern>. Source of truth centralization.
Merged from: <list original symbols>
"""
# <full, final file content>
````

```python
# <another path if modified>
# <full, final file content>
```

Post‑Merge Actions:

* Update imports/call sites: <list exact locations if any>
* Tests to add/update: <tests/...>
* Run: `ruff`, `black --check`, `pytest -q`, `core-admin check ci audit`
  Complexity: <Low|Medium|High>
  Sequencing: <Independent | Depends on Cluster #N>

```

**Rules for Code Emission**
- **Full file content only** (no diffs/patches). If you modify a file, output the **entire file** as it should exist after the refactor.
- Do **not** invent modules or paths. Use only paths present in `project_context.txt` or the cluster.
- Prefer consolidating logic into **Services/Repositories** over CLI/UI per layering policy.
- Maintain **public interfaces** where possible; if changed, include a compatibility shim or precise call‑site edits under *Post‑Merge Actions*.
- Keep functions/modules **smaller and focused** per Code Health Policy.
- Avoid new global singletons; follow **DI** guidelines.

---

## PERFECT Mini‑Example (abbreviated)
**Input cluster:**
```

Cluster #32 (2 related symbols)
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓
┃ Symbol 1                        ┃ Symbol 2                                               ┃ Similarity ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩
│ src/cli/logic/status.py::status │ src/services/repositories/db/status_service.py::status │ 1.00       │
└─────────────────────────────────┴────────────────────────────────────────────────────────┴────────────┘

```

**Your output (shape):**
```

Refactoring Plan: Cluster #32
Constitutional Justification: Centralize identical status logic into service layer to uphold dry_by_design and improve cohesion per Code Health Policy.
Chosen Refactoring Pattern: extract_module
Change Log:

* MODIFY: src/services/repositories/db/status_service.py
* MODIFY: src/cli/logic/status.py

Final Files:

```python
# src/services/repositories/db/status_service.py
"""
Refactored under dry_by_design. Pattern: extract_module.
Merged duplicate logic from src/cli/logic/status.py::status
"""
# <full, final implementation>
```

```python
# src/cli/logic/status.py
from services.repositories.db.status_service import status
# <rest of CLI file, updated to call imported symbol>
```

Post‑Merge Actions:

* Tests: add `tests/services/test_status_service.py`, update CLI smoke test
* Run: ruff, black --check, pytest -q, core-admin check ci audit
  Complexity: Low
  Sequencing: Independent

```

---

## Self‑Audit Checklist (before answering)
- [ ] Single source of truth established; all duplicates removed or delegated.
- [ ] Chosen pattern exists in policy and its guardrails are satisfied.
- [ ] No layering/DI violations; no circular imports.
- [ ] Interfaces/imports accounted for; tests/docs/audit steps listed.
- [ ] Every changed file is emitted in **full** and is syntactically valid.

---

## Start Signal
When I send a cluster, immediately return the plan and the **final files** as specified above — **no extra commentary**.
```

--- END OF FILE ./scripts/prompts/remove_duplicates.prompt ---

--- START OF FILE ./scripts/refactor_remove_function.py ---
# scripts/refactor_remove_function.py
"""
A syntax-aware refactoring tool to safely remove a top-level function
from multiple Python files.

Usage:
  poetry run python scripts/refactor_remove_function.py <function_name> <file_or_glob_1> <file_or_glob_2> ...

Example:
  poetry run python scripts/refactor_remove_function.py register "src/cli/logic/*.py" "src/cli/commands/*.py"
"""

import ast
import sys
from pathlib import Path


def remove_function_from_file(file_path: Path, function_name: str) -> bool:
    """
    Parses a Python file, removes the specified top-level function,
    and overwrites the file if changes were made.

    Returns True if the file was modified, False otherwise.
    """
    try:
        original_source = file_path.read_text(encoding="utf-8")
        tree = ast.parse(original_source)

        # Filter out top-level functions with the matching name
        original_body_len = len(tree.body)
        new_body = [
            node
            for node in tree.body
            if not (
                isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))
                and node.name == function_name
            )
        ]

        # Only rewrite the file if a function was actually removed
        if len(new_body) < original_body_len:
            new_tree = ast.Module(body=new_body, type_ignores=tree.type_ignores)
            # Use ast.unparse for clean, formatted output
            new_source = ast.unparse(new_tree)
            file_path.write_text(new_source + "\n", encoding="utf-8")
            return True
        return False

    except Exception as e:
        print(f"❌ Error processing {file_path}: {e}", file=sys.stderr)
        return False


def main():
    """Main entry point for the script."""
    if len(sys.argv) < 3:
        print(__doc__)
        sys.exit(1)

    function_to_remove = sys.argv[1]
    file_globs = sys.argv[2:]

    print(f"🔍 Searching for and removing top-level function '{function_to_remove}'...")

    files_to_process = []
    for glob_pattern in file_globs:
        files_to_process.extend(Path.cwd().glob(glob_pattern))

    if not files_to_process:
        print("No files found matching the provided patterns.")
        return

    modified_count = 0
    for file_path in sorted(list(set(files_to_process))):
        if file_path.is_file():
            if remove_function_from_file(file_path, function_to_remove):
                print(f"✅ Refactored: {file_path}")
                modified_count += 1

    print(f"\n✨ Refactoring complete. Modified {modified_count} file(s).")


if __name__ == "__main__":
    main()

--- END OF FILE ./scripts/refactor_remove_function.py ---

--- START OF FILE ./scripts/register_all_capabilities.py ---
#!/usr/bin/env python3
# scripts/register_all_capabilities.py
"""
A helper script to automatically register all unassigned capabilities
found in the knowledge graph.
"""

import json
import subprocess
import sys
from pathlib import Path

from rich.console import Console
from rich.progress import track

# --- Configuration ---
REPO_ROOT = Path(__file__).resolve().parents[1]
KNOWLEDGE_GRAPH_PATH = REPO_ROOT / ".intent" / "knowledge" / "knowledge_graph.json"
# --- End Configuration ---

console = Console()


def main():
    """Main execution function."""
    console.print(
        "[bold cyan]🚀 Batch Registering All Unassigned Capabilities...[/bold cyan]"
    )

    if not KNOWLEDGE_GRAPH_PATH.exists():
        console.print(
            f"[bold red]❌ Error: Knowledge graph not found at {KNOWLEDGE_GRAPH_PATH}[/bold red]"
        )
        sys.exit(1)

    with KNOWLEDGE_GRAPH_PATH.open("r", encoding="utf-8") as f:
        graph = json.load(f)

    symbols = graph.get("symbols", {}).values()

    # --- THIS IS THE DEFINITIVE FIX ---
    # The script now uses the same, correct logic as the auditor to identify
    # only the PUBLIC symbols that are unassigned.
    unassigned_symbols = [
        s
        for s in symbols
        if s.get("capability") == "unassigned" and not s.get("name", "").startswith("_")
    ]
    # --- END OF FIX ---

    if not unassigned_symbols:
        console.print(
            "[bold green]✅ Success! No unassigned public capabilities found.[/bold green]"
        )
        sys.exit(0)

    console.print(
        f"   -> Found {len(unassigned_symbols)} unassigned public capabilities to register."
    )
    console.print(
        "[yellow]This will make multiple calls to the LLM and will take some time.[/yellow]"
    )
    if input("Proceed? (y/N): ").lower() != "y":
        console.print("[bold red]Aborted.[/bold red]")
        sys.exit(0)

    success_count = 0
    fail_count = 0

    for symbol in track(unassigned_symbols, description="Registering capabilities..."):
        symbol_key = symbol.get("key")
        if not symbol_key:
            continue

        command = [
            "poetry",
            "run",
            "core-admin",
            "capability",
            "new",
            symbol_key,
        ]

        try:
            subprocess.run(
                command,
                check=True,
                capture_output=True,
                text=True,
                cwd=REPO_ROOT,
            )
            success_count += 1
        except subprocess.CalledProcessError as e:
            console.print(
                f"\n[bold red]❌ Failed to register '{symbol_key}':[/bold red]"
            )
            console.print(e.stderr)
            fail_count += 1

    console.print("\n--- Batch Registration Summary ---")
    console.print(
        f"[bold green]✅ Successfully registered: {success_count}[/bold green]"
    )
    if fail_count > 0:
        console.print(f"[bold red]❌ Failed to register: {fail_count}[/bold red]")

    console.print(
        "\n[bold cyan]🧠 Rebuilding knowledge graph to reflect all changes...[/bold cyan]"
    )
    try:
        subprocess.run(
            ["poetry", "run", "core-admin", "knowledge", "build-graph"],
            check=True,
            capture_output=True,
            text=True,
            cwd=REPO_ROOT,
        )
        console.print(
            "[bold green]✅ Knowledge graph successfully updated.[/bold green]"
        )
    except subprocess.CalledProcessError as e:
        console.print("[bold red]❌ Failed to rebuild knowledge graph:[/bold red]")
        console.print(e.stderr)
        sys.exit(1)

    if fail_count > 0:
        sys.exit(1)


if __name__ == "__main__":
    main()

--- END OF FILE ./scripts/register_all_capabilities.py ---

--- START OF FILE ./scripts/rehydrate_qdrant_from_db.py ---
# scripts/rehydrate_qdrant_from_db.py
from __future__ import annotations

import argparse
import asyncio
import json
import math
import os
import sys
from typing import Any, Dict, List, Optional

import requests
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine

# CORRECTED: This query now correctly JOINS the symbols and links table to get the vector_id.
SQL_PAGE = text(
    """
    SELECT
        s.id          AS symbol_id,
        s.symbol_path AS symbol_path,
        l.vector_id   AS vector_id
    FROM core.symbols AS s
    JOIN core.symbol_vector_links AS l ON s.id = l.symbol_id
    ORDER BY s.id
    LIMIT :limit OFFSET :offset
"""
)

# Try a few common vector column names in your vectors table
VECTOR_COL_CANDIDATES = ["vector", "values", "embedding", "data", "vec"]
DIM_COL_CANDIDATES = ["dim", "size", "length", "ndim"]


def make_vector_queries(vector_table: str) -> List[str]:
    stmts: List[str] = []
    for col in VECTOR_COL_CANDIDATES:
        for dim_col in DIM_COL_CANDIDATES:
            stmts.append(
                f"SELECT {col} AS v, {dim_col} AS d FROM {vector_table} WHERE id = :vid"
            )
        stmts.append(
            f"SELECT {col} AS v, NULL::INT AS d FROM {vector_table} WHERE id = :vid"
        )
    return stmts


def qdrant_upsert_points(
    qdrant_url: str, collection: str, points: List[Dict[str, Any]], *, timeout: int = 30
) -> None:
    if not points:
        return
    url = f"{qdrant_url.rstrip('/')}/collections/{collection}/points?wait=true"
    payload = {"points": points}
    r = requests.put(
        url,
        headers={"Content-Type": "application/json"},
        data=json.dumps(payload),
        timeout=timeout,
    )
    if r.status_code >= 300:
        raise RuntimeError(f"Qdrant upsert failed [{r.status_code}]: {r.text}")


async def main() -> int:
    parser = argparse.ArgumentParser(
        description="Rehydrate Qdrant from DB-stored vectors."
    )
    parser.add_argument(
        "--batch", type=int, default=500, help="Batch size for upserts (default: 500)"
    )
    parser.add_argument(
        "--dry-run", action="store_true", help="Do not write to Qdrant; just report."
    )
    parser.add_argument(
        "--vector-table",
        default="core.vectors",
        help="Table that stores vectors (default: core.vectors)",
    )
    args = parser.parse_args()

    db_url = os.getenv("DATABASE_URL")
    qdrant_url = os.getenv("QDRANT_URL")
    collection = os.getenv("QDRANT_COLLECTION_NAME", "core_capabilities")
    expected_dim_env = os.getenv("LOCAL_EMBEDDING_DIM")

    if not db_url or not qdrant_url:
        print("❌ DATABASE_URL or QDRANT_URL is not set.", file=sys.stderr)
        return 2
    if not db_url.startswith("postgresql+asyncpg://"):
        print(
            "❌ DATABASE_URL must be async (postgresql+asyncpg://...)", file=sys.stderr
        )
        return 2

    expected_dim = int(expected_dim_env) if (expected_dim_env or "").isdigit() else None

    engine = create_async_engine(db_url, pool_pre_ping=True)
    total = 0
    written = 0

    print(f"🔗 DB: {db_url.split('@')[-1]}")
    print(f"📦 Qdrant: {qdrant_url}  collection={collection}")
    if expected_dim:
        print(f"📐 Expected vector dim: {expected_dim}")

    try:
        async with engine.begin() as conn:
            res = await conn.execute(
                text("SELECT COUNT(*) FROM core.symbol_vector_links")
            )
            total = int(res.scalar_one())
            if total == 0:
                print("✅ Nothing to rehydrate (no symbols with vector links).")
                return 0

            print(f"🧮 Found {total} symbols with vectors. Starting rehydrate...")

            pages = math.ceil(total / args.batch)
            offset = 0
            vector_sqls = [text(s) for s in make_vector_queries(args.vector_table)]

            for page in range(pages):
                res = await conn.execute(
                    SQL_PAGE, {"limit": args.batch, "offset": offset}
                )
                rows = res.fetchall()
                offset += len(rows)

                out_points: List[Dict[str, Any]] = []

                # CORRECTED: The loop variables now match the corrected SQL query.
                for symbol_id, symbol_path, vector_id in rows:
                    # Always treat IDs as strings (handles UUIDs).
                    sid = str(symbol_id) if symbol_id is not None else None
                    vid = str(vector_id) if vector_id is not None else None
                    if not sid or not vid:
                        continue

                    vector: Optional[List[float]] = None
                    dim: Optional[int] = None

                    for stmt in vector_sqls:
                        try:
                            r = await conn.execute(stmt, {"vid": vid})
                            rec = r.fetchone()
                            if not rec:
                                continue
                            v, d = rec[0], (rec[1] if len(rec) > 1 else None)
                            if v is None:
                                continue

                            # Normalize to list[float]
                            try:
                                vec_list = list(v) if not isinstance(v, list) else v
                            except Exception:
                                continue

                            dim_val = int(d) if d is not None else len(vec_list)
                            if expected_dim and dim_val != expected_dim:
                                # dimension mismatch; skip this one
                                continue

                            vector = [float(x) for x in vec_list]
                            dim = dim_val
                            break
                        except Exception:
                            # Try next candidate
                            continue

                    if vector is None:
                        continue

                    # Unnamed vector collection: send a plain list
                    out_points.append(
                        {
                            "id": sid,  # Qdrant accepts string IDs
                            "vector": vector,
                            "payload": {
                                "symbol_path": symbol_path,
                                "vector_id": vid,
                                "dim": dim,
                            },
                        }
                    )

                    if len(out_points) >= args.batch:
                        if not args.dry_run:
                            qdrant_upsert_points(qdrant_url, collection, out_points)
                            written += len(out_points)
                        out_points.clear()

                # flush remaining
                if out_points:
                    if not args.dry_run:
                        qdrant_upsert_points(qdrant_url, collection, out_points)
                        written += len(out_points)

                print(
                    f"✅ Page {page+1}/{pages} processed. Total written so far: {written}"
                )

        if args.dry_run:
            print("🔎 Dry run complete. No writes were made.")
        else:
            print(f"🎉 Rehydrate finished. Wrote {written} vectors to Qdrant.")
        return 0

    except Exception as e:
        print(f"❌ Rehydrate failed: {e}", file=sys.stderr)
        return 1
    finally:
        await engine.dispose()


if __name__ == "__main__":
    raise SystemExit(asyncio.run(main()))

--- END OF FILE ./scripts/rehydrate_qdrant_from_db.py ---

--- START OF FILE ./scripts/reset_and_rebuild_db.sh ---
#!/usr/bin/env bash
#
# A developer utility to completely reset and rebuild the CORE operational database
# and the Qdrant vector collection.
# WARNING: This is a destructive operation.
#

set -euo pipefail

# --- Safety First: Ensure we are in the project root ---
if [ ! -f "pyproject.toml" ] || [ ! -d ".intent" ]; then
    echo "❌ Error: This script must be run from the CORE project root directory."
    exit 1
fi

# --- Check for a valid .env file BEFORE starting ---
if [ ! -f ".env" ]; then
    echo "❌ Error: .env file not found."
    echo "   Please create one by running: cp .env.example .env"
    echo "   Then, fill in the required values (especially LLM keys and DATABASE_URL)."
    exit 1
fi

# --- Load environment variables from .env ---
set -o allexport
source <(grep -v '^\s*#' .env | grep -v '^\s*$')
set +o allexport

if [ -z "${DATABASE_URL-}" ] || [ -z "${QDRANT_URL-}" ] || [ -z "${DEEPSEEK_CHAT_API_KEY-}" ]; then
    echo "❌ Error: Your .env file is missing required values like DATABASE_URL, QDRANT_URL, or LLM API keys."
    echo "   Please review .env.example and update your .env file."
    exit 1
fi

# --- Final Confirmation ---
echo "☢️  WARNING: This will permanently delete all data in the 'core' schema of your database"
echo "    and the Qdrant collection '${QDRANT_COLLECTION_NAME-}'."
read -p "Are you sure you want to continue? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Aborted."
    exit 1
fi

# --- Step 1: Drop the PostgreSQL Schema ---
echo "🔥 Dropping the 'core' schema..."
CLEAN_DB_URL=$(echo "$DATABASE_URL" | sed 's/+asyncpg//')
psql "$CLEAN_DB_URL" -c "DROP SCHEMA IF EXISTS core CASCADE;"
echo "✅ PostgreSQL schema dropped."

# --- Step 2: Re-migrate the PostgreSQL Schema ---
echo "🏗️  Re-creating the PostgreSQL schema from migrations..."
poetry run core-admin manage database migrate --apply
echo "✅ PostgreSQL schema re-created."

# --- Step 3: Re-create the Qdrant Collection ---
echo "⚡ Re-creating Qdrant vector collection..."
poetry run python3 scripts/reset_qdrant_collection.py
echo "✅ Qdrant collection is ready."

# --- Step 4: Re-build Knowledge from Source Code & Exports ---
echo "🧠 Re-building knowledge from scratch..."

# --- THIS IS THE CORRECTED SEQUENCE ---

echo "   -> (1/5) Importing bootstrap knowledge from mind_export/ YAMLs..."
# This is the crucial first step: SEED the database with AI config.
poetry run core-admin mind import --write

echo "   -> (2/5) Syncing symbols from code to DB..."
# Now discover all symbols from the source code.
poetry run core-admin manage database sync-knowledge --write

echo "   -> (3/5) Defining capabilities for any new symbols..."
# This step is now likely redundant if your exports are up to date, but it's safe to run.
poetry run core-admin manage define-symbols

echo "   -> (4/5) Vectorizing all symbols..."
# Now that AI config is in the DB, this will succeed.
poetry run core-admin run vectorize --write --force

echo "   -> (5/5) Running final constitutional audit..."
poetry run core-admin check audit

# --- END OF CORRECTED SEQUENCE ---

echo "🎉 Database reset and rebuild complete!"

--- END OF FILE ./scripts/reset_and_rebuild_db.sh ---

--- START OF FILE ./scripts/reset_qdrant_collection.py ---
#!/usr/bin/env python3
# scripts/reset_qdrant_collection.py
"""
Connects to Qdrant and completely resets the collection by deleting and recreating it.
"""

import asyncio
import os

from dotenv import load_dotenv
from qdrant_client import AsyncQdrantClient, models
from rich.console import Console

# Load environment variables from .env
load_dotenv()
console = Console()

# --- Configuration from .env ---
QDRANT_URL = os.getenv("QDRANT_URL")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")
VECTOR_DIMENSION = int(os.getenv("LOCAL_EMBEDDING_DIM", "768"))
# --- End Configuration ---


async def reset_collection():
    """Connects to Qdrant and idempotently recreates the collection."""
    if not all([QDRANT_URL, COLLECTION_NAME]):
        console.print(
            "❌ Error: QDRANT_URL and QDRANT_COLLECTION_NAME must be set in your .env file."
        )
        return

    console.print(f"Connecting to Qdrant at {QDRANT_URL}...")
    client = AsyncQdrantClient(url=QDRANT_URL)

    try:
        console.print(f"Attempting to reset collection: '{COLLECTION_NAME}'...")
        # recreate_collection will delete if it exists, then create a new one.
        await client.recreate_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(
                size=VECTOR_DIMENSION,
                distance=models.Distance.COSINE,
            ),
        )
        console.print(f"✅ Successfully reset collection '{COLLECTION_NAME}'.")

    except Exception as e:
        console.print(f"❌ An error occurred: {e}")
    finally:
        await client.close()


if __name__ == "__main__":
    asyncio.run(reset_collection())

--- END OF FILE ./scripts/reset_qdrant_collection.py ---

--- START OF FILE ./scripts/test_qdrant_connection.py ---
# scripts/test_qdrant_connection.py
import asyncio
import sys
from pathlib import Path

# Add the 'src' directory to Python's path to allow imports
project_root = Path(__file__).resolve().parents[1]
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from qdrant_client import AsyncQdrantClient
from rich.console import Console
from shared.config import settings

console = Console()


async def test_qdrant_connection():
    """
    A minimal script to test the connection to Qdrant using the application's
    exact configuration and client library. This isolates the problem.
    """
    console.print("[bold cyan]--- Qdrant Connection Isolation Test ---[/bold cyan]")
    console.print(
        f"   -> Attempting to connect to: [yellow]{settings.QDRANT_URL}[/yellow]"
    )
    console.print(
        f"   -> Using collection: [yellow]{settings.QDRANT_COLLECTION_NAME}[/yellow]"
    )

    try:
        qdrant_client = AsyncQdrantClient(
            url=settings.QDRANT_URL, api_key=settings.model_extra.get("QDRANT_API_KEY")
        )

        # 1. First, get ANY point from the collection to have a valid ID to test with.
        console.print(
            "\n[bold]Step 1: Fetching a single point to get a valid ID...[/bold]"
        )
        scroll_result, _ = await qdrant_client.scroll(
            collection_name=settings.QDRANT_COLLECTION_NAME,
            limit=1,
            with_payload=False,
            with_vectors=False,
        )

        if not scroll_result:
            console.print(
                "[bold yellow]⚠️ The collection is empty. This is not an error, but the test cannot proceed.[/bold yellow]"
            )
            console.print(
                "   -> Run `bash scripts/reset_and_rebuild_db.sh` to populate the database."
            )
            return

        test_point_id = scroll_result[0].id
        console.print(
            f"   -> Found a valid point to test with. ID: [green]{test_point_id}[/green]"
        )

        # 2. Now, attempt the EXACT operation that is failing in the main app.
        console.print(
            "\n[bold]Step 2: Attempting to retrieve the point by its ID...[/bold]"
        )
        records = await qdrant_client.retrieve(
            collection_name=settings.QDRANT_COLLECTION_NAME,
            ids=[test_point_id],
            with_vectors=True,
        )

        if records:
            console.print(
                "[bold green]✅ SUCCESS: The client successfully retrieved the vector.[/bold green]"
            )
            console.print("   -> This means the connection is working perfectly.")
            console.print(
                "\n[bold red]DIAGNOSIS:[/bold red] The problem is NOT in the client or network, but somewhere inside the main CORE application's complex logic."
            )
        else:
            console.print(
                "[bold red]❌ FAILURE: The client failed to retrieve a known-good point.[/bold red]"
            )

    except Exception as e:
        console.print("\n[bold red]❌ TEST FAILED WITH AN EXCEPTION[/bold red]")
        console.print(f"   -> Exception Type: {type(e).__name__}")
        console.print(f"   -> Error Message: {e}")
        console.print(
            "\n[bold red]DIAGNOSIS:[/bold red] This proves the problem is in the connection between the `qdrant-client` library and your Docker/network setup. The main application code is not the issue."
        )


if __name__ == "__main__":
    asyncio.run(test_qdrant_connection())

--- END OF FILE ./scripts/test_qdrant_connection.py ---

--- START OF FILE ./scripts/verify_a1_readiness.py ---
# scripts/verify_a1_readiness.py
"""
A standalone script to clearly demonstrate the architectural gap preventing
A1 autonomy from functioning correctly. It provides evidence by comparing the
result of the correct validation command against the one currently being
used by the A1 executor's pre-flight check.
"""

import subprocess
import sys
from pathlib import Path

from rich.console import Console
from rich.panel import Panel

console = Console()

# --- Configuration ---
REPO_ROOT = Path(__file__).resolve().parents[1]
KNOWN_GOOD_FILE = "src/shared/logger.py"

# The command that we know is correct and works
CORRECT_VALIDATION_COMMAND = [
    "poetry",
    "run",
    "core-admin",
    "check",
    "validate",
    KNOWN_GOOD_FILE,
]

# The command that the buggy `micro apply` script is currently trying to run
BUGGY_PREFLIGHT_COMMAND = [
    "poetry",
    "run",
    "core-admin",
    "validate",
    "code",
    KNOWN_GOOD_FILE,
]
# --- End Configuration ---


def run_command(command: list[str]) -> tuple[bool, str]:
    """Runs a command and returns (success_bool, combined_output_str)."""
    try:
        result = subprocess.run(
            command,
            cwd=REPO_ROOT,
            capture_output=True,
            text=True,
            check=False,  # We want to capture failures, not crash
        )
        output = result.stdout + "\n" + result.stderr
        return result.returncode == 0, output.strip()
    except FileNotFoundError:
        return False, f"Command not found: {command[0]}"


def main():
    """Main execution function for the verification script."""
    console.print(Panel("[bold cyan]CORE A1 Readiness Verification Script[/bold cyan]"))

    # --- TEST A: ESTABLISH GROUND TRUTH ---
    console.print("\n[bold]STEP 1: Verifying the canonical validation tool...[/bold]")
    console.print(
        f"  -> Running correct command: `{' '.join(CORRECT_VALIDATION_COMMAND)}`"
    )
    success_a, output_a = run_command(CORRECT_VALIDATION_COMMAND)

    if not success_a:
        console.print(
            "[bold red]❌ TEST FAILED: The baseline validation tool itself is broken.[/bold red]"
        )
        console.print("Output:")
        console.print(output_a)
        sys.exit(1)

    console.print(
        "[bold green]  -> ✅ SUCCESS: The canonical validation tool is healthy.[/bold green]"
    )
    console.print("     This proves the system *is capable* of validating a file.")

    # --- TEST B: SIMULATE THE BUGGY A1 PRE-FLIGHT CHECK ---
    console.print(
        "\n[bold]STEP 2: Simulating the A1 micro-proposal pre-flight check...[/bold]"
    )
    console.print(
        f"  -> Running the command currently used by `micro apply`: `{' '.join(BUGGY_PREFLIGHT_COMMAND)}`"
    )
    success_b, output_b = run_command(BUGGY_PREFLIGHT_COMMAND)

    console.print("\n" + "=" * 50)
    console.print("[bold]VERIFICATION ANALYSIS[/bold]")
    console.print("=" * 50)

    if success_b:
        console.print(
            "[bold red]UNEXPECTED RESULT:[/bold red] The buggy pre-flight check succeeded."
        )
        console.print(
            "This indicates a different problem than anticipated. Please review the output:"
        )
        console.print(output_b)
        sys.exit(1)

    if "No such command 'validate'" in output_b:
        console.print(
            "[bold green]✅ EVIDENCE CONFIRMED: The A1 pre-flight check is failing as expected.[/bold green]"
        )
        console.print("\n[bold]Conclusion:[/bold]")
        console.print(
            "The system is not yet in A1 because of a simple but critical architectural disconnect:"
        )
        console.print(
            "1. The **correct** validation command is `core-admin check validate` (as proven in Step 1)."
        )
        console.print(
            "2. The **A1 executor** is incorrectly trying to call `core-admin validate code` (as proven by the failure in Step 2)."
        )
        console.print(
            "\nThe A1 autonomy loop is correctly halting because its pre-flight check is calling a non-existent command."
        )
        console.print(
            "\nThis script provides the clear evidence that the final step is to fix this wiring."
        )
    else:
        console.print(
            "[bold yellow]UNEXPECTED FAILURE:[/bold yellow] The pre-flight check failed, but for a different reason."
        )
        console.print("Please review the captured output to diagnose the issue:")
        console.print(Panel(output_b, title="Captured Output from Buggy Command"))

    sys.exit(0)


if __name__ == "__main__":
    main()

--- END OF FILE ./scripts/verify_a1_readiness.py ---

--- START OF FILE ./scripts/verify_live_config.py ---
# scripts/verify_live_config.py
import asyncio
import sys
from pathlib import Path

# Add the 'src' directory to Python's path to allow imports
project_root = Path(__file__).resolve().parents[1]
src_path = project_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from rich.console import Console
from rich.table import Table
from services.database.session_manager import get_session
from shared.config import settings
from sqlalchemy import text

console = Console()


async def inspect_live_database_config():
    """
    Connects to the database using the EXACT same method as the application
    and prints the live cognitive roles configuration it sees.
    """
    console.print(
        "[bold cyan]--- Live Application Configuration Inspector ---[/bold cyan]"
    )
    try:
        # Print the database URL the application is actually using
        console.print(
            "\n[bold]1. Database Connection String Used by Application:[/bold]"
        )
        console.print(f"[yellow]{settings.DATABASE_URL}[/yellow]")

        async with get_session() as session:
            console.print(
                "\n[bold]2. Live Data in 'core.cognitive_roles' Table (as seen by the app):[/bold]"
            )
            result = await session.execute(
                text("SELECT role, assigned_resource FROM core.cognitive_roles")
            )
            live_data = [dict(row._mapping) for row in result]

            table = Table(show_header=True, header_style="bold magenta")
            table.add_column("role")
            table.add_column("assigned_resource")
            for row in live_data:
                table.add_row(row["role"], row["assigned_resource"])
            console.print(table)

            # Final Diagnosis
            reviewer_role = next(
                (item for item in live_data if item["role"] == "CodeReviewer"), None
            )
            if reviewer_role and reviewer_role["assigned_resource"] == "ollama_local":
                console.print(
                    "\n[bold red]DIAGNOSIS:[/bold red] The application is connected to a database that still contains the OLD data."
                )
                console.print(
                    "This proves the application is NOT connecting to the same database as your pgAdmin/psql client."
                )
            elif (
                reviewer_role and reviewer_role["assigned_resource"] == "deepseek_chat"
            ):
                console.print(
                    "\n[bold green]DIAGNOSIS:[/bold green] The application IS seeing the correct data. The problem lies elsewhere."
                )
            else:
                console.print(
                    "\n[bold yellow]DIAGNOSIS:[/bold yellow] The 'CodeReviewer' role data is missing or unexpected."
                )

    except Exception as e:
        console.print(
            f"\n[bold red]❌ An error occurred while trying to connect or query: {e}[/bold red]"
        )
        console.print(
            "   This may indicate the DATABASE_URL points to a non-existent or inaccessible database."
        )


if __name__ == "__main__":
    asyncio.run(inspect_live_database_config())

--- END OF FILE ./scripts/verify_live_config.py ---

--- START OF FILE ./sql/001_consolidated_schema.sql ---
-- =============================================================================
-- CORE v2.1 — Self-Improving System Schema
-- Designed for A1+ Autonomy with Qdrant Vector Integration
--
-- Design Principles:
-- - UUID type consistency (native uuid everywhere, no text UUIDs)
-- - symbol_path as natural key, id as immutable PK
-- - Production-ready materialized view management
-- - Full observability and audit trails
-- =============================================================================

CREATE SCHEMA IF NOT EXISTS core;

-- Helper function for auto-updating timestamps
CREATE OR REPLACE FUNCTION core.set_updated_at() RETURNS trigger
    LANGUAGE plpgsql AS $$
BEGIN
  NEW.updated_at = NOW();
  RETURN NEW;
END;
$$;

-- =============================================================================
-- SECTION 1: KNOWLEDGE LAYER (What exists in the codebase)
-- =============================================================================

-- Core code symbols discovered via AST analysis
CREATE TABLE IF NOT EXISTS core.symbols (
    -- Primary key: Immutable UUID for referential integrity
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Natural key: Human-readable, unique, but may change during refactoring
    symbol_path text NOT NULL UNIQUE,

    -- Location & structure
    module text NOT NULL,                    -- File path
    qualname text NOT NULL,                  -- Qualified name
    kind text NOT NULL CHECK (kind IN ('function', 'class', 'method', 'module')),

    -- Structure & fingerprinting
    ast_signature text NOT NULL,             -- Structural signature
    fingerprint text NOT NULL,               -- Hash (non-unique: same pattern, different contexts)

    -- Lifecycle
    state text DEFAULT 'discovered' NOT NULL CHECK (
        state IN ('discovered', 'classified', 'bound', 'verified', 'deprecated')
    ),
    health_status text DEFAULT 'unknown' CHECK (
        health_status IN ('healthy', 'needs_review', 'deprecated', 'broken', 'unknown')
    ),
    is_public boolean NOT NULL DEFAULT true,

    -- History tracking for autonomous refactoring
    previous_paths text[],                   -- Track symbol renames/moves

    -- Capability key and AI-generated description
    key text,
    intent text,

    -- Vectorization state tracking
    embedding_model text DEFAULT 'text-embedding-3-small',
    last_embedded timestamptz, -- Timestamp of the last successful vectorization

    -- Timestamps
    first_seen timestamptz DEFAULT now() NOT NULL,
    last_seen timestamptz DEFAULT now() NOT NULL,
    last_modified timestamptz DEFAULT now() NOT NULL,
    created_at timestamptz DEFAULT now() NOT NULL,
    updated_at timestamptz DEFAULT now() NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_symbols_module ON core.symbols(module);
CREATE INDEX IF NOT EXISTS idx_symbols_kind ON core.symbols(kind);
CREATE INDEX IF NOT EXISTS idx_symbols_state ON core.symbols(state);
CREATE INDEX IF NOT EXISTS idx_symbols_health ON core.symbols(health_status);
CREATE INDEX IF NOT EXISTS idx_symbols_qualname ON core.symbols(qualname);
CREATE INDEX IF NOT EXISTS idx_symbols_fingerprint ON core.symbols(fingerprint);

-- Lookup helper for natural key usage
CREATE OR REPLACE FUNCTION core.get_symbol_id(path text)
RETURNS uuid AS $$
    SELECT id FROM core.symbols WHERE symbol_path = path;
$$ LANGUAGE sql STABLE;

COMMENT ON FUNCTION core.get_symbol_id IS
    'Helper to look up symbol UUID by its natural key (symbol_path). Usage: get_symbol_id(''my.module:MyClass'')';

-- System capabilities (what CORE can do)
CREATE TABLE IF NOT EXISTS core.capabilities (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    name text NOT NULL,
    domain text DEFAULT 'general' NOT NULL,
    title text NOT NULL,
    objective text,
    owner text NOT NULL,

    -- Implementation tracking (arrays of UUIDs)
    entry_points uuid[] DEFAULT '{}',        -- Main symbol IDs
    dependencies jsonb DEFAULT '[]'::jsonb,  -- Required capability names
    test_coverage numeric(5,2),              -- 0-100%

    -- Metadata
    tags jsonb DEFAULT '[]'::jsonb NOT NULL CHECK (jsonb_typeof(tags) = 'array'),
    status text DEFAULT 'Active' CHECK (status IN ('Active', 'Draft', 'Deprecated')),

    created_at timestamptz DEFAULT now() NOT NULL,
    updated_at timestamptz DEFAULT now() NOT NULL,

    UNIQUE(domain, name)
);

CREATE INDEX IF NOT EXISTS idx_capabilities_domain ON core.capabilities(domain);
CREATE INDEX IF NOT EXISTS idx_capabilities_status ON core.capabilities(status);
CREATE INDEX IF NOT EXISTS idx_capabilities_entry_points ON core.capabilities USING GIN(entry_points);

COMMENT ON COLUMN core.capabilities.entry_points IS
    'Array of symbol UUIDs that serve as primary entry points for this capability';

-- Link symbols to capabilities they implement
CREATE TABLE IF NOT EXISTS core.symbol_capability_links (
    symbol_id uuid NOT NULL REFERENCES core.symbols(id) ON DELETE CASCADE,
    capability_id uuid NOT NULL REFERENCES core.capabilities(id) ON DELETE CASCADE,
    confidence numeric NOT NULL CHECK (confidence BETWEEN 0 AND 1),
    source text NOT NULL CHECK (source IN ('auditor-infer', 'manual', 'rule', 'llm-classified')),
    verified boolean DEFAULT false NOT NULL,
    created_at timestamptz DEFAULT now() NOT NULL,
    PRIMARY KEY (symbol_id, capability_id, source)
);

CREATE INDEX IF NOT EXISTS idx_links_capability ON core.symbol_capability_links(capability_id);
CREATE INDEX IF NOT EXISTS idx_links_verified ON core.symbol_capability_links(verified);

-- Domains for organizing capabilities
CREATE TABLE IF NOT EXISTS core.domains (
    key text PRIMARY KEY,
    title text NOT NULL,
    description text,
    created_at timestamptz DEFAULT now() NOT NULL
);

-- =============================================================================
-- SECTION 2: GOVERNANCE LAYER (Constitutional compliance)
-- =============================================================================

-- Change proposals requiring approval
CREATE TABLE IF NOT EXISTS core.proposals (
    id bigserial PRIMARY KEY,
    target_path text NOT NULL,
    content_sha256 char(64) NOT NULL,
    justification text NOT NULL,
    risk_tier text DEFAULT 'low' CHECK (risk_tier IN ('low', 'medium', 'high')),
    is_critical boolean DEFAULT false NOT NULL,
    status text DEFAULT 'open' NOT NULL CHECK (
        status IN ('open', 'approved', 'rejected', 'superseded')
    ),
    created_at timestamptz DEFAULT now() NOT NULL,
    created_by text NOT NULL
);

-- Cryptographic approval signatures
CREATE TABLE IF NOT EXISTS core.proposal_signatures (
    proposal_id bigint NOT NULL REFERENCES core.proposals(id) ON DELETE CASCADE,
    approver_identity text NOT NULL,
    signature_base64 text NOT NULL,
    signed_at timestamptz DEFAULT now() NOT NULL,
    is_valid boolean DEFAULT true NOT NULL,
    PRIMARY KEY (proposal_id, approver_identity)
);

-- Audit runs tracking
CREATE TABLE IF NOT EXISTS core.audit_runs (
    id bigserial PRIMARY KEY,
    source text NOT NULL,
    commit_sha char(40),
    score numeric(4,3),
    passed boolean NOT NULL,
    violations_found integer DEFAULT 0,
    started_at timestamptz DEFAULT now() NOT NULL,
    finished_at timestamptz
);

CREATE INDEX IF NOT EXISTS idx_audit_runs_passed ON core.audit_runs(passed, started_at DESC);

-- =============================================================================
-- SECTION 3: OPERATIONAL LAYER (What's happening right now)
-- =============================================================================

-- LLM resources available to cognitive roles
CREATE TABLE IF NOT EXISTS core.llm_resources (
    name text PRIMARY KEY,
    env_prefix text NOT NULL UNIQUE,
    provided_capabilities jsonb DEFAULT '[]'::jsonb CHECK (jsonb_typeof(provided_capabilities) = 'array'),
    performance_metadata jsonb,
    is_available boolean DEFAULT true,
    created_at timestamptz DEFAULT now() NOT NULL
);

-- AI cognitive roles (specialized agents)
CREATE TABLE IF NOT EXISTS core.cognitive_roles (
    role text PRIMARY KEY,
    description text,
    assigned_resource text REFERENCES core.llm_resources(name),
    required_capabilities jsonb DEFAULT '[]'::jsonb CHECK (jsonb_typeof(required_capabilities) = 'array'),
    max_concurrent_tasks integer DEFAULT 1,
    specialization jsonb,                    -- {"good_at": [...], "avoid": [...]}
    is_active boolean DEFAULT true,
    created_at timestamptz DEFAULT now() NOT NULL
);

-- Task queue: what agents need to do
CREATE TABLE IF NOT EXISTS core.tasks (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    intent text NOT NULL,                    -- User's request
    assigned_role text REFERENCES core.cognitive_roles(role),
    parent_task_id uuid REFERENCES core.tasks(id),  -- For decomposition

    -- Execution state
    status text DEFAULT 'pending' NOT NULL CHECK (
        status IN ('pending', 'planning', 'executing', 'validating', 'completed', 'failed', 'blocked')
    ),
    plan jsonb,                              -- Agent's execution plan
    context jsonb DEFAULT '{}'::jsonb,       -- Working memory for this task
    error_message text,
    failure_reason text,

    -- Vector retrieval context (from Qdrant) - native UUID arrays
    relevant_symbols uuid[],                 -- Symbol UUIDs from vector search
    context_retrieval_query text,            -- What we searched for
    context_retrieved_at timestamptz,
    context_tokens_used integer,

    -- Constitutional compliance
    requires_approval boolean DEFAULT false,
    proposal_id bigint REFERENCES core.proposals(id), -- Links to governance

    -- Metrics
    estimated_complexity integer CHECK (estimated_complexity BETWEEN 1 AND 10),
    actual_duration_seconds integer,

    -- Timestamps
    created_at timestamptz DEFAULT now() NOT NULL,
    started_at timestamptz,
    completed_at timestamptz
);

CREATE INDEX IF NOT EXISTS idx_tasks_status ON core.tasks(status) WHERE status IN ('pending', 'executing', 'blocked');
CREATE INDEX IF NOT EXISTS idx_tasks_role ON core.tasks(assigned_role);
CREATE INDEX IF NOT EXISTS idx_tasks_parent ON core.tasks(parent_task_id);
CREATE INDEX IF NOT EXISTS idx_tasks_created ON core.tasks(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_tasks_relevant_symbols ON core.tasks USING GIN(relevant_symbols);

COMMENT ON COLUMN core.tasks.relevant_symbols IS
    'Array of symbol UUIDs retrieved from Qdrant vector search for this task context';

-- Link tasks to proposals they generated
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_constraint
        WHERE conname = 'fk_tasks_proposal' AND conrelid = 'core.tasks'::regclass
    ) THEN
        ALTER TABLE core.tasks
        ADD CONSTRAINT fk_tasks_proposal
        FOREIGN KEY (proposal_id) REFERENCES core.proposals(id);
    END IF;
END;
$$;

-- Constitutional violations detected by auditor
CREATE TABLE IF NOT EXISTS core.constitutional_violations (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    rule_id text NOT NULL,
    symbol_id uuid REFERENCES core.symbols(id),
    task_id uuid REFERENCES core.tasks(id),
    severity text NOT NULL CHECK (severity IN ('info', 'warning', 'error', 'critical')),
    description text NOT NULL,
    detected_at timestamptz DEFAULT now() NOT NULL,
    resolved_at timestamptz,
    resolution_notes text
);

CREATE INDEX IF NOT EXISTS idx_violations_unresolved ON core.constitutional_violations(severity, detected_at)
    WHERE resolved_at IS NULL;
CREATE INDEX IF NOT EXISTS idx_violations_symbol ON core.constitutional_violations(symbol_id);
CREATE INDEX IF NOT EXISTS idx_violations_task ON core.constitutional_violations(task_id);

-- Action log: everything agents do
CREATE TABLE IF NOT EXISTS core.actions (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    task_id uuid NOT NULL REFERENCES core.tasks(id) ON DELETE CASCADE,
    action_type text NOT NULL CHECK (
        action_type IN ('file_read', 'file_write', 'symbol_analysis', 'llm_call',
                       'shell_command', 'validation', 'vector_search', 'test_run')
    ),
    target text,                             -- File path, symbol ID, command, etc.
    payload jsonb,                           -- Input details
    result jsonb,                            -- Output/response
    success boolean NOT NULL,
    cognitive_role text NOT NULL,
    reasoning text,                          -- Why this action?
    duration_ms integer,
    created_at timestamptz DEFAULT now() NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_actions_task ON core.actions(task_id);
CREATE INDEX IF NOT EXISTS idx_actions_type ON core.actions(action_type);
CREATE INDEX IF NOT EXISTS idx_actions_created ON core.actions(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_actions_success ON core.actions(success) WHERE success = false;

-- Agent decisions: choice points for debugging
CREATE TABLE IF NOT EXISTS core.agent_decisions (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    task_id uuid NOT NULL REFERENCES core.tasks(id),
    decision_point text NOT NULL,            -- "What to do next?"
    options_considered jsonb NOT NULL,       -- All possible choices
    chosen_option text NOT NULL,
    reasoning text NOT NULL,                 -- WHY this choice?
    confidence numeric(3,2) NOT NULL CHECK (confidence BETWEEN 0 AND 1),
    was_correct boolean,                     -- Post-hoc evaluation
    decided_at timestamptz DEFAULT now() NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_decisions_task ON core.agent_decisions(task_id);
CREATE INDEX IF NOT EXISTS idx_decisions_confidence ON core.agent_decisions(confidence);

-- Short-term agent memory (expires)
CREATE TABLE IF NOT EXISTS core.agent_memory (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    cognitive_role text NOT NULL,
    memory_type text NOT NULL CHECK (memory_type IN ('fact', 'observation', 'decision', 'pattern', 'error')),
    content text NOT NULL,
    related_task_id uuid REFERENCES core.tasks(id),
    relevance_score numeric(3,2) DEFAULT 1.0 CHECK (relevance_score BETWEEN 0 AND 1),
    expires_at timestamptz,                  -- NULL = permanent
    created_at timestamptz DEFAULT now() NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_memory_role_type ON core.agent_memory(cognitive_role, memory_type);
CREATE INDEX IF NOT EXISTS idx_memory_expires ON core.agent_memory(expires_at) WHERE expires_at IS NOT NULL;
CREATE INDEX IF NOT EXISTS idx_memory_relevance ON core.agent_memory(relevance_score DESC);

-- =============================================================================
-- SECTION 4: VECTOR INTEGRATION LAYER (Qdrant sync)
-- =============================================================================

-- Link table between symbols and their vectors in Qdrant
CREATE TABLE IF NOT EXISTS core.symbol_vector_links (
    symbol_id uuid PRIMARY KEY NOT NULL REFERENCES core.symbols(id) ON DELETE CASCADE,
    vector_id UUID NOT NULL, -- The UUID ID used in Qdrant
    embedding_model text NOT NULL,
    embedding_version integer NOT NULL,
    created_at timestamptz DEFAULT now() NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_symbol_vector_links_vector_id ON core.symbol_vector_links(vector_id);


-- Track Qdrant synchronization
CREATE TABLE IF NOT EXISTS core.vector_sync_log (
    id bigserial PRIMARY KEY,
    operation text NOT NULL CHECK (operation IN ('upsert', 'delete', 'bulk_update', 'reindex')),
    symbol_ids uuid[],                       -- Native UUID array
    qdrant_collection text NOT NULL,
    success boolean NOT NULL,
    error_message text,
    batch_size integer,
    duration_ms integer,
    synced_at timestamptz DEFAULT now() NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_vector_sync_failed ON core.vector_sync_log(success, synced_at) WHERE success = false;
CREATE INDEX IF NOT EXISTS idx_vector_sync_collection ON core.vector_sync_log(qdrant_collection);
CREATE INDEX IF NOT EXISTS idx_vector_sync_symbols ON core.vector_sync_log USING GIN(symbol_ids);

-- Track retrieval quality for optimization
CREATE TABLE IF NOT EXISTS core.retrieval_feedback (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    task_id uuid NOT NULL REFERENCES core.tasks(id),
    query text NOT NULL,
    retrieved_symbols uuid[],                -- Native UUID array
    actually_used_symbols uuid[],            -- Which ones were actually modified/read?
    retrieval_quality integer CHECK (retrieval_quality BETWEEN 1 AND 5),
    notes text,
    created_at timestamptz DEFAULT now() NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_retrieval_task ON core.retrieval_feedback(task_id);
CREATE INDEX IF NOT EXISTS idx_retrieval_quality ON core.retrieval_feedback(retrieval_quality);
CREATE INDEX IF NOT EXISTS idx_retrieval_symbols ON core.retrieval_feedback USING GIN(retrieved_symbols);
CREATE INDEX IF NOT EXISTS idx_retrieval_used ON core.retrieval_feedback USING GIN(actually_used_symbols);

-- Semantic cache for LLM responses
CREATE TABLE IF NOT EXISTS core.semantic_cache (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    query_hash text NOT NULL UNIQUE,
    query_text text NOT NULL,
    vector_id text,                          -- Also in Qdrant for semantic lookup
    response_text text NOT NULL,
    cognitive_role text,
    llm_model text NOT NULL,
    tokens_used integer,
    confidence numeric(3,2) CHECK (confidence BETWEEN 0 AND 1),
    hit_count integer DEFAULT 0,
    expires_at timestamptz,
    created_at timestamptz DEFAULT now() NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_cache_hash ON core.semantic_cache(query_hash);
CREATE INDEX IF NOT EXISTS idx_cache_expires ON core.semantic_cache(expires_at) WHERE expires_at IS NOT NULL;
CREATE INDEX IF NOT EXISTS idx_cache_hits ON core.semantic_cache(hit_count DESC);

-- =============================================================================
-- SECTION 5: LEARNING & FEEDBACK LAYER
-- =============================================================================

-- General feedback loop
CREATE TABLE IF NOT EXISTS core.feedback (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    task_id uuid REFERENCES core.tasks(id),
    action_id uuid REFERENCES core.actions(id),
    feedback_type text NOT NULL CHECK (
        feedback_type IN ('success', 'failure', 'improvement', 'validation_error', 'user_correction')
    ),
    message text NOT NULL,
    corrective_action text,
    applied boolean DEFAULT false,
    created_at timestamptz DEFAULT now() NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_feedback_task ON core.feedback(task_id);
CREATE INDEX IF NOT EXISTS idx_feedback_applied ON core.feedback(applied) WHERE applied = false;

-- =============================================================================
-- SECTION 6: SYSTEM METADATA
-- =============================================================================

-- CLI commands exposed by CORE
CREATE TABLE IF NOT EXISTS core.cli_commands (
    name text PRIMARY KEY,
    module text NOT NULL,
    entrypoint text NOT NULL,
    summary text,
    category text
);

-- Runtime services
CREATE TABLE IF NOT EXISTS core.runtime_services (
    name text PRIMARY KEY,
    implementation text NOT NULL UNIQUE,
    is_active boolean DEFAULT true
);

-- Migration tracking
CREATE TABLE IF NOT EXISTS core._migrations (
    id text PRIMARY KEY,
    applied_at timestamptz DEFAULT now() NOT NULL
);

-- Export manifests
CREATE TABLE IF NOT EXISTS core.export_manifests (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    exported_at timestamptz DEFAULT now() NOT NULL,
    who text,
    environment text,
    notes text
);

CREATE TABLE IF NOT EXISTS core.export_digests (
    path text PRIMARY KEY,
    sha256 text NOT NULL,
    manifest_id uuid NOT NULL REFERENCES core.export_manifests(id) ON DELETE CASCADE,
    exported_at timestamptz DEFAULT now() NOT NULL
);

-- Mission statement (The Northstar)
CREATE TABLE IF NOT EXISTS core.northstar (
    id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    mission text NOT NULL,
    updated_at timestamptz DEFAULT now() NOT NULL
);

-- Runtime configuration
CREATE TABLE IF NOT EXISTS core.runtime_settings (
    key TEXT PRIMARY KEY,
    value TEXT,
    description TEXT,
    is_secret BOOLEAN NOT NULL DEFAULT FALSE,
    last_updated TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

COMMENT ON TABLE core.runtime_settings IS 'Single source of truth for runtime configuration, loaded from .env and managed by `core-admin manage dotenv sync`.';
COMMENT ON COLUMN core.runtime_settings.is_secret IS 'If true, the value should be handled with care.';


-- =============================================================================
-- SECTION 7: MATERIALIZED VIEW MANAGEMENT (Production-Ready)
-- =============================================================================

-- Track materialized view refresh operations
CREATE TABLE IF NOT EXISTS core.mv_refresh_log (
    view_name text PRIMARY KEY,
    last_refresh_started timestamptz,
    last_refresh_completed timestamptz,
    last_refresh_duration_ms integer,
    rows_affected integer,
    triggered_by text
);

-- Refresh function with logging and observability
CREATE OR REPLACE FUNCTION core.refresh_materialized_view(view_name text)
RETURNS TABLE(
    duration_ms integer,
    rows_affected integer
) AS $$
DECLARE
    start_time timestamptz := now();
    rows_count integer;
    duration integer;
BEGIN
    INSERT INTO core.mv_refresh_log (view_name, last_refresh_started, triggered_by)
    VALUES (view_name, start_time, current_user)
    ON CONFLICT (view_name)
    DO UPDATE SET last_refresh_started = start_time, triggered_by = current_user;

    EXECUTE format('REFRESH MATERIALIZED VIEW CONCURRENTLY %I', view_name);

    EXECUTE format('SELECT COUNT(*) FROM %I', view_name) INTO rows_count;

    duration := EXTRACT(EPOCH FROM (now() - start_time)) * 1000;

    UPDATE core.mv_refresh_log
    SET last_refresh_completed = now(),
        last_refresh_duration_ms = duration,
        rows_affected = rows_count
    WHERE mv_refresh_log.view_name = refresh_materialized_view.view_name;

    RETURN QUERY SELECT duration, rows_count;
END;
$$ LANGUAGE plpgsql;

COMMENT ON FUNCTION core.refresh_materialized_view IS
    'Refresh a materialized view with logging. Usage: SELECT * FROM core.refresh_materialized_view(''core.mv_symbol_usage_patterns'');';

-- =============================================================================
-- SECTION 8: OPERATIONAL VIEWS
-- =============================================================================

CREATE OR REPLACE VIEW core.v_symbols_needing_embedding AS
SELECT s.id, s.module, s.qualname, s.symbol_path, s.ast_signature, s.fingerprint
FROM core.symbols s
WHERE s.last_embedded IS NULL OR s.last_modified > s.last_embedded
ORDER BY s.last_modified DESC;

CREATE OR REPLACE VIEW core.v_orphan_symbols AS
SELECT s.id, s.symbol_path, s.module, s.qualname, s.kind, s.state, s.health_status
FROM core.symbols s
LEFT JOIN core.symbol_capability_links l ON l.symbol_id = s.id
WHERE l.symbol_id IS NULL
  AND s.state != 'deprecated'
  AND s.health_status != 'deprecated'
ORDER BY s.last_modified DESC;

CREATE OR REPLACE VIEW core.v_verified_coverage AS
SELECT
    c.id AS capability_id,
    c.name,
    c.domain,
    COUNT(l.symbol_id) AS verified_symbols,
    c.test_coverage,
    c.status
FROM core.capabilities c
LEFT JOIN core.symbol_capability_links l ON l.capability_id = c.id AND l.verified = true
GROUP BY c.id, c.name, c.domain, c.test_coverage, c.status
ORDER BY c.domain, c.name;

CREATE OR REPLACE VIEW core.v_agent_workload AS
SELECT
    cr.role,
    cr.is_active,
    COUNT(t.id) FILTER (WHERE t.status = 'executing') as active_tasks,
    COUNT(t.id) FILTER (WHERE t.status = 'pending') as queued_tasks,
    COUNT(t.id) FILTER (WHERE t.status = 'blocked') as blocked_tasks,
    cr.max_concurrent_tasks,
    (cr.max_concurrent_tasks - COUNT(t.id) FILTER (WHERE t.status = 'executing')) as available_slots,
    cr.assigned_resource
FROM core.cognitive_roles cr
LEFT JOIN core.tasks t ON t.assigned_role = cr.role
    AND t.status IN ('pending', 'executing', 'blocked')
GROUP BY cr.role, cr.is_active, cr.max_concurrent_tasks, cr.assigned_resource
ORDER BY cr.role;

CREATE OR REPLACE VIEW core.v_agent_context AS
SELECT
    t.id as task_id,
    t.intent,
    t.assigned_role,
    t.status,
    t.relevant_symbols,
    array_length(t.relevant_symbols, 1) as context_symbol_count,
    (SELECT json_agg(json_build_object(
        'action', a.action_type,
        'success', a.success,
        'target', a.target,
        'reasoning', a.reasoning
    ) ORDER BY a.created_at DESC)
    FROM core.actions a
    WHERE a.task_id = t.id
    LIMIT 10) as recent_actions,
    (SELECT json_agg(json_build_object(
        'type', am.memory_type,
        'content', am.content,
        'score', am.relevance_score
    ) ORDER BY am.relevance_score DESC)
    FROM core.agent_memory am
    WHERE am.cognitive_role = t.assigned_role
      AND (am.expires_at IS NULL OR am.expires_at > now())
    LIMIT 5) as active_memories,
    (SELECT json_agg(json_build_object(
        'point', ad.decision_point,
        'chosen', ad.chosen_option,
        'reasoning', ad.reasoning,
        'confidence', ad.confidence
    ) ORDER BY ad.decided_at DESC)
    FROM core.agent_decisions ad
    WHERE ad.task_id = t.id
    LIMIT 5) as recent_decisions
FROM core.tasks t
WHERE t.status IN ('pending', 'executing', 'planning')
ORDER BY t.created_at;

CREATE OR REPLACE VIEW core.knowledge_graph AS
SELECT
    s.id as uuid,
    s.symbol_path,
    s.module as file_path,
    s.qualname as name,
    s.kind as type,
    s.state as status,
    s.health_status,
    s.is_public,
    s.fingerprint as structural_hash,
    s.updated_at AS last_updated,
    s.key as capability,
    s.intent,
    vl.vector_id,
    COALESCE(
        (SELECT json_agg(DISTINCT c.name ORDER BY c.name)
         FROM core.symbol_capability_links l
         JOIN core.capabilities c ON c.id = l.capability_id
         WHERE l.symbol_id = s.id),
        '[]'::json
    ) as capabilities_array,
    (s.kind = 'class') AS is_class,
    (s.qualname LIKE 'Test%' OR s.qualname LIKE 'test_%') AS is_test,
    (SELECT COUNT(*) FROM core.actions a WHERE a.target = s.symbol_path) as action_count
FROM core.symbols s
LEFT JOIN core.symbol_vector_links vl ON s.id = vl.symbol_id
ORDER BY s.updated_at DESC;

CREATE OR REPLACE VIEW core.v_stale_materialized_views AS
SELECT
    view_name,
    last_refresh_completed,
    now() - last_refresh_completed as age,
    last_refresh_duration_ms,
    rows_affected,
    (
        last_refresh_completed IS NULL
        OR last_refresh_completed < now() - interval '10 minutes'
    ) as is_stale
FROM core.mv_refresh_log
WHERE (last_refresh_completed IS NULL OR last_refresh_completed < now() - interval '10 minutes')
ORDER BY last_refresh_completed NULLS FIRST;

-- =============================================================================
-- SECTION 9: TRIGGERS
-- =============================================================================

DO $$
BEGIN
    IF NOT EXISTS (SELECT 1 FROM pg_trigger WHERE tgname = 'trg_capabilities_updated_at') THEN
        CREATE TRIGGER trg_capabilities_updated_at
            BEFORE UPDATE ON core.capabilities
            FOR EACH ROW EXECUTE FUNCTION core.set_updated_at();
    END IF;

    IF NOT EXISTS (SELECT 1 FROM pg_trigger WHERE tgname = 'trg_symbols_updated_at') THEN
        CREATE TRIGGER trg_symbols_updated_at
            BEFORE UPDATE ON core.symbols
            FOR EACH ROW EXECUTE FUNCTION core.set_updated_at();
    END IF;
END$$;

-- =============================================================================
-- SECTION 10: MATERIALIZED VIEW FOR ANALYTICS
-- =============================================================================

CREATE MATERIALIZED VIEW IF NOT EXISTS core.mv_symbol_usage_patterns AS
SELECT
    s.id,
    s.symbol_path,
    s.module,
    s.kind,
    s.state,
    s.health_status,
    COUNT(DISTINCT a.task_id) FILTER (WHERE a.action_type = 'file_write') as times_modified,
    COUNT(DISTINCT a.task_id) FILTER (WHERE a.action_type = 'file_read') as times_read,
    COUNT(DISTINCT rf.task_id) as times_retrieved,
    CASE
        WHEN COUNT(DISTINCT rf.task_id) > 0
        THEN COUNT(DISTINCT a.task_id) FILTER (WHERE a.action_type IN ('file_write', 'file_read'))::numeric
             / COUNT(DISTINCT rf.task_id)
        ELSE 0
    END as retrieval_precision,
    array_agg(DISTINCT c.name) FILTER (WHERE c.name IS NOT NULL) as associated_capabilities,
    MAX(a.created_at) as last_action_at,
    MAX(rf.created_at) as last_retrieved_at
FROM core.symbols s
LEFT JOIN core.actions a ON a.target = s.symbol_path
LEFT JOIN core.retrieval_feedback rf ON s.id = ANY(rf.retrieved_symbols)
LEFT JOIN core.symbol_capability_links l ON l.symbol_id = s.id
LEFT JOIN core.capabilities c ON c.id = l.capability_id
GROUP BY s.id, s.symbol_path, s.module, s.kind, s.state, s.health_status;

CREATE UNIQUE INDEX IF NOT EXISTS idx_mv_usage_id ON core.mv_symbol_usage_patterns(id);
CREATE INDEX IF NOT EXISTS idx_mv_usage_precision ON core.mv_symbol_usage_patterns(retrieval_precision DESC);
CREATE INDEX IF NOT EXISTS idx_mv_usage_modified ON core.mv_symbol_usage_patterns(times_modified DESC);
CREATE INDEX IF NOT EXISTS idx_mv_usage_last_action ON core.mv_symbol_usage_patterns(last_action_at DESC NULLS LAST);

COMMENT ON MATERIALIZED VIEW core.mv_symbol_usage_patterns IS
    'Analytics view for symbol usage patterns. Refresh with: SELECT * FROM core.refresh_materialized_view(''core.mv_symbol_usage_patterns'');';

INSERT INTO core.mv_refresh_log (view_name, triggered_by)
VALUES ('core.mv_symbol_usage_patterns', 'schema_init')
ON CONFLICT (view_name) DO NOTHING;

--- END OF FILE ./sql/001_consolidated_schema.sql ---

--- START OF FILE ./src/api/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/api/__init__.py ---

--- START OF FILE ./src/api/v1/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/api/v1/__init__.py ---

--- START OF FILE ./src/api/v1/development_routes.py ---
# src/api/v1/development_routes.py
"""
Provides API endpoints for initiating and managing autonomous development cycles.
"""

from __future__ import annotations

from fastapi import APIRouter, BackgroundTasks, Depends, Request
from features.autonomy.autonomous_developer import develop_from_goal
from pydantic import BaseModel
from services.database.models import Task

# --- THIS IS THE FIX: Correct the import path ---
from services.database.session_manager import get_db_session

# --- END OF FIX ---
from shared.context import CoreContext
from sqlalchemy.ext.asyncio import AsyncSession

router = APIRouter()


# ID: 7b83814d-b747-4c17-b054-9e8f2b8b8325
class DevelopmentGoal(BaseModel):
    goal: str


@router.post("/develop/goal", status_code=202)
# ID: de19ab6c-6bb6-4d9c-98bd-f1b3783b2188
async def start_development_cycle(
    request: Request,
    payload: DevelopmentGoal,
    background_tasks: BackgroundTasks,
    # --- THIS IS THE FIX: Use the imported dependency provider ---
    session: AsyncSession = Depends(get_db_session),
    # --- END OF FIX ---
):
    """
    Accepts a high-level goal, creates a task record, and starts the
    autonomous development cycle in the background.
    """
    core_context: CoreContext = request.app.state.core_context

    new_task = Task(
        intent=payload.goal, assigned_role="AutonomousDeveloper", status="planning"
    )
    session.add(new_task)
    await session.commit()
    await session.refresh(new_task)

    background_tasks.add_task(
        develop_from_goal, core_context, payload.goal, task_id=new_task.id
    )

    return {"task_id": str(new_task.id), "status": "Task accepted and running."}

--- END OF FILE ./src/api/v1/development_routes.py ---

--- START OF FILE ./src/api/v1/knowledge_routes.py ---
# src/api/v1/knowledge_routes.py
from __future__ import annotations

from core.knowledge_service import KnowledgeService
from fastapi import APIRouter

# Prefix aligns with test path: /v1/knowledge/capabilities
router = APIRouter(prefix="/knowledge")


@router.get("/capabilities")
# ID: 0016df93-d0e5-45b0-b5b8-8f4170de3d9d
async def list_capabilities() -> dict:
    """
    Return known capabilities.

    Tests expect a 200 on GET /v1/knowledge/capabilities and a JSON object
    with a 'capabilities' key.
    """
    service = KnowledgeService()
    caps = await service.list_capabilities()
    return {"capabilities": caps}

--- END OF FILE ./src/api/v1/knowledge_routes.py ---

--- START OF FILE ./src/cli/admin_cli.py ---
# src/cli/admin_cli.py
"""
The single, canonical entry point for the core-admin CLI.
This module assembles all command groups into a single Typer application.
"""

from __future__ import annotations

import typer
from cli.commands import check, enrich, fix, inspect, manage, mind, run, search, submit
from cli.interactive import launch_interactive_menu
from cli.logic import audit as audit_logic
from core.cognitive_service import CognitiveService
from core.file_handler import FileHandler
from core.git_service import GitService
from core.knowledge_service import KnowledgeService
from features.governance.audit_context import AuditorContext
from rich.console import Console
from services.clients.qdrant_client import QdrantService
from shared.config import settings
from shared.context import CoreContext
from shared.logger import getLogger
from shared.models import PlannerConfig

console = Console()
log = getLogger("admin_cli")

app = typer.Typer(
    name="core-admin",
    help="""
    CORE: The Self-Improving System Architect's Toolkit.
    This CLI is the primary interface for operating and governing the CORE system.
    """,
    no_args_is_help=False,
)

core_context = CoreContext(
    git_service=GitService(settings.REPO_PATH),
    cognitive_service=CognitiveService(settings.REPO_PATH),
    knowledge_service=KnowledgeService(settings.REPO_PATH),
    qdrant_service=QdrantService(),
    auditor_context=AuditorContext(settings.REPO_PATH),
    file_handler=FileHandler(str(settings.REPO_PATH)),
    planner_config=PlannerConfig(),
)


# ID: 2cefad7a-83b8-4263-b882-5a62eae5b092
def register_all_commands(app_instance: typer.Typer) -> None:
    """Register all command groups and inject context declaratively."""
    # 1. Add top-level command groups (verbs)
    app_instance.add_typer(check.check_app, name="check")
    app_instance.add_typer(enrich.enrich_app, name="enrich")
    app_instance.add_typer(fix.fix_app, name="fix")
    app_instance.add_typer(inspect.inspect_app, name="inspect")
    app_instance.add_typer(manage.manage_app, name="manage")
    app_instance.add_typer(mind.mind_app, name="mind")
    app_instance.add_typer(run.run_app, name="run")
    app_instance.add_typer(search.search_app, name="search")
    app_instance.add_typer(submit.submit_app, name="submit")

    # 2. Inject context directly into the modules that need it.
    modules_with_context = [
        check,
        enrich,
        fix,
        inspect,
        manage,
        run,
        search,
        submit,
        audit_logic,
    ]
    for module in modules_with_context:
        # This is the corrected, direct way to set the context.
        module._context = core_context


register_all_commands(app)


@app.callback(invoke_without_command=True)
# ID: 74b366e2-d1fc-44c6-a9bc-e8fe222d1ad8
def main(ctx: typer.Context):
    """If no command is specified, launch the interactive menu."""
    ctx.obj = core_context
    if ctx.invoked_subcommand is None:
        console.print(
            "[bold green]No command specified. Launching interactive menu...[/bold green]"
        )
        launch_interactive_menu()


if __name__ == "__main__":
    app()

--- END OF FILE ./src/cli/admin_cli.py ---

--- START OF FILE ./src/cli/commands/__init__.py ---
# src/cli/commands_v2/__init__.py
"""Package marker for the V2 CLI command structure."""

from __future__ import annotations

--- END OF FILE ./src/cli/commands/__init__.py ---

--- START OF FILE ./src/cli/commands/check.py ---
# src/cli/commands/check.py
"""
Registers and implements the verb-based 'check' command group.
Refactored under dry_by_design to use the canonical context setter.
"""

from __future__ import annotations

from typing import Optional

import typer
from cli.logic.audit import audit, lint, test_system
from cli.logic.diagnostics import policy_coverage
from shared.context import CoreContext
from shared.logger import getLogger

log = getLogger("check_command")
check_app = typer.Typer(
    help="Read-only validation and health checks.",
    no_args_is_help=True,
)

_context: Optional[CoreContext] = None


check_app.command("audit", help="Run the full constitutional self-audit.")(audit)
check_app.command("lint", help="Check code formatting and quality.")(lint)
check_app.command("tests", help="Run the pytest suite.")(test_system)
check_app.command("diagnostics", help="Audit the constitution for policy coverage.")(
    policy_coverage
)

--- END OF FILE ./src/cli/commands/check.py ---

--- START OF FILE ./src/cli/commands/enrich.py ---
# src/cli/commands/enrich.py
"""
Registers the 'enrich' command group.
"""

from __future__ import annotations

import asyncio
from typing import Optional

import typer
from features.self_healing.enrichment_service import enrich_symbols
from rich.console import Console
from shared.context import CoreContext

console = Console()
enrich_app = typer.Typer(help="Autonomous tools to enrich the system's knowledge base.")

_context: Optional[CoreContext] = None


@enrich_app.command("symbols")
# ID: 7ecf56f5-c723-45f1-b1a8-4dbb19868968
def enrich_symbols_command(
    write: bool = typer.Option(
        False, "--write", help="Apply the generated descriptions to the database."
    ),
):
    """Uses an AI agent to write descriptions for symbols that have placeholders."""
    if not _context:
        raise typer.Exit("Context not set for enrich symbols command.")
    asyncio.run(enrich_symbols(_context.cognitive_service, dry_run=not write))

--- END OF FILE ./src/cli/commands/enrich.py ---

--- START OF FILE ./src/cli/commands/fix.py ---
# src/cli/commands/fix.py
"""
Registers the 'fix' command group.
"""

from __future__ import annotations

import asyncio
from pathlib import Path
from typing import Optional

import typer
from cli.logic.proposals_micro import micro_propose, propose_and_apply_autonomously
from features.maintenance.command_sync_service import sync_commands_to_db
from features.self_healing.capability_tagging_service import (
    tag_unassigned_capabilities,
)
from features.self_healing.clarity_service import fix_clarity
from features.self_healing.code_style_service import format_code
from features.self_healing.complexity_service import complexity_outliers
from features.self_healing.duplicate_id_service import resolve_duplicate_ids
from features.self_healing.id_tagging_service import assign_missing_ids
from features.self_healing.linelength_service import fix_line_lengths
from features.self_healing.policy_id_service import add_missing_policy_ids
from features.self_healing.prune_orphaned_vectors import (
    main_sync as prune_orphaned_vectors,
)
from features.self_healing.purge_legacy_tags_service import purge_legacy_tags
from rich.console import Console
from shared.context import CoreContext
from shared.logger import getLogger

log = getLogger("core_admin.fix")
console = Console()
fix_app = typer.Typer(
    help="Self-healing tools that write changes to the codebase.",
    no_args_is_help=True,
)

_context: Optional[CoreContext] = None


def _ensure_context() -> CoreContext:
    """Raises a clear error if context is not set."""
    if not _context:
        console.print("[red]Error: Context not initialized.[/red]")
        raise typer.Exit(code=1)
    return _context


def _print_completion_message(
    operation: str,
    total: int,
    dry_run: bool,
    next_step: Optional[str] = None,
) -> None:
    """Prints a consistent completion message for fix commands."""
    console.print(f"\n--- {operation} Complete ---")
    if total == 0 and not dry_run:
        console.print("[bold green]✅ No changes were needed.[/bold green]")
        return
    if dry_run:
        console.print(f"💧 DRY RUN: Found {total} item(s) to fix.")
        console.print("   Run with '--write' to apply these changes.")
    else:
        console.print(f"✅ APPLIED: Successfully updated {total} item(s).")
        if next_step:
            console.print(f"\n[bold]NEXT STEP:[/bold] {next_step}")


@fix_app.command(
    "code-style", help="Auto-format all code to be constitutionally compliant."
)
# ID: b4f50422-a599-4734-87b6-598fabe4474d
def format_code_wrapper() -> None:
    """Wrapper that calls the dedicated code style service directly."""
    format_code()


@fix_app.command(
    "headers", help="Enforces constitutional header conventions on Python files."
)
# ID: 4869877d-a484-425d-88ff-8a63b44ca6ef
def fix_headers_cmd(
    write: bool = typer.Option(
        False, "--write", help="Apply the changes autonomously."
    ),
) -> None:
    """User-friendly wrapper for the header fixing logic, now using the A1 loop."""
    ctx = _ensure_context()
    goal = "Fix all Python file headers in the `src` directory to be constitutionally compliant."
    if write:
        console.print(
            "[bold cyan]🚀 Initiating A1 self-healing for file headers...[/bold cyan]"
        )
        asyncio.run(propose_and_apply_autonomously(context=ctx, goal=goal))
    else:
        console.print(
            "[bold yellow]-- DRY RUN: Generating autonomous plan for fixing headers... --[/bold yellow]"
        )
        asyncio.run(micro_propose(context=ctx, goal=goal))


@fix_app.command(
    "docstrings", help="Adds missing docstrings using the A1 autonomy loop."
)
# ID: 75479ad2-99ab-4f7c-ad07-0c502f66a96e
def fix_docstrings_command(
    write: bool = typer.Option(
        False, "--write", help="Propose and apply the fix autonomously."
    ),
) -> None:
    """Uses the A1 micro-proposal loop to find and add missing docstrings."""
    ctx = _ensure_context()
    goal = "Add missing docstrings to all Python functions and methods in the `src` directory."
    if write:
        asyncio.run(propose_and_apply_autonomously(context=ctx, goal=goal))
    else:
        console.print(
            "[bold yellow]-- DRY RUN: Generating autonomous plan without applying it. --[/bold yellow]"
        )
        asyncio.run(micro_propose(context=ctx, goal=goal))


fix_app.command("line-lengths", help="Refactors files with long lines.")(
    fix_line_lengths
)
fix_app.command("clarity", help="Refactors a file for clarity.")(fix_clarity)


@fix_app.command(
    "complexity", help="Refactors complex code for better separation of concerns."
)
# ID: 2e61896f-cfa3-42fd-95ac-e8e1a7d04111
def complexity_command(
    file_path: Path = typer.Argument(
        ...,
        help="The path to a specific file to refactor for complexity.",
        exists=True,
        dir_okay=False,
        resolve_path=True,
    ),
    write: bool = typer.Option(
        False, "--write", help="Apply the refactoring to the file."
    ),
) -> None:
    """Identifies and refactors complexity outliers to improve separation of concerns."""
    complexity_outliers(file_path=file_path, dry_run=not write)


# --- START: THIS COMMAND WAS ACCIDENTALLY DELETED AND IS NOW RESTORED ---
@fix_app.command(
    "ids", help="Assigns a stable '# ID: <uuid>' to all untagged public symbols."
)
# ID: acc209a3-b54c-474d-9ac9-2cc3d04fb24a
def assign_ids_command(
    write: bool = typer.Option(
        False, "--write", help="Apply the changes to the files."
    ),
) -> None:
    """CLI wrapper for the symbol ID tagging service."""
    total_assigned = assign_missing_ids(dry_run=not write)
    _print_completion_message(
        operation="ID Assignment",
        total=total_assigned,
        dry_run=not write,
        next_step="Run 'poetry run core-admin manage database sync-knowledge --write' to update the database.",
    )


# --- END: RESTORED COMMAND ---


@fix_app.command(
    "purge-legacy-tags", help="Removes obsolete '# CAPABILITY:' tags from source code."
)
# ID: c1a30bf9-4a8c-48ac-9fe4-6621dec472ab
def purge_legacy_tags_command(
    write: bool = typer.Option(
        False, "--write", help="Apply the changes to the files."
    ),
) -> None:
    """CLI wrapper for the legacy tag purging service."""
    total_removed = purge_legacy_tags(dry_run=not write)
    _print_completion_message(
        operation="Purge",
        total=total_removed,
        dry_run=not write,
        next_step="Run 'poetry run core-admin manage database sync-knowledge --write' to update the database.",
    )


@fix_app.command(
    "policy-ids", help="Adds a unique `policy_id` UUID to any policy file missing one."
)
# ID: a4a70ed7-2bcd-42c0-9edc-40c6efa796cb
def fix_policy_ids_command(
    write: bool = typer.Option(
        False, "--write", help="Apply the changes to the files."
    ),
) -> None:
    """CLI wrapper for the policy ID migration service."""
    total_updated = add_missing_policy_ids(dry_run=not write)
    _print_completion_message(
        operation="Policy ID Migration",
        total=total_updated,
        dry_run=not write,
        next_step="Run 'poetry run core-admin check audit' to verify constitutional compliance.",
    )


@fix_app.command(
    "tags",
    help="Use an AI agent to suggest and apply capability tags to untagged symbols.",
)
# ID: 3159ad83-bea2-4a10-9d3a-b4598f4f3d1c
def fix_tags_command(
    file_path: Optional[Path] = typer.Argument(
        None,
        help="Optional: A specific file to process. If omitted, all files are scanned.",
        exists=True,
        dir_okay=False,
        resolve_path=True,
    ),
    write: bool = typer.Option(
        False, "--write", help="Apply the suggested tags directly to the files."
    ),
) -> None:
    """Wrapper for the CapabilityTaggerAgent that writes to the database."""
    ctx = _ensure_context()
    tag_unassigned_capabilities(
        cognitive_service=ctx.cognitive_service,
        knowledge_service=ctx.knowledge_service,
        file_path=file_path,
        write=write,
    )


@fix_app.command(
    "db-registry", help="Syncs the live CLI command structure to the database."
)
# ID: fc47ab7d-c9bf-4476-a703-4099c50e3946
def sync_db_registry_command() -> None:
    """CLI wrapper for the command sync service."""
    from cli.admin_cli import app as main_app

    asyncio.run(sync_commands_to_db(main_app))


@fix_app.command(
    "duplicate-ids", help="Finds and fixes duplicate '# ID:' tags in the codebase."
)
# ID: 6d709af4-5ab4-45bc-9360-f05b50d930c9
def fix_duplicate_ids_command(
    write: bool = typer.Option(False, "--write", help="Apply fixes to source files."),
) -> None:
    """CLI wrapper for the duplicate ID resolution service."""
    asyncio.run(resolve_duplicate_ids(dry_run=not write))


fix_app.command(
    "orphaned-vectors",
    help="Finds and deletes vectors in Qdrant that no longer exist in the main DB.",
)(prune_orphaned_vectors)

--- END OF FILE ./src/cli/commands/fix.py ---

--- START OF FILE ./src/cli/commands/inspect.py ---
# src/cli/commands/inspect.py
"""
Registers the new, verb-based 'inspect' command group.
Refactored under dry_by_design to use the canonical context setter.
"""

from __future__ import annotations

import asyncio
from typing import Optional

import typer
from cli.logic.diagnostics import cli_tree
from cli.logic.duplicates import inspect_duplicates
from cli.logic.guard_cli import register_guard
from cli.logic.knowledge import find_common_knowledge
from cli.logic.status import status
from cli.logic.symbol_drift import inspect_symbol_drift
from cli.logic.vector_drift import inspect_vector_drift
from shared.context import CoreContext

inspect_app = typer.Typer(
    help="Read-only commands to inspect system state and configuration.",
    no_args_is_help=True,
)

_context: Optional[CoreContext] = None


# ID: e1f2a3b4-c5d6-7e8f-9a0b-1c2d3e4f5a6b
def set_context(context: CoreContext):
    """Sets the shared context for the logic layer."""
    global _context
    _context = context


register_guard(inspect_app)
inspect_app.command("status")(status)
inspect_app.command("command-tree")(cli_tree)
inspect_app.command(
    "symbol-drift",
    help="Detects drift between symbols on the filesystem and in the database.",
)(inspect_symbol_drift)
inspect_app.command(
    "vector-drift",
    help="Verifies perfect synchronization between PostgreSQL and Qdrant.",
)(lambda: asyncio.run(inspect_vector_drift()))
inspect_app.command(
    "common-knowledge",
    help="Finds structurally identical helper functions that can be consolidated.",
)(find_common_knowledge)


@inspect_app.command(
    "duplicates", help="Runs only the semantic code duplication check."
)
# ID: ad11a02b-077d-4e17-b5bb-00ed77392bbd
def duplicates_command(
    threshold: float = typer.Option(
        0.80,
        "--threshold",
        "-t",
        help="The minimum similarity score to consider a duplicate.",
        min=0.5,
        max=1.0,
    ),
):
    """Wrapper to pass context and threshold to the inspect_duplicates logic."""
    if not _context:
        raise typer.Exit("Context not set for duplicates command.")
    inspect_duplicates(context=_context, threshold=threshold)

--- END OF FILE ./src/cli/commands/inspect.py ---

--- START OF FILE ./src/cli/commands/manage.py ---
# src/cli/commands/manage.py
"""
Registers the new, verb-based 'manage' command group with subgroups.
"""

from __future__ import annotations

import asyncio
from pathlib import Path

import typer
from cli.logic.byor import initialize_repository
from cli.logic.db import export_data, migrate_db
from cli.logic.project_docs import docs as project_docs
from cli.logic.proposal_service import (
    proposals_approve,
    proposals_list,
    proposals_sign,
)
from cli.logic.proposals_micro import micro_apply, micro_propose
from cli.logic.sync import sync_knowledge_base
from cli.logic.sync_manifest import sync_manifest
from features.governance.key_management_service import keygen
from features.maintenance.dotenv_sync_service import run_dotenv_sync
from features.maintenance.migration_service import run_ssot_migration
from features.project_lifecycle.definition_service import define_new_symbols
from features.project_lifecycle.scaffolding_service import new_project
from rich.console import Console
from shared.context import CoreContext

console = Console()
manage_app = typer.Typer(
    help="State-changing administrative tasks for the system.",
    no_args_is_help=True,
)

# NOTE: We are no longer using a module-level _context or set_context function.

db_sub_app = typer.Typer(
    help="Manage the database schema and data.", no_args_is_help=True
)
db_sub_app.command("migrate")(migrate_db)
db_sub_app.command("export")(export_data)
db_sub_app.command("sync-knowledge")(sync_knowledge_base)
db_sub_app.command("sync-manifest")(sync_manifest)


@db_sub_app.command(
    "migrate-ssot",
    help="One-time data migration from legacy files to the SSOT database.",
)
# ID: 3151802b-97cc-45aa-a4e0-c3bdb0b2e30d
def migrate_ssot_command(
    write: bool = typer.Option(
        False, "--write", help="Apply the migration to the database."
    ),
):
    asyncio.run(run_ssot_migration(dry_run=not write))


manage_app.add_typer(db_sub_app, name="database")

dotenv_sub_app = typer.Typer(
    help="Manage runtime configuration from .env.", no_args_is_help=True
)


@dotenv_sub_app.command(
    "sync",
    help="Sync settings from .env to the database, governed by runtime_requirements.yaml.",
)
# ID: bc1da88a-0fa5-45dc-9d34-57075abbcfcd
def dotenv_sync_command(
    write: bool = typer.Option(
        False, "--write", help="Apply the sync to the database."
    ),
):
    asyncio.run(run_dotenv_sync(dry_run=not write))


manage_app.add_typer(dotenv_sub_app, name="dotenv")

project_sub_app = typer.Typer(help="Manage CORE projects.", no_args_is_help=True)
project_sub_app.command("new")(new_project)
project_sub_app.command("onboard")(initialize_repository)
project_sub_app.command("docs")(project_docs)
manage_app.add_typer(project_sub_app, name="project")

proposals_sub_app = typer.Typer(
    help="Manage constitutional amendment proposals.", no_args_is_help=True
)
proposals_sub_app.command("list")(proposals_list)
proposals_sub_app.command("sign")(proposals_sign)


@proposals_sub_app.command("approve")
# ID: e50e9a6d-3efd-41e5-a472-1ce5d8ad2563
def approve_command_wrapper(
    ctx: typer.Context,  # <-- ADD THIS
    proposal_name: str = typer.Argument(
        ..., help="Filename of the proposal to approve."
    ),
):
    core_context: CoreContext = ctx.obj  # <-- ADD THIS
    proposals_approve(context=core_context, proposal_name=proposal_name)


@proposals_sub_app.command("micro-apply")
# ID: 486036d8-0553-4630-9f53-bc02fced4f73
def micro_apply_command(
    ctx: typer.Context,  # <-- ADD THIS
    proposal_path: Path = typer.Argument(..., exists=True),
):
    """Validates and applies a micro-proposal JSON file."""
    core_context: CoreContext = ctx.obj  # <-- ADD THIS
    asyncio.run(micro_apply(context=core_context, proposal_path=proposal_path))


@proposals_sub_app.command("micro-propose")
# ID: 45f5e69d-31f9-41ab-b019-1770136a06ea
def micro_propose_command(
    ctx: typer.Context,  # <-- ADD THIS
    goal: str = typer.Argument(...),
):
    """Generates a micro-proposal for a given goal without applying it."""
    core_context: CoreContext = ctx.obj  # <-- ADD THIS
    asyncio.run(micro_propose(context=core_context, goal=goal))


manage_app.add_typer(proposals_sub_app, name="proposals")

keys_sub_app = typer.Typer(
    help="Manage operator cryptographic keys.", no_args_is_help=True
)
keys_sub_app.command("generate")(keygen)
manage_app.add_typer(keys_sub_app, name="keys")


@manage_app.command(
    "define-symbols",
    help="Defines all undefined capabilities one by one using an AI agent.",
)
# ID: 63ef4a80-6f41-4700-8653-64a853a1f279
def define_symbols_command(
    ctx: typer.Context,  # <-- ADD THIS
):
    """Synchronous wrapper that calls the refactored definition service."""
    console.print(
        "[bold yellow]Running asynchronous symbol definition...[/bold yellow]"
    )
    core_context: CoreContext = ctx.obj  # <-- ADD THIS
    try:
        cognitive_service = core_context.cognitive_service
        asyncio.run(define_new_symbols(cognitive_service))
    except Exception as e:
        console.print(
            f"[bold red]An unexpected error occurred: {e}[/bold red]", highlight=False
        )
        raise typer.Exit(code=1)

--- END OF FILE ./src/cli/commands/manage.py ---

--- START OF FILE ./src/cli/commands/mind.py ---
"""
Registers the new 'mind' command group for managing the Working Mind's SSOT.
"""

from __future__ import annotations

import asyncio
from typing import Optional

import typer
from cli.logic.knowledge_sync import run_diff, run_import, run_snapshot, run_verify

mind_app = typer.Typer(
    help="Commands to manage the Working Mind (DB-as-SSOT).", no_args_is_help=True
)


@mind_app.command(
    "snapshot",
    help="Export the database to canonical YAML files in .intent/mind_export/.",
)
# ID: 92cdf207-d8c3-4665-a520-ddbd3714882b
def snapshot_command(
    env: Optional[str] = typer.Option(
        None, "--env", help="Environment tag (e.g., 'dev', 'prod')."
    ),
    note: Optional[str] = typer.Option(
        None, "--note", help="A brief note to store with the export manifest."
    ),
):
    """CLI wrapper for the snapshot logic."""
    asyncio.run(run_snapshot(env=env, note=note))


@mind_app.command(
    "diff", help="Compare the live database with the exported YAML files."
)
# ID: 818e271e-8878-429a-ac31-156f8902893e
def diff_command(
    as_json: bool = typer.Option(
        False, "--json", help="Output the diff in machine-readable JSON format."
    ),
):
    """CLI wrapper for the diff logic."""
    asyncio.run(run_diff(as_json=as_json))


@mind_app.command(
    "import", help="Import the exported YAML files into the database (idempotent)."
)
# ID: 661dded5-1fc8-4bff-a6b3-8912e937595d
def import_command(
    write: bool = typer.Option(
        False, "--write", help="Apply the import to the database."
    ),
):
    """CLI wrapper for the import logic."""
    asyncio.run(run_import(dry_run=not write))


@mind_app.command(
    "verify", help="Recomputes digests for exported files and fails on mismatch."
)
# ID: a1118547-c161-471f-9113-f15259a3be05
def verify_command():
    """CLI wrapper for the verification logic."""
    if not run_verify():
        raise typer.Exit(code=1)

--- END OF FILE ./src/cli/commands/mind.py ---

--- START OF FILE ./src/cli/commands/run.py ---
# src/cli/commands/run.py
"""
Registers and implements the 'run' command group for executing complex,
multi-step processes and autonomous cycles.
"""

from __future__ import annotations

import asyncio
from pathlib import Path
from typing import Optional

import typer
from cli.logic.run import develop
from features.introspection.vectorization_service import run_vectorize
from shared.context import CoreContext

run_app = typer.Typer(
    help="Commands for executing complex processes and autonomous cycles."
)

# NOTE: We are no longer using a module-level _context or set_context function.


@run_app.command("develop")
# ID: d10d0d34-054c-4923-bda9-1264f6d85813
def develop_command(
    ctx: typer.Context,  # <-- ADD THIS ARGUMENT
    goal: Optional[str] = typer.Argument(
        None,
        help="The high-level development goal for CORE to achieve.",
        show_default=False,
    ),
    from_file: Optional[Path] = typer.Option(
        None,
        "--from-file",
        "-f",
        help="Path to a file containing the development goal.",
        exists=True,
        dir_okay=False,
        resolve_path=True,
        show_default=False,
    ),
):
    """Orchestrates the autonomous development process from a high-level goal."""
    # --- START MODIFICATION ---
    # Get the context directly from the Typer context object. This is the robust way.
    core_context: CoreContext = ctx.obj
    # --- END MODIFICATION ---

    develop(context=core_context, goal=goal, from_file=from_file)


@run_app.command("vectorize")
# ID: 61b0c0e6-41ad-4050-bb23-54d39ef9e248
def vectorize_command(
    ctx: typer.Context,  # <-- ADD THIS ARGUMENT
    write: bool = typer.Option(False, "--write", help="Persist changes to Qdrant."),
    force: bool = typer.Option(
        False, "--force", help="Force re-vectorization of all capabilities."
    ),
):
    """Scan capabilities from the DB, generate embeddings, and upsert to Qdrant."""
    # --- START MODIFICATION ---
    # Get the context directly from the Typer context object.
    core_context: CoreContext = ctx.obj
    # --- END MODIFICATION ---

    cog = core_context.cognitive_service
    asyncio.run(run_vectorize(cognitive_service=cog, dry_run=not write, force=force))

--- END OF FILE ./src/cli/commands/run.py ---

--- START OF FILE ./src/cli/commands/search.py ---
# src/cli/commands/search.py
"""
Registers the new, verb-based 'search' command group.
Refactored under dry_by_design to use the canonical context setter.
"""

from __future__ import annotations

import asyncio
from typing import Optional

import typer
from cli.logic.hub import hub_search
from rich.console import Console
from rich.table import Table
from shared.context import CoreContext

console = Console()
search_app = typer.Typer(
    help="Discover capabilities and commands.",
    no_args_is_help=True,
)

_context: Optional[CoreContext] = None


# ID: ce6ffa34-2440-4188-bc95-0f6703651b9a
def search_knowledge_command(context: CoreContext, query: str, limit: int = 5) -> None:
    """Synchronous wrapper around async search."""

    async def _run() -> None:
        console.print(
            f"🧠 Searching for capabilities related to: '[cyan]{query}[/cyan]'..."
        )
        try:
            cognitive_service = context.cognitive_service
            results = await cognitive_service.search_capabilities(query, limit=limit)
            if not results:
                console.print("[yellow]No relevant capabilities found.[/yellow]")
                return

            table = Table(title="Top Matching Capabilities")
            table.add_column("Score", style="magenta", justify="right")
            table.add_column("Capability Key", style="cyan")
            table.add_column("Description", style="green")
            for hit in results:
                payload = hit.get("payload", {}) or {}
                key = payload.get("key", "N/A")
                description = (
                    payload.get("description") or "No description provided."
                ).strip()
                score = f"{hit.get('score', 0):.4f}"
                table.add_row(score, key, description)
            console.print(table)
        except Exception as e:
            console.print(f"[bold red]❌ Search failed: {e}[/bold red]")
            raise typer.Exit(code=1)

    asyncio.run(_run())


@search_app.command("capabilities")
# ID: 22dd2048-ebe7-490b-81f7-632d276585e6
def search_capabilities_wrapper(
    query: str,
    limit: int = 5,
):
    """Performs a semantic search for capabilities in the knowledge base."""
    if not _context:
        raise typer.Exit("Context not set for search capabilities command.")
    search_knowledge_command(context=_context, query=query, limit=limit)


search_app.command("commands")(hub_search)

--- END OF FILE ./src/cli/commands/search.py ---

--- START OF FILE ./src/cli/commands/secrets.py ---
# src/cli/commands/secrets.py
"""
CLI commands for managing encrypted secrets.

Usage:
    poetry run core-admin secrets generate-key
    poetry run core-admin secrets set anthropic.api_key
    poetry run core-admin secrets get anthropic.api_key
    poetry run core-admin secrets list
    poetry run core-admin secrets migrate-from-env
"""

import os
from typing import Optional

import typer
from rich.console import Console
from rich.table import Table

from core.secrets_service import SecretsService, get_secrets_service
from services.database.session_manager import get_session

app = typer.Typer(help="Manage encrypted secrets in the database")
console = Console()


@app.command()
def generate_key():
    """
    Generate a new master encryption key.

    Save this to CORE_MASTER_KEY in your .env file.
    Keep it safe - if you lose it, you lose all secrets!
    """
    key = SecretsService.generate_master_key()

    console.print("\n[bold green]Generated Master Key:[/bold green]")
    console.print(f"[yellow]{key}[/yellow]")
    console.print("\n[bold]Add this to your .env file:[/bold]")
    console.print(f'CORE_MASTER_KEY="{key}"')
    console.print("\n[red]⚠️  Keep this key safe! Store it securely (e.g., password manager)[/red]")


@app.command()
def set(
    key: str = typer.Argument(..., help="Secret key (e.g., anthropic.api_key)"),
    value: Optional[str] = typer.Option(None, "--value", help="Secret value (prompted if not provided)"),
    description: Optional[str] = typer.Option(None, "--description", "-d", help="Human-readable description"),
):
    """
    Store an encrypted secret in the database.

    Example:
        poetry run core-admin secrets set anthropic.api_key --value "sk-ant-..."
    """
    if not value:
        value = typer.prompt("Enter secret value", hide_input=True)

    async def _set():
        async with get_session() as db:
            secrets = await get_secrets_service(db)
            await secrets.set_secret(db, key, value, description)

    import asyncio
    asyncio.run(_set())

    console.print(f"[green]✓[/green] Secret '{key}' stored successfully")


@app.command()
def get(
    key: str = typer.Argument(..., help="Secret key to retrieve"),
    show: bool = typer.Option(False, "--show", help="Display the secret value (WARNING: visible in terminal)"),
):
    """
    Retrieve a secret from the database.

    By default, does not display the value (for security).
    Use --show to display it.
    """
    async def _get():
        async with get_session() as db:
            secrets = await get_secrets_service(db)
            return await secrets.get_secret(db, key, audit_context="cli")

    import asyncio
    value = asyncio.run(_get())

    if show:
        console.print(f"[yellow]{value}[/yellow]")
    else:
        console.print(f"[green]✓[/green] Secret '{key}' exists (use --show to display)")


@app.command()
def list():
    """
    List all secret keys in the database (does not show values).
    """
    async def _list():
        async with get_session() as db:
            secrets = await get_secrets_service(db)
            return await secrets.list_secrets(db)

    import asyncio
    secrets_list = asyncio.run(_list())

    if not secrets_list:
        console.print("[yellow]No secrets found in database[/yellow]")
        return

    table = Table(title="Encrypted Secrets")
    table.add_column("Key", style="cyan")
    table.add_column("Description", style="white")
    table.add_column("Last Updated", style="dim")

    for secret in secrets_list:
        table.add_row(
            secret["key"],
            secret["description"] or "",
            str(secret["last_updated"]) if secret["last_updated"] else "N/A",
        )

    console.print(table)


@app.command()
def delete(
    key: str = typer.Argument(..., help="Secret key to delete"),
    confirm: bool = typer.Option(False, "--yes", "-y", help="Skip confirmation prompt"),
):
    """
    Delete a secret from the database.
    """
    if not confirm:
        confirmed = typer.confirm(f"Are you sure you want to delete secret '{key}'?")
        if not confirmed:
            console.print("[yellow]Aborted[/yellow]")
            return

    async def _delete():
        async with get_session() as db:
            secrets = await get_secrets_service(db)
            await secrets.delete_secret(db, key)

    import asyncio
    asyncio.run(_delete())

    console.print(f"[green]✓[/green] Secret '{key}' deleted")


@app.command()
def migrate_from_env(
    env_file: str = typer.Option(".env", "--file", "-f", help="Path to .env file to migrate from"),
    dry_run: bool = typer.Option(False, "--dry-run", help="Show what would be migrated without doing it"),
):
    """
    Migrate API keys from .env file to encrypted database storage.

    This command:
    1. Reads API keys from .env
    2. Encrypts them
    3. Stores them in core.runtime_settings
    4. Shows you which lines to DELETE from .env

    Example:
        poetry run core-admin secrets migrate-from-env
        poetry run core-admin secrets migrate-from-env --dry-run
    """
    from dotenv import dotenv_values

    # Load env vars
    env_vars = dotenv_values(env_file)

    if not env_vars:
        console.print(f"[red]Error:[/red] Could not read {env_file}")
        return

    # Check for master key
    master_key = os.getenv("CORE_MASTER_KEY")
    if not master_key:
        console.print("[red]Error:[/red] CORE_MASTER_KEY not set in environment")
        console.print("Run: poetry run core-admin secrets generate-key")
        return

    # Find API keys in env
    api_key_vars = {
        k: v for k, v in env_vars.items()
        if "API_KEY" in k and v and v != '""' and v != "''"
    }

    if not api_key_vars:
        console.print("[yellow]No API keys found in .env file[/yellow]")
        return

    console.print(f"\n[bold]Found {len(api_key_vars)} API keys to migrate:[/bold]")
    for key in api_key_vars.keys():
        console.print(f"  • {key}")

    if dry_run:
        console.print("\n[yellow]Dry run - no changes made[/yellow]")
        return

    # Migrate
    async def _migrate():
        async with get_session() as db:
            return await SecretsService.migrate_from_env(db, api_key_vars, master_key)

    import asyncio
    migrated = asyncio.run(_migrate())

    console.print(f"\n[green]✓ Migrated {len(migrated)} secrets to database[/green]")

    # Show what to delete from .env
    console.print("\n[bold]Now delete these lines from .env:[/bold]")
    for env_name in migrated.keys():
        console.print(f'  [red]-[/red] {env_name}="..."')

    console.print("\n[yellow]⚠️  Keep CORE_MASTER_KEY in .env![/yellow]")


@app.command()
def rotate(
    key: str = typer.Argument(..., help="Secret key to rotate"),
    new_value: Optional[str] = typer.Option(None, "--value", help="New secret value"),
):
    """
    Rotate a secret (change its value while keeping audit trail).
    """
    if not new_value:
        new_value = typer.prompt("Enter new secret value", hide_input=True)

    async def _rotate():
        async with get_session() as db:
            secrets = await get_secrets_service(db)
            await secrets.rotate_secret(db, key, new_value)

    import asyncio
    asyncio.run(_rotate())

    console.print(f"[green]✓[/green] Secret '{key}' rotated successfully")


if __name__ == "__main__":
    app()
--- END OF FILE ./src/cli/commands/secrets.py ---

--- START OF FILE ./src/cli/commands/submit.py ---
# src/cli/commands/submit.py
"""
Registers the new, high-level 'submit' workflow command.
"""

from __future__ import annotations

import asyncio
from typing import Optional

import typer
from features.project_lifecycle.integration_service import integrate_changes
from shared.context import CoreContext

submit_app = typer.Typer(
    help="High-level workflow commands for developers.",
    no_args_is_help=True,
)

_context: Optional[CoreContext] = None


@submit_app.command(
    "changes",
    help="The primary workflow to integrate staged code changes into the system.",
)
# ID: 2d1e8a9f-7b6c-4d5e-8f9a-0b1c2d3e4f5a
def integrate_command(
    ctx: typer.Context,
    commit_message: str = typer.Option(
        ..., "-m", "--message", help="The git commit message for this integration."
    ),
):
    """Orchestrates the full, autonomous integration of staged code changes."""
    core_context: CoreContext = ctx.obj
    asyncio.run(integrate_changes(context=core_context, commit_message=commit_message))

--- END OF FILE ./src/cli/commands/submit.py ---

--- START OF FILE ./src/cli/interactive.py ---
# src/cli/interactive.py
"""
Implements the interactive, menu-driven TUI for the CORE Admin CLI.
This provides a user-friendly way to discover and run commands.
"""

from __future__ import annotations

import sys
from typing import Callable, Dict

from rich.console import Console
from rich.panel import Panel
from shared.utils.subprocess_utils import run_poetry_command

console = Console()


def _show_menu(title: str, options: Dict[str, str], actions: Dict[str, Callable]):
    """Generic helper to display a menu, get input, and execute an action."""
    while True:
        console.clear()
        console.print(Panel(f"[bold cyan]{title}[/bold cyan]"))
        for key, text in options.items():
            console.print(f"  [{key}] {text}")

        console.print("\n  [b] Back to main menu")
        console.print("  [q] Quit")
        choice = console.input("\nEnter your choice: ").lower()

        if choice == "b":
            return
        if choice == "q":
            sys.exit(0)

        action = actions.get(choice)
        if action:
            try:
                action()
            except Exception as e:
                console.print(f"[bold red]Command failed: {e}[/bold red]")
            console.print(
                "\n[bold green]Press Enter to return to the menu...[/bold green]"
            )
            input()
        else:
            console.print(
                f"[bold red]Invalid choice '{choice}'. Please try again.[/bold red]"
            )
            input("Press Enter to continue...")


# ID: e4f81e87-71c1-41c1-bfed-fdba926db71f
def show_development_menu():
    """Displays the AI Development & Self-Healing submenu."""
    _show_menu(
        title="AI Development & Self-Healing",
        options={
            "1": "Chat with CORE (Translate idea to command)",
            "2": "Develop (Execute a high-level goal)",
            "3": "Fix Headers (Run AI-powered style fixer)",
        },
        actions={
            "1": lambda: run_poetry_command(
                "Translating goal...",
                ["core-admin", "chat", console.input("Enter your goal: ")],
            ),
            "2": lambda: run_poetry_command(
                "Executing goal...",
                [
                    "core-admin",
                    "run",
                    "develop",
                    console.input("Enter the full development goal: "),
                ],
            ),
            "3": lambda: run_poetry_command(
                "Fixing headers...", ["core-admin", "fix", "headers", "--write"]
            ),
        },
    )


# ID: 91af5862-021e-4c3b-ba18-51deb032382c
def show_governance_menu():
    """Displays the Constitutional Governance submenu."""
    _show_menu(
        title="Constitutional Governance",
        options={
            "1": "List Proposals",
            "2": "Sign a Proposal",
            "3": "Approve a Proposal",
            "4": "Generate a new Operator Key",
            "5": "Review Constitution (AI Peer Review)",
        },
        actions={
            "1": lambda: run_poetry_command(
                "Listing proposals...", ["core-admin", "manage", "proposals", "list"]
            ),
            "2": lambda: run_poetry_command(
                "Signing proposal...",
                [
                    "core-admin",
                    "manage",
                    "proposals",
                    "sign",
                    console.input("Enter proposal filename to sign: "),
                ],
            ),
            "3": lambda: run_poetry_command(
                "Approving proposal...",
                [
                    "core-admin",
                    "manage",
                    "proposals",
                    "approve",
                    console.input("Enter proposal filename to approve: "),
                ],
            ),
            "4": lambda: run_poetry_command(
                "Generating key...",
                [
                    "core-admin",
                    "manage",
                    "keys",
                    "generate",
                    console.input("Enter identity for key (e.g., email): "),
                ],
            ),
            "5": lambda: run_poetry_command(
                "Reviewing constitution...", ["core-admin", "review", "constitution"]
            ),
        },
    )


# ID: 38f63e99-7a3d-4734-9aaa-188e99e44846
def show_system_menu():
    """Displays the System Health & CI submenu."""
    _show_menu(
        title="System Health & CI",
        options={
            "1": "Run Full Check (lint, test, audit)",
            "2": "Run Only Tests",
            "3": "Format All Code",
        },
        actions={
            "1": lambda: run_poetry_command(
                "Running system check...", ["core-admin", "check", "system"]
            ),
            "2": lambda: run_poetry_command(
                "Running tests...", ["core-admin", "check", "tests"]
            ),
            "3": lambda: run_poetry_command(
                "Formatting code...", ["core-admin", "fix", "code-style"]
            ),
        },
    )


# ID: b13f7aa2-3d3a-4442-af86-19bfb95ccfb9
def show_project_lifecycle_menu():
    """Displays the Project Lifecycle submenu."""
    _show_menu(
        title="Project Lifecycle",
        options={
            "1": "Create New Governed Application",
            "2": "Onboard Existing Repository (BYOR)",
        },
        actions={
            "1": lambda: run_poetry_command(
                "Creating new application...",
                [
                    "core-admin",
                    "manage",
                    "project",
                    "new",
                    console.input("Enter the name for the new application: "),
                    "--write",
                ],
            ),
            "2": lambda: run_poetry_command(
                "Onboarding repository...",
                [
                    "core-admin",
                    "manage",
                    "project",
                    "onboard",
                    console.input("Enter the path to the existing repository: "),
                    "--write",
                ],
            ),
        },
    )


# ID: 0493a7e1-3b54-478c-b22f-490a36be8b61
def launch_interactive_menu():
    """The main entry point for the interactive TUI menu."""
    while True:
        console.clear()
        console.print(
            Panel(
                "[bold green]🏛️ Welcome to the CORE Interactive Shell[/bold green]",
                subtitle="Select a command group",
            )
        )
        console.print("[bold cyan]1.[/bold cyan] AI Development & Self-Healing")
        console.print("[bold cyan]2.[/bold cyan] Constitutional Governance")
        console.print("[bold cyan]3.[/bold cyan] System Health & CI")
        console.print("[bold cyan]4.[/bold cyan] Project Lifecycle")
        console.print("\n[bold red]q.[/bold red] Quit")

        choice = console.input("\nEnter your choice: ")

        if choice == "1":
            show_development_menu()
        elif choice == "2":
            show_governance_menu()
        elif choice == "3":
            show_system_menu()
        elif choice == "4":
            show_project_lifecycle_menu()
        elif choice.lower() == "q":
            break

--- END OF FILE ./src/cli/interactive.py ---

--- START OF FILE ./src/cli/logic/__init__.py ---
# src/cli/commands/__init__.py
"""
This file marks the 'commands' directory as a Python package,
allowing command modules to be imported from here.
"""

from __future__ import annotations

--- END OF FILE ./src/cli/logic/__init__.py ---

--- START OF FILE ./src/cli/logic/agent.py ---
# src/cli/logic/agent.py
"""
Provides a CLI interface for human operators to directly invoke autonomous agent capabilities like application scaffolding.
"""

from __future__ import annotations

import json
import textwrap
from typing import Any

import typer
from features.project_lifecycle.scaffolding_service import Scaffolder
from shared.context import CoreContext
from shared.logger import getLogger

log = getLogger("core_admin.agent")
agent_app = typer.Typer(help="Directly invoke autonomous agent capabilities.")


def _extract_json_from_response(text: str) -> Any:
    """Helper to extract JSON from LLM responses for scaffolding."""
    import re

    match = re.search(r"```json\s*(\{[\s\S]*?\}|\[[\s\S]*?\])\s*```", text, re.DOTALL)
    if match:
        return json.loads(match.group(1))
    return json.loads(text)


# ID: 610428b9-0edd-43be-ae98-8077f1444ad9
async def scaffold_new_application(
    context: CoreContext,
    project_name: str,
    goal: str,
    initialize_git: bool = False,
) -> tuple[bool, str]:
    """Uses an LLM to plan and generate a new, multi-file application."""
    log.info(f"🌱 Starting to scaffold new application '{project_name}'...")
    cognitive_service = context.cognitive_service
    await cognitive_service.initialize()

    prompt_template = textwrap.dedent(
        """
        You are a senior software architect. Your task is to design the file structure and content for a new Python application based on a high-level goal.

        **Goal:** "{goal}"

        **Instructions:**
        1.  Think step-by-step about the necessary files for a minimal, working version.
        2.  Your output MUST be a single, valid JSON object with file paths as keys and content as values.
        3.  Include a `pyproject.toml` and a simple `src/main.py`.
        4.  Keep the code simple, clean, and functional.
        """
    ).strip()

    final_prompt = prompt_template.format(goal=goal)
    try:
        planner_client = await cognitive_service.aget_client_for_role("Planner")
        response_text = await planner_client.make_request_async(
            final_prompt, user_id="scaffolding_agent"
        )
        file_structure = _extract_json_from_response(response_text)

        if not isinstance(file_structure, dict):
            raise ValueError("LLM did not return a valid JSON object of files.")

        log.info(f"   -> LLM planned a structure with {len(file_structure)} files.")

        scaffolder = Scaffolder(project_name=project_name)
        scaffolder.scaffold_base_structure()

        for rel_path, content in file_structure.items():
            scaffolder.write_file(rel_path, content)

        log.info("   -> Adding starter test and CI workflow...")
        test_template_path = scaffolder.starter_kit_path / "test_main.py.template"
        ci_template_path = scaffolder.starter_kit_path / "ci.yml.template"

        if test_template_path.exists():
            test_content = test_template_path.read_text(encoding="utf-8").format(
                project_name=project_name
            )
            scaffolder.write_file("tests/test_main.py", test_content)

        if ci_template_path.exists():
            ci_content = ci_template_path.read_text(encoding="utf-8")
            scaffolder.write_file(".github/workflows/ci.yml", ci_content)

        if initialize_git:
            git_service = context.git_service
            log.info(
                f"   -> Initializing new Git repository in {scaffolder.project_root}..."
            )
            # --- THIS IS THE FIX ---
            # Use the sanctioned GitService instead of subprocess
            git_service.init(scaffolder.project_root)
            # Re-initialize the service to operate within the new repo
            scoped_git_service = context.git_service.__class__(scaffolder.project_root)
            scoped_git_service.add_all()
            scoped_git_service.commit(
                f"feat(scaffold): Initial commit for '{project_name}'"
            )
            # --- END OF FIX ---

        return (
            True,
            f"✅ Successfully scaffolded '{project_name}'.",
        )

    except Exception as e:
        log.error(f"❌ Scaffolding failed: {e}", exc_info=True)
        return False, f"Scaffolding failed: {str(e)}"


@agent_app.command("scaffold")
# ID: f2ade2e5-5b27-513d-ae87-90aa3d3687a6
async def agent_scaffold(
    ctx: typer.Context,
    name: str = typer.Argument(..., help="The directory name for the new application."),
    goal: str = typer.Argument(..., help="A high-level goal for the application."),
    git_init: bool = typer.Option(
        True, "--git/--no-git", help="Initialize a Git repository."
    ),
):
    """Uses an LLM agent to autonomously scaffold a new application."""
    log.info(f"🤖 Invoking Agent to scaffold application '{name}'...")
    log.info(f"   -> Goal: '{goal}'")

    core_context: CoreContext = ctx.obj
    success, message = await scaffold_new_application(
        context=core_context,
        project_name=name,
        goal=goal,
        initialize_git=git_init,
    )

    if success:
        typer.secho(f"\n{message}", fg=typer.colors.GREEN)
    else:
        typer.secho(f"\n{message}", fg=typer.colors.RED)
        raise typer.Exit(code=1)

--- END OF FILE ./src/cli/logic/agent.py ---

--- START OF FILE ./src/cli/logic/audit.py ---
# src/cli/logic/audit.py
"""
Implements high-level CI and system health checks, including the main constitutional audit.
"""

from __future__ import annotations

import asyncio
from collections import defaultdict
from pathlib import Path
from typing import Optional

import typer
from features.governance.constitutional_auditor import ConstitutionalAuditor
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from shared.context import CoreContext
from shared.models import AuditFinding, AuditSeverity
from shared.utils.subprocess_utils import run_poetry_command

from .cli_utils import find_test_file_for_capability_async

console = Console()

# Global variable to store context, set by the main admin_cli.py
_context: Optional[CoreContext] = None


# ID: 8afdeab9-fc81-4d7c-b05f-dd27f936b3e6
def lint():
    """Checks code formatting and quality using Black and Ruff."""
    run_poetry_command(
        "🔎 Checking code format with Black...", ["black", "--check", "src", "tests"]
    )
    run_poetry_command(
        "🔎 Checking code quality with Ruff...", ["ruff", "check", "src", "tests"]
    )


# ID: f4d514f7-e277-446e-98ff-06e881710a99
def test_system(
    target: str | None = typer.Argument(
        None, help="Optional: A specific test file path or a capability ID."
    ),
):
    """Run the pytest suite, optionally targeting a specific test file or capability."""

    async def _async_test_system():
        command = ["pytest"]
        description = "🧪 Running all tests with pytest..."
        if isinstance(target, str):
            target_path = Path(target)
            if target_path.exists() and target_path.is_file():
                command.append(str(target_path))
                description = f"🧪 Running tests for file: {target}"
            else:
                test_file = await find_test_file_for_capability_async(target)
                if test_file:
                    command.append(str(test_file))
                    description = (
                        f"🧪 Running tests for ID '{target}' in {test_file.name}..."
                    )
                else:
                    console.print(
                        f"❌ Could not find a test file for target: '{target}'."
                    )
                    raise typer.Exit(code=1)
        run_poetry_command(description, command)

    asyncio.run(_async_test_system())


# ID: f7bc6512-03d2-4bf9-b718-6fb9323e38ea
def audit(
    severity: str = typer.Option(
        "warning",
        "--severity",
        "-s",
        help="Filter findings by minimum severity level (info, warning, error).",
        case_sensitive=False,
    ),
    verbose: bool = typer.Option(
        False,
        "--verbose",
        "-v",
        help="Show all individual findings instead of a summary.",
    ),
):
    """Run a full constitutional self-audit and print a summary of findings."""
    if _context is None:
        console.print("[bold red]Error: Context not initialized for audit[/bold red]")
        raise typer.Exit(code=1)

    async def _async_audit():
        auditor = ConstitutionalAuditor(_context.auditor_context)
        all_findings_dicts = await auditor.run_full_audit_async()

        # --- THIS IS THE FIX ---
        # Re-hydrate the dictionaries into proper AuditFinding objects,
        # correctly converting the severity string back to an Enum member.
        severity_map = {str(s): s for s in AuditSeverity}
        all_findings = []
        for f_dict in all_findings_dicts:
            severity_str = f_dict.get("severity", "info")
            f_dict["severity"] = severity_map.get(severity_str, AuditSeverity.INFO)
            all_findings.append(AuditFinding(**f_dict))
        # --- END OF FIX ---

        unassigned_count = len(
            [f for f in all_findings if f.check_id == "linkage.capability.unassigned"]
        )
        blocking_errors = [f for f in all_findings if f.severity.is_blocking]
        passed = not bool(blocking_errors)

        try:
            min_severity = AuditSeverity[severity.upper()]
        except KeyError:
            console.print(
                f"[bold red]Invalid severity level '{severity}'. Must be 'info', 'warning', or 'error'.[/bold red]"
            )
            raise typer.Exit(code=1)

        filtered_findings = [f for f in all_findings if f.severity >= min_severity]

        summary_table = Table.grid(expand=True, padding=(0, 1))
        summary_table.add_column(justify="left")
        summary_table.add_column(justify="right", style="bold")
        errors = [f for f in all_findings if f.severity.is_blocking]
        warnings = [f for f in all_findings if f.severity == AuditSeverity.WARNING]
        summary_table.add_row("Errors:", f"[red]{len(errors)}[/red]")
        summary_table.add_row("Warnings:", f"[yellow]{len(warnings)}[/yellow]")
        summary_table.add_row("Unassigned Symbols:", f"[cyan]{unassigned_count}[/cyan]")

        title = "✅ AUDIT PASSED" if passed else "❌ AUDIT FAILED"
        style = "bold green" if passed else "bold red"
        console.print(Panel(summary_table, title=title, style=style, expand=False))

        if filtered_findings:
            if verbose:
                _print_verbose_findings(filtered_findings)
            else:
                _print_summary_findings(filtered_findings)

        if not passed:
            raise typer.Exit(1)

    asyncio.run(_async_audit())


def _print_verbose_findings(findings: list[AuditFinding]):
    """Prints every single finding in a detailed table."""
    console.print("\n[bold]Audit Findings (Verbose):[/bold]")
    table = Table()
    table.add_column("Severity", style="bold")
    table.add_column("Check ID")
    table.add_column("Message")
    table.add_column("File:Line")

    for f in sorted(findings, key=lambda x: x.severity, reverse=True):
        color = {"error": "red", "warning": "yellow", "info": "cyan"}.get(
            str(f.severity), "white"
        )
        loc = (
            f"{f.file_path}:{f.line_number}"
            if f.file_path and f.line_number
            else f.file_path or ""
        )
        table.add_row(
            f"[{color}]{str(f.severity).upper()}[/{color}]",
            f.check_id,
            f.message,
            loc,
        )
    console.print(table)


def _print_summary_findings(findings: list[AuditFinding]):
    """Groups findings by check ID and prints a summary table."""
    console.print("\n[bold]Audit Findings (Summary):[/bold]")
    grouped = defaultdict(list)
    for f in findings:
        grouped[f.check_id].append(f)

    table = Table()
    table.add_column("Severity", style="bold")
    table.add_column("Count", justify="right")
    table.add_column("Check ID")
    table.add_column("Sample Message")

    # Sort by severity (errors first), then by count
    sorted_check_ids = sorted(
        grouped.keys(),
        key=lambda k: (max(f.severity for f in grouped[k]), len(grouped[k])),
        reverse=True,
    )

    for check_id in sorted_check_ids:
        items = grouped[check_id]
        sample = items[0]
        count = len(items)
        severity = sample.severity

        color = {"error": "red", "warning": "yellow", "info": "cyan"}.get(
            str(severity), "white"
        )
        table.add_row(
            f"[{color}]{str(severity).upper()}[/{color}]",
            str(count),
            check_id,
            sample.message,
        )
    console.print(table)
    console.print("\nRun with '--verbose' to see all individual findings.")

--- END OF FILE ./src/cli/logic/audit.py ---

--- START OF FILE ./src/cli/logic/audit_capability_domains.py ---
# src/cli/logic/audit_capability_domains.py
"""
Provides functionality for the audit_capability_domains module.
"""

from __future__ import annotations

import typer

# CORRECTED IMPORT: Now points to the single source of truth for sessions.
from services.database.session_manager import get_session
from sqlalchemy import text


async def _audit_queries(limit: int):
    """Audit capabilities database for data quality issues,
    returning counts of total capabilities and lists of keys with
    zero tags, multiple primary domains, legacy domain mismatches,
    and inactive domain tags."""
    async with get_session() as session:
        total = (
            await session.execute(text("select count(*) as c from core.capabilities"))
        ).scalar_one()

        zero_tags_stmt = text(
            """
            select c.key
            from core.capabilities c
            where not exists (
              select 1 from core.capability_domains d
              where d.capability_key = c.key
            )
            limit :lim
            """
        ).bindparams(lim=limit)
        zero_tags_rows = (await session.execute(zero_tags_stmt)).scalars().all()

        multi_primary_stmt = text(
            """
            select capability_key
            from core.capability_domains
            group by capability_key
            having sum(case when is_primary then 1 else 0 end) > 1
            limit :lim
            """
        ).bindparams(lim=limit)
        multi_primary_rows = (await session.execute(multi_primary_stmt)).scalars().all()

        legacy_mismatch_stmt = text(
            """
            select c.key
            from core.capabilities c
            where c.domain is not null
              and not exists (
                select 1 from core.capability_domains d
                where d.capability_key = c.key
                  and d.domain_key = c.domain
              )
            limit :lim
            """
        ).bindparams(lim=limit)
        legacy_mismatch_rows = (
            (await session.execute(legacy_mismatch_stmt)).scalars().all()
        )

        inactive_domain_tags_stmt = text(
            """
            select distinct d.capability_key
            from core.capability_domains d
            join core.domains dm on dm.key = d.domain_key
            where dm.status != 'active'
            limit :lim
            """
        ).bindparams(lim=limit)
        inactive_tag_rows = (
            (await session.execute(inactive_domain_tags_stmt)).scalars().all()
        )

        return (
            total,
            zero_tags_rows,
            multi_primary_rows,
            legacy_mismatch_rows,
            inactive_tag_rows,
        )


# ID: a2d0d438-253f-49ba-82be-10eb2a2a7749
def audit_capability_domains(
    limit: int = typer.Option(
        20, "--limit", help="Max sample keys to show for each finding"
    ),
):
    """Audit capability domains for common tagging issues and display findings with sample keys."""
    total, zero_tags, multi_primary, legacy_mismatch, inactive_tags = typer.run(
        _audit_queries, limit
    )

    typer.echo(f"Total capabilities: {total}")
    typer.echo(f"Zero tags: {len(zero_tags)}  {zero_tags}")
    typer.echo(f"Multiple primary tags: {len(multi_primary)}  {multi_primary}")
    typer.echo(
        f"Legacy domain not among tags: {len(legacy_mismatch)}  {legacy_mismatch}"
    )
    typer.echo(f"Tags on INACTIVE domains: {len(inactive_tags)}  {inactive_tags}")

--- END OF FILE ./src/cli/logic/audit_capability_domains.py ---

--- START OF FILE ./src/cli/logic/build.py ---
"""
Registers and implements the 'build' command group for generating
artifacts from the database or constitution.
"""

from __future__ import annotations

import typer
from features.introspection.generate_capability_docs import (
    main as generate_capability_docs,
)

build_app = typer.Typer(
    help="Commands to build artifacts (e.g., documentation) from the database."
)
build_app.command(
    "capability-docs",
    help="Generate the capability reference documentation from the DB.",
)(generate_capability_docs)

--- END OF FILE ./src/cli/logic/build.py ---

--- START OF FILE ./src/cli/logic/byor.py ---
# src/cli/logic/byor.py
"""
Implements the 'byor-init' command to analyze external repositories and scaffold minimal CORE governance structures.
"""

from __future__ import annotations

from pathlib import Path

import typer
import yaml
from features.introspection.knowledge_graph_service import KnowledgeGraphBuilder
from shared.logger import getLogger

log = getLogger("core_admin.byor")
CORE_ROOT = Path(__file__).resolve().parents[2]
TEMPLATES_DIR = (
    CORE_ROOT / "src" / "features" / "project_lifecycle" / "starter_kits" / "default"
)


# ID: 2c141c77-c07b-48a3-b001-d607d6ed9a39
def initialize_repository(
    path: Path = typer.Argument(
        ...,
        help="The path to the external repository to analyze.",
        exists=True,
        file_okay=False,
        dir_okay=True,
        resolve_path=True,
    ),
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--write",
        help="Show the proposed .intent/ scaffold without writing files. Use --write to apply.",
    ),
):
    """
    Analyzes an external repository and scaffolds a minimal `.intent/` constitution.
    """
    log.info(f"🚀 Starting analysis of repository at: {path}")

    log.info("   -> Step 1: Building Knowledge Graph of the target repository...")
    try:
        builder = KnowledgeGraphBuilder(root_path=path)
        graph = builder.build()
        total_symbols = len(graph.get("symbols", {}))
        log.info(
            f"   -> ✅ Knowledge Graph built successfully. Found {total_symbols} symbols."
        )
    except Exception as e:
        log.error(f"   -> ❌ Failed to build Knowledge Graph: {e}", exc_info=True)
        raise typer.Exit(code=1)

    log.info("   -> Step 2: Generating starter constitution from analysis...")

    domains = builder.domain_map
    source_structure_content = {
        "structure": [
            {
                "domain": name,
                "path": path_str,
                "description": f"Domain for '{name}' inferred by CORE.",
                "allowed_imports": [name, "shared"],
            }
            for path_str, name in domains.items()
        ]
    }

    discovered_capabilities = sorted(
        list(
            set(
                s["capability"]
                for s in graph.get("symbols", {}).values()
                if s.get("capability") != "unassigned"
            )
        )
    )
    project_manifest_content = {
        "name": path.name,
        "version": "0.1.0-core-scaffold",
        "intent": "A high-level description of what this project is intended to do.",
        "required_capabilities": discovered_capabilities,
    }

    (TEMPLATES_DIR / "capability_tags.yaml.template").read_text()
    capability_tags_content = {
        "tags": [
            {
                "name": cap,
                "description": "A clear explanation of what this capability does.",
            }
            for cap in discovered_capabilities
        ]
    }

    files_to_generate = {
        ".intent/knowledge/source_structure.yaml": source_structure_content,
        ".intent/project_manifest.yaml": project_manifest_content,
        ".intent/knowledge/capability_tags.yaml": capability_tags_content,
        ".intent/mission/principles.yaml": (
            TEMPLATES_DIR / "principles.yaml"
        ).read_text(),
        ".intent/policies/safety_policies.yaml": (
            TEMPLATES_DIR / "safety_policies.yaml"
        ).read_text(),
    }

    if dry_run:
        log.info("\n💧 Dry Run Mode: No files will be written.")
        for rel_path, content in files_to_generate.items():
            typer.secho(f"\n📄 Proposed `{rel_path}`:", fg=typer.colors.YELLOW)
            if isinstance(content, dict):
                typer.echo(yaml.dump(content, indent=2))
            else:
                typer.echo(content)
    else:
        log.info("\n💾 **Write Mode:** Applying changes to disk.")
        for rel_path, content in files_to_generate.items():
            target_path = path / rel_path
            target_path.parent.mkdir(parents=True, exist_ok=True)
            if isinstance(content, dict):
                target_path.write_text(yaml.dump(content, indent=2))
            else:
                target_path.write_text(content)
            typer.secho(
                f"   -> ✅ Wrote starter file to {target_path}", fg=typer.colors.GREEN
            )

    log.info("\n🎉 BYOR initialization complete.")


# The obsolete `register` function has been removed.

--- END OF FILE ./src/cli/logic/byor.py ---

--- START OF FILE ./src/cli/logic/capability.py ---
# src/cli/logic/capability.py
"""
Provides the 'core-admin capability' command group for managing capabilities
in a constitutionally-aligned way. THIS MODULE IS NOW DEPRECATED and will be
removed after the DB-centric migration is complete.
"""

from __future__ import annotations

import typer
from rich.console import Console

console = Console()

capability_app = typer.Typer(help="[DEPRECATED] Create and manage capabilities.")


@capability_app.command("new")
# ID: c2111920-a102-52e0-b8f5-1278411d4bae
def capability_new_deprecated():
    """[DEPRECATED] This command is now obsolete. Use 'knowledge sync' instead."""
    console.print(
        "[bold yellow]⚠️  This command is deprecated and will be removed.[/bold yellow]"
    )
    console.print(
        "   -> Please use '[cyan]poetry run core-admin knowledge sync[/cyan]' to synchronize symbols."
    )

--- END OF FILE ./src/cli/logic/capability.py ---

--- START OF FILE ./src/cli/logic/chat.py ---
# src/cli/logic/chat.py
"""
Implements the 'core-admin chat' command for conversational interaction.
"""

from __future__ import annotations

import json
import subprocess

import typer
from core.agents.intent_translator import IntentTranslator
from core.cognitive_service import CognitiveService
from dotenv import load_dotenv
from shared.config import settings
from shared.logger import getLogger
from shared.utils.parsing import extract_json_from_response

log = getLogger("core_admin.chat")
load_dotenv()


# ID: ab5e8f95-ba22-4845-9903-2aa02b618dd2
def chat(user_input: str = typer.Argument(..., help="Your goal in natural language.")):
    """
    Assesses your natural language goal and provides a clear, actionable command.
    """
    if not settings.LLM_ENABLED:
        log.error(
            "❌ The 'chat' command requires LLMs to be enabled in your .env file."
        )
        raise typer.Exit(code=1)

    log.info(f"Translating user goal: '{user_input}'")

    try:
        help_text_result = subprocess.run(
            ["poetry", "run", "core-admin", "--help"],
            capture_output=True,
            text=True,
            check=True,
        )
        help_text = help_text_result.stdout
        help_file = settings.REPO_PATH / "reports" / "cli_help.txt"
        help_file.parent.mkdir(exist_ok=True)
        help_file.write_text(help_text, encoding="utf-8")

        cognitive_service = CognitiveService(settings.REPO_PATH)
        translator = IntentTranslator(cognitive_service)
        response_text = translator.translate(user_input)

        response_json = extract_json_from_response(response_text)
        if not response_json:
            raise json.JSONDecodeError(
                "No valid JSON found in response.", response_text, 0
            )

        if "command" in response_json:
            command = response_json["command"]
            typer.secho("\n✅ AI Suggestion:", fg=typer.colors.GREEN)
            typer.echo("Here is the recommended command to achieve your goal:")
            typer.secho(f"\n  {command}\n", fg=typer.colors.CYAN)
        elif "error" in response_json:
            error_message = response_json["error"]
            typer.secho("\n⚠️ AI Assessment:", fg=typer.colors.YELLOW)
            typer.echo(error_message)
        else:
            raise KeyError("AI response missing 'command' or 'error' key.")

    except (json.JSONDecodeError, KeyError) as e:
        log.error(f"Failed to parse the AI's translation: {e}")
        typer.echo("The AI returned a response I couldn't understand. Raw response:")
        typer.echo(response_text)
        raise typer.Exit(code=1)
    except subprocess.CalledProcessError as e:
        log.error(f"Failed to generate CLI help text: {e.stderr}")
        raise typer.Exit(code=1)
    except Exception as e:
        log.error(f"An unexpected error occurred: {e}", exc_info=True)
        raise typer.Exit(code=1)


# The obsolete `register` function has been removed.

--- END OF FILE ./src/cli/logic/chat.py ---

--- START OF FILE ./src/cli/logic/check.py ---
"""
Registers and implements the 'check' command group by composing
sub-groups for CI and diagnostic commands.
"""

from __future__ import annotations

import typer
from cli.commands.ci import ci_app
from cli.commands.diagnostics import diagnostics_app

check_app = typer.Typer(
    help="Read-only checks to validate constitutional and code health."
)
check_app.add_typer(ci_app, name="ci", help="High-level CI and system health checks.")
check_app.add_typer(
    diagnostics_app, name="diagnostics", help="Deep diagnostic and integrity checks."
)

--- END OF FILE ./src/cli/logic/check.py ---

--- START OF FILE ./src/cli/logic/cli_utils.py ---
# src/cli/logic/cli_utils.py
"""
Provides centralized, reusable utilities for standardizing the console output
and execution of all `core-admin` commands.
"""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

import typer
from core.knowledge_service import KnowledgeService
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ed25519
from rich.console import Console
from shared.config import settings
from shared.logger import getLogger

log = getLogger("core_admin.cli_utils")
console = Console()


# The old, problematic set_context function has been removed.


# ID: 76d0313a-1d12-4ea2-9c98-e1d44283bb86
async def find_test_file_for_capability_async(capability_key: str) -> Optional[Path]:
    """
    Asynchronously finds the test file corresponding to a given capability key.
    """
    log.debug(f"Searching for test file for capability: '{capability_key}'")
    try:
        knowledge_service = KnowledgeService(settings.REPO_PATH)
        graph = await knowledge_service.get_graph()
        symbols = graph.get("symbols", {})

        source_file_str = None
        for symbol in symbols.values():
            if symbol.get("key") == capability_key:
                source_file_str = symbol.get("file_path")
                break

        if not source_file_str:
            log.warning(f"Capability '{capability_key}' not found in knowledge graph.")
            return None

        p = Path(source_file_str)
        test_file_path = (
            settings.REPO_PATH / "tests" / p.relative_to("src")
        ).with_name(f"test_{p.name}")

        if test_file_path.exists():
            log.debug(f"Found corresponding test file at: {test_file_path}")
            return test_file_path
        else:
            log.warning(f"Conventional test file not found at: {test_file_path}")
            return None
    except Exception as e:
        log.error(f"Error processing knowledge graph: {e}")
        return None


# ID: 8143391a-d9a7-5015-9c47-63647ca9eaee
def save_yaml_file(path: Path, data: Dict[str, Any]) -> None:
    """Saves data to a YAML file with consistent sorting."""
    import yaml

    path.write_text(yaml.dump(data, sort_keys=True), encoding="utf-8")


# ID: f4f2c106-6de1-57a7-8799-bc7b447de680
def load_private_key() -> ed25519.Ed25519PrivateKey:
    """Loads the operator's private key."""
    key_path = settings.KEY_STORAGE_DIR / "private.key"
    if not key_path.exists():
        log.error(
            "❌ Private key not found. Please run 'core-admin keygen' to create one."
        )
        raise typer.Exit(code=1)
    return serialization.load_pem_private_key(key_path.read_bytes(), password=None)


# ID: eebbca97-f3ba-46f0-a6dd-af189bfaf93c
def archive_rollback_plan(proposal_name: str, proposal: Dict[str, Any]) -> None:
    """Archives a proposal's rollback plan upon approval."""
    rollback_plan = proposal.get("rollback_plan")
    if not rollback_plan:
        return
    rollbacks_dir = settings.MIND / "constitution" / "rollbacks"
    rollbacks_dir.mkdir(parents=True, exist_ok=True)
    archive_path = (
        rollbacks_dir
        / f"{datetime.utcnow().strftime('%Y%m%d%H%M%S')}-{proposal_name}.json"
    )
    archive_path.write_text(
        json.dumps(
            {
                "proposal_name": proposal_name,
                "target_path": proposal.get("target_path"),
                "justification": proposal.get("justification"),
                "rollback_plan": rollback_plan,
            },
            indent=2,
        ),
        encoding="utf-8",
    )
    log.info(f"📖 Rollback plan archived to {archive_path}")


# ID: d17feffd-94a1-5636-b502-ce58fc16b644
def should_fail(report: dict, fail_on: str) -> bool:
    """
    Determines if the CLI should exit with an error code based on the drift
    report and the specified fail condition.
    """
    if fail_on == "missing":
        return bool(report.get("missing_in_code"))
    if fail_on == "undeclared":
        return bool(report.get("undeclared_in_manifest"))
    return bool(
        report.get("missing_in_code")
        or report.get("undeclared_in_manifest")
        or report.get("mismatched_mappings")
    )

--- END OF FILE ./src/cli/logic/cli_utils.py ---

--- START OF FILE ./src/cli/logic/context.py ---
# src/cli/logic/context.py
"""
This module is being phased out in favor of direct context injection in admin_cli.py.
It is kept for backward compatibility during the transition.
"""

from __future__ import annotations

--- END OF FILE ./src/cli/logic/context.py ---

--- START OF FILE ./src/cli/logic/db.py ---
# src/cli/logic/db.py
"""
Registers the top-level 'db' command group for managing the CORE operational database.
"""

from __future__ import annotations

import asyncio

import typer
import yaml
from rich.console import Console
from services.database.session_manager import get_session
from services.repositories.db.migration_service import migrate_db
from shared.config import settings
from sqlalchemy import text

from .status import status
from .sync_domains import sync_domains

console = Console()
db_app = typer.Typer(
    help="Commands for managing the CORE operational database (migrations, syncs, status, exports)."
)


async def _export_domains():
    """Fetches domains from the DB and writes them to domains.yaml."""
    console.print("   -> Exporting `core.domains` to YAML...")
    async with get_session() as session:
        result = await session.execute(
            text(
                "SELECT key as name, title, description FROM core.domains ORDER BY key"
            )
        )
        domains_data = [dict(row._mapping) for row in result]

    output_path = settings.MIND / "knowledge" / "domains.yaml"

    output_path.parent.mkdir(parents=True, exist_ok=True)
    yaml_content = {"version": 2, "domains": domains_data}
    output_path.write_text(yaml.dump(yaml_content, indent=2, sort_keys=False), "utf-8")
    console.print(
        f"      -> Wrote {len(domains_data)} domains to {output_path.relative_to(settings.REPO_PATH)}"
    )


async def _export_vector_metadata():
    """Fetches vector metadata from the DB and writes it to a report."""
    console.print("   -> Exporting vector metadata from `core.symbols` to YAML...")
    async with get_session() as session:
        result = await session.execute(
            text(
                """
            SELECT uuid, symbol_path, vector_id
            FROM core.symbols
            WHERE vector_id IS NOT NULL
            ORDER BY symbol_path
        """
            )
        )
        vector_data = [dict(row._mapping) for row in result]

    output_path = settings.REPO_PATH / "reports" / "vector_metadata_export.yaml"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(yaml.dump(vector_data, indent=2, sort_keys=False), "utf-8")
    console.print(
        f"      -> Wrote metadata for {len(vector_data)} vectors to {output_path.relative_to(settings.REPO_PATH)}"
    )


@db_app.command(
    "export", help="Export operational data from the database to read-only files."
)
# ID: a226b858-1e99-4443-8d18-a2cf0ecafba3
def export_data():
    """Exports DB tables to their canonical, read-only YAML file representations."""
    console.print(
        "[bold cyan]🚀 Exporting operational data from Database to files...[/bold cyan]"
    )

    async def _run_exports():
        await _export_domains()
        await _export_vector_metadata()

    asyncio.run(_run_exports())
    console.print("[bold green]✅ Export complete.[/bold green]")


# --- COMMAND REGISTRATION ---
db_app.command("status")(status)
db_app.command("sync-domains")(sync_domains)
db_app.command("migrate")(migrate_db)

--- END OF FILE ./src/cli/logic/db.py ---

--- START OF FILE ./src/cli/logic/db_manage.py ---
from __future__ import annotations

import typer

# Generic DB commands: `core-admin db ...`
from .db import app as db_app

# Knowledge DB sub-commands live under `knowledge db ...`
# We assemble a small "knowledge" group here and mount its `db` sub-app,
# so existing commands like `core-admin knowledge db import-from-graph` keep working.
from .db import app as knowledge_db_app

# Top-level Typer app exposed by this module
app = typer.Typer(help="Database management meta-commands")

# Mount groups
app.add_typer(db_app, name="db")

knowledge_app = typer.Typer(help="Knowledge operations")
knowledge_app.add_typer(knowledge_db_app, name="db")
app.add_typer(knowledge_app, name="knowledge")

__all__ = ["app"]

--- END OF FILE ./src/cli/logic/db_manage.py ---

--- START OF FILE ./src/cli/logic/diagnostics.py ---
# src/cli/logic/diagnostics.py
"""
Implements deep diagnostic checks for system integrity and constitutional alignment.
"""

from __future__ import annotations

import asyncio
import json

import jsonschema
import typer
import yaml
from features.governance.audit_context import AuditorContext
from features.governance.checks.domain_placement import DomainPlacementCheck
from features.governance.checks.legacy_tag_check import LegacyTagCheck
from features.governance.policy_coverage_service import PolicyCoverageService
from features.introspection.audit_unassigned_capabilities import get_unassigned_symbols
from features.introspection.graph_analysis_service import find_semantic_clusters
from rich.console import Console
from rich.table import Table
from rich.tree import Tree
from ruamel.yaml import YAML
from shared.config import settings
from shared.models import AuditSeverity
from shared.utils.constitutional_parser import get_all_constitutional_paths

console = Console()
yaml_loader = YAML(typ="safe")
diagnostics_app = typer.Typer(help="Deep diagnostic and integrity checks.")


async def _async_find_clusters(n_clusters: int):
    """Async helper that contains the core logic for the command."""
    console.print(
        f"🚀 Finding semantic clusters with [bold cyan]n_clusters={n_clusters}[/bold cyan]..."
    )
    clusters = await find_semantic_clusters(n_clusters=n_clusters)

    if not clusters:
        console.print("⚠️  No clusters found.")
        return

    console.print(f"✅ Found {len(clusters)} clusters. Displaying all, sorted by size.")

    for i, cluster in enumerate(clusters):
        if not cluster:
            continue

        table = Table(
            title=f"Semantic Cluster #{i + 1} ({len(cluster)} symbols)",
            show_header=True,
            header_style="bold magenta",
        )
        table.add_column("Symbol Key", style="cyan", no_wrap=True)

        for symbol_key in sorted(cluster):
            table.add_row(symbol_key)

        console.print(table)


@diagnostics_app.command(
    "find-clusters",
    help="Finds and displays all semantic capability clusters, sorted by size.",
)
# ID: 308457e8-a235-525c-9058-503a93b7755e
def find_clusters_command_sync(
    n_clusters: int = typer.Option(
        25, "--n-clusters", "-n", help="The number of clusters to find."
    ),
):
    """Synchronous Typer wrapper for the async clustering logic."""
    asyncio.run(_async_find_clusters(n_clusters))


def _add_cli_nodes(tree_node: Tree, cli_app: typer.Typer):
    for cmd_info in sorted(cli_app.registered_commands, key=lambda c: c.name or ""):
        if not cmd_info.name:
            continue
        help_text = cmd_info.help.split("\n")[0] if cmd_info.help else ""
        tree_node.add(
            f"[bold yellow]⚡ {cmd_info.name}[/bold yellow] [dim]- {help_text}[/dim]"
        )
    for group_info in sorted(cli_app.registered_groups, key=lambda g: g.name or ""):
        if not group_info.name:
            continue
        help_text = (
            group_info.typer_instance.info.help.split("\n")[0]
            if group_info.typer_instance.info.help
            else ""
        )
        branch = tree_node.add(
            f"[cyan]📂 {group_info.name}[/cyan] [dim]- {help_text}[/dim]"
        )
        _add_cli_nodes(branch, group_info.typer_instance)


@diagnostics_app.command(
    "cli-tree", help="Displays a hierarchical tree view of all available CLI commands."
)
# ID: 3a196cd2-fa47-532d-ba25-ef8d8c0a1cf2
def cli_tree():
    """Builds and displays the CLI command tree."""
    from cli.admin_cli import app as main_app

    console.print("[bold cyan]🚀 Building CLI Command Tree...[/bold cyan]")
    tree = Tree(
        "[bold magenta]🏛️ CORE Admin CLI Commands[/bold magenta]",
        guide_style="bold bright_blue",
    )
    _add_cli_nodes(tree, main_app)
    console.print(tree)


@diagnostics_app.command(
    "policy-coverage", help="Audits the constitution for policy coverage and integrity."
)
# ID: 603fdf61-88b8-5d61-a76e-5ff7cb43d590
def policy_coverage():
    """
    Runs a meta-audit on all .intent/charter/policies/ to ensure they are
    well-formed and covered by the governance model.
    """
    console.print(
        "[bold cyan]🚀 Running Constitutional Policy Coverage Audit...[/bold cyan]"
    )
    service = PolicyCoverageService()
    report = service.run()

    console.print(f"Report ID: [dim]{report.report_id}[/dim]")
    console.print(f"Policies Seen: {report.summary['policies_seen']}")
    console.print(f"Rules Found: {report.summary['rules_found']}")
    console.print(f"Uncovered Rules: {report.summary['uncovered_rules']}")

    if report.summary["uncovered_rules"] > 0:
        table = Table(title="Uncovered Policy Rules")
        table.add_column("Policy", style="cyan")
        table.add_column("Rule ID", style="magenta")
        table.add_column("Enforcement", style="yellow")
        for record in report.records:
            if not record["covered"]:
                table.add_row(
                    record["policy_id"], record["rule_id"], record["enforcement"]
                )
        console.print(table)

    if report.exit_code != 0:
        console.print(
            f"\n[bold red]❌ Audit Failed with exit code: {report.exit_code}[/bold red]"
        )
        raise typer.Exit(code=report.exit_code)
    else:
        console.print(
            "\n[bold green]✅ All active policies are well-formed and covered.[/bold green]"
        )


@diagnostics_app.command(
    "debug-meta", help="Prints the auditor's view of all required constitutional files."
)
# ID: 3e865e92-b3e4-5c0e-8edc-686ecbb1892b
def debug_meta_paths():
    """A diagnostic tool that prints all file paths indexed in meta.yaml."""
    console.print(
        "[bold yellow]--- Auditor's Interpretation of meta.yaml ---[/bold yellow]"
    )
    required_paths = get_all_constitutional_paths(settings._meta_config, settings.MIND)
    for path in sorted(list(required_paths)):
        console.print(path)


@diagnostics_app.command(
    "unassigned-symbols", help="Finds symbols without a universal # ID tag."
)
# ID: e77e8ffc-9dd0-5cf1-8b9f-4044358fbed1
def unassigned_symbols():
    unassigned = get_unassigned_symbols()
    if not unassigned:
        console.print(
            "[bold green]✅ Success! All governable symbols have an assigned ID tag.[/bold green]"
        )
        return
    console.print(
        f"\n[bold red]❌ Found {len(unassigned)} symbols with no assigned ID:[/bold red]"
    )
    table = Table(title="Untagged Symbols ('Orphaned Logic')")
    table.add_column("Symbol Key", style="cyan", no_wrap=True)
    table.add_column("File", style="yellow")
    table.add_column("Line", style="magenta")
    for symbol in sorted(unassigned, key=lambda s: s["key"]):
        table.add_row(symbol["key"], symbol["file"], str(symbol["line_number"]))
    console.print(table)
    console.print("\n[bold]Action Required:[/bold] Run 'knowledge sync' to assign IDs.")


@diagnostics_app.command(
    "manifest-hygiene",
    help="Checks for capabilities declared in the wrong domain manifest file.",
)
# ID: 36231b48-6f2b-5448-ada1-5830022ae33c
def manifest_hygiene():
    context = AuditorContext(settings.REPO_PATH)
    check = DomainPlacementCheck(context)
    findings = check.execute()
    if not findings:
        console.print(
            "[bold green]✅ All capabilities correctly placed in domain manifests[/bold green]"
        )
        raise typer.Exit(code=0)
    errors = [f for f in findings if f.severity == AuditSeverity.ERROR]
    if errors:
        console.print(f"[bold red]🚨 {len(errors)} CRITICAL errors found:[/bold red]")
        for f in errors:
            console.print(f"  [red]{f}[/red]")
    if warnings := [f for f in findings if f.severity == AuditSeverity.WARNING]:
        console.print(f"[bold yellow]⚠️  {len(warnings)} warnings found:[/bold yellow]")
        for f in warnings:
            console.print(f"  [yellow]{f}[/yellow]")
    raise typer.Exit(code=1 if errors else 0)


@diagnostics_app.command(
    "cli-registry", help="Validates the CLI registry against its constitutional schema."
)
# ID: de84cf6f-c051-5c20-a1dd-ab1eeaa6359d
def cli_registry():
    meta_content = (settings.REPO_PATH / ".intent" / "meta.yaml").read_text("utf-8")
    meta = yaml.safe_load(meta_content) or {}
    knowledge = meta.get("mind", {}).get("knowledge", {})
    schemas = meta.get("charter", {}).get("schemas", {})
    registry_rel = knowledge.get("cli_registry", "mind/knowledge/cli_registry.yaml")
    schema_rel = schemas.get(
        "cli_registry_schema", "charter/schemas/cli_registry_schema.json"
    )
    registry_path = (settings.REPO_PATH / registry_rel).resolve()
    schema_path = (settings.REPO_PATH / schema_rel).resolve()
    if not registry_path.exists():
        typer.secho(
            f"ERROR: CLI registry not found: {registry_path}",
            err=True,
            fg=typer.colors.RED,
        )
        raise typer.Exit(1)
    if not schema_path.exists():
        typer.secho(
            f"ERROR: CLI registry schema not found: {schema_path}",
            err=True,
            fg=typer.colors.RED,
        )
        raise typer.Exit(1)
    registry_content = registry_path.read_text("utf-8")
    registry = yaml.safe_load(registry_content) or {}
    schema_content = schema_path.read_text(encoding="utf-8")
    schema = json.loads(schema_content)
    validator = jsonschema.Draft202012Validator(schema)
    errors = sorted(validator.iter_errors(registry), key=lambda e: e.path)
    if errors:
        typer.secho(
            f"❌ CLI registry failed validation against {schema_rel}",
            err=True,
            fg=typer.colors.RED,
        )
        for idx, err in enumerate(errors, 1):
            loc = "/".join(map(str, err.path)) or "(root)"
            typer.secho(
                f"  {idx}. at {loc}: {err.message}", err=True, fg=typer.colors.RED
            )
        raise typer.Exit(1)
    typer.secho(f"✅ CLI registry is valid: {registry_rel}", fg=typer.colors.GREEN)


@diagnostics_app.command("legacy-tags", help="Scans the codebase for obsolete tags.")
# ID: 2bb166e3-7586-5472-aee6-3fb061b23674
def check_legacy_tags():
    """Runs only the LegacyTagCheck to find obsolete capability tags."""

    async def _async_check_legacy_tags():
        console.print(
            "[bold cyan]🚀 Running standalone legacy tag check...[/bold cyan]"
        )
        context = AuditorContext(settings.REPO_PATH)
        await context.load_knowledge_graph()
        check = LegacyTagCheck(context)
        findings = check.execute()
        if not findings:
            console.print("[bold green]✅ Success! No legacy tags found.[/bold green]")
            return

        console.print(
            f"\n[bold red]❌ Found {len(findings)} instance(s) of legacy tags:[/bold red]"
        )
        table = Table(title="Obsolete Tag Violations")
        table.add_column("File Path", style="cyan", no_wrap=True)
        table.add_column("Line", style="magenta")
        table.add_column("Message", style="red")
        for finding in findings:
            table.add_row(finding.file_path, str(finding.line_number), finding.message)

        console.print(table)

        raise typer.Exit(code=1)

    asyncio.run(_async_check_legacy_tags())


# The obsolete inspect_vector_drift function has been removed.

--- END OF FILE ./src/cli/logic/diagnostics.py ---

--- START OF FILE ./src/cli/logic/duplicates.py ---
# src/cli/logic/duplicates.py
"""
Implements the dedicated 'inspect duplicates' command, providing a focused tool
to run only the semantic duplication check with clustering.
"""

from __future__ import annotations

import asyncio
from typing import List

import networkx as nx
import typer
from features.governance.audit_context import AuditorContext
from features.governance.checks.duplication_check import DuplicationCheck
from rich.console import Console
from rich.table import Table
from shared.context import CoreContext
from shared.models import AuditFinding

console = Console()


def _group_findings(findings: list[AuditFinding]) -> List[List[AuditFinding]]:
    """Groups individual finding pairs into clusters of related duplicates."""
    graph = nx.Graph()
    finding_map = {}

    for finding in findings:
        symbol1 = finding.context.get("symbol_a")
        symbol2 = finding.context.get("symbol_b")
        if symbol1 and symbol2:
            graph.add_edge(symbol1, symbol2)
            finding_map[tuple(sorted((symbol1, symbol2)))] = finding

    clusters = list(nx.connected_components(graph))
    grouped_findings = []

    for cluster in clusters:
        cluster_findings = []
        for i, node1 in enumerate(list(cluster)):
            for node2 in list(cluster)[i + 1 :]:
                key = tuple(sorted((node1, node2)))
                if key in finding_map:
                    cluster_findings.append(finding_map[key])

        if cluster_findings:
            # --- THIS IS THE FIX ---
            # The sorting key now correctly and safely reads the similarity score
            # from the finding's 'context' dictionary, preventing the ValueError.
            cluster_findings.sort(
                key=lambda f: float(f.context.get("similarity", 0)), reverse=True
            )
            # --- END OF FIX ---
            grouped_findings.append(cluster_findings)

    return grouped_findings


async def _async_inspect_duplicates(context: CoreContext, threshold: float):
    """The core async logic for running only the duplication check."""
    if context is None:
        console.print(
            "[bold red]Error: Context not initialized for inspect duplicates[/bold red]"
        )
        raise typer.Exit(code=1)

    console.print(
        f"[bold cyan]🚀 Running semantic duplication check with threshold: {threshold}...[/bold cyan]"
    )

    auditor_context = AuditorContext(context.git_service.repo_path)
    await auditor_context.load_knowledge_graph()
    duplication_check = DuplicationCheck(auditor_context)

    findings: list[AuditFinding] = await duplication_check.execute(threshold=threshold)

    if not findings:
        console.print("[bold green]✅ No semantic duplicates found.[/bold green]")
        return

    grouped_findings = _group_findings(findings)

    console.print(
        f"\n[bold yellow]Found {len(findings)} duplicate pairs, forming {len(grouped_findings)} cluster(s):[/bold yellow]"
    )

    for i, cluster in enumerate(grouped_findings, 1):
        all_symbols_in_cluster = set()
        for f in cluster:
            all_symbols_in_cluster.add(f.context["symbol_a"])
            all_symbols_in_cluster.add(f.context["symbol_b"])

        title = f"Cluster #{i} ({len(all_symbols_in_cluster)} related symbols)"
        table = Table(show_header=True, header_style="bold magenta", title=title)
        table.add_column("Symbol 1", style="cyan")
        table.add_column("Symbol 2", style="cyan")
        table.add_column("Similarity", style="yellow")

        for finding in cluster:
            table.add_row(
                finding.context["symbol_a"],
                finding.context["symbol_b"],
                finding.context["similarity"],
            )

        console.print(table)


# ID: 1a1b2c3d-4e5f-6a7b-8c9d-0e1f2a3b4c5d
def inspect_duplicates(context: CoreContext, threshold: float):
    """Runs only the semantic duplication check and reports the findings."""
    asyncio.run(_async_inspect_duplicates(context, threshold))

--- END OF FILE ./src/cli/logic/duplicates.py ---

--- START OF FILE ./src/cli/logic/embeddings_cli.py ---
"""
CLI wiring for embeddings & vectorization commands.
Exposes: `core-admin knowledge vectorize [--write|--dry-run] [--cap capability --cap ...]`
"""

from __future__ import annotations

from pathlib import Path
from typing import Optional, Set

import typer
from core.cognitive_service import CognitiveService
from core.knowledge_service import KnowledgeService
from services.clients.qdrant_client import QdrantService
from shared.logger import getLogger

from .knowledge_orchestrator import run_vectorize

log = getLogger("core_admin.embeddings_cli")
app = typer.Typer(
    name="knowledge", no_args_is_help=True, help="Knowledge graph & embeddings commands"
)


@app.command("vectorize")
# ID: 90b5ca34-823b-4584-9d61-383ff4a4e29f
def vectorize_cmd(
    write: bool = typer.Option(
        False, "--write", help="Persist changes to knowledge graph after run."
    ),
    dry_run: bool = typer.Option(
        False, "--dry-run", help="Do not upsert to Qdrant, simulate only."
    ),
    verbose: bool = typer.Option(
        False, "--verbose", help="Verbose logging / stack traces."
    ),
    cap: Optional[list[str]] = typer.Option(
        None, "--cap", help="Limit to specific capability keys (repeatable)."
    ),
    flush_every: int = typer.Option(
        10, "--flush-every", help="Flush/save cadence (N processed chunks)."
    ),
):
    """
    Vectorize code chunks into Qdrant with per-chunk idempotency.
    """
    repo_root = Path(".").resolve()
    ks = KnowledgeService()
    knowledge = ks.load_graph()
    symbols_map: dict = knowledge.get("symbols", knowledge)
    cognitive = CognitiveService()
    qdrant = QdrantService()
    targets: Optional[Set[str]] = set(cap) if cap else None
    typer.echo("🚀 Starting capability vectorization process (per-chunk idempotent)…")
    import asyncio

    asyncio.run(
        run_vectorize(
            repo_root=repo_root,
            symbols_map=symbols_map,
            cognitive_service=cognitive,
            qdrant_service=qdrant,
            dry_run=dry_run,
            verbose=verbose,
            target_capabilities=targets,
            flush_every=flush_every,
        )
    )
    if write and (not dry_run):
        ks.save_graph(knowledge)
        typer.echo("📝 Saved updated knowledge graph.")
    else:
        typer.echo("ℹ️ Not saving graph (use --write and disable --dry-run to persist).")

--- END OF FILE ./src/cli/logic/embeddings_cli.py ---

--- START OF FILE ./src/cli/logic/guard.py ---
"""
Intent: Governance/validation guard commands exposed to the operator.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml
from rich import print as rprint
from rich.panel import Panel
from rich.table import Table
from shared.logger import getLogger

log = getLogger("core_admin")


def _find_manifest_path(root: Path, explicit: Optional[Path]) -> Optional[Path]:
    """Locate and return the path to the project manifest file, or None."""
    if explicit and explicit.exists():
        return explicit
    for p in (root / ".intent/project_manifest.yaml", root / ".intent/manifest.yaml"):
        if p.exists():
            return p
    return None


def _load_raw_manifest(root: Path, explicit: Optional[Path]) -> Dict[str, Any]:
    """Loads and parses a YAML manifest file, returning an empty dict if not found."""
    path = _find_manifest_path(root, explicit)
    if not path:
        return {}
    data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    return data


def _ux_defaults(root: Path, explicit: Optional[Path]) -> Dict[str, Any]:
    """Extracts and returns UX-related default values from the manifest."""
    raw = _load_raw_manifest(root, explicit)
    ux = raw.get("operator_experience", {}).get("guard", {}).get("drift", {})
    return {
        "default_format": ux.get("default_format", "json"),
        "default_fail_on": ux.get("default_fail_on", "any"),
        "strict_default": bool(ux.get("strict_default", False)),
        "evidence_json": bool(ux.get("evidence_json", True)),
        "evidence_path": ux.get("evidence_path", "reports/drift_report.json"),
        "labels": ux.get(
            "labels",
            {
                "none": "NONE",
                "success": "✅ No capability drift",
                "failure": "🚨 Drift detected",
            },
        ),
    }


def _is_clean(report: dict) -> bool:
    """Determines if a report is clean."""
    return not (
        report.get("missing_in_code")
        or report.get("undeclared_in_manifest")
        or report.get("mismatched_mappings")
    )


def _print_table(report_dict: dict, labels: Dict[str, str]) -> None:
    """Prints a formatted table of the drift report."""
    table = Table(show_header=True, header_style="bold", title="Capability Drift")
    table.add_column("Section", style="bold")
    table.add_column("Values")

    # ID: 2b132e4d-1d2b-48e3-92bb-cbaea04dfd0d
    def row(title: str, items: List[str]):
        """Adds a row with a formatted list of items."""
        if not items:
            table.add_row(title, f"[bold green]{labels['none']}[/bold green]")
        else:
            table.add_row(
                title, f'[yellow]{'\\n'.join((f'- {it}' for it in items))}[/yellow]'
            )

    row("Missing in code", report_dict.get("missing_in_code", []))
    row("Undeclared in manifest", report_dict.get("undeclared_in_manifest", []))
    mismatches = report_dict.get("mismatched_mappings", [])
    if not mismatches:
        table.add_row(
            "Mismatched mappings", f"[bold green]{labels['none']}[/bold green]"
        )
    else:
        lines = [
            f"- {m.get('capability')}: manifest(...) != code(...)" for m in mismatches
        ]
        table.add_row(
            "Mismatched mappings", "[yellow]" + "\n".join(lines) + "[/yellow]"
        )
    status = (
        f"[bold green]{labels['success']}[/bold green]"
        if _is_clean(report_dict)
        else f"[bold red]{labels['failure']}[/bold red]"
    )
    rprint(Panel.fit(table, title=status))


def _print_pretty(report_dict: dict, labels: Dict[str, str]) -> None:
    """Prints a user-friendly summary of the drift report."""
    _print_table(report_dict, labels)

--- END OF FILE ./src/cli/logic/guard.py ---

--- START OF FILE ./src/cli/logic/guard_cli.py ---
# src/cli/logic/guard_cli.py
"""
CLI-facing guard registration helpers.
"""

from __future__ import annotations

import asyncio
import json
from pathlib import Path
from typing import Any, Dict, Optional

import typer
from features.introspection.drift_detector import write_report
from features.introspection.drift_service import run_drift_analysis_async

# --- CORRECTED IMPORTS ---
from .cli_utils import should_fail
from .guard import _print_pretty, _ux_defaults

__all__ = ["register_guard"]


# ID: a083eccb-0f7d-4230-b32c-4f9d9ae80ace
def register_guard(app: typer.Typer) -> None:
    """
    Registers the 'guard' command group with the CLI.
    """
    guard = typer.Typer(help="Governance/validation guards")
    app.add_typer(guard, name="guard")

    @guard.command("drift")
    # ID: 9c69d559-0c4a-4431-918b-14b3d588da91
    def drift(
        root: Path = typer.Option(Path("."), help="Repository root."),
        manifest_path: Optional[Path] = typer.Option(
            None, help="Explicit manifest path (deprecated)."
        ),
        output: Optional[Path] = typer.Option(
            None, help="Path for JSON evidence report."
        ),
        format: Optional[str] = typer.Option(None, help="json|table|pretty"),
        fail_on: Optional[str] = typer.Option(None, help="any|missing|undeclared"),
    ) -> None:
        """Compares manifest vs code to detect capability drift."""
        try:
            ux = _ux_defaults(root, manifest_path)
            fmt = (format or ux["default_format"]).lower()
            fail_policy = (fail_on or ux["default_fail_on"]).lower()

            report = asyncio.run(run_drift_analysis_async(root))
            report_dict: Dict[str, Any] = report.to_dict()

            if ux["evidence_json"]:
                write_report(output or (root / ux["evidence_path"]), report)

            if fmt in ("table", "pretty"):
                _print_pretty(report_dict, ux["labels"])
            else:
                typer.echo(json.dumps(report_dict, indent=2))

            if should_fail(report_dict, fail_policy):
                raise typer.Exit(code=2)
        except FileNotFoundError as e:
            typer.secho(
                f"Error: A required constitutional file was not found: {e}",
                fg=typer.colors.RED,
            )
            raise typer.Exit(code=1)

--- END OF FILE ./src/cli/logic/guard_cli.py ---

--- START OF FILE ./src/cli/logic/hub.py ---
"""
Central Hub: discover and locate CORE tools from a single place.

This reads from the DB-backed CLI registry (core.cli_commands). If empty, it
helps you populate it via `core-admin knowledge sync` or `migrate-ssot`.
"""

from __future__ import annotations

import asyncio
import importlib
import inspect
from pathlib import Path
from typing import List, Optional

import typer
from rich.console import Console
from rich.table import Table
from services.database.models import CliCommand
from services.database.session_manager import get_session
from shared.config import settings
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

console = Console()
hub_app = typer.Typer(help="Central hub for discovering and locating CORE tools.")


async def _fetch_commands(session: AsyncSession) -> list[CliCommand]:
    rows = (await session.execute(select(CliCommand))).scalars().all()
    return list(rows or [])


def _format_command_name(cmd: CliCommand) -> str:
    return getattr(cmd, "name", "") or ""


def _shorten(s: Optional[str], n: int = 80) -> str:
    if not s:
        return "—"
    return s if len(s) <= n else s[: n - 1] + "…"


def _module_file(module_path: str) -> Optional[Path]:
    try:
        mod = importlib.import_module(module_path)
        f = inspect.getsourcefile(mod)
        return Path(f).resolve() if f else None
    except Exception:
        return None


def _desc_for(c: CliCommand) -> str:
    """Best-effort description across possible schemas (be resilient to missing fields)."""
    for attr in ("description", "help", "summary", "doc"):
        v = getattr(c, attr, None)
        if isinstance(v, str) and v.strip():
            return v
    return ""


@hub_app.command("list")
# ID: 89be20b9-1d77-408f-9f59-3ac2ca169144
def hub_list():
    """Show all registered CLI commands from the DB registry."""

    async def _run():
        async with get_session() as session:
            cmds = await _fetch_commands(session)
        if not cmds:
            console.print(
                "[bold yellow]No CLI registry entries in DB.[/bold yellow] Run: [bold]core-admin knowledge sync[/bold]"
            )
            raise typer.Exit(code=2)
        table = Table(title="All CLI commands in registry")
        table.add_column("#", justify="right", style="dim")
        table.add_column("Command", style="cyan")
        table.add_column("Module", style="magenta")
        table.add_column("Entrypoint", style="green")
        table.add_column("Description")
        for i, c in enumerate(cmds, 1):
            table.add_row(
                str(i),
                _format_command_name(c),
                getattr(c, "module", "") or "",
                getattr(c, "entrypoint", "") or "",
                _shorten(_desc_for(c), 100),
            )
        console.print(table)

    asyncio.run(_run())


@hub_app.command("search")
# ID: 8ac36c7c-867c-4f17-9503-5b5199cb813e
def hub_search(
    term: str = typer.Argument(
        ..., help="Term to search in command names/descriptions."
    ),
    limit: int = typer.Option(25, "--limit", "-l", help="Max results."),
):
    """Fuzzy search across CLI commands from the registry."""

    async def _run():
        async with get_session() as session:
            cmds = await _fetch_commands(session)
        if not cmds:
            console.print(
                "[bold yellow]No CLI registry entries found in DB.[/bold yellow]\nTry:\n  • core-admin knowledge migrate-ssot    (if you still have legacy YAML)\n  • core-admin knowledge sync            (introspect and populate)\n"
            )
            raise typer.Exit(code=2)
        term_l = term.lower()
        hits: List[CliCommand] = []
        for c in cmds:
            name = (_format_command_name(c) or "").lower()
            desc = _desc_for(c).lower()
            if term_l in name or (desc and term_l in desc):
                hits.append(c)
        hits = hits[:limit]
        if not hits:
            console.print("[yellow]No matches.[/yellow]")
            raise typer.Exit(code=0)
        table = Table(title=f"Hub search: “{term}”")
        table.add_column("Command", style="cyan")
        table.add_column("Module", style="magenta")
        table.add_column("Entrypoint", style="green")
        table.add_column("Description", style="white")
        for c in hits:
            table.add_row(
                _format_command_name(c),
                getattr(c, "module", "") or "",
                getattr(c, "entrypoint", "") or "",
                _shorten(_desc_for(c), 100),
            )
        console.print(table)

    asyncio.run(_run())


@hub_app.command("whereis")
# ID: 263425b5-3e99-4e3b-a89f-0fc4b88d3fdd
def hub_whereis(
    command: str = typer.Argument(
        ...,
        help="Exact command name as stored (e.g., 'proposals.micro.apply' or 'knowledge.sync')",
    ),
):
    """Show module, entrypoint, and file path for a command."""

    async def _run():
        async with get_session() as session:
            cmds = await _fetch_commands(session)
        if not cmds:
            console.print(
                "[bold yellow]No CLI registry in DB.[/bold yellow] Run [bold]core-admin knowledge sync[/bold] first."
            )
            raise typer.Exit(code=2)
        matches = [c for c in cmds if _format_command_name(c) == command]
        if not matches:
            matches = [c for c in cmds if _format_command_name(c).endswith(command)]
        if not matches:
            console.print("[yellow]No such command in registry.[/yellow]")
            raise typer.Exit(code=1)
        c = matches[0]
        path = (
            _module_file(getattr(c, "module", "") or "")
            if getattr(c, "module", None)
            else None
        )
        console.print(f"[bold]Command:[/bold] {_format_command_name(c)}")
        console.print(f"[bold]Module:[/bold]  {getattr(c, 'module', '') or '—'}")
        console.print(f"[bold]Entrypoint:[/bold] {getattr(c, 'entrypoint', '') or '—'}")
        console.print(f"[bold]File:[/bold]    {(path if path else '—')}")

    asyncio.run(_run())


@hub_app.command("doctor")
# ID: a09b6ebe-6a2a-4030-b85c-e9f127e74171
def hub_doctor():
    """Quick health checks for discoverability + SSOT surfaces."""

    async def _run():
        ok = True
        async with get_session() as session:
            try:
                cmds = await _fetch_commands(session)
                if cmds:
                    console.print(f"✅ CLI registry entries in DB: {len(cmds)}")
                else:
                    ok = False
                    console.print("❌ No CLI registry entries in DB.")
                    console.print("   → Run: core-admin knowledge sync")
            except Exception as e:
                ok = False
                console.print(f"❌ DB error while reading CLI registry: {e}")
        snapshots = [
            settings.MIND / "knowledge" / "cli_registry.yaml",
            settings.MIND / "knowledge" / "resource_manifest.yaml",
            settings.MIND / "knowledge" / "cognitive_roles.yaml",
        ]
        missing = [p for p in snapshots if not p.exists()]
        if missing:
            console.print("⚠️  Missing YAML exports:")
            for p in missing:
                console.print(f"   • {p}")
            console.print("   → Run: core-admin knowledge export-ssot")
        else:
            console.print("✅ YAML exports present.")
        console.print(
            "\nTip: run [bold]core-admin knowledge canary --skip-tests[/bold] before big ops."
        )
        raise typer.Exit(code=0 if ok else 1)

    asyncio.run(_run())

--- END OF FILE ./src/cli/logic/hub.py ---

--- START OF FILE ./src/cli/logic/init.py ---
from __future__ import annotations

import typer

from .init import init_db as _init_db
from .list_audits import list_audits as _list_audits
from .log_audit import log_audit as _log_audit
from .report import report as _report
from .status import status as _status

app = typer.Typer(help="Generic DB commands (migrations, status, audits).")

# Register commands
app.command("status")(_status)
app.command("init")(_init_db)
app.command("log-audit")(_log_audit)
app.command("list-audits")(_list_audits)
app.command("report")(_report)

--- END OF FILE ./src/cli/logic/init.py ---

--- START OF FILE ./src/cli/logic/knowledge.py ---
# src/cli/logic/knowledge.py
"""
Implements the logic for knowledge-related CLI commands, such as finding
common, duplicated helper functions across the codebase.
"""

from __future__ import annotations

import asyncio

from features.self_healing.knowledge_consolidation_service import (
    find_structurally_similar_helpers,
)
from rich.console import Console
from rich.table import Table

console = Console()


# ID: a4b9c1d8-f3e2-4b1e-a9d5-f8c3d7f4b1e9
def find_common_knowledge(
    min_occurrences: int = 3,
    max_lines: int = 10,
):
    """
    CLI logic to find and display structurally similar helper functions.
    """
    console.print(
        "[bold cyan]🔍 Scanning for structurally similar helper functions...[/bold cyan]"
    )

    duplicates = asyncio.run(
        asyncio.to_thread(find_structurally_similar_helpers, min_occurrences, max_lines)
    )

    if not duplicates:
        console.print(
            "[bold green]✅ No common helper functions found meeting the criteria.[/bold green]"
        )
        return

    console.print(
        f"\n[bold yellow]Found {len(duplicates)} cluster(s) of duplicated helper functions:[/bold yellow]"
    )

    for i, (hash_val, locations) in enumerate(duplicates.items(), 1):
        table = Table(
            title=f"Cluster #{i} (Found {len(locations)} times)",
            show_header=True,
            header_style="bold magenta",
        )
        table.add_column("File Path", style="cyan")
        table.add_column("Line", style="magenta", justify="right")

        for file_path, line_num in sorted(locations):
            table.add_row(file_path, str(line_num))

        console.print(table)

    console.print(
        "\n[bold]Next Step:[/bold] Use these findings to refactor and consolidate helpers into `src/shared/utils/` to uphold the `dry_by_design` principle."
    )

--- END OF FILE ./src/cli/logic/knowledge.py ---

--- START OF FILE ./src/cli/logic/knowledge_sync/__init__.py ---
# src/cli/logic/knowledge_sync/__init__.py
"""
Initialization module for the knowledge synchronization package.
"""

from .diff import run_diff
from .import_ import run_import
from .snapshot import run_snapshot
from .verify import run_verify

__all__ = ["run_snapshot", "run_diff", "run_import", "run_verify"]

--- END OF FILE ./src/cli/logic/knowledge_sync/__init__.py ---

--- START OF FILE ./src/cli/logic/knowledge_sync/diff.py ---
# src/cli/logic/knowledge_sync/diff.py
"""
Compares database state with exported YAML files to detect drift in the CORE Working Mind.
"""

from __future__ import annotations

import asyncio
import json
from typing import Any, Dict, List

from rich.console import Console
from shared.config import settings

from .snapshot import fetch_capabilities, fetch_links, fetch_northstar, fetch_symbols
from .utils import _get_diff_links_key, canonicalize, read_yaml

console = Console()
EXPORT_DIR = settings.REPO_PATH / ".intent" / "mind_export"


# ID: e066d956-a155-42b5-be8c-adb693371b99
def diff_sets(
    db_items: List[Dict[str, Any]], file_items: List[Dict[str, Any]], key: str
) -> Dict[str, Any]:
    """Compares two lists of dictionaries based on a key and returns the differences.

    Args:
        db_items: List of items from the database.
        file_items: List of items from the YAML file.
        key: The key to compare items by.

    Returns:
        Dictionary with 'only_db', 'only_file', and 'changed' lists.
    """
    db_map = {str(it.get(key)): it for it in db_items if it.get(key)}
    file_map = {str(it.get(key)): it for it in file_items if it.get(key)}

    only_db = sorted([k for k in db_map if k not in file_map])
    only_file = sorted([k for k in file_map if k not in db_map])

    changed = []
    for k in sorted(db_map.keys() & file_map.keys()):
        db_item = {
            kk: vv
            for kk, vv in db_map[k].items()
            if kk not in ("created_at", "updated_at", "first_seen", "last_seen")
        }
        file_item = {
            kk: vv
            for kk, vv in file_map[k].items()
            if kk not in ("created_at", "updated_at", "first_seen", "last_seen")
        }
        if canonicalize(db_item) != canonicalize(file_item):
            changed.append(k)

    return {"only_db": only_db, "only_file": only_file, "changed": changed}


# ID: b12e8b4a-a760-436a-9529-3919081a1a43
async def run_diff(as_json: bool) -> None:
    """Compares database state with exported YAML files and outputs differences.

    Args:
        as_json: If True, outputs the diff as JSON; otherwise, uses human-readable format.
    """
    if not EXPORT_DIR.exists():
        console.print(
            f"[bold red]Export directory not found: {EXPORT_DIR}. "
            "Please run 'snapshot' first.[/bold red]"
        )
        return

    console.print("🔄 Comparing database state with exported YAML files...")

    db_caps, db_syms, db_links, db_north = await asyncio.gather(
        fetch_capabilities(), fetch_symbols(), fetch_links(), fetch_northstar()
    )

    file_caps = read_yaml(EXPORT_DIR / "capabilities.yaml").get("items", [])
    file_syms = read_yaml(EXPORT_DIR / "symbols.yaml").get("items", [])
    file_links = read_yaml(EXPORT_DIR / "links.yaml").get("items", [])
    file_north = read_yaml(EXPORT_DIR / "northstar.yaml").get("items", [])

    output = {
        "capabilities": diff_sets(db_caps, file_caps, "id"),
        "symbols": diff_sets(db_syms, file_syms, "id"),
        "links": diff_sets(
            [dict(it, key=_get_diff_links_key(it)) for it in db_links],
            [dict(it, key=_get_diff_links_key(it)) for it in file_links],
            "key",
        ),
        "northstar": {"changed": canonicalize(db_north) != canonicalize(file_north)},
    }

    if as_json:
        console.print(json.dumps(output, indent=2))
    else:
        console.print("\n[bold]Diff Summary (Database <-> Files):[/bold]")
        for k, v in output.items():
            if k == "northstar":
                status = (
                    "[red]Changed[/red]" if v["changed"] else "[green]No change[/green]"
                )
                console.print(f"  - [cyan]{k.capitalize()}[/cyan]: {status}")
                continue

            counts = (
                f"DB-only: {len(v['only_db'])}, "
                f"File-only: {len(v['only_file'])}, "
                f"Changed: {len(v['changed'])}"
            )
            is_clean = not any(v.values())
            status = (
                "[green]Clean[/green]"
                if is_clean
                else "[yellow]Drift detected[/yellow]"
            )
            console.print(f"  - [cyan]{k.capitalize()}[/cyan]: {status} ({counts})")

--- END OF FILE ./src/cli/logic/knowledge_sync/diff.py ---

--- START OF FILE ./src/cli/logic/knowledge_sync/import_.py ---
# src/cli/logic/knowledge_sync/import_.py
"""
Handles importing YAML files into the database for the CORE Working Mind.
"""

from __future__ import annotations

from typing import Any, Dict

from rich.console import Console
from services.database.models import (
    Capability,
    CognitiveRole,
    LlmResource,
    Northstar,
    Symbol,
    SymbolCapabilityLink,
)
from services.database.session_manager import get_session
from shared.config import settings
from sqlalchemy import text
from sqlalchemy.dialects.postgresql import insert as pg_insert

from .utils import _get_items_from_doc, compute_digest, read_yaml

console = Console()
EXPORT_DIR = settings.REPO_PATH / ".intent" / "mind_export"
YAML_FILES = {
    "capabilities": "capabilities.yaml",
    "symbols": "symbols.yaml",
    "links": "links.yaml",
    "northstar": "northstar.yaml",
    "cognitive_roles": "cognitive_roles.yaml",
    "resource_manifest": "resource_manifest.yaml",
}


async def _upsert_items(session, table_model, items, index_elements):
    """Generic upsert function for SSOT tables.

    Args:
        session: Database session.
        table_model: SQLAlchemy model class.
        items: List of items to upsert.
        index_elements: Columns to use for conflict resolution.
    """
    if not items:
        return
    stmt = pg_insert(table_model).values(items)
    update_dict = {
        c.name: getattr(stmt.excluded, c.name)
        for c in stmt.table.columns
        if not c.primary_key
    }
    upsert_stmt = stmt.on_conflict_do_update(
        index_elements=index_elements,
        set_=update_dict,
    )
    await session.execute(upsert_stmt)


async def _import_capabilities(session, doc: Dict[str, Any]) -> None:
    """Import capabilities into the database.

    Args:
        session: Database session.
        doc: YAML document containing capabilities.
    """
    console.print("  -> Importing capabilities...")
    await _upsert_items(session, Capability, doc.get("items", []), ["id"])


async def _import_symbols(session, doc: Dict[str, Any]) -> None:
    """Import symbols into the database, fixing missing symbol_path if necessary."""
    console.print("  -> Importing symbols...")
    items = doc.get("items", [])
    for item in items:
        if "symbol_path" not in item or not item["symbol_path"]:
            module = item.get("module")
            qualname = item.get("qualname")
            if module and qualname:
                file_path = "src/" + module.replace(".", "/") + ".py"
                item["symbol_path"] = f"{file_path}::{qualname}"

    await _upsert_items(session, Symbol, items, ["id"])


async def _import_links(session, doc: Dict[str, Any]) -> None:
    """Import symbol-capability links into the database.

    Args:
        session: Database session.
        doc: YAML document containing links.
    """
    console.print("  -> Importing links...")
    links_items = doc.get("items", [])
    if links_items:
        await session.execute(text("DELETE FROM core.symbol_capability_links;"))
        await _upsert_items(
            session,
            SymbolCapabilityLink,
            links_items,
            ["symbol_id", "capability_id", "source"],
        )


async def _import_northstar(session, doc: Dict[str, Any]) -> None:
    """Import North Star mission into the database.

    Args:
        session: Database session.
        doc: YAML document containing North Star data.
    """
    console.print("  -> Importing North Star...")
    await _upsert_items(session, Northstar, doc.get("items", []), ["id"])


async def _import_llm_resources(session, doc: Dict[str, Any]) -> None:
    """Import LLM resources into the database.

    Args:
        session: Database session.
        doc: YAML document containing LLM resources.
    """
    console.print("  -> Importing LLM resources...")
    await _upsert_items(session, LlmResource, doc.get("llm_resources", []), ["name"])


async def _import_cognitive_roles(session, doc: Dict[str, Any]) -> None:
    """Import cognitive roles into the database.

    Args:
        session: Database session.
        doc: YAML document containing cognitive roles.
    """
    console.print("  -> Importing cognitive roles...")
    await _upsert_items(
        session, CognitiveRole, doc.get("cognitive_roles", []), ["role"]
    )


# ID: a5c43fa1-1137-426d-a98f-a8f0e9265cf7
async def run_import(dry_run: bool) -> None:
    """Imports YAML files into the database, with optional dry run.

    Args:
        dry_run: If True, prints actions without executing them.
    """
    if not EXPORT_DIR.exists():
        console.print(
            f"[bold red]Export directory not found: {EXPORT_DIR}. Cannot import.[/bold red]"
        )
        return

    # Load all YAML documents
    docs = {
        name: read_yaml(EXPORT_DIR / filename) for name, filename in YAML_FILES.items()
    }

    # Verify digests for files that have them
    for name, doc in docs.items():
        if "digest" in doc and "items" in doc:
            if doc["digest"] != compute_digest(doc["items"]):
                console.print(
                    f"[bold red]Digest mismatch in {name}.yaml! "
                    "Aborting import. Run 'snapshot' to regenerate.[/bold red]"
                )
                return

    if dry_run:
        console.print(
            "[bold yellow]-- DRY RUN: The following actions would be taken --[/bold yellow]"
        )
        for name, doc in docs.items():
            count = len(_get_items_from_doc(doc, name))
            console.print(f"  - Upsert {count} {name}.")
        return

    async with get_session() as session:
        async with session.begin():
            await _import_capabilities(session, docs["capabilities"])
            await _import_symbols(session, docs["symbols"])
            await _import_links(session, docs["links"])
            await _import_northstar(session, docs["northstar"])
            await _import_llm_resources(session, docs["resource_manifest"])
            await _import_cognitive_roles(session, docs["cognitive_roles"])

    console.print(
        "[bold green]✅ Import complete. Database is synchronized with YAML files.[/bold green]"
    )

--- END OF FILE ./src/cli/logic/knowledge_sync/import_.py ---

--- START OF FILE ./src/cli/logic/knowledge_sync/snapshot.py ---
# src/cli/logic/knowledge_sync/snapshot.py
"""
Handles snapshot operations to export database state to YAML files for the CORE Working Mind.
"""

from __future__ import annotations

import asyncio
import getpass
from typing import Any, List

from rich.console import Console
from services.database.session_manager import get_session
from shared.config import settings
from shared.time import now_iso
from sqlalchemy import text

from .utils import write_yaml

console = Console()
EXPORT_DIR = settings.REPO_PATH / ".intent" / "mind_export"


# ID: 0e4f98b0-6132-435f-b463-9f27c447302a
async def fetch_capabilities() -> List[dict[str, Any]]:
    """Reads all capabilities from the database, ordered consistently.

    Returns:
        List of capability dictionaries.
    """
    async with get_session() as session:
        result = await session.execute(
            text(
                "SELECT id, name, objective, owner, domain, tags, status "
                "FROM core.capabilities ORDER BY lower(domain), lower(name), id"
            )
        )
        return [dict(row._mapping) for row in result]


# ID: 03445002-3060-4d3f-bc0b-27c6ccdc2fe9
async def fetch_symbols() -> List[dict[str, Any]]:
    """Reads all symbols from the database, ordered consistently.

    Returns:
        List of symbol dictionaries.
    """
    async with get_session() as session:
        result = await session.execute(
            text(
                "SELECT id, symbol_path, module, qualname, kind, ast_signature, fingerprint, state "
                "FROM core.symbols ORDER BY fingerprint, id"
            )
        )
        return [dict(row._mapping) for row in result]


# ID: 323d778b-4ed7-4d65-9d8d-9077fb880bb9
async def fetch_links() -> List[dict[str, Any]]:
    """Reads all symbol-capability links from the database, ordered consistently.

    Returns:
        List of link dictionaries.
    """
    async with get_session() as session:
        result = await session.execute(
            text(
                "SELECT symbol_id, capability_id, confidence, source, verified "
                "FROM core.symbol_capability_links "
                "ORDER BY capability_id, symbol_id, source"
            )
        )
        rows = [dict(row._mapping) for row in result]
        for r in rows:
            if "confidence" in r and r["confidence"] is not None:
                r["confidence"] = float(r["confidence"])
        return rows


# ID: 9f94dca6-1d04-41db-8970-b09fdc803222
async def fetch_northstar() -> List[dict[str, Any]]:
    """Reads the current North Star mission from the database.

    Returns:
        List containing the North Star dictionary.
    """
    async with get_session() as session:
        result = await session.execute(
            text(
                "SELECT id, mission FROM core.northstar "
                "ORDER BY updated_at DESC LIMIT 1"
            )
        )
        return [dict(row._mapping) for row in result]


# ID: dee34d49-638d-41ce-9f29-6941f5d90706
async def run_snapshot(env: str | None, note: str | None) -> None:
    """Exports database state to YAML files in the mind_export directory.

    Args:
        env: Environment name (e.g., 'dev'), defaults to 'dev'.
        note: Optional note for the snapshot.
    """
    EXPORT_DIR.mkdir(parents=True, exist_ok=True)
    exported_at = now_iso()
    who = getpass.getuser()
    env = env or "dev"

    console.print(f"📸 Creating a new snapshot of the database in '{EXPORT_DIR}'...")

    # Fetch all data
    caps, syms, links, north = await asyncio.gather(
        fetch_capabilities(), fetch_symbols(), fetch_links(), fetch_northstar()
    )

    # Write YAML files and collect digests
    snapshots = [
        ("capabilities.yaml", caps),
        ("symbols.yaml", syms),
        ("links.yaml", links),
        ("northstar.yaml", north),
    ]

    digests = [
        (filename, write_yaml(EXPORT_DIR / filename, data, exported_at))
        for filename, data in snapshots
    ]

    # Record in database
    async with get_session() as session:
        async with session.begin():
            result = await session.execute(
                text(
                    "INSERT INTO core.export_manifests (who, environment, notes) "
                    "VALUES (:who, :env, :note) RETURNING id"
                ),
                {"who": who, "env": env, "note": note},
            )
            manifest_id = result.scalar_one()

            for relpath, sha in digests:
                await session.execute(
                    text(
                        """
                        INSERT INTO core.export_digests (path, sha256, manifest_id)
                        VALUES (:path, :sha, :manifest_id)
                        ON CONFLICT (path) DO UPDATE SET
                          sha256 = EXCLUDED.sha256,
                          manifest_id = EXCLUDED.manifest_id,
                          exported_at = NOW()
                        """
                    ),
                    {
                        "path": str(
                            EXPORT_DIR.relative_to(settings.REPO_PATH) / relpath
                        ),
                        "sha": sha,
                        "manifest_id": manifest_id,
                    },
                )

    console.print("[bold green]✅ Snapshot complete.[/bold green]")
    for filename, sha in digests:
        console.print(f"  - Wrote '{filename}' with digest: {sha}")

--- END OF FILE ./src/cli/logic/knowledge_sync/snapshot.py ---

--- START OF FILE ./src/cli/logic/knowledge_sync/utils.py ---
# src/cli/logic/knowledge_sync/utils.py
"""
Shared utilities for knowledge synchronization operations in the CORE Working Mind.
"""

from __future__ import annotations

import hashlib
import json
import uuid
from pathlib import Path
from typing import Any, Dict, List

import yaml
from shared.config_loader import load_yaml_file


# ID: 0a055408-c1c4-54f2-b2d3-28bc47ace016
def canonicalize(obj: Any) -> Any:
    """Recursively sorts dictionary keys and handles UUIDs to ensure a stable, consistent order for hashing."""
    if isinstance(obj, dict):
        return {k: canonicalize(obj[k]) for k in sorted(obj.keys())}
    if isinstance(obj, list):
        return [canonicalize(x) for x in obj]
    if isinstance(obj, uuid.UUID):
        return str(obj)
    return obj


# ID: 96c822f2-6aeb-49e1-866f-53d8d97953c4
def compute_digest(items: List[Dict[str, Any]]) -> str:
    """Creates a unique fingerprint (SHA256) for a list of items."""
    canon = canonicalize(items)
    payload = json.dumps(
        canon, ensure_ascii=False, sort_keys=True, separators=(",", ":")
    ).encode("utf-8")
    return "sha256:" + hashlib.sha256(payload).hexdigest()


# ID: 915326c0-141c-83d4-fe10-2e46-ddae48f4-9813cce0001f0ea091b3c86d5595bf2f
# ID: b91d073b-f19b-42ce-b6a9-afe7594a10a5
def write_yaml(path: Path, items: List[Dict[str, Any]], exported_at: str) -> str:
    """Writes a list of items to a YAML file, including version, timestamp, and digest."""
    stringified_items = [
        {k: (str(v) if isinstance(v, uuid.UUID) else v) for k, v in item.items()}
        for item in items
    ]

    digest = compute_digest(stringified_items)

    doc = {
        "version": 1,
        "exported_at": exported_at,
        "items": stringified_items,
        "digest": digest,
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        yaml.safe_dump(doc, f, allow_unicode=True, sort_keys=False, indent=2)
    return digest


# The local `read_yaml` function is now an alias for the canonical loader.
read_yaml = load_yaml_file


# ID: 75d790d9-b2f7-5757-b2f7-6d790d9b2f7d
def _get_diff_links_key(item: Dict[str, Any]) -> str:
    """Creates a stable composite key for a link dictionary."""
    return f"{str(item.get('symbol_id', ''))}-{str(item.get('capability_id', ''))}-{item.get('source', '')}"


# ID: eab5bc15-09c8-56ca-9103-a160e16f0bce
def _get_items_from_doc(doc: Dict[str, Any], doc_name: str) -> List[Dict[str, Any]]:
    """Extract items from a document using the appropriate key."""
    possible_keys = [doc_name, "items", "llm_resources", "cognitive_roles"]
    items_key = next((k for k in possible_keys if k in doc), None)
    return doc.get(items_key, []) if items_key else []

--- END OF FILE ./src/cli/logic/knowledge_sync/utils.py ---

--- START OF FILE ./src/cli/logic/knowledge_sync/verify.py ---
# src/cli/logic/knowledge_sync/verify.py
"""
Verifies the integrity of exported YAML files by checking their digests.
"""

from __future__ import annotations

from rich.console import Console
from shared.config import settings

from .utils import compute_digest, read_yaml

console = Console()
EXPORT_DIR = settings.REPO_PATH / ".intent" / "mind_export"


# ID: 19b318e0-903d-4f25-8948-2c2680856ba1
def run_verify() -> bool:
    """Checks digests of exported YAML files to ensure integrity.

    Returns:
        bool: True if all digests are valid, False otherwise.
    """
    if not EXPORT_DIR.exists():
        console.print(
            f"[bold red]Export directory not found: {EXPORT_DIR}. Cannot verify.[/bold red]"
        )
        return False

    console.print("🔐 Verifying digests of exported YAML files...")

    files_to_check = [
        "capabilities.yaml",
        "symbols.yaml",
        "links.yaml",
        "northstar.yaml",
    ]
    all_ok = True

    for filename in files_to_check:
        path = EXPORT_DIR / filename
        if not path.exists():
            console.print(
                f"  - [yellow]SKIP[/yellow]: [cyan]{filename}[/cyan] does not exist."
            )
            continue

        doc = read_yaml(path)
        items = doc.get("items", [])
        expected_digest = doc.get("digest")

        if not expected_digest:
            console.print(
                f"  - [red]FAIL[/red]: [cyan]{filename}[/cyan] is missing a digest."
            )
            all_ok = False
            continue

        actual_digest = compute_digest(items)

        if expected_digest == actual_digest:
            console.print(
                f"  - [green]PASS[/green]: [cyan]{filename}[/cyan] digest is valid."
            )
        else:
            console.print(
                f"  - [red]FAIL[/red]: [cyan]{filename}[/cyan] digest mismatch!"
            )
            all_ok = False

    if all_ok:
        console.print("[bold green]✅ All digests are valid.[/bold green]")
    else:
        console.print(
            "[bold red]❌ One or more digests failed verification.[/bold red]"
        )

    return all_ok

--- END OF FILE ./src/cli/logic/knowledge_sync/verify.py ---

--- START OF FILE ./src/cli/logic/list_audits.py ---
# src/cli/logic/list_audits.py
"""
Provides functionality for the list_audits module.
"""

from __future__ import annotations

import asyncio

import typer
from services.database.session_manager import get_session
from sqlalchemy import text


# ID: 09c55085-1d89-46c2-a663-b4e1f2c2c0b5
def list_audits(
    limit: int = typer.Option(
        10, "--limit", help="How many to show (most recent first)"
    ),
) -> None:
    """Show recent rows from core.audit_runs."""

    async def _run():
        stmt = text(
            """
            select id, started_at, source, score, passed
            from core.audit_runs
            order by id desc
            limit :lim
            """
        ).bindparams(lim=limit)

        async with get_session() as session:
            result = await session.execute(stmt)
            rows = result.all()

        if not rows:
            typer.echo("— no audit rows yet —")
            return
        for r in rows:
            when = r.started_at.strftime("%Y-%m-%d %H:%M:%S")
            mark = "✅" if r.passed else "❌"
            typer.echo(f"{r.id:>4}  {when}  {r.source:<7}  score={r.score:.3f}  {mark}")

    asyncio.run(_run())

--- END OF FILE ./src/cli/logic/list_audits.py ---

--- START OF FILE ./src/cli/logic/log_audit.py ---
# src/cli/logic/log_audit.py
"""
Provides functionality for the log_audit module.
"""

from __future__ import annotations

import asyncio

import typer
from services.database.session_manager import get_session
from sqlalchemy import text

from .common import git_commit_sha


# ID: 90625b7b-b201-458d-84a3-895835a005c0
def log_audit(
    score: float = typer.Option(..., "--score", help="Audit score, e.g. 0.92"),
    passed: bool = typer.Option(
        True, "--passed/--failed", help="Mark audit as passed or failed"
    ),
    source: str = typer.Option(
        "manual", "--source", help="Source label: manual|pr|nightly"
    ),
    commit_sha: str = typer.Option(
        "", "--commit", help="Optional git commit SHA (40 chars)"
    ),
) -> None:
    """Insert one row into core.audit_runs."""

    async def _run():
        sha = commit_sha or git_commit_sha()
        stmt = text(
            """
            insert into core.audit_runs (source, commit_sha, score, passed, started_at, finished_at)
            values (:source, :sha, :score, :passed, now(), now())
            returning id
            """
        )
        async with get_session() as session:
            async with session.begin():
                result = await session.execute(
                    stmt, dict(source=source, sha=sha, score=score, passed=passed)
                )
                new_id = result.scalar_one()

        typer.echo(
            f"📝 Logged audit id={new_id} (source={source}, score={score}, passed={passed})"
        )

    asyncio.run(_run())

--- END OF FILE ./src/cli/logic/log_audit.py ---

--- START OF FILE ./src/cli/logic/new.py ---
"""
Handles the 'core-admin new' command for creating new project scaffolds.
Intent: Defines the 'core-admin new' command, a user-facing wrapper
around the Scaffolder tool.
"""

from __future__ import annotations

--- END OF FILE ./src/cli/logic/new.py ---

--- START OF FILE ./src/cli/logic/project_docs.py ---
# src/cli/logic/project_docs.py
"""
CLI wrapper for generating capability documentation.
It reuses the existing Python module entrypoint to keep one source of truth.
"""

from __future__ import annotations

import runpy
import sys

import typer


# ID: 752ead32-df2a-48c5-bb30-3530397e2cd2
def docs(output: str = "docs/10_CAPABILITY_REFERENCE.md") -> None:
    """
    Generate capability documentation into the given output path.
    """
    mod = "features.introspection.generate_capability_docs"
    # Preserve original argv and invoke the module as if run with: python -m ... --output <path>
    argv_backup = sys.argv[:]
    try:
        sys.argv = [mod, "--output", output]
        runpy.run_module(mod, run_name="__main__")
    finally:
        sys.argv = argv_backup
    typer.echo(f"📚 Capability documentation written to: {output}")

--- END OF FILE ./src/cli/logic/project_docs.py ---

--- START OF FILE ./src/cli/logic/proposal_service.py ---
# src/cli/logic/proposal_service.py
"""
Implements the command-line interface for proposal lifecycle management.
This module now serves as the main entry point for ALL proposal types.
"""

from __future__ import annotations

import asyncio
import base64
import tempfile
from datetime import datetime
from pathlib import Path

import typer
from cryptography.hazmat.primitives import serialization
from dotenv import load_dotenv
from features.governance.constitutional_auditor import ConstitutionalAuditor
from rich.console import Console
from rich.table import Table
from shared.config import settings
from shared.config_loader import load_yaml_file
from shared.context import CoreContext
from shared.logger import getLogger
from shared.utils.crypto import generate_approval_token

from .cli_utils import archive_rollback_plan, load_private_key, save_yaml_file

log = getLogger("core_admin.proposals")
console = Console()


# --- Helper functions to replace shutil ---
def _copy_tree(src: Path, dst: Path):
    dst.mkdir(parents=True, exist_ok=True)
    for item in src.iterdir():
        s = src / item.name
        d = dst / item.name
        if s.is_dir():
            if s.name not in [
                ".git",
                ".venv",
                "venv",
                "__pycache__",
                "work",
                "reports",
            ]:
                _copy_tree(s, d)
        else:
            d.write_bytes(s.read_bytes())


def _copy_file(src: Path, dst: Path):
    dst.parent.mkdir(parents=True, exist_ok=True)
    dst.write_bytes(src.read_bytes())


# --- End of helpers ---


# ID: 7dcb045e-19c9-4d84-91fd-70c4de7e8dfe
def proposals_list() -> None:
    # ... (this function is correct, no changes needed)
    log.info("🔍 Finding pending constitutional proposals...")
    proposals_dir = settings.REPO_PATH / ".intent" / "proposals"
    proposals_dir.mkdir(exist_ok=True)
    proposals = sorted(list(proposals_dir.glob("cr-*.yaml")))

    if not proposals:
        log.info("✅ No pending proposals found.")
        return

    log.info(f"Found {len(proposals)} pending proposal(s):")
    approvers_config = load_yaml_file(
        settings.REPO_PATH / ".intent" / "charter" / "constitution" / "approvers.yaml"
    )

    for prop_path in proposals:
        config = load_yaml_file(prop_path)
        justification = config.get("justification", "No justification provided.")
        target_path = config.get("target_path", "")
        quorum_config = approvers_config.get("quorum", {})
        current_mode = quorum_config.get("current_mode", "development")

        critical_paths_source = approvers_config.get(
            "critical_paths_source", "charter/constitution/critical_paths.yaml"
        )
        critical_paths_file = settings.REPO_PATH / ".intent" / critical_paths_source
        critical_paths_config = load_yaml_file(critical_paths_file)
        critical_paths = critical_paths_config.get("paths", [])

        is_critical = any(target_path == p for p in critical_paths)
        required_sigs = quorum_config.get(current_mode, {}).get(
            "critical" if is_critical else "standard", 1
        )
        current_sigs = len(config.get("signatures", []))
        status = (
            "✅ Ready"
            if current_sigs >= required_sigs
            else f"⏳ {current_sigs}/{required_sigs} sigs"
        )

        log.info(f"\n  - **{prop_path.name}**: {justification.strip()}")
        log.info(f"    Target: {target_path}")
        log.info(f"    Status: {status} ({'Critical' if is_critical else 'Standard'})")


# ID: e0b15fef-d8d5-4f39-98b3-18d4eedd8bb5
def proposals_sign(
    proposal_name: str = typer.Argument(
        ..., help="Filename of the proposal to sign (e.g., 'cr-new-policy.yaml')."
    ),
) -> None:
    # ... (this function is correct, no changes needed)
    log.info(f"✍️ Signing proposal: {proposal_name}")
    proposal_path = settings.REPO_PATH / ".intent" / "proposals" / proposal_name
    if not proposal_path.exists():
        log.error(f"❌ Proposal '{proposal_name}' not found.")
        raise typer.Exit(code=1)

    proposal = load_yaml_file(proposal_path)
    private_key = load_private_key()
    token = generate_approval_token(proposal)
    signature = private_key.sign(token.encode("utf-8"))
    identity = typer.prompt(
        "Enter your identity (e.g., name@domain.com) for this signature"
    )

    proposal.setdefault("signatures", [])
    proposal["signatures"] = [
        s for s in proposal["signatures"] if s.get("identity") != identity
    ]
    proposal["signatures"].append(
        {
            "identity": identity,
            "signature_b64": base64.b64encode(signature).decode("utf-8"),
            "token": token,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
    )

    save_yaml_file(proposal_path, proposal)
    log.info("✅ Signature added to proposal file.")


# ID: 9848504e-60ef-44c1-a57c-b7e14edb5809
def proposals_approve(
    context: CoreContext,
    proposal_name: str = typer.Argument(
        ..., help="Filename of the proposal to approve."
    ),
) -> None:
    """Verify signatures, run a canary audit, and apply a valid proposal."""
    log.info(f"🚀 Attempting to approve proposal: {proposal_name}")
    proposal_path = settings.REPO_PATH / ".intent" / "proposals" / proposal_name
    if not proposal_path.exists():
        log.error(f"❌ Proposal '{proposal_name}' not found.")
        raise typer.Exit(code=1)

    proposal = load_yaml_file(proposal_path)
    target_rel_path = proposal.get("target_path")
    if not target_rel_path:
        log.error("❌ Proposal is invalid: missing 'target_path'.")
        raise typer.Exit(code=1)

    log.info("🔐 Verifying cryptographic signatures...")
    approvers_config = load_yaml_file(
        settings.REPO_PATH / ".intent" / "charter" / "constitution" / "approvers.yaml"
    )
    approver_keys = {
        a["identity"]: a["public_key"] for a in approvers_config.get("approvers", [])
    }

    expected_token = generate_approval_token(proposal)
    valid_signatures = 0
    for sig in proposal.get("signatures", []):
        identity = sig.get("identity")
        if sig.get("token") != expected_token:
            log.warning(f"   ⚠️ Stale signature from '{identity}'.")
            continue
        pem = approver_keys.get(identity)
        if not pem:
            log.warning(f"   ⚠️ No public key found for signatory '{identity}'.")
            continue
        try:
            pub_key = serialization.load_pem_public_key(pem.encode("utf-8"))
            pub_key.verify(
                base64.b64decode(sig["signature_b64"]), expected_token.encode("utf-8")
            )
            log.info(f"   ✅ Valid signature from '{identity}'.")
            valid_signatures += 1
        except Exception:
            log.warning(f"   ⚠️ Verification failed for signature from '{identity}'.")
            continue

    quorum_config = approvers_config.get("quorum", {})
    mode = quorum_config.get("current_mode", "development")

    critical_paths_source = approvers_config.get(
        "critical_paths_source", "charter/constitution/critical_paths.yaml"
    )
    critical_paths_file = settings.REPO_PATH / ".intent" / critical_paths_source
    critical_paths_config = load_yaml_file(critical_paths_file)
    critical_paths = critical_paths_config.get("paths", [])

    is_critical = any(str(target_rel_path) == p for p in critical_paths)
    required_sigs = quorum_config.get(mode, {}).get(
        "critical" if is_critical else "standard", 1
    )

    if valid_signatures < required_sigs:
        log.error(
            f"❌ Approval failed: Quorum not met ({valid_signatures}/{required_sigs})."
        )
        raise typer.Exit(code=1)

    log.info("\n🐦 Spinning up canary environment for validation...")
    with tempfile.TemporaryDirectory() as tmp:
        tmp_path = Path(tmp)
        log.info(f"   -> Creating a clean copy of the repository at {tmp_path}...")

        _copy_tree(settings.REPO_PATH, tmp_path)

        canary_env_path = tmp_path / ".env"
        env_file = settings.REPO_PATH / ".env"
        if env_file.exists():
            _copy_file(env_file, canary_env_path)
            log.info("   -> Copied environment configuration to canary.")

        canary_target_path = tmp_path / target_rel_path
        canary_target_path.parent.mkdir(parents=True, exist_ok=True)
        canary_target_path.write_text(proposal.get("content", ""), encoding="utf-8")

        if canary_env_path.exists():
            log.info(f"   -> Loading canary environment from {canary_env_path}...")
            load_dotenv(dotenv_path=canary_env_path, override=True)

        log.info("🔬 Commanding canary to perform a self-audit...")
        auditor = ConstitutionalAuditor(repo_root_override=tmp_path)
        success, findings, unassigned_count = asyncio.run(
            auditor.run_full_audit_async()
        )

        if success:
            log.info("✅ Canary audit PASSED. Change is constitutionally valid.")
            archive_rollback_plan(proposal_name, proposal)
            live_target_path = settings.REPO_PATH / target_rel_path
            live_target_path.parent.mkdir(parents=True, exist_ok=True)
            live_target_path.write_text(proposal.get("content", ""), encoding="utf-8")
            proposal_path.unlink()
            log.info(f"✅ Successfully approved and applied '{proposal_name}'.")
        else:
            log.error(
                "❌ Canary audit FAILED. Proposal rejected; live system untouched."
            )
            if findings:
                console.print("\n[bold red]Canary Audit Findings:[/bold red]")
                table = Table()
                table.add_column("Severity")
                table.add_column("Check ID")
                table.add_column("Message")
                table.add_column("File:Line")
                for f in findings:
                    loc = (
                        f"{f.file_path}:{f.line_number}"
                        if f.line_number
                        else f.file_path
                    )
                    table.add_row(str(f.severity), f.check_id, f.message, loc)
                console.print(table)
            raise typer.Exit(code=1)

--- END OF FILE ./src/cli/logic/proposal_service.py ---

--- START OF FILE ./src/cli/logic/proposals_micro.py ---
# src/cli/logic/proposals_micro.py
"""
Implements the logic for creating and applying autonomous, low-risk micro-proposals.
"""

from __future__ import annotations

import json
import tempfile
import time
import uuid
from pathlib import Path
from typing import Optional

import typer
from core.agents.micro_planner import MicroPlannerAgent
from core.agents.plan_executor import PlanExecutor
from features.governance.micro_proposal_validator import MicroProposalValidator
from rich.console import Console
from shared.action_logger import action_logger
from shared.context import CoreContext
from shared.logger import getLogger
from shared.models import ExecutionTask

console = Console()
log = getLogger("proposals_micro")


# ID: 5336b8a6-6f19-46a1-b8d2-9d3d83e8e3d3
async def micro_propose(
    context: CoreContext,
    goal: str,
) -> Optional[Path]:
    """Uses an agent to create a safe, auto-approvable plan for a goal."""
    console.print(f"🤖 Generating micro-proposal for goal: '[cyan]{goal}[/cyan]'")

    cognitive_service = context.cognitive_service
    planner = MicroPlannerAgent(cognitive_service)

    plan = await planner.create_micro_plan(goal)

    if not plan:
        console.print(
            "[bold red]❌ Agent could not generate a safe plan for this goal.[/bold red]"
        )
        return None

    proposal = {"proposal_id": str(uuid.uuid4()), "goal": goal, "plan": plan}
    proposal_file = (
        Path(tempfile.gettempdir())
        / f"core-micro-proposal-{proposal['proposal_id']}.json"
    )
    proposal_file.write_text(json.dumps(proposal, indent=2))

    console.print(
        "[bold green]✅ Safe micro-proposal generated successfully![/bold green]"
    )
    console.print("Plan details:")
    console.print(json.dumps(plan, indent=2))
    console.print("To apply this plan, run:")
    console.print(
        f"[bold]poetry run core-admin manage proposals micro-apply {proposal_file}[/bold]"
    )
    return proposal_file


# ID: 1494aa0f-9e85-4675-8bc4-7c69529206c4
async def propose_and_apply_autonomously(context: CoreContext, goal: str):
    """
    A single, unified async workflow that proposes a plan and immediately applies it.
    """
    console.print(
        f"[bold cyan]🚀 Initiating A1 self-healing for: '{goal}'...[/bold cyan]"
    )
    proposal_path = await micro_propose(context, goal)

    if proposal_path and proposal_path.exists():
        console.print(
            "\n[bold cyan]-> Plan generated. Proceeding with autonomous application...[/bold cyan]"
        )
        await micro_apply(context=context, proposal_path=proposal_path)
    elif proposal_path:
        console.print(
            f"[bold red]❌ Proposal file was not created at {proposal_path}. Aborting.[/bold red]"
        )
        raise typer.Exit(code=1)
    else:
        console.print(
            "[bold red]❌ Failed to generate a proposal. Aborting.[/bold red]"
        )
        raise typer.Exit(code=1)


# ID: f84ebe0a-f814-4cb3-a54b-5c186d4733c9
async def micro_apply(
    context: CoreContext,
    proposal_path: Path,
):
    """Validates and applies a micro-proposal."""
    console.print(f"🔵 Loading and applying micro-proposal: {proposal_path.name}")
    start_time = time.monotonic()

    try:
        proposal_content = proposal_path.read_text(encoding="utf-8")
        proposal_data = json.loads(proposal_content)
        plan_dicts = proposal_data.get("plan", [])
        plan = [ExecutionTask(**task) for task in plan_dicts]
    except Exception as e:
        console.print(f"[bold red]❌ Error loading proposal file: {e}[/bold red]")
        raise typer.Exit(code=1)

    action_logger.log_event(
        "a1.apply.started",
        {"proposal": proposal_path.name, "goal": proposal_data.get("goal")},
    )

    try:
        console.print(
            "[bold]Step 1/3: Validating plan against constitutional policy...[/bold]"
        )
        validator = MicroProposalValidator()
        is_valid, validation_error = validator.validate(plan)
        if not is_valid:
            raise RuntimeError(f"Plan is constitutionally invalid: {validation_error}")
        console.print("   -> ✅ Plan is valid.")

        console.print(
            "[bold]Step 2/3: Gathering evidence via pre-flight checks...[/bold]"
        )
        console.print("   -> Running full system audit check (in-process)...")

        # This part requires the ExecutionAgent, so we'll simulate for now
        # In a real scenario, you'd call the validation service.
        console.print("   -> ✅ All pre-flight checks passed (simulated for CLI call).")

        console.print("[bold]Step 3/3: Executing the validated plan...[/bold]")
        plan_executor = PlanExecutor(
            context.file_handler, context.git_service, context.planner_config
        )
        await plan_executor.execute_plan(plan)

        duration = time.monotonic() - start_time
        action_logger.log_event(
            "a1.apply.succeeded",
            {"proposal": proposal_path.name, "duration_sec": round(duration, 2)},
        )
        console.print(
            "[bold green]✅ Micro-proposal applied successfully![/bold green]"
        )

    except Exception as e:
        duration = time.monotonic() - start_time
        action_logger.log_event(
            "a1.apply.failed",
            {
                "proposal": proposal_path.name,
                "error": str(e),
                "duration_sec": round(duration, 2),
            },
        )
        console.print(f"[bold red]❌ Error during plan execution: {e}[/bold red]")
        raise typer.Exit(code=1)

--- END OF FILE ./src/cli/logic/proposals_micro.py ---

--- START OF FILE ./src/cli/logic/reconcile.py ---
# src/cli/commands/reconcile.py
"""
Implements the 'knowledge reconcile-from-cli' command to link declared
capabilities to their implementations in the database using the CLI registry as the map.
"""

from __future__ import annotations

import asyncio

import typer
import yaml
from rich.console import Console
from services.repositories.db.engine import get_session
from shared.config import settings
from sqlalchemy import text

console = Console()
CLI_REGISTRY_PATH = (
    settings.REPO_PATH / ".intent" / "mind" / "knowledge" / "cli_registry.yaml"
)


async def _async_reconcile():
    """
    Reads the CLI registry and updates the 'key' in the symbols table for all
    symbols that implement a registered command.
    """
    console.print(
        "[bold cyan]🚀 Reconciling capabilities from CLI registry to database...[/bold cyan]"
    )

    if not CLI_REGISTRY_PATH.exists():
        console.print(
            f"[bold red]❌ CLI Registry not found at {CLI_REGISTRY_PATH}[/bold red]"
        )
        raise typer.Exit(code=1)

    registry = yaml.safe_load(CLI_REGISTRY_PATH.read_text("utf-8"))
    commands = registry.get("commands", [])

    updates_to_perform = []
    for command in commands:
        entrypoint = command.get("entrypoint")
        capabilities = command.get("implements", [])
        if not entrypoint or not capabilities:
            continue

        module_path, function_name = entrypoint.split("::")
        file_path_str = "src/" + module_path.replace(".", "/") + ".py"
        symbol_path = f"{file_path_str}::{function_name}"
        primary_key = capabilities[0]

        updates_to_perform.append(
            {
                "key": primary_key,
                "symbol_path": symbol_path,
            }
        )

    if not updates_to_perform:
        console.print(
            "[yellow]⚠️ No capabilities with entrypoints found in CLI registry.[/yellow]"
        )
        return

    console.print(
        f"   -> Found {len(updates_to_perform)} capability implementations to link."
    )

    linked_count = 0
    async with get_session() as session:
        async with session.begin():
            for update in updates_to_perform:
                stmt = text(
                    """
                    UPDATE core.symbols SET key = :key, updated_at = NOW()
                    WHERE symbol_path = :symbol_path AND key IS NULL;
                    """
                )
                result = await session.execute(stmt, update)
                if result.rowcount > 0:
                    linked_count += 1

    console.print(
        f"[bold green]✅ Successfully linked {linked_count} capabilities.[/bold green]"
    )


# ID: b43fc6d4-413b-47f2-8a0c-7860836913ab
def reconcile_from_cli():
    """Typer-compatible wrapper for the async reconcile logic."""
    asyncio.run(_async_reconcile())

--- END OF FILE ./src/cli/logic/reconcile.py ---

--- START OF FILE ./src/cli/logic/report.py ---
# src/cli/logic/report.py
"""
Provides functionality for the report module.
"""

from __future__ import annotations

import asyncio

import typer
from services.database.session_manager import get_session
from sqlalchemy import text


# ID: 27a79c8d-285f-4e79-8de9-a4a5cba424d4
def report() -> None:
    """Summary by source (count, pass rate, avg score)."""

    async def _run():
        stmt = text(
            """
            select
              source,
              count(*) as total,
              sum(case when passed then 1 else 0 end) as passed_count,
              round(avg(score)::numeric, 3) as avg_score
            from core.audit_runs
            group by source
            order by source
            """
        )

        async with get_session() as session:
            result = await session.execute(stmt)
            rows = result.all()

        if not rows:
            typer.echo("— no data —")
            return

        typer.echo("source   total  passed  pass_rate  avg_score")
        for r in rows:
            pass_rate = (r.passed_count / r.total) * 100.0 if r.total else 0.0
            typer.echo(
                f"{r.source:<7} {r.total:>5}  {r.passed_count:>6}   {pass_rate:>6.1f}%     {float(r.avg_score):>8.3f}"
            )

    asyncio.run(_run())

--- END OF FILE ./src/cli/logic/report.py ---

--- START OF FILE ./src/cli/logic/reviewer.py ---
"""
Provides commands for AI-powered review of the constitution, documentation, and source code files.
"""

from __future__ import annotations

import asyncio
from pathlib import Path
from typing import List, Set

import typer
from core.cognitive_service import CognitiveService
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from shared.config import settings
from shared.logger import getLogger
from shared.utils.constitutional_parser import get_all_constitutional_paths

log = getLogger("core_admin.review")
console = Console()
DOCS_IGNORE_DIRS = {"assets", "archive", "migrations", "examples"}


def _get_bundle_content(files_to_bundle: List[Path], root_dir: Path) -> str:
    bundle_parts = []
    for file_path in sorted(list(files_to_bundle)):
        if file_path.exists() and file_path.is_file():
            try:
                content = file_path.read_text(encoding="utf-8")
                rel_path = file_path.resolve().relative_to(root_dir.resolve())
                bundle_parts.append(f"--- START OF FILE ./{rel_path} ---\n")
                bundle_parts.append(content)
                bundle_parts.append(f"\n--- END OF FILE ./{rel_path} ---\n\n")
            except ValueError:
                log.warning(
                    f"Could not determine relative path for {file_path}. Skipping."
                )
    return "".join(bundle_parts)


def _get_constitutional_files() -> List[Path]:
    """
    Discovers all constitutional files by parsing meta.yaml via the settings object.
    """
    meta_content = settings._meta_config
    relative_paths = get_all_constitutional_paths(meta_content, settings.MIND)
    return [settings.REPO_PATH / p for p in relative_paths]


def _get_docs_files() -> List[Path]:
    root_dir = settings.REPO_PATH
    scan_files = [root_dir / "README.md", root_dir / "CONTRIBUTING.md"]
    docs_dir = root_dir / "docs"
    found_files: Set[Path] = {f for f in scan_files if f.exists()}
    if docs_dir.is_dir():
        for md_file in docs_dir.rglob("*.md"):
            if not any((ignored in md_file.parts for ignored in DOCS_IGNORE_DIRS)):
                found_files.add(md_file)
    return list(found_files)


def _orchestrate_review(
    bundle_name: str,
    prompt_key: str,
    file_gatherer_fn,
    output_path: Path,
    no_send: bool,
):
    log.info(f"🤖 Orchestrating review for: {bundle_name}...")
    try:
        prompt_path = settings.get_path(f"mind.prompts.{prompt_key}")
        review_prompt_template = prompt_path.read_text(encoding="utf-8")
    except FileNotFoundError:
        log.error(
            f"❌ Review prompt '{prompt_key}' not found in meta.yaml. Cannot proceed."
        )
        raise typer.Exit(code=1)
    log.info(f"   -> Loaded review prompt: {prompt_key}")
    log.info("   -> Bundling files for review...")
    files_to_bundle = file_gatherer_fn()
    bundle_content = _get_bundle_content(files_to_bundle, settings.REPO_PATH)
    log.info(f"   -> Bundled {len(files_to_bundle)} files.")
    bundle_output_path = settings.REPO_PATH / "reports" / f"{bundle_name}_bundle.txt"
    bundle_output_path.parent.mkdir(parents=True, exist_ok=True)
    bundle_output_path.write_text(bundle_content, encoding="utf-8")
    log.info(f"   -> Saved review bundle to: {bundle_output_path}")
    final_prompt = f"{review_prompt_template}\n\n{bundle_content}"
    if no_send:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(final_prompt, encoding="utf-8")
        log.info(f"✅ Full prompt bundle for manual review saved to: {output_path}")
        raise typer.Exit()
    log.info("   -> Sending bundle to LLM for analysis. This may take a moment...")
    cognitive_service = CognitiveService(settings.REPO_PATH)
    reviewer = cognitive_service.get_client_for_role("SecurityAnalyst")

    # ID: 9320f90b-3fc5-4979-9c18-c8aa9b36bb7d
    async def run_async_review():
        return await reviewer.make_request_async(
            final_prompt, user_id=f"{bundle_name}_reviewer"
        )

    review_feedback = asyncio.run(run_async_review())
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(review_feedback, encoding="utf-8")
    log.info(f"✅ Successfully received feedback and saved to: {output_path}")
    console.print(f"\n--- {bundle_name.replace('_', ' ').title()} Review Summary ---")
    console.print(Markdown(review_feedback))


# ID: b7d07270-37b5-4ce6-8649-217117646d36
def peer_review(
    output: Path = typer.Option(
        Path("reports/constitutional_review.md"), "--output", "-o"
    ),
    no_send: bool = typer.Option(False, "--no-send"),
):
    """Audits the machine-readable constitution (.intent files) for clarity and consistency."""
    _orchestrate_review(
        "constitutional",
        "constitutional_review",
        _get_constitutional_files,
        output,
        no_send,
    )


# ID: d6590bcd-fc97-4615-905f-6295787b4b53
def docs_clarity_audit(
    output: Path = typer.Option(
        Path("reports/docs_clarity_review.md"), "--output", "-o"
    ),
    no_send: bool = typer.Option(False, "--no-send"),
):
    """Audits the human-readable documentation (.md files) for conceptual clarity."""
    _orchestrate_review(
        "docs_clarity", "docs_clarity_review", _get_docs_files, output, no_send
    )


# ID: af4eed18-bc09-44ce-a41d-fa309820d3c8
def code_review(
    file_path: Path = typer.Argument(
        ..., exists=True, dir_okay=False, resolve_path=True
    ),
):
    """Submits a source file to an AI expert for a peer review and improvement suggestions."""

    async def _async_code_review():
        log.info(
            f"🤖 Submitting '{file_path.relative_to(settings.REPO_PATH)}' for AI peer review..."
        )
        try:
            source_code = file_path.read_text(encoding="utf-8")
            prompt_path = settings.get_path("mind.prompts.code_peer_review")
            review_prompt_template = prompt_path.read_text(encoding="utf-8")
            final_prompt = f"{review_prompt_template}\n\n```python\n{source_code}\n```"
            with console.status(
                "[bold green]Asking AI expert for review...[/bold green]",
                spinner="dots",
            ):
                cognitive_service = CognitiveService(settings.REPO_PATH)
                reviewer_client = cognitive_service.get_client_for_role("CodeReviewer")
                review_feedback = await reviewer_client.make_request_async(
                    final_prompt, user_id="code_review_operator"
                )
            console.print(
                Panel("AI Peer Review Complete", style="bold green", expand=False)
            )
            console.print(Markdown(review_feedback))
        except FileNotFoundError:
            log.error(f"❌ Error: File not found at '{file_path}'")
            raise typer.Exit(code=1)
        except Exception as e:
            log.error(
                f"❌ An unexpected error occurred during peer review: {e}",
                exc_info=True,
            )
            raise typer.Exit(code=1)

    asyncio.run(_async_code_review())

--- END OF FILE ./src/cli/logic/reviewer.py ---

--- START OF FILE ./src/cli/logic/run.py ---
# src/cli/logic/run.py
"""
Registers and implements the 'run' command group for executing complex,
multi-step processes and autonomous cycles.
"""

from __future__ import annotations

import asyncio
from pathlib import Path
from typing import Optional

import typer
from dotenv import load_dotenv
from features.autonomy.autonomous_developer import develop_from_goal
from features.introspection.vectorization_service import run_vectorize
from shared.config import settings
from shared.context import CoreContext
from shared.logger import getLogger

log = getLogger("core_admin.run")
run_app = typer.Typer(
    help="Commands for executing complex processes and autonomous cycles."
)


@run_app.command(
    "develop",
    help="Orchestrates the autonomous development process from a high-level goal.",
)
# ID: 1ddfca35-8fcd-4f5e-925d-f0659f34e2a4
def develop(
    context: CoreContext,
    goal: Optional[str] = typer.Argument(
        None,
        help="The high-level development goal for CORE to achieve.",
        show_default=False,
    ),
    from_file: Optional[Path] = typer.Option(
        None,
        "--from-file",
        "-f",
        help="Path to a file containing the development goal.",
        exists=True,
        dir_okay=False,
        resolve_path=True,
        show_default=False,
    ),
):
    """Orchestrates the autonomous development process from a high-level goal."""
    if not goal and not from_file:
        log.error(
            "❌ You must provide a goal either as an argument or with --from-file."
        )
        raise typer.Exit(code=1)

    if from_file:
        goal_content = from_file.read_text(encoding="utf-8").strip()
    else:
        goal_content = goal.strip()

    load_dotenv()
    if not settings.LLM_ENABLED:
        log.error("❌ The 'develop' command requires LLMs to be enabled.")
        raise typer.Exit(code=1)

    # The CLI now simply calls the dedicated service.
    success, message = asyncio.run(develop_from_goal(context, goal_content))

    if success:
        typer.secho(f"\n✅ Goal execution successful: {message}", fg=typer.colors.GREEN)
        typer.secho(
            "   -> Run 'git status' to see the changes and 'core-admin submit changes' to integrate them.",
            bold=True,
        )
    else:
        typer.secho(f"\n❌ Goal execution failed: {message}", fg=typer.colors.RED)
        raise typer.Exit(code=1)


@run_app.command(
    "vectorize",
    help="Scan capabilities from the DB, generate embeddings, and upsert to Qdrant.",
)
# ID: b6ca020c-68ea-4280-b189-e2e7d453f391
def vectorize_capabilities(
    context: CoreContext,
    dry_run: bool = typer.Option(
        True, "--dry-run/--write", help="Show changes without writing to Qdrant."
    ),
    force: bool = typer.Option(
        False, "--force", help="Force re-vectorization of all capabilities."
    ),
):
    """The CLI wrapper for the database-driven vectorization process."""
    log.info("🚀 Starting capability vectorization process...")
    if not settings.LLM_ENABLED:
        log.error("❌ LLMs must be enabled to generate embeddings.")
        raise typer.Exit(code=1)
    try:
        cog = context.cognitive_service
        asyncio.run(run_vectorize(cognitive_service=cog, dry_run=dry_run, force=force))
    except Exception as e:
        log.error(f"❌ Orchestration failed: {e}", exc_info=True)
        raise typer.Exit(code=1)

--- END OF FILE ./src/cli/logic/run.py ---

--- START OF FILE ./src/cli/logic/status.py ---
# src/cli/logic/status.py
"""
CLI command to check database connectivity and migration status.
This is a thin wrapper around the status service.
"""

from __future__ import annotations

import asyncio

import typer

# This now correctly imports the business logic from the service layer.
from services.repositories.db.status_service import status as get_status_report


# ID: 10235f65-fae8-473a-8a60-f65711b87f43
def status() -> None:
    """Show DB connectivity and migration status by calling the status service."""

    async def _run():
        report = await get_status_report()

        if report.is_connected and report.db_version:
            typer.echo(f"✅ Connected: {report.db_version}")
        else:
            typer.echo("❌ Connection failed.", err=True)
            raise typer.Exit(code=1)

        typer.echo(f"Applied: {sorted(list(report.applied_migrations)) or '—'}")
        typer.echo(f"Pending: {report.pending_migrations or '—'}")

    asyncio.run(_run())

--- END OF FILE ./src/cli/logic/status.py ---

--- START OF FILE ./src/cli/logic/symbol_drift.py ---
# src/cli/logic/symbol_drift.py
"""
Implements the `inspect symbol-drift` command, a diagnostic tool to detect
discrepancies between symbols on the filesystem and those in the database.
"""

from __future__ import annotations

import asyncio

from features.introspection.sync_service import SymbolScanner
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from services.database.session_manager import get_session
from sqlalchemy import text

console = Console()


async def _run_drift_analysis():
    """
    The core logic that scans source, queries the DB, and compares the results.
    """
    console.print("[bold cyan]🚀 Running Symbol Drift Analysis...[/bold cyan]")

    # 1. Scan the filesystem to get the ground truth
    console.print("   -> Scanning 'src/' directory for all public symbols...")
    scanner = SymbolScanner()
    code_symbols = await asyncio.to_thread(scanner.scan)
    code_symbol_paths = {s["symbol_path"] for s in code_symbols}
    console.print(f"      - Found {len(code_symbol_paths)} symbols in source code.")

    # 2. Query the database to get the current state
    console.print("   -> Querying database for all registered symbols...")
    db_symbol_paths = set()
    try:
        async with get_session() as session:
            result = await session.execute(text("SELECT symbol_path FROM core.symbols"))
            db_symbol_paths = {row[0] for row in result}
        console.print(f"      - Found {len(db_symbol_paths)} symbols in the database.")
    except Exception as e:
        console.print(f"[bold red]❌ Database query failed: {e}[/bold red]")
        console.print("   Please ensure your database is running and accessible.")
        return

    # 3. Compare the two sets to find the drift
    ghost_symbols_in_db = sorted(list(db_symbol_paths - code_symbol_paths))
    new_symbols_in_code = sorted(list(code_symbol_paths - db_symbol_paths))

    console.print("\n--- Analysis Complete ---")

    if not ghost_symbols_in_db and not new_symbols_in_code:
        console.print(
            Panel(
                "[bold green]✅ No drift detected.[/bold green]\nThe database is perfectly synchronized with the source code.",
                title="Result",
                border_style="green",
            )
        )
        return

    # Display findings
    if ghost_symbols_in_db:
        table = Table(
            title=f"👻 Found {len(ghost_symbols_in_db)} Ghost Symbols in Database",
            caption="These symbols exist in the DB but NOT in the source code. They should be pruned.",
            show_header=True,
            header_style="bold red",
        )
        table.add_column("Obsolete Symbol Path", style="red")
        for symbol in ghost_symbols_in_db:
            table.add_row(symbol)
        console.print(table)
        console.print(
            "\n[bold]Diagnosis:[/bold] The `sync-knowledge` command is failing to delete obsolete symbols from the database."
        )

    if new_symbols_in_code:
        table = Table(
            title=f"✨ Found {len(new_symbols_in_code)} New Symbols in Source Code",
            caption="These symbols exist in the code but NOT in the DB. They need to be synchronized.",
            show_header=True,
            header_style="bold green",
        )
        table.add_column("New Symbol Path", style="green")
        for symbol in new_symbols_in_code:
            table.add_row(symbol)
        console.print(table)

    console.print(
        "\n[bold]Next Step:[/bold] This report confirms a bug in the sync logic. Please proceed with fixing the `run_sync_with_db` function."
    )


# ID: 1342dd1f-2117-469d-b5a3-9e3379f68197
def inspect_symbol_drift():
    """Synchronous Typer wrapper for the async drift analysis logic."""
    asyncio.run(_run_drift_analysis())

--- END OF FILE ./src/cli/logic/symbol_drift.py ---

--- START OF FILE ./src/cli/logic/sync.py ---
# src/cli/logic/sync.py
"""
Implements the 'knowledge sync' command, the single source of truth for
synchronizing the codebase state (IDs) with the database.
"""

from __future__ import annotations

import asyncio

import typer
from features.introspection.sync_service import run_sync_with_db
from rich.console import Console

console = Console()


async def _async_sync_knowledge(write: bool):
    """Core async logic for the sync command."""
    console.print(
        "[bold cyan]🚀 Synchronizing codebase state with database using temp table strategy...[/bold cyan]"
    )

    if not write:
        console.print(
            "\n[bold yellow]💧 Dry Run: This command no longer supports a dry run due to its database-centric logic.[/bold yellow]"
        )
        console.print("   Run with '--write' to execute the synchronization.")
        return

    stats = await run_sync_with_db()

    console.print("\n--- Knowledge Sync Summary ---")
    console.print(f"   Scanned from code:  [cyan]{stats['scanned']}[/cyan] symbols")
    console.print(f"   New symbols added:  [green]{stats['inserted']}[/green]")
    console.print(f"   Existing symbols updated: [yellow]{stats['updated']}[/yellow]")
    console.print(f"   Obsolete symbols removed: [red]{stats['deleted']}[/red]")
    console.print(
        "\n[bold green]✅ Database is now synchronized with the codebase.[/bold green]"
    )


# ID: 89517800-0799-476e-8078-a184519a76a1
def sync_knowledge_base(
    write: bool = typer.Option(
        False, "--write", help="Apply the changes to the database."
    ),
):
    """Scans the codebase and syncs all symbols and their IDs to the database."""
    asyncio.run(_async_sync_knowledge(write))

--- END OF FILE ./src/cli/logic/sync.py ---

--- START OF FILE ./src/cli/logic/sync_domains.py ---
# src/cli/logic/sync_domains.py
"""
CLI command to synchronize the canonical list of domains to the database.
"""

from __future__ import annotations

import asyncio

import typer
import yaml
from rich.console import Console
from services.database.session_manager import get_session
from shared.config import settings
from sqlalchemy import text

console = Console()


async def _sync_domains():
    """
    Reads the canonical domains.yaml file and upserts them into the core.domains table.
    """
    domains_path = settings.MIND / "knowledge" / "domains.yaml"
    if not domains_path.exists():
        console.print(
            f"[bold red]❌ Error: Constitutional domains file not found at {domains_path}[/bold red]"
        )
        raise typer.Exit(code=1)

    content = yaml.safe_load(domains_path.read_text("utf-8"))
    domains_to_sync = content.get("domains", [])

    if not domains_to_sync:
        console.print(
            "[yellow]⚠️  No domains found in domains.yaml. Nothing to sync.[/yellow]"
        )
        return

    upserted_count = 0
    async with get_session() as session:
        async with session.begin():  # Start a transaction
            for domain_data in domains_to_sync:
                name = domain_data.get("name")
                description = domain_data.get("description", "")
                if not name:
                    continue

                stmt = text(
                    """
                    INSERT INTO core.domains (key, title, description, status)
                    VALUES (:key, :title, :desc, 'active')
                    ON CONFLICT (key) DO UPDATE SET
                        title = EXCLUDED.title,
                        description = EXCLUDED.description;
                """
                )

                await session.execute(
                    stmt,
                    {
                        "key": name,
                        "title": name.replace("_", " ").title(),
                        "desc": description,
                    },
                )
                upserted_count += 1

    console.print(
        f"[bold green]✅ Successfully synced {upserted_count} domains to the database.[/bold green]"
    )


# ID: 5bee5341-7f72-430e-b310-f174af37de20
def sync_domains():
    """Synchronizes the canonical list of domains from .intent/knowledge/domains.yaml to the database."""
    asyncio.run(_sync_domains())

--- END OF FILE ./src/cli/logic/sync_domains.py ---

--- START OF FILE ./src/cli/logic/sync_manifest.py ---
# src/cli/logic/sync_manifest.py
"""
Implements the 'knowledge sync-manifest' command to synchronize the project
manifest with the public symbols stored in the database.
"""

from __future__ import annotations

import asyncio

import typer
from rich.console import Console
from ruamel.yaml import YAML  # Use ruamel.yaml for safer writing
from services.database.session_manager import get_session
from shared.config import settings
from shared.logger import getLogger
from sqlalchemy import text

log = getLogger("core_admin.sync_manifest")
console = Console()

MANIFEST_PATH = settings.REPO_PATH / ".intent" / "mind" / "project_manifest.yaml"


async def _async_sync_manifest():
    """
    Reads all public symbols from the database and updates project_manifest.yaml
    to make it the single source of truth for all declared capabilities.
    """
    console.print(
        "[bold cyan]🚀 Synchronizing project manifest with database...[/bold cyan]"
    )

    if not MANIFEST_PATH.exists():
        log.error(f"❌ Manifest file not found at {MANIFEST_PATH}")
        raise typer.Exit(code=1)

    console.print("   -> Fetching all public symbols from the database...")
    public_symbol_keys = []
    try:
        async with get_session() as session:
            result = await session.execute(
                text(
                    # Fetch keys from symbols that have a non-null key
                    "SELECT key FROM core.symbols WHERE key IS NOT NULL ORDER BY key"
                )
            )
            public_symbol_keys = [row[0] for row in result]
    except Exception as e:
        log.error(f"❌ Database query failed: {e}")
        console.print(
            "[bold red]Error connecting to the database. Is it running?[/bold red]"
        )
        raise typer.Exit(code=1)

    console.print(
        f"   -> Found {len(public_symbol_keys)} public capabilities to declare."
    )

    yaml_handler = YAML()
    yaml_handler.indent(mapping=2, sequence=4, offset=2)

    with MANIFEST_PATH.open("r", encoding="utf-8") as f:
        manifest_data = yaml_handler.load(f)

    manifest_data["capabilities"] = public_symbol_keys

    console.print(f"   -> Updating {MANIFEST_PATH.relative_to(settings.REPO_PATH)}...")

    with MANIFEST_PATH.open("w", encoding="utf-8") as f:
        yaml_handler.dump(manifest_data, f)

    console.print("[bold green]✅ Manifest synchronization complete.[/bold green]")


# ID: fcf8c754-27d0-4449-a3c4-bd3afbcff6ce
def sync_manifest():
    """Synchronizes project_manifest.yaml with the public capabilities in the database."""
    asyncio.run(_async_sync_manifest())

--- END OF FILE ./src/cli/logic/sync_manifest.py ---

--- START OF FILE ./src/cli/logic/system.py ---
# src/cli/logic/system.py
from __future__ import annotations

import asyncio
from typing import Optional

import typer
from core.crate_processing_service import process_crates
from features.project_lifecycle.integration_service import integrate_changes
from rich.console import Console
from shared.context import CoreContext

console = Console()

# Global variable to store context, set by the registration layer.
_context: Optional[CoreContext] = None


# ID: 46b79a8e-3360-4fac-af15-9a52cf0d9a7a
def integrate_command(
    commit_message: str = typer.Option(
        ..., "-m", "--message", help="The git commit message for this integration."
    ),
):
    """Orchestrates the full, autonomous integration of staged code changes."""
    if _context is None:
        console.print(
            "[bold red]Error: Context not initialized for integrate[/bold red]"
        )
        raise typer.Exit(code=1)

    # Pass the context to the underlying service
    asyncio.run(integrate_changes(context=_context, commit_message=commit_message))


# ID: 1f2c3d4e-5f6a-7b8c-9d0e-1f2a3b4c5d6e
def process_crates_command():
    """Finds, validates, and applies all pending autonomous change proposals."""
    asyncio.run(process_crates())

--- END OF FILE ./src/cli/logic/system.py ---

--- START OF FILE ./src/cli/logic/tools.py ---
# src/cli/logic/tools.py
"""
Registers a 'tools' command group for powerful, operator-focused maintenance tasks.
This is the new, governed home for logic from standalone scripts.
"""

from __future__ import annotations

import typer
from features.maintenance.maintenance_service import rewire_imports
from rich.console import Console

console = Console()
tools_app = typer.Typer(
    help="Governed, operator-focused maintenance and refactoring tools."
)


@tools_app.command(
    "rewire-imports",
    help="Run after major refactoring to fix all Python import statements across 'src/'.",
)
# ID: 4d6a0245-20c9-425e-a0cd-a390c8dd063c
def rewire_imports_cli(
    write: bool = typer.Option(
        False, "--write", help="Apply the changes to the files."
    ),
):
    """
    CLI wrapper for the import rewiring service.
    """
    dry_run = not write
    console.print("🚀 Starting architectural import re-wiring script...")
    if dry_run:
        console.print("💧 [yellow]DRY RUN MODE[/yellow]: No files will be changed.")
    else:
        console.print("🟢 [bold green]WRITE MODE[/bold green]: Files will be modified.")

    total_changes = rewire_imports(dry_run=dry_run)

    console.print("\n--- Re-wiring Complete ---")
    if dry_run:
        console.print(
            f"💧 DRY RUN: Found {total_changes} potential import changes to make."
        )
        console.print("   Run with '--write' to apply them.")
    else:
        console.print(f"✅ APPLIED: Made {total_changes} import changes.")

    console.print("\n--- NEXT STEPS ---")
    console.print(
        "1.  VERIFY: Run 'make format' and then 'make check' to ensure compliance."
    )


# The obsolete register function has been removed.

--- END OF FILE ./src/cli/logic/tools.py ---

--- START OF FILE ./src/cli/logic/utils_migration.py ---
# src/system/admin/utils_migration.py
"""
Shared utilities for constitutional migration and domain rationalization.
This is the canonical location for logic used by migration-related tools.
"""

from __future__ import annotations

import re
from pathlib import Path
from typing import Dict

from rich.console import Console
from ruamel.yaml import YAML

yaml_handler = YAML()
yaml_handler.preserve_quotes = True
yaml_handler.indent(mapping=2, sequence=4, offset=2)


# ID: 64bb309f-1cf9-4480-afc4-78130e8357e2
def parse_migration_plan(plan_path: Path) -> Dict[str, str]:
    """Parses the markdown migration plan into a mapping dictionary."""
    if not plan_path.exists():
        raise FileNotFoundError(f"Migration plan not found at: {plan_path}")
    content = plan_path.read_text(encoding="utf-8")
    pattern = re.compile(r"\|\s*`([^`]+)`\s*\|\s*`([^`]+)`\s*\|")
    matches = pattern.findall(content)
    if not matches:
        raise ValueError("No valid domain mappings found in the migration plan.")
    return {old.strip(): new.strip() for old, new in matches}


# ID: 80131c72-c024-4823-8226-f63c5d8c4704
def replacer(
    match: re.Match, domain_map: Dict, console: Console, py_file: Path, repo_root: Path
) -> str:
    """Replacement function for re.subn to update capability tags."""
    old_cap = match.group(1)
    for old_domain, new_domain in domain_map.items():
        if old_cap.startswith(old_domain):
            new_cap = old_cap.replace(old_domain, new_domain, 1)
            if old_cap != new_cap:
                console.print(
                    f"   -> In '{py_file.relative_to(repo_root)}': Renaming tag '{old_cap}' -> '[green]{new_cap}[/green]'"
                )
    return match.group(0)

--- END OF FILE ./src/cli/logic/utils_migration.py ---

--- START OF FILE ./src/cli/logic/validate.py ---
"""
Provides CLI commands for validating constitutional and governance integrity.
This module consolidates and houses the logic from the old src/core/cli tools.
"""

from __future__ import annotations

import ast
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import typer
from jsonschema import ValidationError, validate
from shared.config_loader import load_yaml_file
from shared.logger import getLogger

log = getLogger("core_admin.validate")
validate_app = typer.Typer(help="Commands for validating constitutional integrity.")


def _load_json(path: Path) -> dict:
    """Loads and returns a JSON dictionary from the specified file path."""
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def _validate_schema_pair(pair: Tuple[Path, Path]) -> str | None:
    """Validates a YAML file against a JSON Schema, returning an error message or None."""
    yml_path, schema_path = pair
    if not yml_path.exists():
        return f"Missing file: {yml_path}"
    if not schema_path.exists():
        return f"Missing schema: {schema_path}"
    try:
        data = load_yaml_file(yml_path)
        schema = _load_json(schema_path)
        validate(instance=data, schema=schema)
        typer.echo(f"[OK] {yml_path} ✓")
        return None
    except ValidationError as e:
        path = ".".join(map(str, e.path)) or "(root)"
        return f"[FAIL] {yml_path}: {e.message} at {path}"


@validate_app.command("intent-schema")
# ID: 5e99036c-21f6-452b-8698-d22830e205bf
def validate_intent_schema(
    intent_path: Path = typer.Option(
        Path(".intent"), "--intent-path", help="Path to the .intent directory."
    ),
):
    """Validate policy YAMLs under .intent/charter using their corresponding JSON Schemas."""
    log.info("Running intent schema validation via core-admin...")
    base = intent_path / "charter"
    checks: List[Tuple[Path, Path]] = [
        (
            base / "policies" / "agent_policy.yaml",
            base / "schemas" / "agent_policy_schema.json",
        ),
        (
            base / "policies" / "database_policy.yaml",
            base / "schemas" / "database_policy_schema.json",
        ),
        (
            base / "policies" / "canary_policy.yaml",
            base / "schemas" / "canary_policy_schema.json",
        ),
        (
            base / "policies" / "enforcement_model_policy.yaml",
            base / "schemas" / "enforcement_model_schema.json",
        ),
        (
            base / "policies" / "reporting_policy.yaml",
            base / "schemas" / "reporting_policy_schema.json",
        ),
    ]
    errors = list(filter(None, (_validate_schema_pair(p) for p in checks)))
    if errors:
        typer.echo("\n".join(errors), err=True)
        raise typer.Exit(code=1)
    typer.echo("All checked .intent policy files are valid.")


@dataclass
# ID: c9618bb6-3c14-40f6-b32b-16d50eedfb57
class ReviewContext:
    risk_tier: str = "low"
    score: float = 0.0
    touches_critical_paths: bool = False
    checkpoint: bool = False
    canary: bool = False
    approver_quorum: bool = False


_ALLOWED_NODES = {
    ast.Expression,
    ast.BoolOp,
    ast.BinOp,
    ast.UnaryOp,
    ast.Compare,
    ast.Name,
    ast.Load,
    ast.Constant,
    ast.List,
    ast.Tuple,
    ast.And,
    ast.Or,
    ast.Not,
    ast.In,
    ast.Eq,
    ast.NotEq,
}


def _safe_eval(expr: str, ctx: Dict[str, Any]) -> bool:
    """Safely evaluate a boolean expression string against a context dictionary using AST validation."""
    expr = expr.replace(" true", " True").replace(" false", " False")
    tree = ast.parse(expr, mode="eval")
    for node in ast.walk(tree):
        if type(node) not in _ALLOWED_NODES:
            raise ValueError(f"Unsupported expression node: {type(node).__name__}")
        if isinstance(node, ast.Name) and node.id not in ctx:
            raise ValueError(f"Unknown identifier in condition: {node.id}")
    return bool(eval(compile(tree, "<cond>", "eval"), {"__builtins__": {}}, ctx))


def _merge_contexts(a: ReviewContext, b: ReviewContext) -> ReviewContext:
    return ReviewContext(
        risk_tier=b.risk_tier or a.risk_tier,
        score=b.score if b.score != 0.0 else a.score,
        touches_critical_paths=b.touches_critical_paths or a.touches_critical_paths,
        checkpoint=b.checkpoint or a.checkpoint,
        canary=b.canary or a.canary,
        approver_quorum=b.approver_quorum or a.approver_quorum,
    )


@validate_app.command("risk-gates")
# ID: 77a7ca50-e088-4ffc-a168-5fa58f1471d4
def validate_risk_gates(
    mind_path: Path = typer.Option(
        Path(".intent/mind"), "--mind-path", help="Path to the .intent/mind directory."
    ),
    context: Optional[Path] = typer.Option(None, "--context"),
    risk_tier: str = typer.Option("low", "--risk-tier"),
    score: float = typer.Option(0.0, "--score"),
    touches_critical_paths: bool = typer.Option(
        False, "--touches-critical-paths/--no-touches-critical-paths"
    ),
    checkpoint: bool = typer.Option(False, "--checkpoint/--no-checkpoint"),
    canary: bool = typer.Option(False, "--canary/--no-canary"),
    approver_quorum: bool = typer.Option(
        False, "--approver-quorum/--no-approver-quorum"
    ),
):
    """Enforce risk-tier gates from score_policy.yaml."""
    log.info("Running risk gate validation via core-admin...")
    spath = mind_path / "evaluation" / "score_policy.yaml"
    if not spath.exists():
        typer.echo(f"Missing score policy: {spath}", err=True)
        raise typer.Exit(code=2)
    policy = load_yaml_file(spath)
    gates: Dict[str, Any] = policy.get("risk_tier_gates", {})
    conds: Dict[str, str] = policy.get("gate_conditions", {})
    file_ctx = ReviewContext()
    if context and context.exists():
        raw = load_yaml_file(context)
        file_ctx = ReviewContext(**raw)
    cli_ctx = ReviewContext(
        risk_tier, score, touches_critical_paths, checkpoint, canary, approver_quorum
    )
    ctx = _merge_contexts(file_ctx, cli_ctx)
    violations: List[str] = []
    tier = gates.get(ctx.risk_tier, {})
    min_score = float(tier.get("min_score", 0.0))
    required_flags = set(tier.get("require", []))
    if ctx.score < min_score:
        violations.append(
            f"score {ctx.score:.2f} < min_score {min_score:.2f} for tier '{ctx.risk_tier}'"
        )
    cond_env = ctx.__dict__
    for cond_key, flag_name in [
        ("checkpoint_required_when", "checkpoint"),
        ("canary_required_when", "canary"),
        ("approver_quorum_required_when", "approver_quorum"),
    ]:
        expr = conds.get(cond_key)
        if expr and _safe_eval(expr, cond_env):
            required_flags.add(flag_name)
    for flag in sorted(required_flags):
        if not bool(getattr(ctx, flag, False)):
            violations.append(
                f"required '{flag}' is missing/false for tier '{ctx.risk_tier}'"
            )
    if violations:
        typer.echo("Risk gate violations:", err=True)
        for v in violations:
            typer.echo(f" - {v}", err=True)
        raise typer.Exit(code=1)
    typer.echo("Risk gates satisfied ✓")

--- END OF FILE ./src/cli/logic/validate.py ---

--- START OF FILE ./src/cli/logic/vector_drift.py ---
# src/cli/logic/diagnostics/vector_drift.py
from __future__ import annotations

import asyncio
from typing import Set

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from services.clients.qdrant_client import QdrantService
from services.database.session_manager import get_session
from sqlalchemy import text

console = Console()


async def _fetch_postgres_vector_ids() -> Set[str]:
    """
    Authoritative source of vector IDs is the link table:
      core.symbol_vector_links(symbol_id UUID, vector_id TEXT, ...)
    """
    async with get_session() as session:
        rows = await session.execute(
            text("SELECT vector_id::text FROM core.symbol_vector_links")
        )
        return {r[0] for r in rows}


async def _fetch_qdrant_point_ids() -> Set[str]:
    """
    Fetch all point IDs from Qdrant without payloads/vectors.
    """
    service = QdrantService()
    all_ids: Set[str] = set()
    offset = None

    # Scroll through the whole collection to be robust with >10k points
    while True:
        points, offset = await service.client.scroll(
            collection_name=service.collection_name,
            limit=10_000,
            with_payload=False,
            with_vectors=False,
            offset=offset,
        )
        all_ids.update(str(p.id) for p in points)
        if offset is None:
            break

    return all_ids


# ID: 87360a13-844e-4528-a444-5677e7c83841
async def inspect_vector_drift() -> None:
    console.print(
        "[bold cyan]🚀 Verifying synchronization between PostgreSQL and Qdrant...[/bold cyan]"
    )

    try:
        postgres_ids, qdrant_ids = await asyncio.gather(
            _fetch_postgres_vector_ids(), _fetch_qdrant_point_ids()
        )
    except Exception as e:
        console.print(f"[bold red]❌ Error connecting to a database: {e}[/bold red]")
        return

    console.print(f"   -> Found {len(postgres_ids)} linked vector IDs in PostgreSQL.")
    console.print(f"   -> Found {len(qdrant_ids)} point IDs in Qdrant.")

    missing_in_qdrant = sorted(postgres_ids - qdrant_ids)
    orphans_in_qdrant = sorted(qdrant_ids - postgres_ids)

    console.print("\n--- Verification Result ---")
    if not missing_in_qdrant and not orphans_in_qdrant:
        console.print(
            Panel(
                "[bold green]✅ Perfect Synchronization.[/bold green]\nPostgreSQL and Qdrant are perfectly aligned.",
                title="Status",
                border_style="green",
            )
        )
        return

    if missing_in_qdrant:
        table = Table(
            title=f"⚠️ Missing in Qdrant ({len(missing_in_qdrant)})",
            caption="Exists in Postgres link table but missing from Qdrant.",
            header_style="bold yellow",
        )
        table.add_column("Vector ID (expected in Qdrant)")
        for vid in missing_in_qdrant[:200]:
            table.add_row(vid)
        if len(missing_in_qdrant) > 200:
            table.add_row(f"... and {len(missing_in_qdrant) - 200} more")
        console.print(table)
        console.print(
            "\n[bold]Next step:[/bold] Recreate with `poetry run core-admin knowledge vectorize --write`."
        )

    if orphans_in_qdrant:
        table = Table(
            title=f"🧹 Orphaned in Qdrant ({len(orphans_in_qdrant)})",
            caption="Present in Qdrant but no link in Postgres.",
            header_style="bold magenta",
        )
        table.add_column("Orphaned Point ID (Qdrant only)")
        for pid in orphans_in_qdrant[:200]:
            table.add_row(pid)
        if len(orphans_in_qdrant) > 200:
            table.add_row(f"... and {len(orphans_in_qdrant) - 200} more")
        console.print(table)
        console.print(
            "\n[bold]Next step:[/bold] `poetry run core-admin fix orphaned-vectors --dry-run`, then without `--dry-run`."
        )

--- END OF FILE ./src/cli/logic/vector_drift.py ---

--- START OF FILE ./src/core/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/core/__init__.py ---

--- START OF FILE ./src/core/actions/base.py ---
# src/core/actions/base.py
"""
Defines the base interface for all executable actions in the CORE system.
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from core.agents.plan_executor import PlanExecutorContext
    from shared.models import TaskParams


# ID: 1eaf9a8d-7b6c-4f5a-8b3e-9c7d6e5f4a3b
class ActionHandler(ABC):
    """Abstract base class for a specialist that handles a single action."""

    @property
    @abstractmethod
    # ID: 3b8a13fc-9c44-4829-9613-909640d3e733
    def name(self) -> str:
        """The unique name of the action, e.g., 'read_file'."""
        pass

    @abstractmethod
    # ID: 1780136a-31ee-4db1-bbf8-04c0110b4cca
    async def execute(self, params: TaskParams, context: "PlanExecutorContext"):
        """
        Executes the action.

        Args:
            params: The parameters for this specific task.
            context: The shared execution context, allowing access to file content,
                     the file handler, git service, etc.
        """
        pass

--- END OF FILE ./src/core/actions/base.py ---

--- START OF FILE ./src/core/actions/code_actions.py ---
# src/core/actions/code_actions.py
"""
Action handlers for complex code modification and creation.
"""

from __future__ import annotations

import ast
import textwrap
from typing import Optional, Tuple

from core.validation_pipeline import validate_code_async
from shared.logger import getLogger
from shared.models import PlanExecutionError, TaskParams

from .base import ActionHandler
from .context import PlanExecutorContext

log = getLogger("code_actions")


# --- START: Logic moved from the deleted CodeEditor class ---
def _get_symbol_start_end_lines(
    tree: ast.AST, symbol_name: str
) -> Optional[Tuple[int, int]]:
    """Finds the 1-based start and end line numbers of a symbol."""
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            if node.name == symbol_name:
                # ast.get_source_segment is more reliable if end_lineno is available
                if hasattr(node, "end_lineno") and node.end_lineno is not None:
                    return node.lineno, node.end_lineno
    return None


def _replace_symbol_in_code(
    original_code: str, symbol_name: str, new_code_str: str
) -> str:
    """
    Replaces a function/method in code with a new version using AST to find boundaries.
    """
    try:
        original_tree = ast.parse(original_code)
    except SyntaxError as e:
        raise ValueError(f"Could not parse original code due to syntax error: {e}")

    symbol_location = _get_symbol_start_end_lines(original_tree, symbol_name)
    if not symbol_location:
        raise ValueError(f"Symbol '{symbol_name}' not found in the original code.")

    start_line, end_line = symbol_location
    start_index = start_line - 1
    end_index = end_line

    lines = original_code.splitlines()

    # Determine indentation from the first line of the original symbol
    original_symbol_line = lines[start_index]
    indentation = len(original_symbol_line) - len(original_symbol_line.lstrip(" "))

    # Dedent and re-indent the new code to match the original's indentation
    clean_new_code = textwrap.dedent(new_code_str).strip()
    new_code_lines = [
        f"{' ' * indentation}{line}" for line in clean_new_code.splitlines()
    ]

    code_before = lines[:start_index]
    code_after = lines[end_index:]

    final_lines = code_before + new_code_lines + code_after
    return "\n".join(final_lines)


# --- END: Logic moved from the deleted CodeEditor class ---


# ID: 7a8b9c0d-1e2f-3a4b-5c6d-7e8f9c0d
# ID: 2b764ba7-4bf1-4827-9c47-bec092758cb8
class CreateFileHandler(ActionHandler):
    """Handles the 'create_file' action."""

    @property
    # ID: f650040b-ce19-4663-ab49-3943d3dcca20
    def name(self) -> str:
        return "create_file"

    # ID: 9bd3f774-d4f8-4bc0-93f3-c604742dfe0a
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        file_path, code = params.file_path, params.code
        if not all([file_path, code is not None]):
            raise PlanExecutionError(
                "Missing 'file_path' or 'code' for create_file action."
            )

        full_path = context.file_handler.repo_path / file_path
        if full_path.exists():
            raise FileExistsError(f"File '{file_path}' already exists.")

        validation_result = await validate_code_async(
            file_path, code, auditor_context=context.auditor_context
        )
        if validation_result["status"] == "dirty":
            raise PlanExecutionError(
                f"Generated code for '{file_path}' failed validation.",
                violations=validation_result["violations"],
            )

        context.file_handler.confirm_write(
            context.file_handler.add_pending_write(
                prompt=f"Goal: create file {file_path}",
                suggested_path=file_path,
                code=validation_result["code"],
            )
        )
        if context.git_service.is_git_repo():
            context.git_service.add(file_path)
            context.git_service.commit(f"feat: Create new file {file_path}")


# ID: 8b9c0d1e-2f3a-4b5c-6d7e-8f9a0b1c
# ID: a0a1f246-224f-4881-b0f1-2cfda08a40ac
class EditFileHandler(ActionHandler):
    """Handles the 'edit_file' action."""

    @property
    # ID: 3fc89f7f-c949-4f11-a3ff-ac0ac96795a1
    def name(self) -> str:
        return "edit_file"

    # ID: b6502a6e-c6f5-4fc1-9ad4-21c06ba16b3a
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        file_path_str = params.file_path
        new_content = params.code
        if not all([file_path_str, new_content is not None]):
            raise PlanExecutionError(
                "Missing 'file_path' or 'code' for edit_file action."
            )

        full_path = context.file_handler.repo_path / file_path_str
        if not full_path.exists():
            raise PlanExecutionError(
                f"File to be edited does not exist: {file_path_str}"
            )

        validation_result = await validate_code_async(
            file_path_str, new_content, auditor_context=context.auditor_context
        )
        if validation_result["status"] == "dirty":
            raise PlanExecutionError(
                f"Generated code for '{file_path_str}' failed validation.",
                violations=validation_result["violations"],
            )

        context.file_handler.confirm_write(
            context.file_handler.add_pending_write(
                prompt=f"Goal: edit file {file_path_str}",
                suggested_path=file_path_str,
                code=validation_result["code"],
            )
        )
        if context.git_service.is_git_repo():
            context.git_service.add(file_path_str)
            context.git_service.commit(f"feat: Modify file {file_path_str}")


# ID: 0a1b2c3d-4e5f-6a7b-8c9d-0e1f2a3b4c5d
class EditFunctionHandler(ActionHandler):
    """Handles the 'edit_function' action."""

    @property
    # ID: 2b2a9ab6-ebd5-4846-8165-e796c851b6e2
    def name(self) -> str:
        return "edit_function"

    # ID: 4ca616d7-7ffb-4b3c-acbb-fd6a089e612c
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        file_path, symbol_name, new_code = (
            params.file_path,
            params.symbol_name,
            params.code,
        )
        if not all([file_path, symbol_name, new_code is not None]):
            raise PlanExecutionError(
                "Missing required parameters for edit_function action."
            )

        full_path = context.file_handler.repo_path / file_path
        if not full_path.exists():
            raise FileNotFoundError(
                f"Cannot edit function, file not found: '{file_path}'"
            )

        original_code = full_path.read_text("utf-8")

        # First, validate the new code snippet in isolation
        validation_result = await validate_code_async(
            file_path, new_code, auditor_context=context.auditor_context
        )
        if validation_result["status"] == "dirty":
            raise PlanExecutionError(
                f"Generated code for '{symbol_name}' failed validation.",
                violations=validation_result["violations"],
            )

        validated_code_snippet = validation_result["code"]

        try:
            final_code = _replace_symbol_in_code(
                original_code, symbol_name, validated_code_snippet
            )
        except ValueError as e:
            raise PlanExecutionError(f"Failed to edit code in '{file_path}': {e}")

        context.file_handler.confirm_write(
            context.file_handler.add_pending_write(
                prompt=f"Goal: edit function {symbol_name} in {file_path}",
                suggested_path=file_path,
                code=final_code,
            )
        )
        if context.git_service.is_git_repo():
            context.git_service.add(file_path)
            context.git_service.commit(
                f"feat: Modify function {symbol_name} in {file_path}"
            )

--- END OF FILE ./src/core/actions/code_actions.py ---

--- START OF FILE ./src/core/actions/context.py ---
# src/core/actions/context.py
"""
Defines the execution context for the PlanExecutor.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Dict

if TYPE_CHECKING:
    from core.file_handler import FileHandler
    from core.git_service import GitService
    from features.governance.audit_context import AuditorContext


# ID: 2b3c4d5e-6f7a-8b9c-0d1e2f3a4b5c
@dataclass
# ID: 11693175-bbaf-4a96-b97e-d3c53a6bc1f9
class PlanExecutorContext:
    """A container for services and state shared across all action handlers."""

    file_handler: "FileHandler"
    git_service: "GitService"
    auditor_context: "AuditorContext"
    file_content_cache: Dict[str, str] = field(default_factory=dict)

--- END OF FILE ./src/core/actions/context.py ---

--- START OF FILE ./src/core/actions/file_actions.py ---
# src/core/actions/file_actions.py
"""
Action handlers for basic file system operations like read, list, and delete.
"""

from __future__ import annotations

from shared.logger import getLogger
from shared.models import PlanExecutionError, TaskParams

from .base import ActionHandler
from .context import PlanExecutorContext

log = getLogger("file_actions")


# ID: 3c4d5e6f-7a8b-9c0d-1e2f3a4b5c6d
# ID: c058bf71-924a-497a-8847-449129f96068
class ReadFileHandler(ActionHandler):
    """Handles the 'read_file' action."""

    @property
    # ID: bf0bc446-13a0-4e3e-bbd2-cb6eecc43cab
    def name(self) -> str:
        return "read_file"

    # ID: dbb82014-953a-4832-b0ee-97bd19f348a9
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        file_path_str = params.file_path
        if not file_path_str:
            raise PlanExecutionError("Missing 'file_path' for read_file action.")

        full_path = context.file_handler.repo_path / file_path_str
        if not full_path.exists():
            raise PlanExecutionError(f"File to be read does not exist: {file_path_str}")

        if full_path.is_dir():
            raise PlanExecutionError(
                f"Cannot read '{file_path_str}' because it is a directory."
            )

        content = full_path.read_text(encoding="utf-8")
        context.file_content_cache[file_path_str] = content
        log.info(f"📖 Read file '{file_path_str}' into context.")


# ID: 5e6f7a8b-9c0d-1e2f-3a4b5c6d7f8a
# ID: 90e3703f-1c72-402e-b904-96f0e6341059
class ListFilesHandler(ActionHandler):
    """Handles the 'list_files' action."""

    @property
    # ID: 9d607637-7e59-4c84-9b1b-e7f9097fd066
    def name(self) -> str:
        return "list_files"

    # ID: 497141ed-5562-4b14-a9b9-94cfb09b16e9
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        dir_path_str = params.file_path
        if not dir_path_str:
            raise PlanExecutionError("Missing 'file_path' for list_files action.")

        full_path = context.file_handler.repo_path / dir_path_str
        if not full_path.is_dir():
            raise PlanExecutionError(
                f"Directory to be listed does not exist or is not a directory: {dir_path_str}"
            )

        contents = [item.name for item in full_path.iterdir()]
        context.file_content_cache[dir_path_str] = "\n".join(sorted(contents))
        log.info(f"📁 Listed contents of '{dir_path_str}' into context.")


# ID: 6f7a8b9c-0d1e-2f3a-4b5c6d7e8f9b
# ID: 7dbbbc26-0dd4-4c3e-8938-dd6ba3c715b0
class DeleteFileHandler(ActionHandler):
    """Handles the 'delete_file' action."""

    @property
    # ID: dc164637-5cf6-4999-880b-076db48e7b29
    def name(self) -> str:
        return "delete_file"

    # ID: e7de77a8-59e5-4b53-85ef-1c8a9c893202
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        file_path_str = params.file_path
        if not file_path_str:
            raise PlanExecutionError("Missing 'file_path' for delete_file action.")

        full_path = context.file_handler.repo_path / file_path_str
        if not full_path.exists():
            log.warning(
                f"File '{file_path_str}' to be deleted does not exist. Skipping."
            )
            return

        full_path.unlink()
        log.info(f"🗑️  Deleted file: {file_path_str}")

        if context.git_service.is_git_repo():
            context.git_service.add(file_path_str)  # Stage the deletion
            context.git_service.commit(
                f"refactor(cleanup): Remove obsolete file {file_path_str}"
            )

--- END OF FILE ./src/core/actions/file_actions.py ---

--- START OF FILE ./src/core/actions/governance_actions.py ---
# src/core/actions/governance_actions.py
"""
Action handlers for governance-related operations.
"""

from __future__ import annotations

import uuid

import yaml
from shared.logger import getLogger
from shared.models import PlanExecutionError, TaskParams

from .base import ActionHandler
from .context import PlanExecutorContext

log = getLogger("governance_actions")


# ID: 9c0d1e2f-3a4b-5c6d-7e8f-9a0b1c2d
# ID: 81d33ff2-37a3-4686-a4a7-32c899d20705
class CreateProposalHandler(ActionHandler):
    """Handles the 'create_proposal' action."""

    @property
    # ID: e5562cbd-b81b-49f4-ae61-7ce318aec6fa
    def name(self) -> str:
        return "create_proposal"

    # ID: 426dd8a7-e224-4372-b6c4-40bc735d6c62
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        target_path = params.file_path
        content = params.code
        justification = params.justification

        if not all([target_path, content, justification]):
            raise PlanExecutionError("Missing required parameters for create_proposal.")

        proposal_id = str(uuid.uuid4())[:8]
        proposal_filename = (
            f"cr-{proposal_id}-{target_path.split('/')[-1].replace('.py','')}.yaml"
        )
        proposal_path = (
            context.file_handler.repo_path / ".intent/proposals" / proposal_filename
        )

        proposal_content = {
            "target_path": target_path,
            "action": "replace_file",
            "justification": justification,
            "content": content,
        }

        yaml_content = yaml.dump(
            proposal_content, indent=2, default_flow_style=False, sort_keys=True
        )
        proposal_path.parent.mkdir(parents=True, exist_ok=True)
        proposal_path.write_text(yaml_content, encoding="utf-8")
        log.info(f"🏛️  Created constitutional proposal: {proposal_filename}")

        if context.git_service.is_git_repo():
            context.git_service.add(str(proposal_path))
            context.git_service.commit(
                f"feat(proposal): Create proposal for {target_path}"
            )

--- END OF FILE ./src/core/actions/governance_actions.py ---

--- START OF FILE ./src/core/actions/healing_actions.py ---
# src/core/actions/healing_actions.py
"""
Action handlers for autonomous self-healing capabilities.
"""

from __future__ import annotations

from core.actions.base import ActionHandler
from core.actions.context import PlanExecutorContext
from features.self_healing.code_style_service import format_code
from features.self_healing.docstring_service import _async_fix_docstrings
from features.self_healing.header_service import _run_header_fix_cycle
from shared.config import settings
from shared.models import TaskParams


# ID: 79845741-cb28-483e-a017-1f962570f1fa
class FixDocstringsHandler(ActionHandler):
    """Handles the 'autonomy.self_healing.fix_docstrings' action."""

    @property
    # ID: 36fd4346-26f4-4fce-97dd-8ff24ceb4bc3
    def name(self) -> str:
        """Return the unique identifier for this self-healing module."""
        return "autonomy.self_healing.fix_docstrings"

    # ID: 098a9dcb-dab9-40df-9ef7-150fd21c5770
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        """
        Executes the docstring fixing logic by calling the dedicated service.
        This action does not run in dry-run mode; it always applies changes.
        """
        await _async_fix_docstrings(dry_run=False)


# ID: 229e24e4-67d0-4e63-a610-42858e150ac3
class FixHeadersHandler(ActionHandler):
    """Handles the 'autonomy.self_healing.fix_headers' action."""

    @property
    # ID: 4512e458-3548-4932-982c-71793d166c00
    def name(self) -> str:
        return "autonomy.self_healing.fix_headers"

    # ID: 4828affd-f7da-4995-9493-70372f11a144
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        """Executes the header fixing logic for all Python files."""
        src_dir = settings.REPO_PATH / "src"
        all_py_files = [
            str(p.relative_to(settings.REPO_PATH)) for p in src_dir.rglob("*.py")
        ]
        _run_header_fix_cycle(dry_run=False, all_py_files=all_py_files)


# ID: 363fd253-58df-4603-877c-03cffdc626b1
class FormatCodeHandler(ActionHandler):
    """Handles the 'autonomy.self_healing.format_code' action."""

    @property
    # ID: b90e14b5-7741-4e86-8451-2682c10718f0
    def name(self) -> str:
        return "autonomy.self_healing.format_code"

    # ID: 4f311df7-b97a-4a9c-ab54-1369ec41988e
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        """Executes the code formatting logic by calling the dedicated service."""
        # --- START MODIFICATION ---
        # The handler now passes the file_path from the plan to the service.
        # If no file_path is provided, it defaults to the old behavior.
        format_code(path=params.file_path)
        # --- END MODIFICATION ---

--- END OF FILE ./src/core/actions/healing_actions.py ---

--- START OF FILE ./src/core/actions/registry.py ---
# src/core/actions/registry.py
"""
A registry for discovering and accessing all available ActionHandlers.
"""

from __future__ import annotations

from typing import Dict, List, Optional, Type

from shared.logger import getLogger

from .base import ActionHandler
from .code_actions import CreateFileHandler, EditFileHandler, EditFunctionHandler
from .file_actions import DeleteFileHandler, ListFilesHandler, ReadFileHandler
from .governance_actions import CreateProposalHandler
from .healing_actions import (
    FixDocstringsHandler,
    FixHeadersHandler,
    FormatCodeHandler,
)
from .validation_actions import ValidateCodeHandler

log = getLogger("action_registry")


# ID: b351ac04-4574-409e-a4ad-90a1e8225947
class ActionRegistry:
    """A central registry for all action handlers."""

    def __init__(self):
        self._handlers: Dict[str, ActionHandler] = {}
        self._register_handlers()

    def _register_handlers(self):
        """Discovers and registers all concrete ActionHandler classes."""
        handlers_to_register: List[Type[ActionHandler]] = [
            ReadFileHandler,
            ListFilesHandler,
            DeleteFileHandler,
            CreateFileHandler,
            EditFileHandler,
            CreateProposalHandler,
            EditFunctionHandler,
            FixHeadersHandler,
            FixDocstringsHandler,
            FormatCodeHandler,
            ValidateCodeHandler,
        ]

        for handler_class in handlers_to_register:
            instance = handler_class()
            if instance.name in self._handlers:
                log.warning(
                    f"Duplicate action name '{instance.name}' found. Overwriting."
                )
            self._handlers[instance.name] = instance
        log.info(f"ActionRegistry initialized with {len(self._handlers)} handlers.")

    # ID: c1cf8df7-795d-44a0-92f3-2e7f8b99455d
    def get_handler(self, action_name: str) -> Optional[ActionHandler]:
        """Retrieves a handler instance by its action name."""
        return self._handlers.get(action_name)

--- END OF FILE ./src/core/actions/registry.py ---

--- START OF FILE ./src/core/actions/validation_actions.py ---
# src/core/actions/validation_actions.py
"""
Action handlers for validation and verification tasks.
"""

from __future__ import annotations

from core.actions.base import ActionHandler
from core.actions.context import PlanExecutorContext
from shared.logger import getLogger
from shared.models import TaskParams

log = getLogger("validation_actions")


# ID: a4e534f9-b6a0-4151-a3ee-9c2fcc0ec87f
class ValidateCodeHandler(ActionHandler):
    """A handler for the 'core.validation.validate_code' action."""

    @property
    # ID: 29a5f80d-3a24-4d06-a1ad-0403c376319b
    def name(self) -> str:
        return "core.validation.validate_code"

    # ID: 38c34fa0-6443-46ee-a636-e18a28fb0a81
    async def execute(self, params: TaskParams, context: PlanExecutorContext):
        """This is a no-op as validation is performed before execution."""
        log.info(
            "Step 'core.validation.validate_code' acknowledged. Pre-flight validation already completed."
        )
        # This action does nothing because the real validation happens
        # in the `micro_apply` command's pre-flight check.
        pass

--- END OF FILE ./src/core/actions/validation_actions.py ---

--- START OF FILE ./src/core/agents/__init__.py ---
# src/agents/__init__.py
# Package marker for src/agents — contains CORE's agent implementations.

--- END OF FILE ./src/core/agents/__init__.py ---

--- START OF FILE ./src/core/agents/base_planner.py ---
# src/core/agents/base_planner.py
"""
Provides shared, stateless utility functions for planner agents to reduce code duplication.
This serves the 'dry_by_design' constitutional principle.
"""

from __future__ import annotations

import json
from typing import List

from pydantic import ValidationError
from rich.console import Console
from rich.syntax import Syntax
from shared.config import settings
from shared.logger import getLogger
from shared.models import ExecutionTask, PlanExecutionError
from shared.utils.parsing import extract_json_from_response

from core.prompt_pipeline import PromptPipeline

log = getLogger(__name__)


# ID: a1b2c3d4-e5f6-7a8b-9c0d-1f2a3b4c5d6e
def build_planning_prompt(
    goal: str, prompt_template: str, reconnaissance_report: str
) -> str:
    """Builds the detailed prompt for a planning LLM, including available actions."""
    actions_policy = settings.load(
        "charter.policies.governance.available_actions_policy"
    )
    available_actions = actions_policy.get("actions", [])
    prompt_pipeline = PromptPipeline(settings.REPO_PATH)

    action_descriptions = []
    for action in available_actions:
        desc = f"### Action: `{action['name']}`\n"
        desc += f"**Description:** {action['description']}\n"
        params = action.get("parameters", [])
        if params:
            desc += "**Parameters:**\n"
            for param in params:
                req_str = "(required)" if param.get("required", False) else "(optional)"
                desc += f"- `{param['name']}` ({param.get('type', 'any')} {req_str}): {param.get('description', '')}\n"
        action_descriptions.append(desc)
    action_descriptions_str = "\n".join(action_descriptions)

    base_prompt = prompt_template.format(
        goal=goal,
        action_descriptions=action_descriptions_str,
        reconnaissance_report=reconnaissance_report,
    )
    return prompt_pipeline.process(base_prompt)


# ID: b2c3d4e5-f6a7-b8c9-d0e1-f2a3b4c5d6e7
def parse_and_validate_plan(response_text: str) -> List[ExecutionTask]:
    """Parses the LLM's JSON response and validates it into a list of ExecutionTask objects."""
    console = Console()
    try:
        parsed_json = extract_json_from_response(response_text)
        if not isinstance(parsed_json, list):
            raise ValueError("LLM did not return a valid JSON list for the plan.")

        validated_plan = [ExecutionTask(**task) for task in parsed_json]

        log.info("🧠 The PlannerAgent has created the following execution plan:")
        for i, task in enumerate(validated_plan, 1):
            log.info(f"  {i}. {task.step} (Action: {task.action})")
        log.info("🕵️ The ExecutionAgent will now carry out this plan.")
        try:
            plan_json_str = json.dumps(
                [t.model_dump() for t in validated_plan], indent=2
            )
            console.print(Syntax(plan_json_str, "json", theme="solarized-dark"))
        except Exception:
            log.warning("Could not serialize plan to JSON for logging.")

        return validated_plan
    except (ValueError, ValidationError, json.JSONDecodeError) as e:
        log.warning(f"Plan creation failed validation: {e}")
        raise PlanExecutionError("Failed to create a valid plan.") from e

--- END OF FILE ./src/core/agents/base_planner.py ---

--- START OF FILE ./src/core/agents/coder_agent.py ---
# src/core/agents/coder_agent.py
"""
Provides the CoderAgent, a specialist AI agent responsible for all code
generation, validation, and self-correction tasks within the CORE system.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Dict

from shared.config import get_path_or_none, settings
from shared.logger import getLogger
from shared.models import ExecutionTask

from core.cognitive_service import CognitiveService
from core.prompt_pipeline import PromptPipeline
from core.self_correction_engine import attempt_correction
from core.validation_pipeline import validate_code_async

if TYPE_CHECKING:
    from features.governance.audit_context import AuditorContext

log = getLogger("coder_agent")


# ID: a1b2c3d4-e5f6-7a8b-9c0d-e6f7a8b9c0d1
class CoderAgent:
    """A specialist agent for writing, validating, and fixing code."""

    def __init__(
        self,
        cognitive_service: CognitiveService,
        prompt_pipeline: PromptPipeline,
        auditor_context: "AuditorContext",
    ):
        self.cognitive_service = cognitive_service
        self.prompt_pipeline = prompt_pipeline
        self.auditor_context = auditor_context
        agent_policy = settings.load("charter.policies.agent.agent_policy")
        agent_behavior = agent_policy.get("execution_agent", {})
        self.max_correction_attempts = agent_behavior.get("max_correction_attempts", 2)

    # ID: aaa4928d-501e-4502-9d18-e8e0fd02232a
    async def generate_and_validate_code_for_task(
        self, task: ExecutionTask, high_level_goal: str, context_str: str
    ) -> str:
        """
        The main entry point for the CoderAgent. It orchestrates the
        generate-validate-correct loop and returns clean, validated code.

        Raises:
            Exception: If valid code cannot be produced after all attempts.
        """
        current_code = await self._generate_code_for_task(
            task, high_level_goal, context_str
        )

        for attempt in range(self.max_correction_attempts + 1):
            log.info(f"  -> Validation attempt {attempt + 1}...")
            validation_result = await validate_code_async(
                task.params.file_path,
                current_code,
                auditor_context=self.auditor_context,
            )

            if validation_result["status"] == "clean":
                log.info("  -> ✅ Code is constitutionally valid.")
                return validation_result["code"]

            if attempt >= self.max_correction_attempts:
                raise Exception(
                    f"Self-correction failed after {self.max_correction_attempts + 1} attempts."
                )

            log.warning("  -> ⚠️ Code failed validation. Attempting self-correction.")
            correction_result = await self._attempt_code_correction(
                task, current_code, validation_result, high_level_goal
            )

            if correction_result.get("status") == "success":
                log.info("  -> ✅ Self-correction generated a potential fix.")
                current_code = correction_result["code"]
            else:
                raise Exception("Self-correction failed to produce a valid fix.")

        raise Exception("Could not produce valid code after all attempts.")

    async def _generate_code_for_task(
        self, task: ExecutionTask, goal: str, context_str: str
    ) -> str:
        """Builds the prompt and calls the LLM to generate the initial code."""
        log.info(f"✍️  Generating code for task: '{task.step}'...")
        template_path = get_path_or_none("mind.prompts.standard_task_generator")
        prompt_template = (
            template_path.read_text(encoding="utf-8")
            if template_path and template_path.exists()
            else "Implement step '{step}' for goal '{goal}' targeting {file_path}."
        )

        final_prompt = prompt_template.format(
            goal=goal,
            step=task.step,
            file_path=task.params.file_path,
            symbol_name=task.params.symbol_name or "",
        )
        enriched_prompt = self.prompt_pipeline.process(final_prompt + context_str)
        generator = await self.cognitive_service.aget_client_for_role("Coder")
        return await generator.make_request_async(
            enriched_prompt, user_id="coder_agent"
        )

    async def _attempt_code_correction(
        self, task: ExecutionTask, current_code: str, validation_result: Dict, goal: str
    ) -> Dict:
        """Invokes the self-correction engine for a piece of failed code."""
        correction_context = {
            "file_path": task.params.file_path,
            "code": current_code,
            "violations": validation_result["violations"],
            "original_prompt": goal,
        }
        log.info("  -> 🧬 Invoking self-correction engine...")
        return await attempt_correction(
            correction_context, self.cognitive_service, self.auditor_context
        )

--- END OF FILE ./src/core/agents/coder_agent.py ---

--- START OF FILE ./src/core/agents/deduction_agent.py ---
# src/core/agents/deduction_agent.py
from __future__ import annotations

from pathlib import Path
from typing import Iterable, Optional

import yaml

from services.database.models import CognitiveRole, LlmResource
from shared.config import settings
from shared.logger import getLogger

log = getLogger(__name__)


# ID: 6cdae3a9-a62c-4558-aff2-b51d953dfde8
class DeductionAgent:
    """
    Advises on LLM resource selection for a given role.
    In production it reads policy files; in tests/sandboxes it must be tolerant
    when those files aren’t present.
    """

    def __init__(self, repo_path: Path | str):
        self.repo_path = Path(repo_path)
        self._policy: dict | None = None
        self._load_policies()

    def _load_policies(self) -> None:
        """
        Load selection policy from the Charter if present.
        If not present (common in isolated test sandboxes), degrade gracefully.
        """
        # Preferred constitutional location
        policy_path = (
            settings.MIND.parent / "charter" / "policies" / "agent_policy.yaml"
        )
        if policy_path.exists():
            try:
                self._policy = (
                    yaml.safe_load(policy_path.read_text(encoding="utf-8")) or {}
                )
                if not isinstance(self._policy, dict):
                    log.warning(
                        "Agent policy is not a mapping; ignoring: %s", policy_path
                    )
                    self._policy = {}
                return
            except Exception as e:  # noqa: BLE001
                log.warning(
                    "Failed to load agent policy (%s). Proceeding without it.", e
                )
                self._policy = {}
                return

        # Fallback: don’t crash in tests; proceed without policy
        log.warning(
            "Agent policy not found at %s — proceeding without it.", policy_path
        )
        self._policy = {}

    # ID: d25e3279-3af2-4ded-ae84-787683807c23
    def select_resource(
        self,
        role: CognitiveRole,
        candidates: Iterable[LlmResource],
        task_context: str | None = None,
    ) -> Optional[str]:
        """
        Return a preferred resource name if policy can pick one, else None.
        Policy-light heuristic:
          - Prefer lower performance_metadata.cost_rating if present.
          - Otherwise return None and let the caller decide (e.g., cheapest).
        """
        candidates = list(candidates)
        if not candidates:
            return None

        # If policy defines something more advanced, you can add it here later.

        # Heuristic: prefer lowest cost_rating if available
        best = None
        best_rating = None
        for r in candidates:
            md = getattr(r, "performance_metadata", None) or {}
            rating = md.get("cost_rating")
            if rating is None:
                continue
            try:
                rating = float(rating)
            except Exception:
                continue
            if best_rating is None or rating < best_rating:
                best_rating = rating
                best = r

        return best.name if best is not None else None

--- END OF FILE ./src/core/agents/deduction_agent.py ---

--- START OF FILE ./src/core/agents/execution_agent.py ---
# src/core/agents/execution_agent.py
"""
Provides functionality for the execution_agent module.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, List

from shared.logger import getLogger
from shared.models import ExecutionTask, PlanExecutionError

from core.agents.coder_agent import CoderAgent
from core.agents.plan_executor import PlanExecutor

if TYPE_CHECKING:
    from features.governance.audit_context import AuditorContext

log = getLogger("execution_agent")


# ID: 1fedacd4-8227-4216-b07a-c807bd450550
class ExecutionAgent:
    """Orchestrates the execution of a plan, delegating code generation to the CoderAgent."""

    def __init__(
        self,
        coder_agent: "CoderAgent",
        plan_executor: PlanExecutor,
        auditor_context: "AuditorContext",
    ):
        """Initializes the ExecutionAgent as a pure orchestrator."""
        self.coder_agent = coder_agent
        self.executor = plan_executor
        self.auditor_context = auditor_context

    # ID: 6557eefd-2f5e-4904-998a-e7ad2d8d070f
    async def execute_plan(
        self, high_level_goal: str, plan: List[ExecutionTask]
    ) -> tuple[bool, str]:
        """
        Orchestrates the execution of a plan, delegating code generation to the CoderAgent.
        """
        if not plan:
            return False, "Plan is empty or invalid."

        try:
            log.info("--- Starting Governed Code Generation Phase (Orchestration) ---")

            # Prepare context from previously executed steps (e.g., read_file)
            context_str = ""
            if self.executor.context.file_content_cache:
                context_str += "\n\n--- CONTEXT FROM PREVIOUS STEPS ---\n"
                for path, content in self.executor.context.file_content_cache.items():
                    context_str += f"\n--- Contents of {path} ---\n{content}\n"
                context_str += "--- END CONTEXT ---\n"

            for task in plan:
                if (
                    task.action
                    in ["create_file", "edit_file", "edit_function", "create_proposal"]
                    and task.params.code is None
                ):
                    log.info(
                        f"  -> Delegating code generation for step: '{task.step}' to CoderAgent..."
                    )
                    validated_code = (
                        await self.coder_agent.generate_and_validate_code_for_task(
                            task, high_level_goal, context_str
                        )
                    )
                    task.params.code = validated_code
                    log.info(
                        f"  -> ✅ CoderAgent returned validated code for '{task.step}'."
                    )

            log.info("--- Handing off fully prepared plan to Executor ---")
            await self.executor.execute_plan(plan)
            return True, "✅ Plan executed successfully."

        except PlanExecutionError as e:
            return False, f"Plan execution failed during orchestration: {str(e)}"
        except Exception as e:
            log.error(
                f"An unexpected error occurred during execution: {e}", exc_info=True
            )
            return (
                False,
                f"An unexpected error occurred during plan orchestration: {str(e)}",
            )

--- END OF FILE ./src/core/agents/execution_agent.py ---

--- START OF FILE ./src/core/agents/intent_translator.py ---
# src/agents/intent_translator.py
"""
Implements the IntentTranslator agent,
responsible for converting natural language user requests into structured,
executable goals for the CORE system.
"""

from __future__ import annotations

from core.cognitive_service import CognitiveService
from core.prompt_pipeline import PromptPipeline  # <-- ADD THIS IMPORT
from shared.config import settings
from shared.logger import getLogger

log = getLogger("intent_translator")


# ID: 419c46d8-1368-4447-a480-e040954870e5
class IntentTranslator:
    """An agent that translates natural language into structured goals."""

    def __init__(self, cognitive_service: CognitiveService):
        """Initializes the translator with the CognitiveService."""
        self.cognitive_service = cognitive_service
        self.prompt_pipeline = PromptPipeline(settings.REPO_PATH)  # <-- ADD THIS LINE
        self.prompt_path = settings.MIND / "prompts" / "intent_translator.prompt"
        if not self.prompt_path.exists():
            raise FileNotFoundError(
                "Constitutional prompt for IntentTranslator not found."
            )
        self.prompt_template = self.prompt_path.read_text(encoding="utf-8")

    # ID: 50ca512c-e6d4-475d-942e-4b8af8a4dc3c
    def translate(self, user_input: str) -> str:
        """
        Takes a user's natural language input and translates it into a
        structured goal for the PlannerAgent.
        """
        log.info(f"Translating user intent: '{user_input}'")
        client = self.cognitive_service.get_client_for_role("IntentTranslator")

        # Use the pipeline to inject context into the prompt
        final_prompt = self.prompt_pipeline.process(
            self.prompt_template.format(user_input=user_input)
        )

        structured_goal = client.make_request(final_prompt, user_id="intent_translator")
        log.info(f"Translated goal: '{structured_goal}'")
        return structured_goal

--- END OF FILE ./src/core/agents/intent_translator.py ---

--- START OF FILE ./src/core/agents/micro_planner.py ---
# src/core/agents/micro_planner.py
"""
Implements the MicroPlannerAgent, a specialized agent for generating safe,
low-risk plans that can be auto-approved under the micro_proposal_policy.
"""

from __future__ import annotations

import json
from typing import Any, Dict, List

from shared.config import settings
from shared.logger import getLogger
from shared.models import PlanExecutionError

from core.agents.base_planner import parse_and_validate_plan
from core.cognitive_service import CognitiveService

log = getLogger("micro_planner_agent")


# ID: cc3308b8-f2b2-43ab-b412-0f5067a031a1
class MicroPlannerAgent:
    """Decomposes goals into safe, auto-approvable plans."""

    def __init__(self, cognitive_service: CognitiveService):
        """Initializes the MicroPlannerAgent."""
        self.cognitive_service = cognitive_service
        self.policy = settings.load("charter.policies.agent.micro_proposal_policy")
        self.prompt_template = settings.get_path(
            "mind.prompts.micro_planner"
        ).read_text(encoding="utf-8")

    # ID: f9c908ca-b681-4f2d-9009-ba1ad3c936b3
    async def create_micro_plan(self, goal: str) -> List[Dict[str, Any]]:
        """Creates a safe execution plan from a user goal."""
        policy_content = json.dumps(self.policy, indent=2)

        # The prompt should only require these two variables.
        final_prompt = self.prompt_template.format(
            policy_content=policy_content, user_goal=goal
        )

        planner_client = await self.cognitive_service.aget_client_for_role("Planner")
        response_text = await planner_client.make_request_async(
            final_prompt, user_id="micro_planner_agent"
        )

        try:
            plan = parse_and_validate_plan(response_text)
            return [task.model_dump() for task in plan]
        except PlanExecutionError:
            log.warning(
                "Micro-planner did not return a valid plan. Returning empty plan."
            )
            return []

--- END OF FILE ./src/core/agents/micro_planner.py ---

--- START OF FILE ./src/core/agents/plan_executor.py ---
# src/core/agents/plan_executor.py
"""
Provides a clean, refactored PlanExecutor that acts as a pure orchestrator,
delegating all action-specific logic to dedicated, registered handlers.
"""

from __future__ import annotations

import asyncio
from typing import List

from core.actions.context import PlanExecutorContext
from core.actions.registry import ActionRegistry
from core.file_handler import FileHandler
from core.git_service import GitService
from features.governance.audit_context import AuditorContext
from shared.logger import getLogger
from shared.models import ExecutionTask, PlanExecutionError, PlannerConfig

log = getLogger("plan_executor")


# ID: a2b23de4-07fa-4a66-8f29-783934079956
class PlanExecutor:
    """
    A service that takes a list of ExecutionTasks and orchestrates their
    execution by dispatching them to registered ActionHandlers.
    """

    def __init__(
        self, file_handler: FileHandler, git_service: GitService, config: PlannerConfig
    ):
        """Initializes the executor with necessary dependencies."""
        self.config = config
        self.action_registry = ActionRegistry()

        # Create the shared context that all handlers will use
        self.context = PlanExecutorContext(
            file_handler=file_handler,
            git_service=git_service,
            auditor_context=AuditorContext(file_handler.repo_path),
        )

        # Pre-load the auditor's knowledge graph for performance
        asyncio.create_task(self.context.auditor_context.load_knowledge_graph())

    # ID: 65f105d2-27e4-4fca-8f96-27decc90bca5
    async def execute_plan(self, plan: List[ExecutionTask]):
        """Executes the entire plan by dispatching each task to its handler."""
        for i, task in enumerate(plan, 1):
            log.info(f"--- Executing Step {i}/{len(plan)}: {task.step} ---")

            handler = self.action_registry.get_handler(task.action)
            if not handler:
                log.warning(
                    f"Skipping task: No handler found for action '{task.action}'."
                )
                continue

            await self._execute_task_with_timeout(task, handler)

    async def _execute_task_with_timeout(self, task: ExecutionTask, handler):
        """Executes a single task with timeout protection."""
        timeout = self.config.task_timeout
        try:
            await asyncio.wait_for(
                handler.execute(task.params, self.context), timeout=timeout
            )
        except asyncio.TimeoutError:
            raise PlanExecutionError(f"Task '{task.step}' timed out after {timeout}s")
        except Exception as e:
            log.error(
                f"Error executing action '{task.action}' for step '{task.step}': {e}",
                exc_info=True,
            )
            # Re-raise as a PlanExecutionError to be caught by the execution agent
            raise PlanExecutionError(f"Step '{task.step}' failed: {e}") from e

--- END OF FILE ./src/core/agents/plan_executor.py ---

--- START OF FILE ./src/core/agents/planner_agent.py ---
# src/core/agents/planner_agent.py
"""
The PlannerAgent is responsible for decomposing a high-level user goal
into a concrete, step-by-step execution plan that can be carried out
by the ExecutionAgent.
"""

from __future__ import annotations

from typing import List

from core.agents.base_planner import build_planning_prompt, parse_and_validate_plan
from core.cognitive_service import CognitiveService
from shared.config import settings
from shared.logger import getLogger
from shared.models import ExecutionTask, PlanExecutionError

log = getLogger(__name__)


# ID: 8a33ab90-80db-4455-b1b8-636405897ced
class PlannerAgent:
    """Decomposes goals into executable plans."""

    def __init__(self, cognitive_service: CognitiveService):
        """Initializes the PlannerAgent."""
        self.cognitive_service = cognitive_service
        self.prompt_template = settings.get_path(
            "mind.prompts.planner_agent"
        ).read_text(encoding="utf-8")

    # ID: b918335b-60af-4132-a944-88628a3caa66
    async def create_execution_plan(
        self, goal: str, reconnaissance_report: str = ""
    ) -> List[ExecutionTask]:
        """
        Creates an execution plan from a user goal and a reconnaissance report.
        """
        max_retries = settings.model_extra.get("CORE_MAX_RETRIES", 3)

        prompt = build_planning_prompt(
            goal, self.prompt_template, reconnaissance_report
        )
        client = await self.cognitive_service.aget_client_for_role("Planner")

        for attempt in range(max_retries):
            log.info("🧠 Generating step-by-step plan from reconnaissance context...")
            response_text = await client.make_request_async(prompt)

            if response_text:
                try:
                    return parse_and_validate_plan(response_text)
                except PlanExecutionError as e:
                    log.warning(f"Plan creation attempt {attempt + 1} failed: {e}")
                    if attempt == max_retries - 1:
                        raise PlanExecutionError(
                            "Failed to create a valid plan after max retries."
                        ) from e
        return []

--- END OF FILE ./src/core/agents/planner_agent.py ---

--- START OF FILE ./src/core/agents/reconnaissance_agent.py ---
# src/core/agents/reconnaissance_agent.py
"""
Implements the ReconnaissanceAgent, which performs targeted queries and semantic
search against the knowledge graph to build a minimal, surgical context for the Planner.
"""

from __future__ import annotations

from typing import Any, Dict, List

from core.cognitive_service import CognitiveService
from shared.logger import getLogger

log = getLogger("recon_agent")


# ID: f2d9b442-6f3f-4a62-978c-6d5fb9c20b1d
class ReconnaissanceAgent:
    """Queries the knowledge graph to build a focused context for a task."""

    def __init__(
        self, knowledge_graph: Dict[str, Any], cognitive_service: CognitiveService
    ):
        """Initializes with the knowledge graph and cognitive service for search."""
        self.graph = knowledge_graph
        self.symbols = knowledge_graph.get("symbols", {})
        self.cognitive_service = cognitive_service

    async def _find_relevant_symbols_and_files(
        self, goal: str
    ) -> tuple[List[Dict[str, Any]], List[str]]:
        """Performs a semantic search to find symbols and files relevant to the goal."""
        log.info("   -> Performing semantic search for relevant context...")
        try:
            search_results = await self.cognitive_service.search_capabilities(
                goal, limit=5
            )
            if not search_results:
                return [], []

            relevant_symbols = []
            relevant_files = set()
            for hit in search_results:
                if (payload := hit.get("payload")) and (
                    symbol_key := payload.get("symbol")
                ):
                    if symbol_data := self.symbols.get(symbol_key):
                        relevant_symbols.append(symbol_data)
                        relevant_files.add(symbol_data.get("file"))

            log.info(f"   -> Found relevant files: {list(relevant_files)}")
            log.info(
                f"   -> Found relevant symbols: {[s.get('key') for s in relevant_symbols]}"
            )
            return relevant_symbols, sorted(list(relevant_files))
        except Exception as e:
            log.warning(f"Semantic search for context failed: {e}")
            return [], []

    # ID: f3952e9d-1228-4013-9bc8-91d0b551d3b2
    async def generate_report(self, goal: str) -> str:
        """
        Analyzes a goal, queries the graph, and generates a surgical context report.
        """
        log.info(f"🔬 Conducting reconnaissance for goal: '{goal}'")

        target_symbols, relevant_files = await self._find_relevant_symbols_and_files(
            goal
        )

        report_parts = ["# Reconnaissance Report"]

        if relevant_files:
            report_parts.append("\n## Relevant Files Identified by Semantic Search:")
            for file in relevant_files:
                report_parts.append(f"- `{file}`")
        else:
            report_parts.append(
                "\n- No specific relevant files were identified via semantic search."
            )

        if not target_symbols:
            report_parts.append(
                "\n- No specific code symbols were identified via semantic search."
            )
        else:
            report_parts.append("\n## Relevant Symbols Identified by Semantic Search:")
            for symbol_data in target_symbols:
                callers = self._find_callers(symbol_data.get("name"))
                report_parts.append(f"\n### Symbol: `{symbol_data.get('key', 'N/A')}`")
                report_parts.append(f"- **Type:** {symbol_data.get('type')}")
                report_parts.append(f"- **Location:** `{symbol_data.get('file')}`")
                report_parts.append(f"- **Intent:** {symbol_data.get('intent')}")

                if callers:
                    report_parts.append("- **Referenced By:**")
                    for caller in callers:
                        report_parts.append(f"  - `{caller.get('key')}`")
                else:
                    report_parts.append(
                        "- **Referenced By:** None. This symbol appears to be unreferenced."
                    )

        report_parts.append(
            "\n---\n**Conclusion:** The analysis is complete. Use this information to form a precise plan."
        )
        report = "\n".join(report_parts)
        log.info(f"   -> Generated Surgical Context Report:\n{report}")
        return report

    def _find_callers(self, symbol_name: str | None) -> List[Dict]:
        """Finds all symbols in the graph that call the target symbol."""
        if not symbol_name:
            return []
        return [
            data
            for data in self.symbols.values()
            if symbol_name in data.get("calls", [])
        ]

--- END OF FILE ./src/core/agents/reconnaissance_agent.py ---

--- START OF FILE ./src/core/agents/self_correction_engine.py ---
# src/core/self_correction_engine.py
"""
Handles automated correction of code failures by generating and validating LLM-suggested repairs based on structured violation data.
"""

from __future__ import annotations

import json
from typing import TYPE_CHECKING

from core.cognitive_service import CognitiveService
from core.prompt_pipeline import PromptPipeline
from core.validation_pipeline import validate_code_async
from shared.config import settings
from shared.utils.parsing import parse_write_blocks

if TYPE_CHECKING:
    from features.governance.audit_context import AuditorContext


REPO_PATH = settings.REPO_PATH
pipeline = PromptPipeline(repo_path=REPO_PATH)


# ID: 4f94f86f-4119-4f65-b4a6-adbcd159c071
async def attempt_correction(
    failure_context: dict,
    cognitive_service: CognitiveService,
    auditor_context: "AuditorContext",
) -> dict:
    """Attempts to fix a failed validation or test result using an enriched LLM prompt."""
    generator = await cognitive_service.aget_client_for_role("Coder")

    file_path = failure_context.get("file_path")
    code = failure_context.get("code")
    violations = failure_context.get("violations", [])

    if not all([file_path, code, violations]):
        return {
            "status": "error",
            "message": "Missing required failure context fields.",
        }

    correction_prompt = (
        f"You are CORE's self-correction agent.\n\nA recent code generation attempt failed validation.\n"
        f"Please analyze the violations and fix the code below.\n\nFile: {file_path}\n\n"
        f"[[violations]]\n{json.dumps(violations, indent=2)}\n[[/violations]]\n\n"
        f"[[code]]\n{code.strip()}\n[[/code]]\n\n"
        f"Respond with the full, corrected code in a single write block:\n[[write:{file_path}]]\n<corrected code here>\n[[/write]]"
    )

    final_prompt = pipeline.process(correction_prompt)
    llm_output = await generator.make_request_async(final_prompt, user_id="auto_repair")
    write_blocks = parse_write_blocks(llm_output)

    if not write_blocks:
        return {
            "status": "error",
            "message": "LLM did not produce a valid correction in a write block.",
        }

    path, fixed_code = list(write_blocks.items())[0]

    validation_result = await validate_code_async(path, fixed_code, auditor_context)
    if validation_result["status"] == "dirty":
        return {
            "status": "correction_failed_validation",
            "message": "The corrected code still fails validation.",
            "violations": validation_result["violations"],
        }

    # --- FIX: Return the validated code directly ---
    return {
        "status": "success",
        "code": validation_result["code"],
        "message": "Corrected code generated and validated successfully.",
    }

--- END OF FILE ./src/core/agents/self_correction_engine.py ---

--- START OF FILE ./src/core/agents/tagger_agent.py ---
# src/agents/tagger_agent.py
"""
Implements the CapabilityTaggerAgent, which finds unassigned capabilities
and uses an LLM to suggest constitutionally-valid names for them.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Optional

from rich.console import Console
from rich.table import Table
from shared.config import settings
from shared.logger import getLogger
from shared.utils.parallel_processor import ThrottledParallelProcessor

from core.cognitive_service import CognitiveService
from core.knowledge_service import KnowledgeService

log = getLogger("tagger_agent")


# ID: 444b630b-9cf5-4e70-ad60-4756e34144e8
class CapabilityTaggerAgent:
    """An agent that finds unassigned capabilities and suggests names."""

    def __init__(
        self,
        cognitive_service: CognitiveService,
        knowledge_service: KnowledgeService,
    ):
        """Initializes the agent with the tools it needs."""
        self.cognitive_service = cognitive_service
        self.knowledge_service = knowledge_service
        self.console = Console()
        prompt_path = settings.MIND / "prompts" / "capability_tagger.prompt"
        self.prompt_template = prompt_path.read_text(encoding="utf-8")
        self.existing_capabilities = self.knowledge_service.list_capabilities()
        self.tagger_client = self.cognitive_service.get_client_for_role("CodeReviewer")

    def _extract_symbol_info(self, symbol: Dict[str, Any]) -> Dict[str, Any]:
        """Extracts the relevant information for the prompt from a symbol entry."""
        return {
            "key": symbol.get("key"),
            "name": symbol.get("name"),
            "file": symbol.get("file"),
            "domain": symbol.get("domain"),
            "docstring": symbol.get("docstring"),
        }

    def _build_suggestion_prompt(self, symbol_info: Dict[str, Any]) -> str:
        """Builds the final prompt for AI suggestion request."""
        return self.prompt_template.format(
            existing_capabilities=json.dumps(self.existing_capabilities, indent=2),
            symbol_info=json.dumps(symbol_info, indent=2),
        )

    async def _get_suggestion_for_symbol(
        self, symbol: Dict[str, Any]
    ) -> Optional[Dict[str, str]]:
        """Async worker to get a single tag suggestion from the LLM."""
        symbol_info = self._extract_symbol_info(symbol)
        final_prompt = self._build_suggestion_prompt(symbol_info)
        response = await self.tagger_client.make_request_async(
            final_prompt, user_id="tagger_agent"
        )
        try:
            parsed = json.loads(response)
            suggestion = parsed.get("suggested_capability")

            if suggestion is None:
                return None

            if suggestion:
                return {
                    "key": symbol["key"],
                    "name": symbol["name"],
                    "file": symbol["file"],
                    "suggestion": suggestion,
                }
        except (json.JSONDecodeError, AttributeError):
            log.warning(f"Could not parse suggestion for {symbol['name']}.")
        return None

    # ID: 4c92bdd4-66f8-4292-b9c4-daeb2d7fdff7
    async def suggest_and_apply_tags(
        self, file_path: Path | None = None
    ) -> Optional[Dict[str, Dict]]:
        """
        Finds unassigned public symbols, gets AI-powered suggestions, and returns them.
        """
        log.info("🔍 Searching for unassigned capabilities...")
        all_unassigned = [
            s
            for s in self.knowledge_service.graph.get("symbols", {}).values()
            if s.get("capability") == "unassigned"
        ]
        public_unassigned_symbols = [
            s for s in all_unassigned if not s.get("name", "").startswith("_")
        ]
        log.info(
            f"   -> Filtering to {len(public_unassigned_symbols)} public symbols for AI analysis."
        )

        target_symbols = [
            s
            for s in public_unassigned_symbols
            if not file_path or s.get("file") == str(file_path)
        ]

        if not target_symbols:
            return None

        log.info(f"Found {len(target_symbols)} unassigned public symbols. Analyzing...")

        processor = ThrottledParallelProcessor(description="Analyzing symbols...")
        # --- THIS IS THE KEY LINE ---
        results = await processor.run_async(
            target_symbols, self._get_suggestion_for_symbol
        )
        # --- END KEY LINE ---

        suggestions_to_return = {}
        table = Table(title="🤖 Capability Tagger Agent Suggestions")
        table.add_column("Symbol", style="cyan")
        table.add_column("File", style="green")
        table.add_column("Suggested Capability", style="yellow")

        valid_results = filter(None, results)
        for res in valid_results:
            table.add_row(res["name"], res["file"], res["suggestion"])
            suggestions_to_return[res["key"]] = res

        if not suggestions_to_return:
            return None

        self.console.print(table)
        return suggestions_to_return

--- END OF FILE ./src/core/agents/tagger_agent.py ---

--- START OF FILE ./src/core/black_formatter.py ---
# src/core/black_formatter.py
"""
Formats Python code using the Black formatter with robust error handling for syntax and formatting issues.
"""

from __future__ import annotations

import black


# --- MODIFICATION: The function now returns only the formatted code on success ---
# --- and raises a specific exception on failure, simplifying its contract. ---
# ID: 044478bd-8231-48ff-af43-6bc3c022d69c
def format_code_with_black(code: str) -> str:
    """Formats the given Python code using Black, raising `black.InvalidInput` for syntax errors or `Exception` for other formatting issues."""
    """
    Attempts to format the given Python code using Black.

    Args:
        code: The Python source code to format.

    Returns:
        The formatted code as a string.

    Raises:
        black.InvalidInput: If the code contains a syntax error that Black cannot handle.
        Exception: For other unexpected Black formatting errors.
    """
    try:
        mode = black.FileMode()
        formatted_code = black.format_str(code, mode=mode)
        return formatted_code
    except black.InvalidInput as e:
        # Re-raise with a clear message for the pipeline to catch.
        raise black.InvalidInput(
            f"Black could not format the code due to a syntax error: {e}"
        )
    except Exception as e:
        # Catch any other unexpected errors from Black.
        raise Exception(f"An unexpected error occurred during Black formatting: {e}")

--- END OF FILE ./src/core/black_formatter.py ---

--- START OF FILE ./src/core/capabilities.py ---
# src/core/capabilities.py
"""
Orchestrates the system's self-analysis cycle by executing introspection tools as governed subprocesses.
"""

from __future__ import annotations

import sys

from dotenv import load_dotenv
from shared.logger import getLogger
from shared.utils.subprocess_utils import run_poetry_command

log = getLogger(__name__)


# ID: b36292a6-98b1-44fb-b76a-a2faad96564b
def introspection():
    """
    Runs a full self-analysis cycle to inspect system structure and health.
    This orchestrates the execution of the system's own introspection tools
    as separate, governed processes.
    """
    log.info("🔍 Starting introspection cycle...")

    tools_to_run = [
        ("Knowledge Graph Builder", ["python", "-m", "system.tools.codegraph_builder"]),
        (
            "Constitutional Auditor",
            ["python", "-m", "system.governance.constitutional_auditor"],
        ),
    ]

    all_passed = True
    for name, command in tools_to_run:
        try:
            # Use the sanctioned wrapper instead of direct subprocess call
            run_poetry_command(f"Running {name}...", command)
            log.info(f"✅ {name} completed successfully.")
        except Exception:
            log.error(f"❌ {name} failed.")
            all_passed = False

    log.info("🧠 Introspection cycle completed.")
    return all_passed


if __name__ == "__main__":
    load_dotenv()
    if not introspection():
        sys.exit(1)
    sys.exit(0)

--- END OF FILE ./src/core/capabilities.py ---

--- START OF FILE ./src/core/cognitive_service.py ---
# src/core/cognitive_service.py
from __future__ import annotations

import asyncio
import os
from pathlib import Path
from typing import Any, Dict, Optional

from services.config_service import config_service
from services.database.models import CognitiveRole, LlmResource
from services.database.session_manager import get_session
from services.llm.client import LLMClient
from services.llm.providers.base import AIProvider
from services.llm.providers.ollama import OllamaProvider
from services.llm.providers.openai import OpenAIProvider
from services.llm.resource_selector import ResourceSelector
from shared.logger import getLogger
from sqlalchemy import select

log = getLogger(__name__)


# ID: 507c1d3a-e014-4695-a5c6-2e50f2d8dd4d
class CognitiveService:
    """
    Manages LLM client lifecycle and provides clients for specific cognitive roles.
    Acts as a factory for creating provider-specific clients.
    """

    def __init__(self, repo_path: Path):
        self._repo_path = Path(repo_path)
        self._loaded: bool = False
        self._clients_by_role: Dict[str, LLMClient] = {}
        self._resource_selector: Optional[ResourceSelector] = None
        self._init_lock = asyncio.Lock()
        # Lazy import pattern to avoid import-time side effects in tests
        self.qdrant_service = __import__(
            "services.clients.qdrant_client"
        ).clients.qdrant_client.QdrantService()

    # ID: 68895785-8f99-4c02-9167-7191e35a0a98
    async def initialize(self) -> None:
        """Load resources and roles from DB and prepare the selector."""
        async with self._init_lock:
            if self._loaded:
                return

            try:
                log.info("Initializing CognitiveService from database...")
                async with get_session() as session:
                    res_result = await session.execute(select(LlmResource))
                    role_result = await session.execute(select(CognitiveRole))
                    resources = list(res_result.scalars().all())
                    roles = list(role_result.scalars().all())

                self._resource_selector = ResourceSelector(resources, roles)
                self._loaded = True
                log.info(
                    f"CognitiveService loaded {len(resources)} resources and {len(roles)} roles."
                )
            except Exception as e:
                log.warning(
                    f"DB init failed for CognitiveService ({e}); using empty selector."
                )
                self._resource_selector = ResourceSelector([], [])

    async def _create_provider_for_resource(self, resource: LlmResource) -> AIProvider:
        """
        Create the correct provider. Config is fetched asynchronously from the
        DB-backed config service, with a safe fallback to process env.
        """
        prefix = (resource.env_prefix or "").strip().upper()
        if not prefix:
            raise ValueError(f"Resource '{resource.name}' is missing env_prefix.")

        # First try DB-backed config (what tests mock), then fall back to env
        api_url = await config_service.get(f"{prefix}_API_URL") or os.getenv(
            f"{prefix}_API_URL"
        )
        api_key = await config_service.get(f"{prefix}_API_KEY") or os.getenv(
            f"{prefix}_API_KEY"
        )
        model_name = await config_service.get(f"{prefix}_MODEL_NAME") or os.getenv(
            f"{prefix}_MODEL_NAME"
        )

        if not api_url or not model_name:
            raise ValueError(
                f"Missing required config for resource '{resource.name}' with prefix '{prefix}_'. "
                "Ensure values are present (tests mock the DB-backed config service)."
            )

        # Simple provider routing
        if "ollama" in resource.name.lower() or "11434" in (api_url or ""):
            return OllamaProvider(
                api_url=api_url, model_name=model_name, api_key=api_key
            )

        # Default to OpenAI-compatible (or OpenAI proxy)
        return OpenAIProvider(api_url=api_url, model_name=model_name, api_key=api_key)

    # ID: 8c6c595c-d01b-4eb2-b2ad-3035ec35b480
    async def aget_client_for_role(self, role_name: str) -> LLMClient:
        """Return an LLM client for the given cognitive role."""
        if not self._loaded:
            await self.initialize()

        if role_name in self._clients_by_role:
            return self._clients_by_role[role_name]

        if not self._resource_selector:
            raise RuntimeError("ResourceSelector not initialized.")

        resource = self._resource_selector.select_resource_for_role(role_name)
        if not resource:
            raise RuntimeError(f"No compatible resource found for role '{role_name}'")

        try:
            provider = await self._create_provider_for_resource(resource)
            client = LLMClient(provider)
            self._clients_by_role[role_name] = client
            return client
        except Exception as e:
            raise RuntimeError(
                f"Failed to create client for role '{role_name}': {e}"
            ) from e

    # ID: 13aabd89-2e2b-49a1-94d9-3a4e8bbd434b
    async def get_embedding_for_code(self, source_code: str) -> Optional[list[float]]:
        """Generate an embedding using the Vectorizer role."""
        if not source_code:
            return None
        client = await self.aget_client_for_role("Vectorizer")
        return await client.get_embedding(source_code)

    # ID: 483b9bac-982f-484d-8cec-18354a9f422d
    async def search_capabilities(
        self, query: str, limit: int = 5
    ) -> list[dict[str, Any]]:
        """Semantic search via Qdrant."""
        if not self._loaded:
            await self.initialize()

        try:
            query_vector = await self.get_embedding_for_code(query)
            if not query_vector:
                return []
            return await self.qdrant_service.search_similar(query_vector, limit=limit)
        except Exception as e:
            log.error(f"Semantic search failed: {e}", exc_info=True)
            return []

--- END OF FILE ./src/core/cognitive_service.py ---

--- START OF FILE ./src/core/config_service.py ---
# src/services/config_service.py
"""
Configuration service that reads from database as single source of truth.

Constitutional Principle: Mind/Body/Will Separation
- Mind (.intent/) defines WHAT should be configured
- Database stores the CURRENT state
- This service provides Body (code) access to configuration

Migration Path:
1. Bootstrap: .env → DB (one-time)
2. Runtime: Code reads from DB only
3. Updates: Changes go through constitutional governance
"""

from __future__ import annotations

import os
from typing import Any, Optional

from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from core.secrets_service import get_secrets_service
from shared.logger import getLogger

log = getLogger(__name__)


# ID: config-service-001
class ConfigService:
    """
    Provides configuration from database with caching.

    Usage:
        config = await ConfigService.create(db)
        model_name = await config.get("deepseek_chat.model_name")
        api_key = await config.get_secret("anthropic.api_key")
    """

    def __init__(self, db: AsyncSession, cache: dict[str, Any]):
        self.db = db
        self._cache = cache
        self._secrets_service: Optional[Any] = None

    @classmethod
    async def create(cls, db: AsyncSession) -> ConfigService:
        """
        Factory method to create ConfigService with preloaded cache.

        This loads all non-secret config into memory for performance.
        Secrets are fetched on-demand for security.
        """
        # Load all non-secret config
        query = text("""
            SELECT key, value
            FROM core.runtime_settings
            WHERE is_secret = false
        """)

        result = await db.execute(query)
        cache = {row[0]: row[1] for row in result.fetchall()}

        log.info(f"Loaded {len(cache)} configuration values from database")

        return cls(db, cache)

    async def get(
        self,
        key: str,
        default: Optional[str] = None,
        required: bool = False,
    ) -> Optional[str]:
        """
        Get a non-secret configuration value.

        Args:
            key: Config key (e.g., "deepseek_chat.model_name")
            default: Default value if not found
            required: If True, raise error if not found

        Returns:
            Config value or default

        Raises:
            KeyError: If required=True and key not found
        """
        value = self._cache.get(key)

        if value is None:
            if required:
                raise KeyError(f"Required config key '{key}' not found in database")
            return default

        return value

    async def get_secret(
        self,
        key: str,
        audit_context: Optional[str] = None,
    ) -> str:
        """
        Get a secret configuration value (decrypted).

        Args:
            key: Secret key (e.g., "anthropic.api_key")
            audit_context: Context for audit log

        Returns:
            Decrypted secret value

        Raises:
            KeyError: If secret not found
        """
        if not self._secrets_service:
            self._secrets_service = await get_secrets_service(self.db)

        return await self._secrets_service.get_secret(
            self.db,
            key,
            audit_context=audit_context,
        )

    async def get_int(self, key: str, default: Optional[int] = None) -> Optional[int]:
        """Get config value as integer."""
        value = await self.get(key, default=str(default) if default is not None else None)
        return int(value) if value else None

    async def get_float(self, key: str, default: Optional[float] = None) -> Optional[float]:
        """Get config value as float."""
        value = await self.get(key, default=str(default) if default is not None else None)
        return float(value) if value else None

    async def get_bool(self, key: str, default: bool = False) -> bool:
        """Get config value as boolean."""
        value = await self.get(key, default=str(default))
        if value is None:
            return default
        return value.lower() in ("true", "1", "yes", "on")

    async def set(
        self,
        key: str,
        value: str,
        description: Optional[str] = None,
    ) -> None:
        """
        Set a non-secret configuration value.

        Note: This should go through governance for production changes!
        """
        query = text("""
            INSERT INTO core.runtime_settings (key, value, description, is_secret, last_updated)
            VALUES (:key, :value, :description, false, NOW())
            ON CONFLICT (key)
            DO UPDATE SET
                value = EXCLUDED.value,
                description = COALESCE(EXCLUDED.description, core.runtime_settings.description),
                last_updated = NOW()
        """)

        await self.db.execute(
            query,
            {"key": key, "value": value, "description": description},
        )
        await self.db.commit()

        # Update cache
        self._cache[key] = value

        log.info(f"Config '{key}' set to '{value}'")

    async def reload_cache(self) -> None:
        """Reload non-secret config cache from database."""
        query = text("""
            SELECT key, value
            FROM core.runtime_settings
            WHERE is_secret = false
        """)

        result = await self.db.execute(query)
        self._cache = {row[0]: row[1] for row in result.fetchall()}

        log.info(f"Reloaded {len(self._cache)} configuration values")


# ID: config-service-002
async def bootstrap_config_from_env() -> None:
    """
    Bootstrap database configuration from .env file.

    This should be run ONCE when setting up a new environment.
    After this, all config changes go through the database.

    Usage:
        poetry run core-admin manage config bootstrap
    """
    from dotenv import dotenv_values
    from services.database.session_manager import get_session

    env_vars = dotenv_values(".env")

    # Map env vars to database keys
    config_mapping = {
        # LLM Resources
        "OLLAMA_LOCAL_MODEL_NAME": "ollama_local.model_name",
        "OLLAMA_LOCAL_MAX_CONCURRENT_REQUESTS": "ollama_local.max_concurrent",
        "OLLAMA_LOCAL_SECONDS_BETWEEN_REQUESTS": "ollama_local.rate_limit",

        "DEEPSEEK_CHAT_MODEL_NAME": "deepseek_chat.model_name",
        "DEEPSEEK_CHAT_MAX_CONCURRENT_REQUESTS": "deepseek_chat.max_concurrent",
        "DEEPSEEK_CHAT_SECONDS_BETWEEN_REQUESTS": "deepseek_chat.rate_limit",

        "DEEPSEEK_CODER_MODEL_NAME": "deepseek_coder.model_name",
        "DEEPSEEK_CODER_MAX_CONCURRENT_REQUESTS": "deepseek_coder.max_concurrent",
        "DEEPSEEK_CODER_SECONDS_BETWEEN_REQUESTS": "deepseek_coder.rate_limit",

        "ANTHROPIC_CLAUDE_SONNET_MODEL_NAME": "anthropic.model_name",
        "ANTHROPIC_CLAUDE_SONNET_MAX_CONCURRENT_REQUESTS": "anthropic.max_concurrent",
        "ANTHROPIC_CLAUDE_SONNET_SECONDS_BETWEEN_REQUESTS": "anthropic.rate_limit",

        # Embedding
        "LOCAL_EMBEDDING_MODEL_NAME": "embedding.model_name",
        "LOCAL_EMBEDDING_DIM": "embedding.dimensions",
        "LOCAL_EMBEDDING_MAX_CONCURRENT_REQUESTS": "embedding.max_concurrent",

        # System
        "LLM_REQUEST_TIMEOUT": "llm.default_timeout",
        "CORE_MAX_CONCURRENT_REQUESTS": "llm.default_max_concurrent",
        "LLM_SECONDS_BETWEEN_REQUESTS": "llm.default_rate_limit",
        "LOG_LEVEL": "system.log_level",
        "LLM_ENABLED": "system.llm_enabled",
    }

    async with get_session() as db:
        config = await ConfigService.create(db)

        migrated = 0
        for env_key, db_key in config_mapping.items():
            if env_key in env_vars and env_vars[env_key]:
                await config.set(
                    db_key,
                    env_vars[env_key],
                    description=f"Bootstrapped from {env_key}",
                )
                migrated += 1

        log.info(f"Bootstrapped {migrated} config values from .env to database")


# ID: config-service-003
class LLMResourceConfig:
    """
    Convenience wrapper for LLM resource configuration.

    Usage:
        config = await ConfigService.create(db)
        anthropic = await LLMResourceConfig.for_resource(config, "anthropic")
        api_key = await anthropic.get_api_key()
        model = await anthropic.get_model_name()
    """

    def __init__(self, config: ConfigService, resource_name: str):
        self.config = config
        self.resource_name = resource_name
        self._prefix = resource_name.lower().replace("-", "_")

    @classmethod
    async def for_resource(cls, config: ConfigService, resource_name: str):
        """Create config wrapper for a specific LLM resource."""
        return cls(config, resource_name)

    async def get_api_key(self, audit_context: Optional[str] = None) -> str:
        """Get API key for this resource."""
        key = f"{self._prefix}.api_key"
        return await self.config.get_secret(key, audit_context=audit_context)

    async def get_model_name(self) -> str:
        """Get model name for this resource."""
        key = f"{self._prefix}.model_name"
        return await self.config.get(key, required=True)

    async def get_api_url(self) -> str:
        """Get API URL for this resource."""
        key = f"{self._prefix}.api_url"
        return await self.config.get(key, required=True)

    async def get_max_concurrent(self) -> int:
        """Get max concurrent requests for this resource."""
        key = f"{self._prefix}.max_concurrent"
        default_key = "llm.default_max_concurrent"
        value = await self.config.get(key)
        if not value:
            value = await self.config.get(default_key, default="2")
        return int(value)

    async def get_rate_limit(self) -> float:
        """Get rate limit (seconds between requests) for this resource."""
        key = f"{self._prefix}.rate_limit"
        default_key = "llm.default_rate_limit"
        value = await self.config.get(key)
        if not value:
            value = await self.config.get(default_key, default="2.0")
        return float(value)

    async def get_timeout(self) -> int:
        """Get request timeout for this resource."""
        key = f"{self._prefix}.timeout"
        default_key = "llm.default_timeout"
        value = await self.config.get(key)
        if not value:
            value = await self.config.get(default_key, default="300")
        return int(value)
--- END OF FILE ./src/core/config_service.py ---

--- START OF FILE ./src/core/crate_processing_service.py ---
# src/core/crate_processing_service.py
"""
Provides the core service for processing asynchronous, autonomous change requests (Intent Crates).
"""

from __future__ import annotations

import fnmatch
import tempfile
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List

import jsonschema
import yaml
from features.governance.audit_context import AuditorContext
from features.governance.constitutional_auditor import ConstitutionalAuditor
from features.introspection.knowledge_graph_service import (
    KnowledgeGraphBuilder,
)
from rich.console import Console
from shared.action_logger import action_logger
from shared.config import settings
from shared.logger import getLogger
from shared.models import AuditFinding

log = getLogger("crate_processing_service")
console = Console()


@dataclass
# ID: 32eaf90d-656b-4ef0-a87b-e22a7818b9b0
class Crate:
    """A simple data class representing a validated Intent Crate."""

    path: Path
    manifest: Dict[str, Any]


# ID: 5d7a8b3e-1f2c-4d5e-6f7a-8b9c0d1e2f3a
class CrateProcessingService:
    """
    Orchestrates the lifecycle of an Intent Crate: validation, canary testing, application, and result logging.
    """

    def __init__(self):
        """Initializes the service with its required dependencies and constitutional policies."""
        self.repo_root = settings.REPO_PATH
        self.crate_policy = settings.load(
            "charter.policies.governance.intent_crate_policy"
        )
        self.crate_schema = settings.load(
            "charter.schemas.constitutional.intent_crate_schema"
        )

        self.inbox_path = self.repo_root / "work" / "crates" / "inbox"
        self.processing_path = self.repo_root / "work" / "crates" / "processing"
        self.accepted_path = self.repo_root / "work" / "crates" / "accepted"
        self.rejected_path = self.repo_root / "work" / "crates" / "rejected"

        for path in [
            self.inbox_path,
            self.processing_path,
            self.accepted_path,
            self.rejected_path,
        ]:
            path.mkdir(parents=True, exist_ok=True)

        log.info("CrateProcessingService initialized and constitutionally configured.")

    # ID: 4e3d2c1b-0a9b-8c7d-6e5f-4a3b2c1d0e9f
    def _scan_and_validate_inbox(self) -> List[Crate]:
        """Scans the inbox for crates and validates their manifests."""
        valid_crates = []
        if not self.inbox_path.exists():
            return []

        for item in self.inbox_path.iterdir():
            if not item.is_dir():
                continue

            crate_id = item.name
            action_logger.log_event("crate.validation.started", {"crate_id": crate_id})
            manifest_path = item / "manifest.yaml"
            if not manifest_path.exists():
                reason = "missing manifest.yaml"
                log.warning(f"Skipping invalid crate '{crate_id}': {reason}.")
                action_logger.log_event(
                    "crate.validation.failed", {"crate_id": crate_id, "reason": reason}
                )
                continue

            try:
                manifest_content = settings._load_file_content(manifest_path)
                jsonschema.validate(instance=manifest_content, schema=self.crate_schema)
                valid_crates.append(Crate(path=item, manifest=manifest_content))
                log.info(
                    f"Validated crate '{crate_id}' with intent: '{manifest_content['intent']}'"
                )
            except (ValueError, jsonschema.ValidationError) as e:
                reason = f"Manifest validation failed: {e}"
                log.error(f"Rejecting invalid crate '{crate_id}': {reason}")
                action_logger.log_event(
                    "crate.validation.failed", {"crate_id": crate_id, "reason": str(e)}
                )
                self._move_crate_to_rejected(item, reason)
                continue

        return valid_crates

    def _copy_tree(self, src: Path, dst: Path, ignore_patterns: List[str]):
        """A simple replacement for shutil.copytree to avoid the import."""
        dst.mkdir(parents=True, exist_ok=True)
        for item in src.iterdir():
            if any(fnmatch.fnmatch(item.name, p) for p in ignore_patterns):
                continue
            s = src / item.name
            d = dst / item.name
            if s.is_dir():
                self._copy_tree(s, d, ignore_patterns)
            else:
                d.parent.mkdir(parents=True, exist_ok=True)
                d.write_bytes(s.read_bytes())

    def _copy_file(self, src: Path, dst: Path):
        """A simple replacement for shutil.copy2."""
        dst.parent.mkdir(parents=True, exist_ok=True)
        dst.write_bytes(src.read_bytes())

    # ID: 1b2c3d4e-5f6a-7b8c-9d0e-1f2a3b4c5d6e
    async def _run_canary_validation(
        self, crate: Crate
    ) -> tuple[bool, List[AuditFinding]]:
        """Creates a temporary environment, applies crate changes, and runs a full audit."""
        with tempfile.TemporaryDirectory() as tmpdir:
            canary_path = Path(tmpdir) / "canary_repo"
            console.print(f"   -> Creating canary environment at {canary_path}")

            self._copy_tree(
                self.repo_root,
                canary_path,
                ignore_patterns=[".git", ".venv", "__pycache__", "work", "reports"],
            )
            env_file = self.repo_root / ".env"
            if env_file.exists():
                self._copy_file(env_file, canary_path / ".env")
                console.print(
                    "   -> Copied runtime environment configuration to canary."
                )

            console.print("   -> Applying proposed changes to canary...")
            payload_files = crate.manifest.get("payload_files", [])
            for file_in_payload in payload_files:
                source_path = crate.path / file_in_payload
                if crate.manifest.get("type") == "CONSTITUTIONAL_AMENDMENT":
                    target_path = (
                        canary_path
                        / ".intent/charter/policies/governance"
                        / file_in_payload
                    )
                else:
                    target_path = canary_path / file_in_payload

                self._copy_file(source_path, target_path)

            console.print("   -> Building canary's internal knowledge graph...")
            canary_builder = KnowledgeGraphBuilder(root_path=canary_path)
            canary_builder.build()

            console.print("   -> 🔬 Running full constitutional audit on canary...")
            auditor = ConstitutionalAuditor(AuditorContext(canary_path))
            passed_findings = await auditor.run_full_audit_async()

            passed = not any(f.get("severity") == "error" for f in passed_findings)
            findings = [AuditFinding(**f) for f in passed_findings if not passed]

            if passed:
                console.print("   -> [bold green]✅ Canary audit PASSED.[/bold green]")
                return True, []
            else:
                console.print("   -> [bold red]❌ Canary audit FAILED.[/bold red]")
                return False, findings

    def _apply_accepted_crate(self, crate: Crate):
        """Applies the payload of an accepted crate to the live repository."""
        console.print(
            f"   -> Applying accepted crate '{crate.path.name}' to live system..."
        )
        payload_files = crate.manifest.get("payload_files", [])
        for file_in_payload in payload_files:
            source_path = crate.path / file_in_payload

            if crate.manifest.get("type") == "CONSTITUTIONAL_AMENDMENT":
                target_path = (
                    self.repo_root
                    / ".intent/charter/policies/governance"
                    / file_in_payload
                )
            else:
                target_path = self.repo_root / file_in_payload

            self._copy_file(source_path, target_path)
            console.print(f"      -> Applied '{file_in_payload}'")

    def _write_result_manifest(self, crate_path: Path, status: str, details: Any):
        """Writes a result.yaml file into the processed crate directory."""
        result_content = {
            "status": status,
            "processed_at_utc": datetime.now(timezone.utc).isoformat(),
        }
        if isinstance(details, str):
            result_content["justification"] = details
        elif isinstance(details, list):
            result_content["violations"] = [finding.as_dict() for finding in details]

        result_path = crate_path / "result.yaml"
        result_path.write_text(yaml.dump(result_content, indent=2), "utf-8")

    def _move_crate_to_rejected(self, crate_path: Path, details: Any):
        """Moves a crate to the rejected directory and writes a result manifest."""
        crate_id = crate_path.name
        final_path = self.rejected_path / crate_id

        if final_path.exists():
            import time

            final_path = self.rejected_path / f"{crate_id}_{int(time.time())}"
        crate_path.rename(final_path)

        self._write_result_manifest(final_path, "rejected", details)

        reason_summary = (
            details
            if isinstance(details, str)
            else f"{len(details)} constitutional violations found."
        )
        console.print(f"   -> Moved to rejected. Reason: {reason_summary}")

        log_details = {"crate_id": crate_id}
        if isinstance(details, str):
            log_details["reason"] = details
        else:
            log_details["violations"] = [finding.as_dict() for finding in details]

        action_logger.log_event("crate.processing.rejected", log_details)

    # ID: 9a8b7c6d-5e4f-3a2b-1c0d-9e8f7a6b5c4d
    async def process_pending_crates_async(self):
        """
        The main entry point for the service. It finds and processes all crates in the inbox.
        """
        console.print(
            "[bold cyan]🚀 Starting new crate processing cycle...[/bold cyan]"
        )

        valid_crates = self._scan_and_validate_inbox()
        if not valid_crates:
            console.print("✅ No valid crates found in the inbox. Cycle complete.")
            return

        console.print(f"Found {len(valid_crates)} valid crate(s) to process.")

        for crate in valid_crates:
            crate_id = crate.path.name
            console.print(f"\n[bold]Processing crate: {crate_id}[/bold]")
            try:
                processing_path = self.processing_path / crate_id

                crate.path.rename(processing_path)

                crate.path = processing_path
                console.print(
                    f"   -> Moved to processing: {processing_path.relative_to(self.repo_root)}"
                )
                action_logger.log_event(
                    "crate.processing.started", {"crate_id": crate_id}
                )

                is_safe, findings = await self._run_canary_validation(crate)

                if is_safe:
                    self._apply_accepted_crate(crate)
                    final_path = self.accepted_path / crate.path.name

                    crate.path.rename(final_path)

                    self._write_result_manifest(
                        final_path,
                        "accepted",
                        "Canary audit passed and changes were applied.",
                    )
                    console.print("   -> Moved to accepted.")
                    action_logger.log_event(
                        "crate.processing.accepted",
                        {
                            "crate_id": crate_id,
                            "reason": "Canary audit passed and changes applied.",
                        },
                    )
                else:
                    self._move_crate_to_rejected(crate.path, findings)

            except Exception as e:
                log.error(f"Failed to process crate '{crate_id}': {e}", exc_info=True)
                self._move_crate_to_rejected(
                    crate.path, f"Internal processing error: {e}"
                )
                continue


# ID: 3e2d1c0b-9a8b-7c6d-5e4f-3a2b1c0d9e8f
async def process_crates():
    """High-level function to instantiate and run the service."""
    service = CrateProcessingService()
    await service.process_pending_crates_async()

--- END OF FILE ./src/core/crate_processing_service.py ---

--- START OF FILE ./src/core/errors.py ---
# src/core/errors.py
"""
Centralizes HTTP exception handling to prevent sensitive stack trace leaks and ensure consistent error responses.
"""

from __future__ import annotations

from fastapi import Request
from fastapi.responses import JSONResponse
from starlette import status
from starlette.exceptions import HTTPException as StarletteHTTPException

from shared.logger import getLogger

log = getLogger("core_api.errors")


# ID: 08e2d78e-754e-4050-a426-dcca66d5319c
def register_exception_handlers(app):
    """Registers custom exception handlers with the FastAPI application."""

    @app.exception_handler(StarletteHTTPException)
    # ID: f3baf803-cdab-47ae-8a50-2f612e783819
    async def http_exception_handler(request: Request, exc: StarletteHTTPException):
        """
        Handles FastAPI's built-in HTTP exceptions to ensure consistent
        JSON error responses.
        """
        log.warning(
            f"HTTP Exception: {exc.status_code} {exc.detail} for request: "
            f"{request.method} {request.url.path}"
        )
        return JSONResponse(
            status_code=exc.status_code,
            content={"error": "request_error", "detail": exc.detail},
        )

    @app.exception_handler(Exception)
    # ID: 37d3d6d4-048d-4a31-9e02-93f3a7ddf5bc
    async def unhandled_exception_handler(request: Request, exc: Exception):
        """
        Catches any unhandled exception, logs the full traceback internally,
        and returns a generic 500 Internal Server Error to the client.
        This is a critical security measure to prevent leaking stack traces.
        """
        log.exception(
            f"Unhandled exception for request: {request.method} {request.url.path}"
        )
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": "internal_server_error",
                "detail": "An unexpected internal error occurred.",
            },
        )

    log.info("Registered global exception handlers.")

--- END OF FILE ./src/core/errors.py ---

--- START OF FILE ./src/core/file_classifier.py ---
# src/core/file_classifier.py
"""
File classification utilities for the validation pipeline.

This module provides functionality to classify files based on their extensions,
determining the appropriate validation strategy for each file type.
"""

from __future__ import annotations

from pathlib import Path


# ID: efe53dfb-fd71-4cd1-9f4d-1b1718c4f76a
def get_file_classification(file_path: str) -> str:
    """Determines the file type based on its extension.

    Args:
        file_path: Path to the file to classify

    Returns:
        A string representing the file type ('python', 'yaml', 'text', or 'unknown')
    """
    suffix = Path(file_path).suffix.lower()
    if suffix == ".py":
        return "python"
    if suffix in [".yaml", ".yml"]:
        return "yaml"
    if suffix in [".md", ".txt", ".json"]:
        return "text"
    return "unknown"

--- END OF FILE ./src/core/file_classifier.py ---

--- START OF FILE ./src/core/file_handler.py ---
# src/core/file_handler.py
"""
Provides safe, auditable file operations with staged writes
requiring confirmation for traceability and rollback capabilities.
"""

from __future__ import annotations

import json
import threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict
from uuid import uuid4

from shared.logger import getLogger

log = getLogger(__name__)


# ID: 8e74376f-d709-48be-bf0c-0e286f390f67
class FileHandler:
    """
    Central class for safe, auditable file operations in CORE.
    All writes are staged first and require confirmation. Validation is handled
    by the calling agent via the validation_pipeline.
    """

    def __init__(self, repo_path: str):
        """
        Initialize FileHandler with repository root.
        """
        self.repo_path = Path(repo_path).resolve()
        if not self.repo_path.is_dir():
            raise ValueError(f"Invalid repository path provided: {repo_path}")

        # --- THIS IS THE FIX ---
        # All operational directories are now relative to the repo_path
        # that the handler was initialized with. This makes the handler
        # safe to use in different contexts (like our integration test).
        self.log_dir = self.repo_path / "logs"
        self.pending_dir = self.repo_path / "pending_writes"
        self.undo_log = self.log_dir / "undo_log.jsonl"

        self.log_dir.mkdir(exist_ok=True)
        self.pending_dir.mkdir(exist_ok=True)
        # --- END OF FIX ---

        self.pending_writes: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.Lock()

    # ID: bf348511-75e7-442c-9aac-58ced078e564
    def add_pending_write(self, prompt: str, suggested_path: str, code: str) -> str:
        """
        Stages a pending write operation for later confirmation.
        """
        pending_id = str(uuid4())
        rel_path = Path(suggested_path).as_posix()
        entry = {
            "id": pending_id,
            "prompt": prompt,
            "path": rel_path,
            "code": code,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

        with self._lock:
            self.pending_writes[pending_id] = entry

        pending_file = self.pending_dir / f"{pending_id}.json"
        pending_file.write_text(json.dumps(entry, indent=2), encoding="utf-8")
        return pending_id

    # ID: 6238a0d8-0c8d-4c74-b792-7587dc13807a
    def confirm_write(self, pending_id: str) -> Dict[str, str]:
        """
        Confirms and applies a pending write to disk. Assumes content has been validated.
        """
        with self._lock:
            pending_op = self.pending_writes.pop(pending_id, None)

        pending_file = self.pending_dir / f"{pending_id}.json"
        if pending_file.exists():
            pending_file.unlink(missing_ok=True)

        if not pending_op:
            return {
                "status": "error",
                "message": f"Pending write ID '{pending_id}' not found or already processed.",
            }

        file_rel_path = pending_op["path"]

        try:
            abs_file_path = self.repo_path / file_rel_path

            if not abs_file_path.resolve().is_relative_to(self.repo_path.resolve()):
                raise ValueError(
                    f"Attempted to write outside of repository boundary: {file_rel_path}"
                )

            abs_file_path.parent.mkdir(parents=True, exist_ok=True)
            abs_file_path.write_text(pending_op["code"], encoding="utf-8")

            log.info(f"Wrote to {file_rel_path}")
            return {
                "status": "success",
                "message": f"Wrote to {file_rel_path}",
                "file_path": file_rel_path,
            }
        except Exception as e:
            if pending_op:
                with self._lock:
                    self.pending_writes[pending_id] = pending_op
                pending_file.write_text(
                    json.dumps(pending_op, indent=2), encoding="utf-8"
                )
            return {"status": "error", "message": f"Failed to write file: {str(e)}"}

--- END OF FILE ./src/core/file_handler.py ---

--- START OF FILE ./src/core/git_service.py ---
# src/core/git_service.py
"""
GitService: thin, testable wrapper around git commands used by CORE.

Responsibilities
- Validate repo path and .git presence on init.
- Provide small, composable operations (status, add, commit, etc.).
- Raise RuntimeError with useful stderr/stdout on git failures.
"""

from __future__ import annotations

import subprocess
from pathlib import Path

from shared.logger import getLogger

log = getLogger(__name__)


# ID: c1c9c30d-f864-4d43-8e12-d5263e52c15c
class GitService:
    """Provides basic git operations for agents and services."""

    def __init__(self, repo_path: str | Path):
        """
        Initializes the GitService and validates the repository path.
        """
        self.repo_path = Path(repo_path).resolve()

        # Allow initialization on a path that doesn't yet have .git
        # The is_git_repo() check can be used by callers.
        log.info(f"GitService initialized for path {self.repo_path}")

    def _run_command(self, command: list[str], cwd: Path | None = None) -> str:
        """Runs a git command and returns stdout; raises RuntimeError on failure."""
        try:
            effective_cwd = cwd or self.repo_path
            log.debug(f"Running git command: {' '.join(command)} in {effective_cwd}")
            result = subprocess.run(
                ["git", *command],
                cwd=effective_cwd,
                capture_output=True,
                text=True,
                check=True,
            )
            return result.stdout.strip()
        except subprocess.CalledProcessError as e:
            msg = e.stderr or e.stdout or ""
            log.error(f"Git command failed: {msg}")
            raise RuntimeError(f"Git command failed: {msg}") from e

    # ID: e8ab2870-d868-4c23-9f18-240110560b6d
    def init(self, path: Path):
        """Initializes a new Git repository at the specified path."""
        self._run_command(["init"], cwd=path)

    # ID: 41b4a07f-880b-4180-8e2e-ab7109b07ffc
    def get_current_commit(self) -> str:
        """Returns the hash of the current HEAD commit."""
        return self._run_command(["rev-parse", "HEAD"])

    # ID: eed906a4-ba54-4af9-94fe-9865d6906c96
    def get_staged_files(self) -> list[str]:
        """Returns a list of files that are currently staged for commit."""
        try:
            output = self._run_command(
                ["diff", "--cached", "--name-only", "--diff-filter=ACMR"]
            )
            if not output:
                return []
            return output.splitlines()
        except RuntimeError:
            return []

    # ID: 8d60714d-0214-48a9-be5b-9011e53ad93e
    def is_git_repo(self) -> bool:
        """Returns True if a '.git' directory exists."""
        return (self.repo_path / ".git").exists()

    # ID: b5420530-081f-4fa8-9754-5a00bedd5924
    def status_porcelain(self) -> str:
        """Returns the porcelain status output."""
        return self._run_command(["status", "--porcelain"])

    # ID: 2874a643-2e40-44f0-917f-a928484b2c67
    def add_all(self) -> None:
        """Stages all changes, including untracked files."""
        self._run_command(["add", "-A"])

    # ID: 55ed0386-16c1-458a-9b8f-f3ca0dc73696
    def commit(self, message: str) -> None:
        """
        Commits staged changes with the provided message.
        """
        try:
            # Stage everything one last time to be sure
            self.add_all()
            # Check if there is anything to commit after staging
            if not self.get_staged_files():
                log.info("No changes staged to commit.")
                return

            self._run_command(["commit", "-m", message])
            log.info(f"Committed changes with message: '{message}'")
        except RuntimeError as e:
            emsg = (str(e) or "").lower()
            if "nothing to commit" in emsg or "no changes added to commit" in emsg:
                log.info("No changes staged. Skipping commit.")
                return
            raise

--- END OF FILE ./src/core/git_service.py ---

--- START OF FILE ./src/core/intent_alignment.py ---
# src/core/intent_alignment.py
"""
Lightweight guard to ensure a requested goal aligns with CORE's mission/scope.

- Loads NorthStar/mission text from .intent (best-effort; no hard failures).
- Optional blocklist: .intent/policies/blocked_topics.txt (one term per line).
- Returns (ok: bool, details: dict) with short reason codes only.
"""

from __future__ import annotations

import logging
import re
from pathlib import Path
from typing import Dict, List, Tuple

log = logging.getLogger(__name__)

_INTENT_PATH_CANDIDATES: List[Path] = [
    Path(".intent/mission/northstar.md"),
    Path(".intent/mission/mission.md"),
    Path(".intent/mission/northstar.txt"),
    Path(".intent/NorthStar.md"),
]

_BLOCKLIST_PATH = Path(".intent/policies/blocked_topics.txt")


def _read_text_first(paths: List[Path]) -> str:
    """Finds and reads the first existing file from a list of candidate paths."""
    for p in paths:
        try:
            if p.exists():
                return p.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            log.debug("Failed reading %s", p, exc_info=True)
    return ""


def _read_blocklist() -> List[str]:
    """Reads the blocklist file, returning a list of lowercased, stripped terms."""
    if _BLOCKLIST_PATH.exists():
        try:
            return [
                ln.strip().lower()
                for ln in _BLOCKLIST_PATH.read_text(
                    encoding="utf-8", errors="ignore"
                ).splitlines()
                if ln.strip() and not ln.strip().startswith("#")
            ]
        except Exception:
            log.debug("Failed reading blocklist at %s", _BLOCKLIST_PATH, exc_info=True)
    return []


def _tokenize(text: str) -> List[str]:
    """Converts a string into a list of lowercase alphanumeric tokens."""
    return re.findall(r"[a-zA-Z0-9]+", text.lower())


# ID: f1267ace-1e0a-47f8-8d81-36ce4262913a
def check_goal_alignment(
    goal: str, project_root: Path = Path(".")
) -> Tuple[bool, Dict]:
    """
    Returns (ok, details). details = { 'coverage': float|None, 'violations': [codes...] }
    Violations codes: 'blocked_topic', 'low_mission_overlap'
    """
    violations: List[str] = []
    mission = _read_text_first(_INTENT_PATH_CANDIDATES)
    blocked = _read_blocklist()

    # Blocklist
    goal_l = goal.lower()
    if blocked and any(term in goal_l for term in blocked):
        violations.append("blocked_topic")

    # Mission overlap (very simple lexical overlap)
    coverage = None
    if mission:
        g_tokens = set(_tokenize(goal))
        m_tokens = set(_tokenize(mission))
        if g_tokens:
            overlap = len(g_tokens & m_tokens)
            coverage = round(overlap / max(1, len(g_tokens)), 3)
            if coverage < 0.10:  # conservative default; tune later
                violations.append("low_mission_overlap")

    ok = not violations
    return ok, {"coverage": coverage, "violations": violations}

--- END OF FILE ./src/core/intent_alignment.py ---

--- START OF FILE ./src/core/intent_guard.py ---
# src/core/intent_guard.py
"""
IntentGuard — CORE's Constitutional Enforcement Module
Enforces safety, structure, and intent alignment for all file changes.
Loads governance rules from .intent/policies/*.yaml and prevents unauthorized
self-modifications of the CORE constitution.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from shared.config_loader import load_yaml_file
from shared.logger import getLogger

log = getLogger(__name__)


@dataclass
# ID: 1499a5c2-5fc6-4ea3-8049-21702aa20f6e
class PolicyRule:
    """Structured representation of a policy rule."""

    name: str
    pattern: str
    action: str
    description: str
    severity: str = "error"

    @classmethod
    # ID: db43791c-92bd-435e-8ade-85620f3cf4f6
    def from_dict(cls, data: Dict) -> "PolicyRule":
        """Create PolicyRule from dictionary data."""
        return cls(
            name=data.get("name", "unnamed"),
            pattern=data.get("pattern", ""),
            action=data.get("action", "deny"),
            description=data.get("description", ""),
            severity=data.get("severity", "error"),
        )


@dataclass
# ID: 8bdef506-b2b3-4b1e-9a11-96e8d79282b3
class ViolationReport:
    """Detailed violation report with context."""

    rule_name: str
    path: str
    message: str
    severity: str
    suggested_fix: Optional[str] = None


# ID: 1f189a22-8497-44f9-af8e-00888b0eca0e
class IntentGuard:
    """
    Central enforcement engine for CORE's safety and governance policies.
    """

    def __init__(self, repo_path: Path):
        """Initialize IntentGuard with repository path and load all policies."""
        self.repo_path = Path(repo_path).resolve()
        self.intent_path = self.repo_path / ".intent"
        self.proposals_path = self.intent_path / "proposals"
        self.policies_path = self.intent_path / "charter" / "policies"

        self.rules: List[PolicyRule] = []
        self._load_policies()

        log.info(f"IntentGuard initialized with {len(self.rules)} rules loaded.")

    def _load_policies(self):
        """Load rules from all YAML files in the `.intent/charter/policies/` directory."""
        if not self.policies_path.is_dir():
            log.warning(f"Policies directory not found: {self.policies_path}")
            return

        for policy_file in self.policies_path.glob("*.yaml"):
            try:
                content = load_yaml_file(policy_file)
                if (
                    content
                    and "rules" in content
                    and isinstance(content["rules"], list)
                ):
                    for rule_data in content["rules"]:
                        if isinstance(rule_data, dict):
                            self.rules.append(PolicyRule.from_dict(rule_data))
            except Exception as e:
                log.error(f"Failed to load policy file {policy_file}: {e}")

    # ID: abd3b486-3aaa-4dee-8a99-2a0fbd8f1c28
    def check_transaction(
        self, proposed_paths: List[str]
    ) -> Tuple[bool, List[ViolationReport]]:
        """
        Check if a proposed set of file changes complies with all active rules.
        """
        violations = []
        for path_str in proposed_paths:
            path = (self.repo_path / path_str).resolve()
            violations.extend(self._check_single_path(path, path_str))
        return len(violations) == 0, violations

    def _check_single_path(self, path: Path, path_str: str) -> List[ViolationReport]:
        """Check a single path against all rules."""
        violations = []
        constitutional_violation = self._check_constitutional_integrity(path, path_str)
        if constitutional_violation:
            violations.append(constitutional_violation)
        violations.extend(self._check_policy_rules(path, path_str))
        return violations

    def _check_constitutional_integrity(
        self, path: Path, path_str: str
    ) -> Optional[ViolationReport]:
        """Check if the path violates constitutional immutability rules."""
        try:
            charter_path_resolved = (self.intent_path / "charter").resolve()
            if charter_path_resolved in path.parents or path == charter_path_resolved:
                return self._create_constitutional_violation(path_str)
        except Exception as e:
            log.error(f"Error checking constitutional integrity for {path_str}: {e}")
        return None

    def _create_constitutional_violation(self, path_str: str) -> ViolationReport:
        """Create a constitutional violation report."""
        return ViolationReport(
            rule_name="immutable_charter",
            path=path_str,
            message=f"Direct write to '{path_str}' is forbidden. Changes to the Charter require a formal proposal.",
            severity="error",
        )

    def _check_policy_rules(self, path: Path, path_str: str) -> List[ViolationReport]:
        """Check path against all loaded policy rules."""
        violations = []
        for rule in self.rules:
            try:
                if self._matches_pattern(path_str, rule.pattern):
                    violations.extend(self._apply_rule_action(rule, path_str))
            except Exception as e:
                log.error(f"Error applying rule '{rule.name}' to {path_str}: {e}")
        return violations

    def _apply_rule_action(
        self, rule: PolicyRule, path_str: str
    ) -> List[ViolationReport]:
        """Apply the action for a matched rule."""
        if rule.action == "deny":
            return [
                ViolationReport(
                    rule_name=rule.name,
                    path=path_str,
                    message=f"Rule '{rule.name}' violation: {rule.description}",
                    severity=rule.severity,
                )
            ]
        elif rule.action == "warn":
            log.warning(f"Policy warning for {path_str}: {rule.description}")
        return []

    def _matches_pattern(self, path: str, pattern: str) -> bool:
        """Check if a path matches a given glob pattern."""
        return Path(path).match(pattern)

--- END OF FILE ./src/core/intent_guard.py ---

--- START OF FILE ./src/core/invokers/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/core/invokers/__init__.py ---

--- START OF FILE ./src/core/invokers/capability_invoker.py ---
[EMPTY FILE]
--- END OF FILE ./src/core/invokers/capability_invoker.py ---

--- START OF FILE ./src/core/knowledge_service.py ---
# src/core/knowledge_service.py
"""
Centralized access to CORE's knowledge graph and declared capabilities from the database SSOT.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List

from services.database.session_manager import get_session
from shared.logger import getLogger
from sqlalchemy import text

log = getLogger("knowledge_service")


# ID: f1abb440-15b4-41b7-b36e-e63621c6d332
class KnowledgeService:
    """
    A read-only interface to the knowledge graph, which is sourced exclusively
    from the operational database view `core.knowledge_graph`.
    """

    def __init__(self, repo_path: Path | str = "."):
        self.repo_path = Path(repo_path)
        # The internal cache (`self._graph`) has been removed to enforce SSOT.

    # ID: 49190ab5-945a-4aa8-9500-21b849f217f9
    async def get_graph(self) -> Dict[str, Any]:
        """
        Loads the knowledge graph directly from the database, treating it as the
        single source of truth on every call. Caching is removed to ensure freshness.
        """
        log.info("Loading knowledge graph from database view...")
        symbols_map = {}
        try:
            async with get_session() as session:
                result = await session.execute(
                    text("SELECT * FROM core.knowledge_graph ORDER BY symbol_path")
                )
                for row in result:
                    row_dict = dict(row._mapping)
                    symbol_path = row_dict.get("symbol_path")
                    if symbol_path:
                        # Ensure all UUIDs are converted to strings for consistent use.
                        if "uuid" in row_dict and row_dict["uuid"] is not None:
                            row_dict["uuid"] = str(row_dict["uuid"])
                        if (
                            "vector_id" in row_dict
                            and row_dict["vector_id"] is not None
                        ):
                            row_dict["vector_id"] = str(row_dict["vector_id"])
                        symbols_map[symbol_path] = row_dict

            knowledge_graph = {"symbols": symbols_map}
            log.info(
                f"Successfully loaded {len(symbols_map)} symbols from the database."
            )
            return knowledge_graph

        except Exception as e:
            log.error(
                f"Failed to load knowledge graph from database: {e}", exc_info=True
            )
            # Fallback to an empty graph to prevent crashing.
            return {"symbols": {}}

    # ID: 884e9a28-255c-478c-8af9-46865e45a029
    async def list_capabilities(self) -> List[str]:
        """Returns all capability keys directly from the database."""
        async with get_session() as session:
            result = await session.execute(
                text("SELECT name FROM core.capabilities ORDER BY name")
            )
            return [row[0] for row in result]

    # ID: 9fa22aa4-5c09-46f7-a19c-c29851c92437
    async def search_capabilities(self, query: str, limit: int = 5) -> List[str]:
        """
        This is a placeholder. Real semantic search happens in CognitiveService.
        """
        all_caps = await self.list_capabilities()
        q_lower = query.lower()
        return [c for c in all_caps if q_lower in c.lower()][:limit]

--- END OF FILE ./src/core/knowledge_service.py ---

--- START OF FILE ./src/core/llm_client.py ---
# src/core/llm_client.py
"""
A dedicated, asynchronous client for interacting with LLM APIs.
"""

from __future__ import annotations

from pathlib import Path

import httpx
from shared.logger import getLogger

logger = getLogger(Path(__file__).stem)


# ID: 2dce5867-b6f5-49ee-9419-5c548bbaeebd
class LLMClient:
    """A wrapper for making asynchronous API calls to a specific LLM."""

    def __init__(
        self,
        api_url: str,
        api_key: str,
        model_name: str,
        http_timeout: int = 60,
    ):
        self.api_url = api_url
        self.api_key = api_key
        self.model_name = model_name
        self.http_timeout = http_timeout
        self.base_url = api_url  # For compatibility with test assertions

    # ID: cf51233c-d53a-4f83-bfbf-c694e91e2fb1
    async def make_request(
        self,
        prompt: str,
        system_prompt: str = "You are a helpful assistant.",
        max_tokens: int = 4096,
    ) -> str:
        """
        Makes an asynchronous request to the configured LLM API.
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        # This payload structure is common for OpenAI-compatible APIs
        payload = {
            "model": self.model_name,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            "max_tokens": max_tokens,
        }

        async with httpx.AsyncClient(timeout=self.http_timeout) as client:
            try:
                logger.debug(
                    f"Making request to {self.api_url} with model {self.model_name}"
                )
                response = await client.post(
                    self.api_url, headers=headers, json=payload
                )
                response.raise_for_status()  # Raise an exception for bad status codes

                data = response.json()
                content = (
                    data.get("choices", [{}])[0].get("message", {}).get("content", "")
                )

                if not content:
                    logger.warning("LLM response content is empty.")
                    return ""

                return content.strip()

            except httpx.HTTPStatusError as e:
                logger.error(
                    f"HTTP error occurred: {e.response.status_code} - {e.response.text}"
                )
                raise
            except Exception as e:
                logger.error(f"An unexpected error occurred during LLM request: {e}")
                raise

--- END OF FILE ./src/core/llm_client.py ---

--- START OF FILE ./src/core/main.py ---
# src/core/main.py
from __future__ import annotations

import os
from contextlib import asynccontextmanager

from api.v1 import development_routes, knowledge_routes
from fastapi import FastAPI
from features.governance.audit_context import AuditorContext
from services.clients.qdrant_client import QdrantService
from shared.config import settings
from shared.context import CoreContext
from shared.logger import getLogger
from shared.models import PlannerConfig

from core.cognitive_service import CognitiveService
from core.errors import register_exception_handlers
from core.file_handler import FileHandler
from core.git_service import GitService
from core.knowledge_service import KnowledgeService

log = getLogger("core.main")


@asynccontextmanager
# ID: 955908bb-6979-42b9-95fb-67f61a75db12
async def lifespan(app: FastAPI):
    log.info("🚀 Starting CORE system...")
    core_context = CoreContext(
        git_service=GitService(settings.REPO_PATH),
        cognitive_service=CognitiveService(settings.REPO_PATH),
        knowledge_service=KnowledgeService(settings.REPO_PATH),
        qdrant_service=QdrantService(),
        auditor_context=AuditorContext(settings.REPO_PATH),
        file_handler=FileHandler(str(settings.REPO_PATH)),
        planner_config=PlannerConfig(),
    )
    app.state.core_context = core_context

    # If running under pytest, let tests control initialization order explicitly
    if os.getenv("PYTEST_CURRENT_TEST"):
        core_context._is_test_mode = True  # tests call init/load explicitly

    try:
        if not getattr(core_context, "_is_test_mode", False):
            await core_context.cognitive_service.initialize()
            await core_context.auditor_context.load_knowledge_graph()
        yield
    finally:
        log.info("🛑 CORE system shutting down.")


# ID: ac2f33e7-4aed-4d32-b701-9bc50b622016
def create_app() -> FastAPI:
    app = FastAPI(
        title="CORE - Self-Improving System Architect",
        version="1.0.0",
        lifespan=lifespan,
    )
    app.include_router(knowledge_routes.router, prefix="/v1", tags=["Knowledge"])
    app.include_router(development_routes.router, prefix="/v1", tags=["Development"])
    register_exception_handlers(app)

    @app.get("/health")
    # ID: ee809a0b-21da-4169-a197-cf5df1d9ada8
    def health_check():
        return {"status": "ok"}

    return app

--- END OF FILE ./src/core/main.py ---

--- START OF FILE ./src/core/prompt_pipeline.py ---
# src/core/prompt_pipeline.py
"""
PromptPipeline — CORE's Unified Directive Processor

A single pipeline that processes all [[directive:...]] blocks in a user prompt.
Responsible for:
- Injecting context (e.g., file contents)
- Expanding includes
- Adding analysis from introspection tools
- Enriching with manifest data

This is the central "pre-processor" for all LLM interactions.
"""

from __future__ import annotations

import re
from pathlib import Path

import yaml

# --- FIX: Define a constant for a reasonable file size limit (1MB) ---
MAX_FILE_SIZE_BYTES = 1 * 1024 * 1024


# ID: 55fc4bff-0f88-435c-b988-23861ee401e8
class PromptPipeline:
    """
    Processes and enriches user prompts by resolving directives like
    [[include:...]] and [[analysis:...]]. Ensures the LLM receives full
    context before generating code.
    """

    def __init__(self, repo_path: Path):
        """
        Initialize PromptPipeline with repository root.

        Args:
            repo_path (Path): Root path of the repository.
        """
        self.repo_path = Path(repo_path).resolve()

        # Regex patterns for directive matching
        self.context_pattern = re.compile(r"\[\[context:(.+?)\]\]")
        self.include_pattern = re.compile(r"\[\[include:(.+?)\]\]")
        self.analysis_pattern = re.compile(r"\[\[analysis:(.+?)\]\]")
        self.manifest_pattern = re.compile(r"\[\[manifest:(.+?)\]\]")

    def _replace_context_match(self, match: re.Match) -> str:
        """
        Dynamically replaces a [[context:...]] regex match with file content
        or an error message if the file is missing, unreadable, or exceeds
        size limits.
        """
        file_path = match.group(1).strip()
        abs_path = self.repo_path / file_path
        if abs_path.exists() and abs_path.is_file():
            # --- FIX: Add file size check to prevent memory bloat ---
            if abs_path.stat().st_size > MAX_FILE_SIZE_BYTES:
                return (
                    f"\n❌ Could not include {file_path}: "
                    f"File size exceeds 1MB limit.\n"
                )
            try:
                content = abs_path.read_text(encoding="utf-8")
                return (
                    f"\n--- CONTEXT: {file_path} ---\n"
                    f"{content}\n"
                    f"--- END CONTEXT ---\n"
                )
            except Exception as e:
                return f"\n❌ Could not read {file_path}: {str(e)}\n"
        return f"\n❌ File not found: {file_path}\n"

    def _inject_context(self, prompt: str) -> str:
        """Replaces [[context:file.py]] directives with actual file content."""
        return self.context_pattern.sub(self._replace_context_match, prompt)

    def _replace_include_match(self, match: re.Match) -> str:
        """
        Dynamically replaces an [[include:...]] regex match with file content
        or an error message if the file is missing, unreadable, or exceeds
        size limits.
        """
        file_path = match.group(1).strip()
        abs_path = self.repo_path / file_path
        if abs_path.exists() and abs_path.is_file():
            # --- FIX: Add file size check to prevent memory bloat ---
            if abs_path.stat().st_size > MAX_FILE_SIZE_BYTES:
                return (
                    f"\n❌ Could not include {file_path}: "
                    f"File size exceeds 1MB limit.\n"
                )
            try:
                content = abs_path.read_text(encoding="utf-8")
                return (
                    f"\n--- INCLUDED: {file_path} ---\n"
                    f"{content}\n"
                    f"--- END INCLUDE ---\n"
                )
            except Exception as e:
                return f"\n❌ Could not read {file_path}: {str(e)}\n"
        return f"\n❌ File not found: {file_path}\n"

    def _inject_includes(self, prompt: str) -> str:
        """Replaces [[include:file.py]] directives with file content."""
        return self.include_pattern.sub(self._replace_include_match, prompt)

    def _replace_analysis_match(self, match: re.Match) -> str:
        """
        Dynamically replaces an [[analysis:...]] regex match with a
        placeholder analysis message for the given file path.
        """
        file_path = match.group(1).strip()
        # This functionality is a placeholder.
        return f"\n--- ANALYSIS FOR {file_path} (DEFERRED) ---\n"

    def _inject_analysis(self, prompt: str) -> str:
        """Replaces [[analysis:file.py]] directives with code analysis."""
        return self.analysis_pattern.sub(self._replace_analysis_match, prompt)

    def _replace_manifest_match(self, match: re.Match) -> str:
        """
        Dynamically replaces a [[manifest:...]] regex match with
        manifest data or an error.
        """
        manifest_path = self.repo_path / ".intent" / "project_manifest.yaml"
        if not manifest_path.exists():
            return f"\n❌ Manifest file not found at {manifest_path}\n"

        try:
            manifest = yaml.safe_load(manifest_path.read_text(encoding="utf-8"))
        except Exception:
            return f"\n❌ Could not parse manifest file at {manifest_path}\n"

        field = match.group(1).strip()
        value = manifest
        # Improved logic for nested key access
        for key in field.split("."):
            value = value.get(key) if isinstance(value, dict) else None
            if value is None:
                break

        if value is None:
            return f"\n❌ Manifest field not found: {field}\n"

        # Pretty print for better context
        if isinstance(value, (dict, list)):
            value_str = yaml.dump(value, indent=2)
        else:
            value_str = str(value)

        return (
            f"\n--- MANIFEST: {field} ---\n" f"{value_str}\n" f"--- END MANIFEST ---\n"
        )

    def _inject_manifest(self, prompt: str) -> str:
        """
        Replaces [[manifest:field]] directives with data from
        project_manifest.yaml.
        """
        return self.manifest_pattern.sub(self._replace_manifest_match, prompt)

    # ID: 05c566aa-d219-49bd-8b74-daa023b81e46
    def process(self, prompt: str) -> str:
        """
        Processes the full prompt by sequentially resolving all directives.
        This is the main entry point for prompt enrichment.
        """
        prompt = self._inject_context(prompt)
        prompt = self._inject_includes(prompt)
        prompt = self._inject_analysis(prompt)
        prompt = self._inject_manifest(prompt)
        return prompt

--- END OF FILE ./src/core/prompt_pipeline.py ---

--- START OF FILE ./src/core/python_validator.py ---
# src/core/python_validator.py
"""
Python code validation pipeline.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any, Dict, List, Tuple

import black
from features.governance.checks.import_rules import ImportRulesCheck

# --- START: IMPORT THE NEW SERVICE ---
from features.governance.runtime_validator import RuntimeValidatorService

# --- END: IMPORT THE NEW SERVICE ---
from shared.models import AuditFinding

from core.black_formatter import format_code_with_black
from core.ruff_linter import fix_and_lint_code_with_ruff
from core.syntax_checker import check_syntax

from .validation_policies import PolicyValidator
from .validation_quality import QualityChecker

if TYPE_CHECKING:
    from features.governance.audit_context import AuditorContext

Violation = Dict[str, Any]


# ID: df30ee5a-2cf7-4671-a10b-5d995a28310a
async def validate_python_code_async(
    path_hint: str, code: str, auditor_context: "AuditorContext"
) -> Tuple[str, List[Violation]]:
    """Comprehensive validation pipeline for Python code, now including runtime checks."""
    all_violations: List[Violation] = []

    # --- Step 1: Static Analysis (unchanged) ---
    safety_policy = auditor_context.policies.get("safety_policy", {})
    policy_validator = PolicyValidator(safety_policy.get("rules", []))
    quality_checker = QualityChecker()
    import_checker = ImportRulesCheck(auditor_context)

    try:
        formatted_code = format_code_with_black(code)
    except (black.InvalidInput, Exception) as e:
        all_violations.append(
            {
                "rule": "tooling.black_failure",
                "message": str(e),
                "line": 0,
                "severity": "error",
            }
        )
        return code, all_violations

    fixed_code, ruff_violations = fix_and_lint_code_with_ruff(formatted_code, path_hint)
    all_violations.extend(ruff_violations)

    syntax_violations = check_syntax(path_hint, fixed_code)
    all_violations.extend(syntax_violations)
    if any(v["severity"] == "error" for v in syntax_violations):
        return fixed_code, all_violations

    all_violations.extend(policy_validator.check_semantics(fixed_code, path_hint))
    all_violations.extend(quality_checker.check_for_todo_comments(fixed_code))

    try:
        import_violations = await import_checker.execute_on_content(
            path_hint, fixed_code
        )
        all_violations.extend(import_violations)
    except Exception as e:
        all_violations.append(
            {
                "rule": "import.check_failed",
                "message": str(e),
                "line": 0,
                "severity": "error",
            }
        )

    # --- Step 2: Runtime Validation (NEW) ---
    # Only proceed to runtime tests if all static analysis passed.
    if not any(v.get("severity") == "error" for v in all_violations):
        runtime_validator = RuntimeValidatorService(auditor_context.repo_path)
        passed, details = await runtime_validator.run_tests_in_canary(
            path_hint, fixed_code
        )
        if not passed:
            all_violations.append(
                AuditFinding(
                    check_id="runtime.tests.failed",
                    severity="error",
                    message="Code failed to pass the test suite in an isolated environment.",
                    context={"details": details},
                ).as_dict()
            )

    return fixed_code, all_violations

--- END OF FILE ./src/core/python_validator.py ---

--- START OF FILE ./src/core/ruff_linter.py ---
# src/core/ruff_linter.py
"""
Provides a utility to fix and lint Python code using Ruff's JSON output format.
Runs Ruff lint checks on generated Python code before it's staged.
Returns a success flag and an optional linting message.
"""

from __future__ import annotations

import json
import os
import subprocess
import tempfile
from typing import Any, Dict, List, Tuple

from shared.logger import getLogger

log = getLogger(__name__)
Violation = Dict[str, Any]


# --- MODIFICATION: Complete refactor to use Ruff's JSON output. ---
# --- The function now returns the fixed code and a list of structured violations. ---
# ID: 592ac81a-25a7-4313-9977-41f4dbca3cde
def fix_and_lint_code_with_ruff(
    code: str, display_filename: str = "<code>"
) -> Tuple[str, List[Violation]]:
    """
    Fix and lint the provided Python code using Ruff's JSON output format.

    Args:
        code (str): Source code to fix and lint.
        display_filename (str): Optional display name for readable error messages.

    Returns:
        A tuple containing:
        - The potentially fixed code as a string.
        - A list of structured violation dictionaries for any remaining issues.
    """
    violations = []
    with tempfile.NamedTemporaryFile(
        suffix=".py", mode="w+", delete=False, encoding="utf-8"
    ) as tmp_file:
        tmp_file.write(code)
        tmp_file_path = tmp_file.name

    try:
        # Step 1: Run Ruff with --fix to apply safe fixes. This modifies the temp file.
        subprocess.run(
            ["ruff", "check", tmp_file_path, "--fix", "--exit-zero", "--quiet"],
            capture_output=True,
            text=True,
            check=False,
        )

        # Step 2: Read the potentially modified code back from the file.
        with open(tmp_file_path, "r", encoding="utf-8") as f:
            fixed_code = f.read()

        # Step 3: Run Ruff again without fix, but with JSON output to get remaining violations.
        result = subprocess.run(
            ["ruff", "check", tmp_file_path, "--format", "json", "--exit-zero"],
            capture_output=True,
            text=True,
            check=False,
        )

        # Parse the JSON output for any remaining violations.
        if result.stdout:
            ruff_violations = json.loads(result.stdout)
            for v in ruff_violations:
                violations.append(
                    {
                        "rule": v.get("code", "RUFF-UNKNOWN"),
                        "message": v.get("message", "Unknown Ruff error"),
                        "line": v.get("location", {}).get("row", 0),
                        "severity": "warning",  # Assume all ruff issues are warnings for now
                    }
                )

        return fixed_code, violations

    except FileNotFoundError:
        log.error("Ruff is not installed or not in your PATH. Please install it.")
        # Return a critical violation if the tool itself is missing.
        tool_missing_violation = {
            "rule": "tooling.missing",
            "message": "Ruff is not installed or not in your PATH.",
            "line": 0,
            "severity": "error",
        }
        return code, [tool_missing_violation]
    except json.JSONDecodeError:
        log.error("Failed to parse Ruff's JSON output.")
        return code, []  # Return empty if we can't parse, to avoid crashing.
    except Exception as e:
        log.error(f"An unexpected error occurred during Ruff execution: {e}")
        return code, []
    finally:
        if os.path.exists(tmp_file_path):
            os.remove(tmp_file_path)

--- END OF FILE ./src/core/ruff_linter.py ---

--- START OF FILE ./src/core/secrets_service.py ---
# src/core/secrets_service.py
"""
Encrypted secrets management service.
Stores API keys and sensitive config encrypted in the database.

Constitutional Principle: Safe by Default
- All secrets encrypted at rest using Fernet (symmetric encryption)
- Audit trail for all secret access
- Master key never stored in database
"""

from __future__ import annotations

import base64
import os
from datetime import datetime
from typing import Optional

from cryptography.fernet import Fernet, InvalidToken
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from shared.logger import getLogger

log = getLogger(__name__)


# ID: secrets-service-001
class SecretsService:
    """
    Manages encrypted secrets in the database.

    Usage:
        secrets = SecretsService(master_key)
        await secrets.set_secret("anthropic.api_key", "sk-ant-...")
        api_key = await secrets.get_secret("anthropic.api_key")
    """

    def __init__(self, master_key: str):
        """
        Initialize with master encryption key.

        Args:
            master_key: Base64-encoded Fernet key (generate with: Fernet.generate_key())

        Raises:
            ValueError: If master_key is invalid
        """
        try:
            self.cipher = Fernet(master_key.encode())
        except Exception as e:
            raise ValueError(f"Invalid master key format: {e}")

    @staticmethod
    def generate_master_key() -> str:
        """
        Generate a new Fernet master key.

        Returns:
            Base64-encoded key string (save to CORE_MASTER_KEY in .env)
        """
        return Fernet.generate_key().decode()

    def encrypt(self, plaintext: str) -> str:
        """Encrypt a secret value."""
        if not plaintext:
            raise ValueError("Cannot encrypt empty value")
        return self.cipher.encrypt(plaintext.encode()).decode()

    def decrypt(self, ciphertext: str) -> str:
        """Decrypt a secret value."""
        if not ciphertext:
            raise ValueError("Cannot decrypt empty value")
        try:
            return self.cipher.decrypt(ciphertext.encode()).decode()
        except InvalidToken:
            raise ValueError("Decryption failed - wrong master key or corrupted data")

    async def set_secret(
        self,
        db: AsyncSession,
        key: str,
        value: str,
        description: Optional[str] = None,
    ) -> None:
        """
        Store an encrypted secret in the database.

        Args:
            db: Database session
            key: Secret identifier (e.g., "anthropic.api_key")
            value: Plaintext secret value
            description: Optional human-readable description
        """
        encrypted_value = self.encrypt(value)

        query = text("""
            INSERT INTO core.runtime_settings (key, value, description, is_secret, last_updated)
            VALUES (:key, :value, :description, true, NOW())
            ON CONFLICT (key)
            DO UPDATE SET
                value = EXCLUDED.value,
                description = EXCLUDED.description,
                last_updated = NOW()
        """)

        await db.execute(
            query,
            {
                "key": key,
                "value": encrypted_value,
                "description": description or f"Encrypted secret: {key}",
            },
        )
        await db.commit()

        log.info(f"Secret '{key}' stored successfully (encrypted)")

    async def get_secret(
        self,
        db: AsyncSession,
        key: str,
        audit_context: Optional[str] = None,
    ) -> str:
        """
        Retrieve and decrypt a secret from the database.

        Args:
            db: Database session
            key: Secret identifier
            audit_context: Optional context for audit log (e.g., "planner_agent")

        Returns:
            Decrypted secret value

        Raises:
            KeyError: If secret not found
            ValueError: If decryption fails
        """
        query = text("""
            SELECT value FROM core.runtime_settings
            WHERE key = :key AND is_secret = true
        """)

        result = await db.execute(query, {"key": key})
        row = result.fetchone()

        if not row:
            raise KeyError(f"Secret '{key}' not found in database")

        # Audit the access
        await self._audit_secret_access(db, key, audit_context)

        # Decrypt and return
        return self.decrypt(row[0])

    async def delete_secret(self, db: AsyncSession, key: str) -> None:
        """
        Delete a secret from the database.

        Args:
            db: Database session
            key: Secret identifier
        """
        query = text("""
            DELETE FROM core.runtime_settings
            WHERE key = :key AND is_secret = true
        """)

        result = await db.execute(query, {"key": key})
        await db.commit()

        if result.rowcount == 0:
            log.warning(f"Secret '{key}' not found for deletion")
        else:
            log.info(f"Secret '{key}' deleted")

    async def list_secrets(self, db: AsyncSession) -> list[dict]:
        """
        List all secret keys (not values!) in the database.

        Returns:
            List of dicts with 'key', 'description', 'last_updated'
        """
        query = text("""
            SELECT key, description, last_updated
            FROM core.runtime_settings
            WHERE is_secret = true
            ORDER BY key
        """)

        result = await db.execute(query)
        return [
            {
                "key": row[0],
                "description": row[1],
                "last_updated": row[2],
            }
            for row in result.fetchall()
        ]

    async def rotate_secret(
        self,
        db: AsyncSession,
        key: str,
        new_value: str,
    ) -> None:
        """
        Rotate a secret (change its value).

        This is a convenience method that archives the old value
        and sets the new one.

        Args:
            db: Database session
            key: Secret identifier
            new_value: New plaintext secret value
        """
        # Archive old value (optional - for audit trail)
        try:
            old_value = await self.get_secret(db, key, audit_context="rotation")
            log.info(f"Rotating secret '{key}' (old value archived)")
        except KeyError:
            log.warning(f"Rotating secret '{key}' (no previous value)")

        # Set new value
        await self.set_secret(db, key, new_value, description=f"Rotated on {datetime.utcnow()}")

    async def _audit_secret_access(
        self,
        db: AsyncSession,
        key: str,
        context: Optional[str],
    ) -> None:
        """
        Log secret access for audit trail.

        This creates a record in agent_memory for forensics.
        """
        try:
            query = text("""
                INSERT INTO core.agent_memory (
                    cognitive_role,
                    memory_type,
                    content,
                    relevance_score,
                    created_at
                ) VALUES (
                    :role,
                    'fact',
                    :content,
                    1.0,
                    NOW()
                )
            """)

            await db.execute(
                query,
                {
                    "role": context or "system",
                    "content": f"Accessed secret: {key}",
                },
            )
            # Don't commit here - let the caller control transaction
        except Exception as e:
            # Don't fail secret retrieval if audit fails
            log.error(f"Failed to audit secret access: {e}")

    @staticmethod
    async def migrate_from_env(
        db: AsyncSession,
        env_vars: dict[str, str],
        master_key: str,
    ) -> dict[str, str]:
        """
        Migrate secrets from environment variables to encrypted database.

        Args:
            db: Database session
            env_vars: Dict of env var names to values (e.g., {"ANTHROPIC_API_KEY": "sk-..."})
            master_key: Master encryption key

        Returns:
            Dict of migrated keys to their new database keys
        """
        service = SecretsService(master_key)
        migrated = {}

        # Map env var names to database keys
        env_to_db_key = {
            "ANTHROPIC_CLAUDE_SONNET_API_KEY": "anthropic.api_key",
            "DEEPSEEK_CHAT_API_KEY": "deepseek_chat.api_key",
            "DEEPSEEK_CODER_API_KEY": "deepseek_coder.api_key",
            "OLLAMA_LOCAL_API_KEY": "ollama.api_key",
            "LOCAL_EMBEDDING_API_KEY": "embedding.api_key",
        }

        for env_name, db_key in env_to_db_key.items():
            if env_name in env_vars and env_vars[env_name]:
                await service.set_secret(
                    db,
                    db_key,
                    env_vars[env_name],
                    description=f"Migrated from {env_name}",
                )
                migrated[env_name] = db_key
                log.info(f"Migrated {env_name} → {db_key}")

        return migrated


# ID: secrets-service-002
async def get_secrets_service(db: AsyncSession) -> SecretsService:
    """
    Factory function to create SecretsService with master key from environment.

    This is the primary way to instantiate the service in production code.

    Usage:
        secrets = await get_secrets_service(db)
        api_key = await secrets.get_secret(db, "anthropic.api_key")

    Raises:
        RuntimeError: If CORE_MASTER_KEY not set in environment
    """
    master_key = os.getenv("CORE_MASTER_KEY")
    if not master_key:
        raise RuntimeError(
            "CORE_MASTER_KEY not found in environment. "
            "Generate one with: python -c 'from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())'"
        )

    return SecretsService(master_key)
--- END OF FILE ./src/core/secrets_service.py ---

--- START OF FILE ./src/core/self_correction_engine.py ---
# src/core/self_correction_engine.py
"""
Handles automated correction of code failures by generating and validating LLM-suggested repairs based on structured violation data.
"""

from __future__ import annotations

import json
from typing import TYPE_CHECKING

from core.cognitive_service import CognitiveService
from core.prompt_pipeline import PromptPipeline
from core.validation_pipeline import validate_code_async
from shared.config import settings
from shared.utils.parsing import parse_write_blocks

if TYPE_CHECKING:
    from features.governance.audit_context import AuditorContext


REPO_PATH = settings.REPO_PATH
pipeline = PromptPipeline(repo_path=REPO_PATH)


# ID: c60020bd-5910-406e-ae64-ca227982142d
async def attempt_correction(
    failure_context: dict,
    cognitive_service: CognitiveService,
    auditor_context: "AuditorContext",
) -> dict:
    """Attempts to fix a failed validation or test result using an enriched LLM prompt."""
    # --- THIS IS THE REAL FIX ---
    # Call the asynchronous version of the method: aget_client_for_role
    generator = await cognitive_service.aget_client_for_role("Coder")
    # --- END OF REAL FIX ---

    file_path = failure_context.get("file_path")
    code = failure_context.get("code")
    violations = failure_context.get("violations", [])

    if not all([file_path, code, violations]):
        return {
            "status": "error",
            "message": "Missing required failure context fields.",
        }

    correction_prompt = (
        f"You are CORE's self-correction agent.\n\nA recent code generation attempt failed validation.\n"
        f"Please analyze the violations and fix the code below.\n\nFile: {file_path}\n\n"
        f"[[violations]]\n{json.dumps(violations, indent=2)}\n[[/violations]]\n\n"
        f"[[code]]\n{code.strip()}\n[[/code]]\n\n"
        f"Respond with the full, corrected code in a single write block:\n[[write:{file_path}]]\n<corrected code here>\n[[/write]]"
    )

    final_prompt = pipeline.process(correction_prompt)
    llm_output = await generator.make_request_async(final_prompt, user_id="auto_repair")

    write_blocks = parse_write_blocks(llm_output)

    if not write_blocks:
        return {
            "status": "error",
            "message": "LLM did not produce a valid correction in a write block.",
        }

    path, fixed_code = list(write_blocks.items())[0]

    validation_result = await validate_code_async(
        path, fixed_code, auditor_context=auditor_context
    )
    if validation_result["status"] == "dirty":
        return {
            "status": "correction_failed_validation",
            "message": "The corrected code still fails validation.",
            "violations": validation_result["violations"],
        }

    # This is the simplified return value that the ExecutionAgent now expects.
    return {
        "status": "success",
        "code": validation_result["code"],
        "message": "Corrected code generated and validated successfully.",
    }

--- END OF FILE ./src/core/self_correction_engine.py ---

--- START OF FILE ./src/core/service_registry.py ---
# src/core/service_registry.py
"""
Provides a centralized, lazily-initialized service registry for CORE.
This acts as a simple dependency injection container.
"""

from __future__ import annotations

import asyncio
import importlib
from pathlib import Path
from typing import Any, Dict

from services.repositories.db.engine import get_session
from shared.config import settings
from shared.logger import getLogger
from sqlalchemy import text

log = getLogger("service_registry")


# ID: 06afd27a-3b75-4e6c-a335-7e471365c65d
class ServiceRegistry:
    """A simple singleton service locator and DI container."""

    _instances: Dict[str, Any] = {}
    _service_map: Dict[str, str] = {}
    _initialized = False
    _lock = asyncio.Lock()

    def __init__(self, repo_path: Path | None = None):
        self.repo_path = repo_path or settings.REPO_PATH

    async def _initialize_from_db(self):
        """Loads the service map from the database on first access."""
        async with self._lock:
            if self._initialized:
                return

            log.info("Initializing ServiceRegistry from database...")
            try:
                async with get_session() as session:
                    result = await session.execute(
                        text("SELECT name, implementation FROM core.runtime_services")
                    )
                    for row in result:
                        self._service_map[row.name] = row.implementation
                self._initialized = True
                log.info(
                    f"ServiceRegistry initialized with {len(self._service_map)} services."
                )
            except Exception as e:
                log.critical(
                    f"Failed to initialize ServiceRegistry from DB: {e}", exc_info=True
                )
                # In a real app, you might exit or have a fallback
                self._initialized = False

    def _import_class(self, class_path: str):
        """Dynamically imports a class from a string path."""
        module_path, class_name = class_path.rsplit(".", 1)
        module = importlib.import_module(module_path)
        return getattr(module, class_name)

    # ID: fc217b8c-bba2-4600-aac9-4630903e83d2
    async def get_service(self, name: str) -> Any:
        """Lazily initializes and returns a singleton instance of a service."""
        if not self._initialized:
            await self._initialize_from_db()

        if name not in self._instances:
            if name not in self._service_map:
                raise ValueError(f"Service '{name}' not found in registry.")

            class_path = self._service_map[name]
            service_class = self._import_class(class_path)

            if name in ["knowledge_service", "cognitive_service", "auditor"]:
                self._instances[name] = service_class(self.repo_path)
            else:
                self._instances[name] = service_class()

            log.debug(f"Lazily initialized service: {name}")

        return self._instances[name]


# Global instance
service_registry = ServiceRegistry()

--- END OF FILE ./src/core/service_registry.py ---

--- START OF FILE ./src/core/syntax_checker.py ---
# src/core/syntax_checker.py
"""
Handles Python syntax validation for code before it's staged for write/commit operations.
"""

from __future__ import annotations

# --- THIS IS THE FIX ---
# Add all the necessary imports that were missing.
import ast
from typing import Any, Dict, List

Violation = Dict[str, Any]
# --- END OF FIX ---


# ID: c1e335fb-1ee0-4e76-b6bd-9ed7a7494f14
def check_syntax(file_path: str, code: str) -> List[Violation]:
    """Checks the given Python code for syntax errors and returns a list of violations, if any."""
    """
    Checks whether the given code has valid Python syntax.

    Args:
        file_path (str): File name (used to detect .py files).
        code (str): Source code string.

    Returns:
        A list of violation dictionaries. An empty list means the syntax is valid.
    """
    if not file_path.endswith(".py"):
        return []

    try:
        ast.parse(code)
        return []
    except SyntaxError as e:
        error_line = e.text.strip() if e.text else "<source unavailable>"
        return [
            {
                "rule": "E999",  # Ruff's code for syntax errors
                "message": f"Invalid Python syntax: {e.msg} near '{error_line}'",
                "line": e.lineno,
                "severity": "error",
            }
        ]

--- END OF FILE ./src/core/syntax_checker.py ---

--- START OF FILE ./src/core/test_runner.py ---
# src/core/test_runner.py
"""
Executes pytest on the project's test suite and captures structured results for
system integrity verification.
"""

from __future__ import annotations

import datetime
import json
import os
import subprocess
from pathlib import Path
from typing import Dict

from shared.config import settings
from shared.logger import getLogger

log = getLogger(__name__)


# ID: f22f2743-a396-4ca4-b88b-94cd76ee8572
def run_tests(silent: bool = True) -> Dict[str, str]:
    """Executes pytest on the tests/ directory and returns a structured result."""
    log.info("🧪 Running tests with pytest...")
    result = {
        "exit_code": "-1",
        "stdout": "",
        "stderr": "",
        "summary": "❌ Unknown error",
        "timestamp": datetime.datetime.utcnow().isoformat(),
    }

    repo_root = Path(__file__).resolve().parents[2]
    tests_path = repo_root / "tests"
    cmd = ["pytest", str(tests_path), "--tb=short", "-q"]

    timeout = os.getenv("TEST_RUNNER_TIMEOUT")
    try:
        timeout_val = int(timeout) if timeout else None
    except ValueError:
        timeout_val = None

    try:
        proc = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False,
            timeout=timeout_val,
        )
        result["exit_code"] = str(proc.returncode)
        result["stdout"] = proc.stdout.strip()
        result["stderr"] = proc.stderr.strip()
        result["summary"] = _summarize(proc.stdout)

        if not silent:
            log.info(f"Pytest stdout:\n{proc.stdout}")
            if proc.stderr:
                log.warning(f"Pytest stderr:\n{proc.stderr}")

    except subprocess.TimeoutExpired:
        result["stderr"] = "Test run timed out."
        result["summary"] = "⏰ Timeout"
        log.error("Pytest run timed out.")
    except FileNotFoundError:
        result["stderr"] = "pytest is not installed or not found in PATH."
        result["summary"] = "❌ Pytest not available"
        log.error("Pytest command not found. Is it installed in the environment?")
    except Exception as e:
        result["stderr"] = str(e)
        result["summary"] = "❌ Test run error"
        log.error(f"An unexpected error occurred during test run: {e}", exc_info=True)

    _log_test_result(result)
    _store_failure_if_any(result)

    log.info(f"🏁 Test run complete. Summary: {result['summary']}")
    return result


def _summarize(output: str) -> str:
    """Parses pytest output to find the final summary line."""
    lines = output.strip().splitlines()
    for line in reversed(lines):
        if "passed" in line or "failed" in line or "error" in line:
            return line.strip()
    return "No test summary found."


def _log_test_result(data: Dict[str, str]):
    """Appends a JSON record of a test run to the persistent log file."""
    try:
        log_path = Path(settings.CORE_ACTION_LOG_PATH)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(data) + "\n")
    except Exception as e:
        log.warning(f"Failed to write to persistent test log file: {e}", exc_info=True)


def _store_failure_if_any(data: Dict[str, str]):
    """Saves the details of a failed test run to a dedicated file for easy access."""
    try:
        failure_path = Path("logs/test_failures.json")
        if data.get("exit_code") != "0":
            failure_path.parent.mkdir(parents=True, exist_ok=True)
            payload = {
                "summary": data.get("summary"),
                "stdout": data.get("stdout"),
                "timestamp": data.get("timestamp"),
            }
            failure_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        elif failure_path.exists():
            failure_path.unlink(missing_ok=True)
    except Exception as e:
        log.warning(f"Could not save test failure data: {e}", exc_info=True)

--- END OF FILE ./src/core/test_runner.py ---

--- START OF FILE ./src/core/validation_pipeline.py ---
# src/core/validation_pipeline.py
"""
A context-aware validation pipeline that applies different validation steps based on file type.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any, Dict

from shared.logger import getLogger

from .file_classifier import get_file_classification
from .python_validator import validate_python_code_async
from .yaml_validator import validate_yaml_code

if TYPE_CHECKING:
    from features.governance.audit_context import AuditorContext


log = getLogger(__name__)


# ID: 50694eab-72fa-4e20-8f95-3b9f3d7bcb5e
async def validate_code_async(
    file_path: str,
    code: str,
    quiet: bool = False,
    auditor_context: "AuditorContext" | None = None,
) -> Dict[str, Any]:
    """Validate a file's code by routing it to the appropriate validation pipeline."""
    classification = get_file_classification(file_path)
    if not quiet:
        log.debug(f"Validation: Classifying '{file_path}' as '{classification}'.")

    final_code = code
    violations = []

    if classification == "python":
        if not auditor_context:
            raise ValueError("AuditorContext is required for validating Python code.")
        final_code, violations = await validate_python_code_async(
            file_path, code, auditor_context
        )
    elif classification == "yaml":
        final_code, violations = validate_yaml_code(code)

    is_dirty = any(v.get("severity") == "error" for v in violations)
    status = "dirty" if is_dirty else "clean"

    return {"status": status, "violations": violations, "code": final_code}

--- END OF FILE ./src/core/validation_pipeline.py ---

--- START OF FILE ./src/core/validation_policies.py ---
# src/core/validation_policies.py
"""
Policy-aware validation logic for enforcing safety and security policies.
This module is given pre-loaded policies and scans AST nodes for violations.
"""

from __future__ import annotations

import ast
from pathlib import Path
from typing import Any, Dict, List

Violation = Dict[str, Any]


# ID: dcff1afd-963d-419c-8f66-31978115cfc9
class PolicyValidator:
    """Handles policy-aware validation including safety checks and forbidden patterns."""

    def __init__(self, safety_policy_rules: List[Dict]):
        """
        Initialize the policy validator with pre-loaded safety policy rules.
        """
        self.safety_rules = safety_policy_rules

    def _get_full_attribute_name(self, node: ast.Attribute) -> str:
        """Recursively builds the full name of an attribute call."""
        parts = []
        current = node
        while isinstance(current, ast.Attribute):
            parts.insert(0, current.attr)
            current = current.value
        if isinstance(current, ast.Name):
            parts.insert(0, current.id)
        return ".".join(parts)

    def _find_dangerous_patterns(
        self, tree: ast.AST, file_path: str
    ) -> List[Violation]:
        """Scans the AST for calls and imports forbidden by safety policies."""
        violations: List[Violation] = []
        rules = self.safety_rules

        forbidden_calls = set()
        forbidden_imports = set()

        for rule in rules:
            exclude_patterns = [
                p
                for p in rule.get("scope", {}).get("exclude", [])
                if isinstance(p, str)
            ]
            is_excluded = any(Path(file_path).match(p) for p in exclude_patterns)

            if is_excluded:
                continue

            if rule.get("id") == "no_dangerous_execution":
                patterns = {
                    p.replace("(", "")
                    for p in rule.get("detection", {}).get("patterns", [])
                }
                forbidden_calls.update(patterns)
            elif rule.get("id") == "no_unsafe_imports":
                patterns = {
                    imp.split(" ")[-1]
                    for imp in rule.get("detection", {}).get("forbidden", [])
                }
                forbidden_imports.update(patterns)

        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                full_call_name = ""
                if isinstance(node.func, ast.Name):
                    full_call_name = node.func.id
                elif isinstance(node.func, ast.Attribute):
                    full_call_name = self._get_full_attribute_name(node.func)

                if full_call_name in forbidden_calls:
                    violations.append(
                        {
                            "rule": "safety.dangerous_call",
                            "message": f"Use of forbidden call: '{full_call_name}'",
                            "line": node.lineno,
                            "severity": "error",
                        }
                    )
            elif isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name.split(".")[0] in forbidden_imports:
                        violations.append(
                            {
                                "rule": "safety.forbidden_import",
                                "message": f"Import of forbidden module: '{alias.name}'",
                                "line": node.lineno,
                                "severity": "error",
                            }
                        )
            elif isinstance(node, ast.ImportFrom):
                if node.module and node.module.split(".")[0] in forbidden_imports:
                    violations.append(
                        {
                            "rule": "safety.forbidden_import",
                            "message": f"Import from forbidden module: '{node.module}'",
                            "line": node.lineno,
                            "severity": "error",
                        }
                    )
        return violations

    # ID: d6059c1e-83ab-4c9a-8ebf-e596fa79494d
    def check_semantics(self, code: str, file_path: str) -> List[Violation]:
        """Runs all policy-aware semantic checks on a string of Python code."""
        try:
            tree = ast.parse(code)
        except SyntaxError:
            return []
        return self._find_dangerous_patterns(tree, file_path)

--- END OF FILE ./src/core/validation_policies.py ---

--- START OF FILE ./src/core/validation_quality.py ---
# src/core/validation_quality.py
"""
Code quality validation checks for maintainability and clarity.

This module provides quality-focused validation checks such as detecting
TODO comments and other code clarity issues that don't affect functionality
but impact maintainability.
"""

from __future__ import annotations

from typing import Any, Dict, List

Violation = Dict[str, Any]


# ID: 0c6502f3-6d97-41e8-a618-6ae63a489e8b
class QualityChecker:
    """Handles code quality and clarity validation checks."""

    # ID: 972208ef-200e-4836-851d-f82f24e3b779
    def check_for_todo_comments(self, code: str) -> List[Violation]:
        """Scans source code for TODO/FIXME comments and returns them as violations.

        Args:
            code: The source code to scan for TODO comments

        Returns:
            List of violations for each TODO/FIXME comment found
        """
        violations: List[Violation] = []
        for i, line in enumerate(code.splitlines(), 1):
            if "#" in line:
                comment = line.split("#", 1)[1]
                if "TODO" in comment or "FIXME" in comment:
                    violations.append(
                        {
                            "rule": "clarity.no_todo_comments",
                            "message": f"Unresolved '{comment.strip()}' on line {i}",
                            "line": i,
                            "severity": "warning",
                        }
                    )
        return violations

--- END OF FILE ./src/core/validation_quality.py ---

--- START OF FILE ./src/core/yaml_validator.py ---
# src/core/yaml_validator.py
"""
YAML validation pipeline.

This module provides validation functionality specifically for YAML files,
checking for syntax errors and structural issues.
"""

from __future__ import annotations

from typing import Any, Dict, List, Tuple

import yaml

Violation = Dict[str, Any]


# ID: f3bbf4e9-71b5-4dad-8ad8-ee93b90dd8c0
def validate_yaml_code(code: str) -> Tuple[str, List[Violation]]:
    """Validation pipeline for YAML code.

    This function validates YAML syntax and structure, returning any violations
    found during the validation process.

    Args:
        code: The YAML code to validate

    Returns:
        A tuple containing the original code and list of violations
    """
    violations = []
    try:
        yaml.safe_load(code)
    except yaml.YAMLError as e:
        violations.append(
            {
                "rule": "syntax.yaml",
                "message": f"Invalid YAML format: {e}",
                "line": e.problem_mark.line + 1 if e.problem_mark else 0,
                "severity": "error",
            }
        )
    return code, violations

--- END OF FILE ./src/core/yaml_validator.py ---

--- START OF FILE ./src/features/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/features/__init__.py ---

--- START OF FILE ./src/features/autonomy/autonomous_developer.py ---
# src/features/autonomy/autonomous_developer.py
"""
Provides a dedicated, reusable service for orchestrating the full autonomous
development cycle, from goal to implemented code.
"""

from __future__ import annotations

from core.agents.coder_agent import CoderAgent
from core.agents.execution_agent import ExecutionAgent
from core.agents.plan_executor import PlanExecutor
from core.agents.planner_agent import PlannerAgent
from core.agents.reconnaissance_agent import ReconnaissanceAgent
from core.prompt_pipeline import PromptPipeline
from services.database.models import Task
from services.database.session_manager import get_session
from shared.context import CoreContext
from shared.logger import getLogger
from shared.models import PlanExecutionError
from sqlalchemy import update

log = getLogger("autonomous_developer")


# ID: 7a8b9c0d-1e2f-3a4b-5c6d-7e8f9a0b
# ID: f40722fc-751e-4643-81d7-99509b5baa91
async def develop_from_goal(
    context: CoreContext, goal: str, task_id: str | None = None
):
    """
    Runs the full, end-to-end autonomous development cycle for a given goal.
    This is the single source of truth for the A2 development loop.
    Now includes robust error handling and database status updates.
    """
    try:
        log.info(f"🚀 Initiating autonomous development cycle for goal: '{goal}'")

        # --- START OF NEW ROUTER LOGIC ---
        goal_lower = goal.lower()
        if "create" in goal_lower and (
            "new file" in goal_lower or "new function" in goal_lower
        ):
            log.info(
                "   -> Intent classified as 'CREATE_FILE'. Using specialized planner."
            )
            # Use a dummy recon report since we know we're creating something new
            context_report = "# Reconnaissance Report\n\n- No relevant files found. Proceeding with file creation."
            # Use the specialized planner agent for creation
            planner = PlannerAgent(context.cognitive_service)
            # Temporarily override the prompt template for this specific task
            planner.prompt_template = context.settings.get_path(
                "mind.prompts.create_file_planner"
            ).read_text(encoding="utf-8")
        else:
            log.info(
                "   -> Intent classified as 'GENERAL'. Using standard reconnaissance and planning."
            )
            # 1. Reconnaissance (only for general tasks now)
            recon_agent = ReconnaissanceAgent(
                await context.knowledge_service.get_graph(), context.cognitive_service
            )
            context_report = await recon_agent.generate_report(goal)
            # 2. Planning
            planner = PlannerAgent(context.cognitive_service)
        # --- END OF NEW ROUTER LOGIC ---

        plan = await planner.create_execution_plan(goal, context_report)
        if not plan:
            raise PlanExecutionError(
                "PlannerAgent failed to create a valid execution plan."
            )

        # 3. Execution (this part remains the same)
        prompt_pipeline = PromptPipeline(context.git_service.repo_path)
        plan_executor = PlanExecutor(
            context.file_handler, context.git_service, context.planner_config
        )
        coder_agent = CoderAgent(
            cognitive_service=context.cognitive_service,
            prompt_pipeline=prompt_pipeline,
            auditor_context=context.auditor_context,
        )
        executor_agent = ExecutionAgent(
            coder_agent=coder_agent,
            plan_executor=plan_executor,
            auditor_context=context.auditor_context,
        )

        success, message = await executor_agent.execute_plan(
            high_level_goal=goal, plan=plan
        )

        if not success:
            raise PlanExecutionError(f"Execution failed: {message}")

        if task_id:
            async with get_session() as session:
                async with session.begin():
                    stmt = (
                        update(Task)
                        .where(Task.id == task_id)
                        .values(status="completed")
                    )
                    await session.execute(stmt)

    except (PlanExecutionError, Exception) as e:
        error_message = f"Autonomous development cycle failed: {e}"
        log.error(error_message, exc_info=True)
        if task_id:
            async with get_session() as session:
                async with session.begin():
                    stmt = (
                        update(Task)
                        .where(Task.id == task_id)
                        .values(status="failed", failure_reason=error_message)
                    )
                    await session.execute(stmt)

--- END OF FILE ./src/features/autonomy/autonomous_developer.py ---

--- START OF FILE ./src/features/autonomy/micro_proposal_executor.py ---
# src/features/autonomy/micro_proposal_executor.py
"""
Service for validating and applying micro-proposals to enable safe, autonomous
changes to the CORE codebase, adhering to the micro_proposal_policy.yaml and
enforcing safe_by_default and reason_with_purpose principles.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional

from shared.logger import getLogger
from shared.models import CheckResult
from shared.path_utils import get_repo_root
from shared.utils.yaml_processor import strict_yaml_processor

log = getLogger("micro_proposal_executor")


@dataclass
# ID: 5b337f4b-e1c5-43f2-a26e-17bc7ceee474
class MicroProposal:
    """Internal data structure for a micro-proposal with target file, action, and content."""

    file_path: str
    action: str
    content: str
    validation_report_id: Optional[str] = None


# ID: 9f3a2e7b-5c4d-4b9e-a2f0-8d7a9e3d6e2c
class MicroProposalExecutor:
    """
    Validates and applies micro-proposals for safe, autonomous changes as defined
    by micro_proposal_policy.yaml, ensuring compliance with safe_by_default and
    reason_with_purpose principles.
    """

    def __init__(self, repo_root: Optional[Path] = None) -> None:
        """
        Initialize the executor with the repository root and load the policy.

        Args:
            repo_root: Path to the repository root, defaults to detected root.
        """
        self.repo_root = repo_root or get_repo_root()
        self.policy_path = (
            self.repo_root / ".intent/charter/policies/agent/micro_proposal_policy.yaml"
        )
        self.policy = self._load_policy()
        log.debug("MicroProposalExecutor initialized")

    def _load_policy(self) -> Dict:
        """
        Load and validate the micro_proposal_policy.yaml.

        Returns:
            Dict: The parsed policy content.

        Raises:
            ValueError: If the policy file is missing or invalid.
        """
        try:
            policy = strict_yaml_processor.load_strict(self.policy_path)
            if not policy:
                raise ValueError("Micro-proposal policy is empty or invalid")
            return policy
        except ValueError as e:
            log.error(f"Failed to load micro-proposal policy: {e}")
            raise

    def _check_safe_actions(self, action: str) -> CheckResult:
        """
        Verify if the action is in the allowed_actions list.

        Args:
            action: The action to validate.

        Returns:
            CheckResult: Result of the safe actions check.
        """
        safe_actions_rule = next(
            (rule for rule in self.policy["rules"] if rule["id"] == "safe_actions"),
            None,
        )
        if not safe_actions_rule:
            return CheckResult(
                policy_id=self.policy["policy_id"],
                rule_id="safe_actions",
                severity="error",
                message="Safe actions rule not found in policy",
                path=None,
            )

        if action not in safe_actions_rule["allowed_actions"]:
            return CheckResult(
                policy_id=self.policy["policy_id"],
                rule_id="safe_actions",
                severity="error",
                message=f"Action '{action}' is not in allowed actions: {safe_actions_rule['allowed_actions']}",
                path=None,
            )
        return CheckResult(
            policy_id=self.policy["policy_id"],
            rule_id="safe_actions",
            severity="pass",
            message=f"Action '{action}' is allowed",
            path=None,
        )

    def _check_safe_paths(self, file_path: str) -> CheckResult:
        """
        Verify if the file_path complies with allowed and forbidden paths.

        Args:
            file_path: The file path to validate.

        Returns:
            CheckResult: Result of the safe paths check.
        """
        from fnmatch import fnmatch

        safe_paths_rule = next(
            (rule for rule in self.policy["rules"] if rule["id"] == "safe_paths"), None
        )
        if not safe_paths_rule:
            return CheckResult(
                policy_id=self.policy["policy_id"],
                rule_id="safe_paths",
                severity="error",
                message="Safe paths rule not found in policy",
                path=file_path,
            )

        path_obj = Path(file_path)
        is_allowed = any(
            fnmatch(str(path_obj), pattern)
            for pattern in safe_paths_rule["allowed_paths"]
        )
        is_forbidden = any(
            fnmatch(str(path_obj), pattern)
            for pattern in safe_paths_rule["forbidden_paths"]
        )

        if is_forbidden:
            return CheckResult(
                policy_id=self.policy["policy_id"],
                rule_id="safe_paths",
                severity="error",
                message=f"File path '{file_path}' matches forbidden pattern",
                path=file_path,
            )
        if not is_allowed:
            return CheckResult(
                policy_id=self.policy["policy_id"],
                rule_id="safe_paths",
                severity="error",
                message=f"File path '{file_path}' does not match any allowed pattern",
                path=file_path,
            )
        return CheckResult(
            policy_id=self.policy["policy_id"],
            rule_id="safe_paths",
            severity="pass",
            message=f"File path '{file_path}' is allowed",
            path=file_path,
        )

    def _check_validation_report(self, report_id: Optional[str]) -> CheckResult:
        """
        Verify if a validation report ID is provided and valid (placeholder).

        Args:
            report_id: The validation report ID to check.

        Returns:
            CheckResult: Result of the validation report check.
        """
        if not report_id:
            return CheckResult(
                policy_id=self.policy["policy_id"],
                rule_id="require_validation",
                severity="error",
                message="No validation report ID provided",
                path=None,
            )
        # Placeholder for actual validation report check (e.g., query DB or file)
        return CheckResult(
            policy_id=self.policy["policy_id"],
            rule_id="require_validation",
            severity="pass",
            message=f"Validation report '{report_id}' accepted (placeholder)",
            path=None,
        )

    # ID: 7c2e8d9a-6f3e-4c7a-b3f1-9e8a7f4c5d3b
    def validate_proposal(self, proposal: MicroProposal) -> List[CheckResult]:
        """
        Validate a micro-proposal against safe_actions, safe_paths, and
        require_validation rules from micro_proposal_policy.yaml.

        Args:
            proposal: The MicroProposal to validate.

        Returns:
            List[CheckResult]: List of validation results detailing compliance or violations.
        """
        results = []
        log.debug(
            f"Validating micro-proposal for action '{proposal.action}' on '{proposal.file_path}'"
        )

        # Check safe actions
        results.append(self._check_safe_actions(proposal.action))

        # Check safe paths
        results.append(self._check_safe_paths(proposal.file_path))

        # Check validation report
        results.append(self._check_validation_report(proposal.validation_report_id))

        # Log validation outcome
        errors = [r for r in results if r.severity == "error"]
        if errors:
            log.error(
                f"Micro-proposal validation failed: {[(r.rule_id, r.message) for r in errors]}"
            )
        else:
            log.info("Micro-proposal passed all validation checks")

        return results

    # ID: 5d4f9e8b-8c2f-4d9a-a4e2-0f7b6a5c4e3a
    async def apply_proposal(self, proposal: MicroProposal) -> bool:
        """
        Apply a validated micro-proposal by executing the specified action.

        Args:
            proposal: The MicroProposal to apply, expected to have passed validation.

        Returns:
            bool: True if the proposal was applied successfully, False otherwise.
        """
        validation_results = self.validate_proposal(proposal)
        if any(result.severity == "error" for result in validation_results):
            log.error("Cannot apply proposal due to validation errors")
            return False

        try:
            if proposal.action == "autonomy.self_healing.format_code":
                # Placeholder for formatting logic (e.g., invoke black)
                Path(proposal.file_path).write_text(proposal.content, encoding="utf-8")
                log.info(f"Applied format_code to {proposal.file_path}")
            elif proposal.action == "autonomy.self_healing.fix_docstrings":
                # Placeholder for docstring fixing logic
                Path(proposal.file_path).write_text(proposal.content, encoding="utf-8")
                log.info(f"Applied fix_docstrings to {proposal.file_path}")
            elif proposal.action == "autonomy.self_healing.fix_headers":
                # Placeholder for header fixing logic
                Path(proposal.file_path).write_text(proposal.content, encoding="utf-8")
                log.info(f"Applied fix_headers to {proposal.file_path}")
            else:
                log.error(f"Unsupported action: {proposal.action}")
                return False

            return True
        except Exception as e:
            log.error(f"Failed to apply micro-proposal: {e}")
            return False

--- END OF FILE ./src/features/autonomy/micro_proposal_executor.py ---

--- START OF FILE ./src/features/demo/hello_world.py ---
# src/features/demo/hello_world.py


# ID: 3615ba5c-4515-4435-b62b-a0e945430872
def print_greeting():
    """Prints a simple greeting to the console."""
    print("Hello from the CORE system!")

--- END OF FILE ./src/features/demo/hello_world.py ---

--- START OF FILE ./src/features/governance/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/features/governance/__init__.py ---

--- START OF FILE ./src/features/governance/audit_context.py ---
# src/features/governance/audit_context.py
"""
AuditorContext: central view of constitutional artifacts and the knowledge graph
for governance checks and audits.
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Any, Dict, List

import yaml
from core.knowledge_service import KnowledgeService
from shared.logger import getLogger
from shared.models import AuditFinding

log = getLogger("audit_context")


# ID: 245a7de6-5465-41d2-a588-2da4cc86d72f
class AuditorContext:
    """
    Provides access to '.intent' artifacts and the in-memory knowledge graph.
    Tests import this symbol directly from features.governance.audit_context.
    """

    def __init__(self, repo_path: Path):
        self.repo_path = Path(repo_path).resolve()
        self.intent_path = self.repo_path / ".intent"
        self.mind_path = self.intent_path / "mind"
        self.charter_path = self.intent_path / "charter"
        self.src_dir = self.repo_path / "src"

        # Optional: last audit results
        self.last_findings: List[AuditFinding] = []

        # Load constitutional data
        self.meta: Dict[str, Any] = self._load_yaml(self.intent_path / "meta.yaml")
        self.policies: Dict[str, Any] = self._load_policies()
        self.source_structure: Dict[str, Any] = self._load_yaml(
            self.mind_path / "knowledge" / "source_structure.yaml"
        )

        # Knowledge graph placeholders
        self.knowledge_graph: Dict[str, Any] = {"symbols": {}}
        self.symbols_list: list = []
        self.symbols_map: dict = {}

        log.debug("AuditorContext initialized.")

    # ID: d5feef3c-c5c7-4f3c-b940-46a154af4778
    async def load_knowledge_graph(self) -> None:
        """Load the knowledge graph from the service (async)."""
        service = KnowledgeService(self.repo_path)
        self.knowledge_graph = await service.get_graph()
        self.symbols_map = self.knowledge_graph.get("symbols", {})
        self.symbols_list = list(self.symbols_map.values())
        log.info(f"Loaded knowledge graph with {len(self.symbols_list)} symbols.")

    # -------------------- helpers -------------------- #

    def _load_yaml(self, path: Path) -> Dict[str, Any]:
        if not path.exists():
            log.warning(f"YAML not found: {path}")
            return {}
        try:
            return yaml.safe_load(path.read_text("utf-8")) or {}
        except Exception as e:
            log.error(f"Failed to parse YAML {path}: {e}")
            return {}

    def _load_policies(self) -> Dict[str, Any]:
        policies_dir = self.charter_path / "policies"
        if not policies_dir.is_dir():
            log.warning(f"Policies directory missing: {policies_dir}")
            return {}
        out: Dict[str, Any] = {}
        for f in policies_dir.glob("**/*_policy.yaml"):
            content = self._load_yaml(f)
            if content:
                out[f.stem] = content
        return out

    @property
    # ID: 95118d68-5e7f-407c-9747-7cce100fe482
    def python_files(self) -> list[Path]:
        paths: list[Path] = []
        for root, dirs, files in os.walk(self.repo_path):
            dirs[:] = [
                d
                for d in dirs
                if d
                not in {".git", "__pycache__", ".pytest_cache", ".venv", "node_modules"}
            ]
            for name in files:
                if name.endswith(".py"):
                    paths.append(Path(root) / name)
        return paths


# Make the export explicit to avoid rare import edge cases in test boot
__all__ = ["AuditorContext"]

--- END OF FILE ./src/features/governance/audit_context.py ---

--- START OF FILE ./src/features/governance/audit_postprocessor.py ---
# src/features/governance/audit_postprocessor.py
"""
Post-processing utilities for Constitutional Auditor findings.

This module provides:
  1) Severity downgrade for "dead-public-symbol" findings when the symbol
     has an allowed `entry_point_type` (as declared in
     .intent/mind/knowledge/entry_point_patterns.yaml).
  2) Auto-generated reports of all symbols auto-ignored-by-pattern to keep
     human visibility without polluting audit_ignore_policy.yaml.

Usage (programmatic):
    from src.features.governance.audit_postprocessor import (
        EntryPointAllowList,
        apply_entry_point_downgrade_and_report,
    )

    processed_findings = apply_entry_point_downgrade_and_report(
        findings=raw_findings,
        symbol_index=symbol_index,  # dict[str, dict] with entry_point_type, etc.
        reports_dir="reports",
        allow_list=EntryPointAllowList.default(),
        dead_rule_ids={"dead_public_symbol", "dead-public-symbol"},
        downgrade_to="info",  # or "warn"
        write_reports=True,
    )

Usage (CLI; optional):
    python -m src.features.governance.audit_postprocessor \
        --in findings.json --symbols symbols.json --out findings.processed.json --reports reports

Expectations:
  - `findings` is a list[dict] with keys like:
        rule_id: str
        severity: str  ("error"|"warn"|"info")
        symbol_key: str  (e.g., "src/foo.py::Foo.bar")
        message: str
        [any other fields are preserved]

  - `symbol_index` is a dict[str, dict] mapping symbol_key -> metadata dict, e.g.:
        {
          "entry_point_type": "cli_wrapper" | "data_model" | ...,
          "entry_point_justification": "matched pattern X",
          "pattern_name": "cli_wrapper_function",
          ...
        }

  - Caller persists the returned list if using programmatic API.

Notes:
  - This is intentionally narrow-scoped and duck-typed to avoid coupling
    to specific internal Finding/Symbol classes.
"""

from __future__ import annotations

import argparse

# src/features/governance/audit_postprocessor.py
import json
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Iterable, Mapping, MutableMapping, Sequence


# ID: 34bd4ecc-62ce-4d54-b72b-bfd2b14324ed
class EntryPointAllowList:
    """
    Allow-list of entry_point_type values for which we downgrade "dead-public-symbol"
    findings. This mirrors the generalized patterns you codified in
    `.intent/mind/knowledge/entry_point_patterns.yaml`.

    You can extend/override via constructor or by using .default() and modifying the set.
    """

    def __init__(self, allowed_types: Iterable[str]) -> None:
        self.allowed = {t.strip() for t in allowed_types if t and t.strip()}

    @classmethod
    # ID: f789f14f-26bc-4cc4-b889-17c55c6c5f77
    def default(cls) -> "EntryPointAllowList":
        return cls(
            allowed_types=[
                # Structural/data constructs
                "data_model",
                "enum",
                "magic_method",
                "visitor_method",
                "base_class",
                "boilerplate_method",
                # CLI & wrappers
                "cli_command",
                "cli_wrapper",
                "registry_accessor",
                # Orchestration/factories
                "orchestrator",
                "factory",
                # Providers/adapters/clients
                "provider_method",
                "client_surface",
                "client_adapter",
                "io_handler",
                "git_adapter",
                "utility_function",
                # Knowledge & governance pipelines
                "knowledge_core",
                "governance_check",
                "auditor_pipeline",
                # Capabilities
                "capability",
            ]
        )

    def __contains__(self, entry_point_type: str | None) -> bool:
        return bool(entry_point_type) and entry_point_type in self.allowed


def _now_iso() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def _safe_symbol_meta(
    symbol_index: Mapping[str, Mapping[str, object]], symbol_key: str
) -> Mapping[str, object]:
    return symbol_index.get(symbol_key, {}) or {}


# ID: b96e63c3-67b3-44b2-a19a-197368a8aba0
def apply_entry_point_downgrade_and_report(
    *,
    findings: Sequence[MutableMapping[str, object]],
    symbol_index: Mapping[str, Mapping[str, object]],
    reports_dir: str | Path = "reports",
    allow_list: EntryPointAllowList | None = None,
    dead_rule_ids: Iterable[str] = ("dead_public_symbol", "dead-public-symbol"),
    downgrade_to: str = "info",  # could be "warn" if you want a gentle nudge
    write_reports: bool = True,
) -> list[MutableMapping[str, object]]:
    """
    Process a list of findings and:
      - Downgrade severity for dead-public-symbol findings whose symbol entry_point_type
        is allowed by policy.
      - Generate a report listing all auto-ignored symbols (grouped by pattern/type).

    Returns a new list of findings (mutating the original items in place).
    """
    allow = allow_list or EntryPointAllowList.default()
    dead_ids = {r.strip() for r in dead_rule_ids if r and r.strip()}
    processed: list[MutableMapping[str, object]] = []
    auto_ignored: list[dict[str, object]] = []

    for f in findings:
        # Duck-typed access:
        rule_id = str(f.get("rule_id", "") or "")
        symbol_key = str(f.get("symbol_key", "") or "")
        severity = str(f.get("severity", "") or "").lower()

        if rule_id in dead_ids and symbol_key:
            meta = _safe_symbol_meta(symbol_index, symbol_key)
            ep_type = str(meta.get("entry_point_type", "") or "")
            pattern_name = str(meta.get("pattern_name", "") or "")
            justification = str(meta.get("entry_point_justification", "") or "")

            if ep_type in allow:
                # Downgrade severity (only if current is higher)
                if severity in {"error", "warn"}:
                    f["severity"] = downgrade_to
                # Track for auto-ignored report
                auto_ignored.append(
                    {
                        "symbol_key": symbol_key,
                        "entry_point_type": ep_type,
                        "pattern_name": pattern_name or None,
                        "justification": justification or None,
                        "original_rule_id": rule_id,
                        "downgraded_to": f["severity"],
                    }
                )

        processed.append(f)

    if write_reports:
        _write_reports(reports_dir, auto_ignored)

    return processed


def _write_reports(
    reports_dir: str | Path, auto_ignored: Sequence[Mapping[str, object]]
) -> None:
    """
    Emit both JSON and Markdown summaries of auto-ignored-by-pattern symbols.
    These are ephemeral audit artifacts (not part of the Constitution).
    """
    reports_path = Path(reports_dir)
    reports_path.mkdir(parents=True, exist_ok=True)

    ts = _now_iso()
    json_path = reports_path / "audit_auto_ignored.json"
    md_path = reports_path / "audit_auto_ignored.md"

    payload = {
        "generated_at": ts,
        "total_auto_ignored": len(auto_ignored),
        "items": list(auto_ignored),
    }
    json_path.write_text(
        json.dumps(payload, indent=2, ensure_ascii=False), encoding="utf-8"
    )

    # Markdown summary grouped by entry_point_type then pattern_name
    grouped: dict[str, dict[str, list[str]]] = {}
    for item in auto_ignored:
        ep = str(item.get("entry_point_type") or "unknown")
        pat = str(item.get("pattern_name") or "—")
        grouped.setdefault(ep, {}).setdefault(pat, []).append(
            str(item.get("symbol_key") or "")
        )

    lines = [
        "# Audit Auto-Ignored Symbols",
        "",
        f"- Generated: `{ts}`",
        f"- Total auto-ignored: **{len(auto_ignored)}**",
        "",
    ]

    for ep_type in sorted(grouped.keys()):
        lines.append(f"## {ep_type}")
        for pattern_name in sorted(grouped[ep_type].keys()):
            syms = grouped[ep_type][pattern_name]
            lines.append(f"### Pattern: {pattern_name}  _(n={len(syms)})_")
            for s in sorted(syms):
                lines.append(f"- `{s}`")
            lines.append("")  # blank line

    md_path.write_text("\n".join(lines), encoding="utf-8")


# -----------------------------
# Optional CLI entrypoint
# -----------------------------
def _load_json(path: Path) -> object:
    return json.loads(path.read_text(encoding="utf-8"))


def _save_json(path: Path, data: object) -> None:
    path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")


# ID: a373b218-70a0-40fb-89e3-6815b9f76d2b
def main(argv: list[str] | None = None) -> int:
    """
    Minimal CLI to post-process existing auditor outputs.

    Example:
      python -m src.features.governance.audit_postprocessor \
        --in reports/audit_findings.json \
        --symbols reports/symbol_index.json \
        --out reports/audit_findings.processed.json \
        --reports reports \
        --downgrade-to info
    """
    parser = argparse.ArgumentParser(description="Audit findings post-processor")
    parser.add_argument(
        "--in", dest="in_path", required=True, help="Input findings JSON path"
    )
    parser.add_argument(
        "--symbols", dest="symbols_path", required=True, help="Symbol index JSON path"
    )
    parser.add_argument(
        "--out",
        dest="out_path",
        required=True,
        help="Output (processed findings) JSON path",
    )
    parser.add_argument(
        "--reports", dest="reports_dir", default="reports", help="Reports directory"
    )
    parser.add_argument(
        "--downgrade-to",
        dest="downgrade_to",
        default="info",
        choices=["info", "warn"],
        help="Target severity for allowed entry points",
    )
    parser.add_argument(
        "--dead-rule-id",
        dest="dead_rule_ids",
        action="append",
        default=None,
        help="Add/override dead-public-symbol rule id(s). Can be passed multiple times.",
    )

    args = parser.parse_args(argv or sys.argv[1:])

    in_path = Path(args.in_path)
    symbols_path = Path(args.symbols_path)
    out_path = Path(args.out_path)
    reports_dir = Path(args.reports_dir)

    findings_obj = _load_json(in_path)
    symbols_obj = _load_json(symbols_path)

    if not isinstance(findings_obj, list):
        print("ERROR: findings JSON must be a list of objects.", file=sys.stderr)
        return 2
    if not isinstance(symbols_obj, dict):
        print(
            "ERROR: symbols JSON must be an object mapping symbol_key to metadata.",
            file=sys.stderr,
        )
        return 2

    processed = apply_entry_point_downgrade_and_report(
        findings=findings_obj,  # type: ignore[arg-type]
        symbol_index=symbols_obj,  # type: ignore[arg-type]
        reports_dir=reports_dir,
        allow_list=EntryPointAllowList.default(),
        dead_rule_ids=args.dead_rule_ids
        or ("dead_public_symbol", "dead-public-symbol"),
        downgrade_to=args.downgrade_to,
        write_reports=True,
    )

    _save_json(out_path, processed)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

--- END OF FILE ./src/features/governance/audit_postprocessor.py ---

--- START OF FILE ./src/features/governance/checks/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/features/governance/checks/__init__.py ---

--- START OF FILE ./src/features/governance/checks/base_check.py ---
# src/features/governance/checks/base_check.py
"""
Provides a shared base class for all constitutional audit checks to inherit from.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from features.governance.audit_context import AuditorContext


# ID: 2cb0374b-a487-4dce-bab1-c2ee8a693b0a
class BaseCheck:
    """A base class for audit checks, providing a shared context."""

    def __init__(self, context: "AuditorContext"):
        """
        Initializes the check with a shared auditor context.
        This common initializer serves the 'dry_by_design' principle.
        """
        self.context = context
        self.repo_root = context.repo_path
        self.intent_path = context.intent_path
        self.src_dir = context.src_dir

--- END OF FILE ./src/features/governance/checks/base_check.py ---

--- START OF FILE ./src/features/governance/checks/capability_coverage.py ---
# src/features/governance/checks/capability_coverage.py
"""
A constitutional audit check to ensure that all capabilities declared in the
project manifest are implemented in the database.
"""

from __future__ import annotations

from typing import List, Set

from shared.models import AuditFinding, AuditSeverity

from features.governance.audit_context import AuditorContext


# ID: 979ce56f-7f3c-40e7-8736-ce219bab6ad8
class CapabilityCoverageCheck:
    """
    Verifies that every capability in the manifest has a corresponding
    implementation entry in the database's symbols table.
    """

    def __init__(self, context: AuditorContext):
        self.context = context

    # ID: e0730fb8-2616-42b2-915b-48f30ff4ac17
    def execute(self) -> List[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings = []

        manifest_path = self.context.mind_path / "project_manifest.yaml"
        if not manifest_path.exists():
            findings.append(
                AuditFinding(
                    check_id="manifest.missing.project_manifest",
                    severity=AuditSeverity.ERROR,
                    message="The project_manifest.yaml file is missing from .intent/mind/.",
                    file_path=str(manifest_path.relative_to(self.context.repo_path)),
                )
            )
            return findings

        manifest_content = self.context._load_yaml(manifest_path)
        declared_capabilities: Set[str] = set(manifest_content.get("capabilities", []))

        # --- THIS IS THE CORRECT LOGIC ---
        # The source of truth for implementation is the database, not code comments.
        # The view aliases 'key' to 'capability', so we must use that name here.
        implemented_capabilities: Set[str] = {
            s["capability"]
            for s in self.context.knowledge_graph.get("symbols", {}).values()
            if s.get("capability")
        }
        # --- END OF CORRECT LOGIC ---

        missing_implementations = declared_capabilities - implemented_capabilities

        for cap_key in sorted(list(missing_implementations)):
            findings.append(
                AuditFinding(
                    check_id="capability.coverage.missing_implementation",
                    severity=AuditSeverity.WARNING,
                    message=f"Capability '{cap_key}' is declared in the manifest but has no implementation linked in the database.",
                    file_path=str(manifest_path.relative_to(self.context.repo_path)),
                )
            )

        return findings

--- END OF FILE ./src/features/governance/checks/capability_coverage.py ---

--- START OF FILE ./src/features/governance/checks/dependency_injection_check.py ---
# src/features/governance/checks/dependency_injection_check.py
"""
A constitutional audit check to enforce the Dependency Injection (DI) policy.
"""

from __future__ import annotations

import ast
from pathlib import Path
from typing import List

from shared.models import AuditFinding, AuditSeverity

from features.governance.checks.base_check import BaseCheck


# ID: 7cdf99c0-ebe2-4a36-9a6f-9d32dd6ee1db
class DependencyInjectionCheck(BaseCheck):
    """
    Ensures that services and features do not directly instantiate their dependencies,
    and do not use forbidden global imports like `get_session`.
    """

    def __init__(self, context):
        super().__init__(context)
        self.policy = self.context.policies.get("dependency_injection_policy", {})

    # ID: b854f7a7-fe4f-4ec6-8b9d-04bd32c98102
    def execute(self) -> List[AuditFinding]:
        """Runs the DI check by scanning source files for policy violations."""
        findings = []
        rules = self.policy.get("rules", [])
        if not rules:
            return findings

        for rule in rules:
            if rule.get("id") == "di.no_direct_instantiation":
                findings.extend(self._check_forbidden_instantiations(rule))
            elif rule.get("id") == "di.no_global_session_import":
                findings.extend(self._check_forbidden_imports(rule))

        return findings

    def _check_forbidden_instantiations(self, rule: dict) -> List[AuditFinding]:
        """Finds direct instantiations of major services."""
        findings = []
        forbidden_calls = set(rule.get("forbidden_instantiations", []))
        scope = rule.get("scope", [])
        exclusions = rule.get("exclusions", [])

        for file_path in self._get_files_in_scope(scope, exclusions):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):
                        if node.func.id in forbidden_calls:
                            findings.append(
                                AuditFinding(
                                    check_id=rule["id"],
                                    severity=AuditSeverity.ERROR,
                                    message=f"Direct instantiation of '{node.func.id}' is forbidden. Inject it via the constructor.",
                                    file_path=str(
                                        file_path.relative_to(self.repo_root)
                                    ),
                                    line_number=node.lineno,
                                    category="architectural",  # <-- ADD THIS LINE
                                )
                            )
            except Exception:
                continue
        return findings

    def _check_forbidden_imports(self, rule: dict) -> List[AuditFinding]:
        """Finds direct imports of forbidden functions like get_session."""
        findings = []
        forbidden_imports = set(rule.get("forbidden_imports", []))
        scope = rule.get("scope", [])
        exclusions = rule.get("exclusions", [])

        for file_path in self._get_files_in_scope(scope, exclusions):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if (
                        isinstance(node, ast.ImportFrom)
                        and node.module in forbidden_imports
                    ):
                        findings.append(
                            AuditFinding(
                                check_id=rule["id"],
                                severity=AuditSeverity.ERROR,
                                message=f"Direct import of '{node.module}' is forbidden. Inject the dependency instead.",
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=node.lineno,
                                category="architectural",  # <-- ADD THIS LINE
                            )
                        )
            except Exception:
                continue
        return findings

    def _get_files_in_scope(
        self, scope: List[str], exclusions: List[str]
    ) -> List[Path]:
        """Helper to get all files matching the scope and exclusion globs."""
        files = []
        for glob_pattern in scope:
            for file_path in self.repo_root.glob(glob_pattern):
                if file_path.is_file() and not any(
                    file_path.match(ex) for ex in exclusions
                ):
                    files.append(file_path)
        return list(set(files))

--- END OF FILE ./src/features/governance/checks/dependency_injection_check.py ---

--- START OF FILE ./src/features/governance/checks/domain_placement.py ---
# src/features/governance/checks/domain_placement.py
"""
A constitutional audit check to ensure capabilities are declared in the
correct domain manifest file.
"""

from __future__ import annotations

from typing import List

from shared.models import AuditFinding, AuditSeverity
from shared.utils.yaml_processor import yaml_processor

from features.governance.audit_context import AuditorContext


# ID: 0cd8ad5a-ed46-4f18-8335-f95b747d6164
class DomainPlacementCheck:
    """
    Validates that capability keys declared in a domain manifest file
    match the domain of that file.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        self.domains_dir = self.context.mind_path / "knowledge" / "domains"

    # ID: 7eb75aef-6463-450d-8088-e9a64e3d85c8
    def execute(self) -> List[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings = []
        if not self.domains_dir.is_dir():
            return findings

        for domain_file in self.domains_dir.glob("*.yaml"):
            domain_name = domain_file.stem
            manifest_content = yaml_processor.load(domain_file)
            if not manifest_content:
                continue

            capabilities = manifest_content.get("tags", [])
            if not isinstance(capabilities, list):
                continue

            for cap in capabilities:
                if isinstance(cap, dict) and "key" in cap:
                    cap_key = cap["key"]
                    if not cap_key.startswith(f"{domain_name}."):
                        findings.append(
                            AuditFinding(
                                check_id="domain.placement.mismatch",
                                severity=AuditSeverity.ERROR,
                                message=f"Capability '{cap_key}' is misplaced in '{domain_file.name}'. It should be in a '{cap_key.split('.')[0]}.yaml' manifest.",
                                file_path=str(
                                    domain_file.relative_to(self.context.repo_path)
                                ),
                            )
                        )
        return findings

--- END OF FILE ./src/features/governance/checks/domain_placement.py ---

--- START OF FILE ./src/features/governance/checks/duplication_check.py ---
# src/features/governance/checks/duplication_check.py
"""
A constitutional audit check to find semantically duplicate or near-duplicate
symbols (functions/classes) using the Qdrant vector database.
"""

from __future__ import annotations

import asyncio
from typing import Any, Dict, List

import networkx as nx
from rich.progress import track
from services.clients.qdrant_client import QdrantService
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity

from features.governance.audit_context import AuditorContext

log = getLogger("duplication_check")


def _group_findings(findings: list[AuditFinding]) -> List[List[AuditFinding]]:
    """Groups individual finding pairs into clusters of related duplicates."""
    # This helper function is correct and does not need changes.
    graph = nx.Graph()
    finding_map = {}

    for finding in findings:
        symbol1 = finding.context.get("symbol_a")
        symbol2 = finding.context.get("symbol_b")
        if symbol1 and symbol2:
            graph.add_edge(symbol1, symbol2)
            finding_map[tuple(sorted((symbol1, symbol2)))] = finding

    clusters = list(nx.connected_components(graph))
    grouped_findings = []

    for cluster in clusters:
        cluster_findings = []
        for i, node1 in enumerate(list(cluster)):
            for node2 in list(cluster)[i + 1 :]:
                key = tuple(sorted((node1, node2)))
                if key in finding_map:
                    cluster_findings.append(finding_map[key])

        if cluster_findings:
            cluster_findings.sort(
                key=lambda f: float(f.context.get("similarity", 0)), reverse=True
            )
            grouped_findings.append(cluster_findings)

    return grouped_findings


# ID: 79150815-dfca-4b22-9b01-bdc01d14702e
class DuplicationCheck:
    """
    Enforces the 'dry_by_design' principle by finding semantically similar symbols.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        self.symbols = self.context.knowledge_graph.get("symbols", {})
        # --- FIX: We no longer create a shared client here. ---
        # self.qdrant_service = QdrantService()
        ignore_policy = self.context.policies.get("audit_ignore_policy", {})
        self.ignored_symbol_keys = {
            item["key"]
            for item in ignore_policy.get("symbol_ignores", [])
            if "key" in item
        }

    async def _check_single_symbol(
        self, symbol: Dict[str, Any], threshold: float
    ) -> List[AuditFinding]:
        """Checks a single symbol for duplicates against the Qdrant index."""
        findings = []
        symbol_key = symbol.get("symbol_path")

        # --- THIS IS THE DEFINITIVE FIX ---
        # Each concurrent task now gets its own private, fresh client instance.
        qdrant_service = QdrantService()
        # --- END OF FIX ---

        point_id = str(symbol.get("vector_id")) if symbol.get("vector_id") else None

        if not point_id or symbol_key in self.ignored_symbol_keys:
            return []

        try:
            query_vector = await qdrant_service.get_vector_by_id(point_id=point_id)
            if not query_vector:
                return []

            similar_hits = await qdrant_service.search_similar(
                query_vector=query_vector, limit=5
            )

            for hit in similar_hits:
                if not hit.get("payload"):
                    continue

                hit_symbol_key = hit["payload"].get("chunk_id")
                if (
                    not hit_symbol_key
                    or hit_symbol_key == symbol_key
                    or hit_symbol_key in self.ignored_symbol_keys
                ):
                    continue

                if hit["score"] > threshold:
                    if symbol_key < hit_symbol_key:
                        symbol_a, symbol_b = symbol_key, hit_symbol_key
                    else:
                        symbol_a, symbol_b = hit_symbol_key, symbol_key

                    findings.append(
                        AuditFinding(
                            check_id="code.style.semantic-duplication",
                            severity=AuditSeverity.WARNING,
                            message=f"Potential duplicate logic found between '{symbol_a.split('::')[-1]}' and '{symbol_b.split('::')[-1]}'.",
                            file_path=symbol.get("file_path"),
                            context={
                                "symbol_a": symbol_a,
                                "symbol_b": symbol_b,
                                "similarity": f"{hit['score']:.2f}",
                            },
                        )
                    )
        except Exception as e:
            log.warning(f"Could not perform duplication check for '{symbol_key}': {e}")

        return findings

    # ID: a74388ba-140f-4cf6-aa58-de9d61374038
    async def execute(self, threshold: float = 0.80) -> List[AuditFinding]:
        """
        Asynchronously runs the duplication check across all vectorized symbols.
        """
        symbols_to_check = list(self.symbols.values())

        if not symbols_to_check:
            return []

        tasks = [
            self._check_single_symbol(symbol, threshold) for symbol in symbols_to_check
        ]

        results = []
        for future in track(
            asyncio.as_completed(tasks),
            description="Checking for duplicate code...",
            total=len(tasks),
        ):
            results.extend(await future)

        unique_findings = {}
        for finding in results:
            key_tuple = tuple(
                sorted((finding.context["symbol_a"], finding.context["symbol_b"]))
            )
            if key_tuple not in unique_findings:
                unique_findings[key_tuple] = finding

        return list(unique_findings.values())

--- END OF FILE ./src/features/governance/checks/duplication_check.py ---

--- START OF FILE ./src/features/governance/checks/environment_checks.py ---
# src/features/governance/checks/environment_checks.py
"""
Audits the system's runtime environment for required configuration.
"""

from __future__ import annotations

import os

from shared.models import AuditFinding, AuditSeverity

from features.governance.checks.base_check import BaseCheck


# ID: 0c3965b7-b3f3-4fb6-bbbb-c94a1ffae3fe
class EnvironmentChecks(BaseCheck):
    """Container for environment and runtime configuration checks."""

    def __init__(self, context):
        super().__init__(context)
        self.requirements = self.context.policies.get("runtime_requirements", {})

    # ID: 0c0e7695-b11e-4ad8-9e74-23d5f79dad00
    def execute(self) -> list[AuditFinding]:
        """
        Verifies that required environment variables specified in
        runtime_requirements.yaml are set.
        """
        findings = []
        required_vars = self.requirements.get("variables", {})

        for name, config in required_vars.items():
            if config.get("required") and not os.getenv(name):
                msg = (
                    f"Required environment variable '{name}' is not set. "
                    f"Description: {config.get('description', 'No description.')}"
                )
                findings.append(
                    AuditFinding(
                        check_id="environment.variable.missing",
                        severity=AuditSeverity.ERROR,
                        message=msg,
                        file_path=".env",
                    )
                )
        return findings

--- END OF FILE ./src/features/governance/checks/environment_checks.py ---

--- START OF FILE ./src/features/governance/checks/file_checks.py ---
# src/features/governance/checks/file_checks.py
"""
Audits file existence and orphan detection for constitutional governance files.
"""

from __future__ import annotations

from typing import List, Set

from shared.config import settings
from shared.models import AuditFinding, AuditSeverity
from shared.utils.constitutional_parser import get_all_constitutional_paths

from features.governance.checks.base_check import BaseCheck

KNOWN_UNINDEXED_FILES = {
    ".intent/charter/constitution/approvers.yaml.example",
    ".intent/keys/private.key",
}

DEPRECATED_KNOWLEDGE_FILES = [
    ".intent/knowledge/cli_registry.yaml",
    ".intent/knowledge/resource_manifest.yaml",
    ".intent/knowledge/cognitive_roles.yaml",
]


# ID: 37b5ae2f-c3c2-4db4-9677-f16fd788c908
class FileChecks(BaseCheck):
    """Container for file-based constitutional checks."""

    # ID: 56481071-3a0c-437d-ba57-533bc03d9ed6
    def execute(self) -> List[AuditFinding]:
        """Runs all file-related checks."""
        meta_content = settings._meta_config
        required_files = get_all_constitutional_paths(meta_content, self.intent_path)
        findings = self._check_required_files(required_files)
        findings.extend(self._check_for_orphaned_intent_files(required_files))
        findings.extend(self._check_for_deprecated_files())
        return findings

    def _check_for_deprecated_files(self) -> List[AuditFinding]:
        """Verify that files constitutionally replaced by the database do not exist."""
        findings: List[AuditFinding] = []
        for file_rel_path in DEPRECATED_KNOWLEDGE_FILES:
            full_path = self.repo_root / file_rel_path
            if full_path.exists():
                findings.append(
                    AuditFinding(
                        check_id="config.ssot.deprecated-file",
                        severity=AuditSeverity.ERROR,
                        message=f"Deprecated knowledge file exists: '{file_rel_path}'. The database is the SSOT.",
                        file_path=file_rel_path,
                    )
                )
        return findings

    def _check_required_files(self, required_files: Set[str]) -> List[AuditFinding]:
        """Verify that all files declared in meta.yaml exist on disk."""
        findings: List[AuditFinding] = []
        for file_rel_path in sorted(required_files):
            full_path = self.repo_root / file_rel_path
            if not full_path.exists():
                findings.append(
                    AuditFinding(
                        check_id="config.meta.missing-file",
                        severity=AuditSeverity.ERROR,
                        message=f"File declared in meta.yaml is missing: '{file_rel_path}'",
                        file_path=file_rel_path,
                    )
                )
        return findings

    def _check_for_orphaned_intent_files(
        self, declared_files: Set[str]
    ) -> List[AuditFinding]:
        """Find .intent files not referenced in meta.yaml."""
        findings: List[AuditFinding] = []
        all_known_files = declared_files.union(KNOWN_UNINDEXED_FILES)
        if (self.intent_path / "proposals/README.md").exists():
            all_known_files.add(".intent/proposals/README.md")
        physical_files: Set[str] = {
            str(p.relative_to(self.repo_root)).replace("\\", "/")
            for p in self.intent_path.rglob("*")
            if p.is_file()
        }
        orphaned_files = sorted(physical_files - all_known_files)
        for orphan in orphaned_files:
            if "prompts" in orphan or "reports" in orphan:
                continue
            findings.append(
                AuditFinding(
                    check_id="config.meta.orphaned-file",
                    severity=AuditSeverity.WARNING,
                    message=f"Orphaned file in .intent/: '{orphan}'. Add to meta.yaml or remove.",
                    file_path=orphan,
                )
            )
        return findings

--- END OF FILE ./src/features/governance/checks/file_checks.py ---

--- START OF FILE ./src/features/governance/checks/health_checks.py ---
# src/features/governance/checks/health_checks.py
"""
Audits codebase health for complexity, atomicity, and line length violations.
"""

from __future__ import annotations

import ast
import statistics
from pathlib import Path
from typing import List

from radon.visitors import ComplexityVisitor
from shared.models import AuditFinding, AuditSeverity

from features.governance.checks.base_check import BaseCheck


# ID: 64e34c49-4bad-4d35-8de7-df4f67b51adc
class HealthChecks(BaseCheck):
    """Container for codebase health constitutional checks."""

    def __init__(self, context):
        super().__init__(context)
        self.health_policy = self.context.policies.get("code_health_policy", {})

    # ID: 1fe5dc5f-42fb-4bf9-a955-f560d7aca429
    def execute(self) -> list[AuditFinding]:
        """Measures code complexity and atomicity against defined policies."""
        policy_rules = self.health_policy.get("rules", {})
        file_line_counts = {}
        all_violations = []
        unique_files = {
            s["file_path"]
            for s in self.context.symbols_list
            if s.get("file_path", "").startswith("src/")
        }
        for file_path_str in sorted(list(unique_files)):
            if not file_path_str.endswith(".py"):
                continue
            file_path = self.repo_root / file_path_str
            logical_lines, violations = self._analyze_python_file(
                file_path, policy_rules
            )
            if logical_lines > 0:
                file_line_counts[file_path] = logical_lines
            all_violations.extend(violations)
        all_violations.extend(
            self._find_file_size_outliers(file_line_counts, policy_rules)
        )
        return all_violations

    def _analyze_python_file(
        self, file_path: Path, rules: dict
    ) -> tuple[int, List[AuditFinding]]:
        """Analyze a single Python file for health violations."""
        try:
            source_code = file_path.read_text(encoding="utf-8")
            logical_lines = self._count_logical_lines(source_code)
            if logical_lines > rules.get("max_module_lloc", 300):
                return logical_lines, [
                    AuditFinding(
                        check_id="code.complexity.module-too-long",
                        severity=AuditSeverity.WARNING,
                        message=f"Module has {logical_lines} lines (limit: {rules.get('max_module_lloc', 300)}).",
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                ]
            syntax_tree = ast.parse(source_code)
            complexity_visitor = ComplexityVisitor.from_ast(syntax_tree)
            violations = self._check_function_metrics(
                complexity_visitor,
                rules,
                str(file_path.relative_to(self.repo_root)),
            )
            return logical_lines, violations
        except Exception:
            return 0, []

    def _count_logical_lines(self, source_code: str) -> int:
        return sum(
            1
            for line in source_code.splitlines()
            if line.strip() and not line.strip().startswith("#")
        )

    def _check_function_metrics(
        self,
        visitor: ComplexityVisitor,
        rules: dict,
        file_path_str: str,
    ) -> List[AuditFinding]:
        violations = []
        for function in visitor.functions:
            if function.cognitive_complexity > rules.get(
                "max_cognitive_complexity", 15
            ):
                violations.append(
                    AuditFinding(
                        check_id="code.complexity.function-too-complex",
                        severity=AuditSeverity.WARNING,
                        message=f"Function '{function.name}' complexity is {function.cognitive_complexity} (limit: {rules.get('max_cognitive_complexity', 15)}).",
                        file_path=file_path_str,
                    )
                )
            if function.lloc > rules.get("max_function_lloc", 80):
                violations.append(
                    AuditFinding(
                        check_id="code.complexity.function-too-long",
                        severity=AuditSeverity.WARNING,
                        message=f"Function '{function.name}' has {function.lloc} lines (limit: {rules.get('max_function_lloc', 80)}).",
                        file_path=file_path_str,
                    )
                )
        return violations

    def _find_file_size_outliers(
        self, file_line_counts: dict, rules: dict
    ) -> List[AuditFinding]:
        if len(file_line_counts) < 3:
            return []
        violations = []
        line_count_values = list(file_line_counts.values())
        average_lines = statistics.mean(line_count_values)
        standard_deviation = statistics.stdev(line_count_values)
        outlier_threshold = average_lines + (
            rules.get("outlier_standard_deviations", 2.0) * standard_deviation
        )
        for file_path, line_count in file_line_counts.items():
            if line_count > outlier_threshold:
                violations.append(
                    AuditFinding(
                        check_id="code.complexity.module-outlier",
                        severity=AuditSeverity.WARNING,
                        message=f"Module size outlier ({line_count} lines vs avg of {average_lines:.0f}). Consider refactoring.",
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                )
        return violations

--- END OF FILE ./src/features/governance/checks/health_checks.py ---

--- START OF FILE ./src/features/governance/checks/id_coverage_check.py ---
# src/features/governance/checks/id_coverage_check.py
"""
A constitutional audit check to enforce that every public symbol in the codebase
has a registered ID in the database.
"""

from __future__ import annotations

import ast
from typing import List

from shared.ast_utility import find_symbol_id_and_def_line
from shared.models import AuditFinding, AuditSeverity

from features.governance.checks.base_check import BaseCheck


# ID: 3501ed8c-8366-4ad7-9ab4-7dcf4c045c70
class IdCoverageCheck(BaseCheck):
    """
    Ensures every public function/class in `src/` has a valid, DB-registered ID tag.
    """

    # ID: f69a1a2e-26cd-4cc2-8fdc-7f18e0e77d0c
    def execute(self) -> List[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings = []
        for file_path in self.context.src_dir.rglob("*.py"):
            try:
                content = file_path.read_text("utf-8")
                source_lines = content.splitlines()
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if not isinstance(
                        node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                    ):
                        continue

                    # Rule 1: Must be a public symbol
                    if node.name.startswith("_"):
                        continue

                    # Use the robust utility to find the ID and definition line
                    id_result = find_symbol_id_and_def_line(node, source_lines)

                    if not id_result.has_id:
                        findings.append(
                            AuditFinding(
                                check_id="linkage.id.missing-tag",
                                severity=AuditSeverity.ERROR,
                                message=f"Public symbol '{node.name}' is missing its required '# ID:' tag.",
                                file_path=str(
                                    file_path.relative_to(self.context.repo_path)
                                ),
                                line_number=id_result.definition_line_num,
                            )
                        )

            except Exception:
                # Silently ignore files that cannot be parsed
                continue

        return findings

--- END OF FILE ./src/features/governance/checks/id_coverage_check.py ---

--- START OF FILE ./src/features/governance/checks/id_uniqueness_check.py ---
# src/features/governance/checks/id_uniqueness_check.py
"""
A constitutional audit check to enforce that every # ID tag is unique across the codebase.
"""

from __future__ import annotations

import re
from collections import defaultdict
from typing import Dict, List

from shared.models import AuditFinding, AuditSeverity

from features.governance.checks.base_check import BaseCheck

# Pre-compiled regex for efficiency to find '# ID: <uuid>'
ID_TAG_REGEX = re.compile(
    r"#\s*ID:\s*([0-9a-fA-F]{8}-([0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12})"
)


# ID: ddaabb9e-5e9a-4574-b458-dbed610e64e5
class IdUniquenessCheck(BaseCheck):
    """
    Scans the entire source code to ensure that every assigned symbol ID (UUID) is unique.
    This prevents data corruption from accidental copy-paste errors during development.
    """

    # ID: f2a3b4c5-d6e7-f8a9-b0c1-d2e3f4a5b6c7
    def execute(self) -> List[AuditFinding]:
        """
        Runs the check by scanning all Python files in `src/` and returns findings for any duplicate UUIDs.
        """
        # A dictionary to store locations of each UUID: {uuid: [("file/path.py", line_num), ...]}
        uuid_locations: Dict[str, List[tuple[str, int]]] = defaultdict(list)

        src_dir = self.context.repo_path / "src"
        for file_path in src_dir.rglob("*.py"):
            try:
                content = file_path.read_text("utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    match = ID_TAG_REGEX.search(line)
                    if match:
                        found_uuid = match.group(1)
                        rel_path = str(file_path.relative_to(self.context.repo_path))
                        uuid_locations[found_uuid].append((rel_path, i))
            except Exception:
                # Silently ignore files that can't be read or parsed
                continue

        findings = []
        for found_uuid, locations in uuid_locations.items():
            if len(locations) > 1:
                # Found a duplicate!
                locations_str = ", ".join(
                    [f"{path}:{line}" for path, line in locations]
                )
                findings.append(
                    AuditFinding(
                        check_id="linkage.id.duplicate",
                        severity=AuditSeverity.ERROR,
                        message=f"Duplicate ID tag found: {found_uuid}",
                        context={"locations": locations_str},
                    )
                )

        return findings

--- END OF FILE ./src/features/governance/checks/id_uniqueness_check.py ---

--- START OF FILE ./src/features/governance/checks/import_rules.py ---
# src/features/governance/checks/import_rules.py
"""
A constitutional audit check to enforce architectural import rules as
defined in the source_structure.yaml manifest.
"""

from __future__ import annotations

import ast
from pathlib import Path
from typing import Dict, List, Set

from services.database.session_manager import get_session
from shared.models import AuditFinding, AuditSeverity
from sqlalchemy import text

from features.governance.audit_context import AuditorContext
from features.governance.checks.base_check import BaseCheck


def _scan_imports(file_path: Path, content: str | None = None) -> List[str]:
    """
    Parse a Python file or its content and extract all imported module paths.
    """
    imports = []
    try:
        source = (
            content if content is not None else file_path.read_text(encoding="utf-8")
        )
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    if node.level > 0:
                        base = ".".join(file_path.parts[1:-1])
                        if node.level > 1:
                            base = ".".join(base.split(".")[: -(node.level - 1)])
                        imports.append(f"{base}.{node.module}")
                    else:
                        imports.append(node.module)

    except Exception:
        pass

    return imports


# ID: 0690cf39-3739-449e-9228-2c7c8526209b
class ImportRulesCheck(BaseCheck):
    """
    Ensures that code files only import modules from their allowed domains.
    This check now reads its configuration from the database.
    """

    def __init__(self, context: AuditorContext):
        super().__init__(context)
        self.domain_map: Dict[str, str] = {}
        self.import_rules: Dict[str, Set[str]] = {}

    async def _load_rules_from_db(self):
        """Loads domain maps and import rules from the database."""
        if self.domain_map:
            return

        async with get_session() as session:
            await session.execute(text("SELECT key FROM core.domains"))

        structure = self.context.source_structure.get("structure", [])
        for domain_info in structure:
            path_str = domain_info.get("path")
            domain_name = domain_info.get("domain")
            if path_str and domain_name:
                self.domain_map[path_str] = domain_name

        for domain_info in structure:
            domain_name = domain_info.get("domain")
            allowed_imports = domain_info.get("allowed_imports", [])
            if domain_name:
                self.import_rules[domain_name] = set(allowed_imports)

    def _get_domain_for_path_str(self, file_path_str: str) -> str | None:
        """Finds the domain for a given relative file path string."""
        for domain_path_prefix, domain_name in self.domain_map.items():
            if file_path_str.startswith(domain_path_prefix):
                return domain_name
        return None

    # ID: f1a7dedb-d5e4-442d-8957-b7f974778bc5
    async def execute(self) -> List[AuditFinding]:
        """
        Runs the check by scanning all source files and validating their imports.
        """
        await self._load_rules_from_db()
        findings = []
        # Use self.src_dir provided by the BaseCheck
        for file_path in self.src_dir.rglob("*.py"):
            findings.extend(self._check_file_imports(file_path, file_content=None))
        return findings

    # ID: 31287af5-d942-4a1d-b06d-d0570026d035
    async def execute_on_content(
        self, file_path_str: str, file_content: str
    ) -> List[AuditFinding]:
        """
        Runs the import check on a string of content instead of a file on disk.
        """
        await self._load_rules_from_db()
        # Use self.repo_root provided by the BaseCheck
        file_path = self.repo_root / file_path_str
        return self._check_file_imports(file_path, file_content)

    def _check_file_imports(
        self, file_path: Path, file_content: str | None
    ) -> List[AuditFinding]:
        """Core logic to check imports for a given file path and optional content."""
        findings = []
        # Use self.repo_root provided by the BaseCheck
        file_rel_path_str = str(file_path.relative_to(self.repo_root))
        file_domain = self._get_domain_for_path_str(file_rel_path_str)
        if not file_domain:
            return []

        allowed_imports_for_domain = self.import_rules.get(file_domain, set())
        imported_modules = _scan_imports(file_path, content=file_content)

        for module_str in imported_modules:
            imported_package = module_str.split(".")[0]

            if not any(
                imported_package.startswith(p)
                for p in ["src", "cli", "core", "features", "services", "shared"]
            ):
                continue

            if imported_package in allowed_imports_for_domain:
                continue

            if imported_package == file_domain:
                continue

            findings.append(
                AuditFinding(
                    check_id="architecture.import_violation",
                    severity=AuditSeverity.ERROR,
                    message=f"Illegal import of '{module_str}' in domain '{file_domain}'. Allowed: {sorted(list(allowed_imports_for_domain))}",
                    file_path=file_rel_path_str,
                )
            )
        return findings

--- END OF FILE ./src/features/governance/checks/import_rules.py ---

--- START OF FILE ./src/features/governance/checks/knowledge_source_check.py ---
"""
Compares DB single-source-of-truth tables with their (legacy) YAML exports.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import yaml
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, async_sessionmaker

# Configuration
TABLE_CONFIGS = {
    "cli_registry": {
        "yaml_paths": [
            ".intent/mind/knowledge/cli_registry.yaml",
            ".intent/mind/knowledge/cli_registry.yml",
        ],
        "table": "core.cli_commands",
        "yaml_key": "commands",
        "primary_key": "name",
        "preferred_order": ["name", "module", "entrypoint", "enabled"],
    },
    "resource_manifest": {
        "yaml_paths": [
            ".intent/mind/knowledge/resource_manifest.yaml",
            ".intent/mind/knowledge/resource_manifest.yml",
        ],
        "table": "core.llm_resources",
        "yaml_key": "llm_resources",
        "primary_key": "name",
        "preferred_order": ["name", "provider", "model", "enabled"],
    },
    "cognitive_roles": {
        "yaml_paths": [
            ".intent/mind/knowledge/cognitive_roles.yaml",
            ".intent/mind/knowledge/cognitive_roles.yml",
        ],
        "table": "core.cognitive_roles",
        "yaml_key": "cognitive_roles",
        "primary_key": "role",
        "preferred_order": ["name", "description", "enabled"],
    },
}

FIELD_PRIORITY = [
    "name",
    "role",
    "module",
    "entrypoint",
    "provider",
    "model",
    "description",
    "enabled",
]


@dataclass
# ID: 55de1540-39da-4a5d-9e40-b0614cfe655f
class CheckResult:
    name: str
    passed: bool
    details: Dict[str, Any]


# ID: 81d6e8ed-a6f6-444c-acda-9064896c5111
class KnowledgeSourceCheck:
    """
    Compares DB single-source-of-truth tables with their (legacy) YAML exports under:
      .intent/mind/knowledge/{cli_registry, resource_manifest, cognitive_roles}.yaml

    Behavior:
      - If a YAML file is missing and `require_yaml_exports=False` (default), that section is SKIPPED.
      - If a YAML file exists, it is compared with the DB rows (adaptive to actual DB columns).
      - Any drift in an existing YAML file FAILS the check.

    Set `require_yaml_exports=True` to enforce the presence of YAML exports.
    """

    NAME = "knowledge_source_check"

    def __init__(
        self,
        repo_root: Path,
        engine: AsyncEngine,
        session_factory: async_sessionmaker[AsyncSession],
        reports_dir: Path | None = None,
        require_yaml_exports: bool = False,
    ) -> None:
        self.repo_root = repo_root
        self.engine = engine
        self.session_factory = session_factory
        self.reports_dir = reports_dir or repo_root / "reports" / "knowledge_ssot"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        self.require_yaml_exports = require_yaml_exports

    # ---------- Public API ----------
    # ID: b846d3ab-5762-4bc8-9dfc-f3fa060da29c
    async def execute(self) -> CheckResult:
        """Execute the knowledge source check and return results."""
        # Resolve YAML paths
        yaml_paths = {
            section: self._resolve_yaml(*config["yaml_paths"])
            for section, config in TABLE_CONFIGS.items()
        }

        # Fetch all database tables
        section_results = {}
        async with self.session_factory() as session:
            for section, config in TABLE_CONFIGS.items():
                schema, table = config["table"].split(".")
                db_rows, db_cols = await self._fetch_table(
                    session, schema, table, config["preferred_order"]
                )

                section_results[section] = await self._compare_section(
                    label=section,
                    yaml_path=yaml_paths[section],
                    db_rows=db_rows,
                    db_cols=db_cols,
                    yaml_key=config["yaml_key"],
                    primary_key=config["primary_key"],
                )

        # Determine overall pass/fail status
        passed = self._determine_overall_status(section_results)

        # Build and save report
        report = self._build_report(passed, yaml_paths, section_results)
        self._save_report(report)

        return CheckResult(name=self.NAME, passed=passed, details=report)

    # ---------- Section comparison ----------
    async def _compare_section(
        self,
        *,
        label: str,
        yaml_path: Path | None,
        db_rows: List[Dict[str, Any]],
        db_cols: List[str],
        yaml_key: str,
        primary_key: str,
    ) -> Dict[str, Any]:
        """Compare a single section (YAML vs DB)."""
        # Handle missing YAML file
        if yaml_path is None:
            return self._handle_missing_yaml()

        # Load and compare
        yaml_items = self._read_yaml(yaml_path, yaml_key)
        compare_fields = self._determine_compare_fields(yaml_items, db_cols)
        diff = self._diff_records(yaml_items, db_rows, primary_key, compare_fields)

        status = "passed" if self._is_diff_clean(diff) else "failed"
        return {
            "status": status,
            "yaml": str(yaml_path),
            "compare_fields": list(compare_fields),
            "diff": diff,
        }

    def _handle_missing_yaml(self) -> Dict[str, Any]:
        """Handle the case where a YAML file is missing."""
        if self.require_yaml_exports:
            return {
                "status": "failed",
                "reason": "yaml_missing_and_required",
                "diff": {
                    "missing_in_db": [],
                    "missing_in_yaml": [],
                    "mismatched": [],
                },
            }
        return {"status": "skipped", "reason": "yaml_missing", "diff": None}

    # ---------- Database operations ----------
    async def _fetch_table(
        self,
        session: AsyncSession,
        schema: str,
        table: str,
        preferred_order: List[str],
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """Fetch all rows and columns from a database table."""
        cols = await self._list_columns(session, schema, table)
        if not cols:
            return [], []

        # Query the table
        select_cols = ", ".join([f'"{c}"' for c in cols])
        sql = text(f'SELECT {select_cols} FROM "{schema}"."{table}"')
        rows = (await session.execute(sql)).mappings().all()

        # Order columns consistently
        ordered_cols = self._order_columns(cols, preferred_order)
        data = [{k: dict(r).get(k) for k in ordered_cols} for r in rows]

        return data, ordered_cols

    async def _list_columns(
        self, session: AsyncSession, schema: str, table: str
    ) -> List[str]:
        """Get the list of columns for a table from information_schema."""
        sql = text(
            """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = :schema AND table_name = :table
            ORDER BY ordinal_position
            """
        )
        rows = (
            await session.execute(sql, {"schema": schema, "table": table})
        ).mappings()
        return [r["column_name"] for r in rows]

    # ---------- YAML operations ----------
    def _resolve_yaml(self, *candidate_rel_paths: str) -> Path | None:
        """Find the first existing YAML file from a list of candidates."""
        for rel in candidate_rel_paths:
            p = self.repo_root / rel
            if p.exists():
                return p
        return None

    def _read_yaml(self, path: Path, key: str) -> List[Dict[str, Any]]:
        """Read a YAML file and extract items by key."""
        try:
            data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
            if not isinstance(data, dict):
                return []

            items = data.get(key, [])
            return items if isinstance(items, list) else []
        except Exception:
            return []

    # ---------- Comparison logic ----------
    def _determine_compare_fields(
        self, yaml_items: List[Dict[str, Any]], db_cols: List[str]
    ) -> Tuple[str, ...]:
        """Determine which fields to compare based on YAML and DB columns."""
        yaml_keys = set()
        for item in yaml_items:
            if isinstance(item, dict):
                yaml_keys.update(item.keys())

        # Include primary key possibilities
        common_keys = (yaml_keys & set(db_cols)) | {"name", "role"}
        return self._order_fields(common_keys)

    def _diff_records(
        self,
        yaml_items: List[Dict[str, Any]],
        db_items: List[Dict[str, Any]],
        primary_key: str,
        compare_fields: Tuple[str, ...],
    ) -> Dict[str, Any]:
        """Compare YAML and DB records and return differences."""
        # Build indexes by primary key
        yaml_index = self._build_index(yaml_items, primary_key)
        db_index = self._build_index(db_items, primary_key)

        # Find missing records
        missing_in_db = sorted(set(yaml_index.keys()) - set(db_index.keys()))
        missing_in_yaml = sorted(set(db_index.keys()) - set(yaml_index.keys()))

        # Find mismatched records
        mismatched = self._find_mismatches(yaml_index, db_index, compare_fields)

        return {
            "missing_in_db": missing_in_db,
            "missing_in_yaml": missing_in_yaml,
            "mismatched": mismatched,
        }

    def _build_index(
        self, items: List[Dict[str, Any]], key: str
    ) -> Dict[str, Dict[str, Any]]:
        """Build an index of items by their primary key."""
        return {
            str(item.get(key)).strip(): item
            for item in items
            if isinstance(item, dict) and item.get(key) is not None
        }

    def _find_mismatches(
        self,
        yaml_index: Dict[str, Dict[str, Any]],
        db_index: Dict[str, Dict[str, Any]],
        compare_fields: Tuple[str, ...],
    ) -> List[Dict[str, Any]]:
        """Find records that exist in both but have different field values."""
        mismatched = []
        common_keys = set(yaml_index.keys()) & set(db_index.keys())

        for key in sorted(common_keys):
            yaml_record = yaml_index[key]
            db_record = db_index[key]

            field_diffs = self._compare_records(yaml_record, db_record, compare_fields)

            if field_diffs:
                mismatched.append({"name": key, "fields": field_diffs})

        return mismatched

    def _compare_records(
        self,
        yaml_record: Dict[str, Any],
        db_record: Dict[str, Any],
        compare_fields: Tuple[str, ...],
    ) -> Dict[str, Dict[str, Any]]:
        """Compare two records field by field."""
        diffs = {}

        for field in compare_fields:
            # Skip fields not present in either record
            if field not in yaml_record and field not in db_record:
                continue

            yaml_val = yaml_record.get(field)
            db_val = db_record.get(field)

            # Normalize: treat empty strings and None as equivalent
            if self._values_equivalent(yaml_val, db_val):
                continue

            if yaml_val != db_val:
                diffs[field] = {"yaml": yaml_val, "db": db_val}

        return diffs

    @staticmethod
    def _values_equivalent(val1: Any, val2: Any) -> bool:
        """Check if two values are equivalent (treating None and empty string as same)."""
        return (val1 is None or val1 == "") and (val2 is None or val2 == "")

    @staticmethod
    def _is_diff_clean(diff: Dict[str, Any]) -> bool:
        """Check if a diff shows no differences."""
        return (
            not diff["missing_in_db"]
            and not diff["missing_in_yaml"]
            and not diff["mismatched"]
        )

    # ---------- Utility functions ----------
    @staticmethod
    def _order_columns(cols: List[str], preferred: List[str]) -> List[str]:
        """Order columns with preferred ones first, rest alphabetically."""
        return [c for c in preferred if c in cols] + [
            c for c in cols if c not in preferred
        ]

    @staticmethod
    def _order_fields(fields: set) -> Tuple[str, ...]:
        """Order fields with priority fields first, rest alphabetically."""
        ordered = [f for f in FIELD_PRIORITY if f in fields] + [
            f for f in sorted(fields) if f not in FIELD_PRIORITY
        ]
        return tuple(ordered)

    def _determine_overall_status(
        self, section_results: Dict[str, Dict[str, Any]]
    ) -> bool:
        """Determine if the overall check passed based on section results."""
        any_failed = any(
            result.get("status") == "failed" for result in section_results.values()
        )

        if self.require_yaml_exports:
            any_skipped = any(
                result.get("status") == "skipped" for result in section_results.values()
            )
            return not any_failed and not any_skipped

        return not any_failed

    def _build_report(
        self,
        passed: bool,
        yaml_paths: Dict[str, Path | None],
        section_results: Dict[str, Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Build the complete report structure."""
        return {
            "check": self.NAME,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "passed": passed,
            "require_yaml_exports": self.require_yaml_exports,
            "sources": {
                "yaml_paths": {
                    k: str(v) if isinstance(v, Path) else None
                    for k, v in yaml_paths.items()
                },
                "db_tables": {
                    section: config["table"]
                    for section, config in TABLE_CONFIGS.items()
                },
            },
            "sections": section_results,
        }

    def _save_report(self, report: Dict[str, Any]) -> None:
        """Save the report to a timestamped JSON file."""
        report_path = self.reports_dir / (
            datetime.utcnow().strftime("%Y%m%d_%H%M%S") + ".json"
        )
        report_path.write_text(json.dumps(report, indent=2), encoding="utf-8")

--- END OF FILE ./src/features/governance/checks/knowledge_source_check.py ---

--- START OF FILE ./src/features/governance/checks/legacy_tag_check.py ---
# src/features/governance/checks/legacy_tag_check.py
from __future__ import annotations

import re

from shared.models import AuditFinding, AuditSeverity

from features.governance.checks.base_check import BaseCheck


# ID: 0649c22b-9336-490b-9ffd-25e202924301
class LegacyTagCheck(BaseCheck):
    # ID: 94e602d4-47da-455d-be69-fe7a037bcb2b
    def execute(self) -> list[AuditFinding]:
        findings = []
        pattern = re.compile(r"#\s*CAPABILITY:", re.IGNORECASE)
        exclude_dirs = {
            ".git",
            ".venv",
            "__pycache__",
            ".pytest_cache",
            ".ruff_cache",
            "reports",
        }
        exclude_files = {"poetry.lock", "project_context.txt"}
        binary_extensions = {
            ".png",
            ".jpg",
            ".jpeg",
            ".gif",
            ".ico",
            ".pyc",
            ".so",
            ".o",
            ".zip",
            ".gz",
            ".pdf",
        }

        # --- THIS IS THE FIX ---
        # The loop now correctly uses self.repo_root, which is set by the BaseCheck parent class.
        for file_path in self.repo_root.rglob("*"):
            if not file_path.is_file():
                continue

            if any(part in exclude_dirs for part in file_path.parts):
                continue
            if file_path.name in exclude_files:
                continue
            if file_path.suffix in binary_extensions:
                continue

            try:
                content = file_path.read_text(encoding="utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    if pattern.search(line):
                        findings.append(
                            AuditFinding(
                                check_id="style.no_legacy_capability_tags",
                                severity=AuditSeverity.ERROR,
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=i,
                            )
                        )
            except UnicodeDecodeError:
                continue
            except Exception:
                continue

        return findings

--- END OF FILE ./src/features/governance/checks/legacy_tag_check.py ---

--- START OF FILE ./src/features/governance/checks/manifest_lint.py ---
# src/features/governance/checks/manifest_lint.py
"""
Audits capability manifests for quality issues like placeholder text.
"""

from __future__ import annotations

from typing import List

from shared.models import AuditFinding, AuditSeverity

from features.governance.checks.base_check import BaseCheck


# ID: 3de1c035-00f6-4de2-b778-2b7baaf4594b
class ManifestLintCheck(BaseCheck):
    """Checks for placeholder text in capability manifests."""

    def __init__(self, context):
        super().__init__(context)
        self.linter_policy = self.context.policies.get("capability_linter_policy", {})

    # ID: 6831b833-92a9-4f37-adc9-c3eb7dd3b3d7
    def execute(self) -> List[AuditFinding]:
        """Finds capabilities with placeholder descriptions."""
        findings = []
        rule = next(
            (
                r
                for r in self.linter_policy.get("rules", [])
                if r.get("id") == "caps.no_placeholder_text"
            ),
            None,
        )
        if not rule:
            return []

        for symbol in self.context.symbols_list:
            description = symbol.get("intent", "") or ""
            if any(
                f.lower() in description.lower() for f in ["TBD", "N/A", "Auto-added"]
            ):
                findings.append(
                    AuditFinding(
                        check_id="manifest.lint.placeholder",
                        severity=AuditSeverity.WARNING,
                        message=f"Capability '{symbol.get('key')}' has a placeholder description: '{description}'",
                        file_path=symbol.get("file"),
                        line_number=symbol.get("line_number"),
                    )
                )
        return findings

--- END OF FILE ./src/features/governance/checks/manifest_lint.py ---

--- START OF FILE ./src/features/governance/checks/naming_conventions.py ---
# src/features/governance/checks/naming_conventions.py
"""
A constitutional audit check to enforce file and symbol naming conventions
as defined in the naming_conventions_policy.yaml.
"""

from __future__ import annotations

import re
from typing import List

from shared.models import AuditFinding, AuditSeverity

from features.governance.audit_context import AuditorContext


# ID: 48100636-3970-4d7b-835a-1a4279ef3717
class NamingConventionsCheck:
    """
    Ensures that file names match the patterns defined in the constitution.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        self.policy = self.context.policies.get("naming_conventions_policy", {})

    # ID: 3ceda015-448e-4745-9b09-573cc37edeb1
    def execute(self) -> List[AuditFinding]:
        """
        Runs the check by scanning all repository files against the policy rules.
        """
        findings = []
        rules = self.policy.get("rules", [])
        if not rules:
            return findings

        for rule in rules:
            scope_glob = rule.get("scope", "**/*")
            pattern = rule.get("pattern")
            exclusions = rule.get("exclusions", [])

            if not pattern:
                continue

            try:
                compiled_pattern = re.compile(pattern)
            except re.error:
                # Invalid regex in policy, skip this rule
                continue

            for file_path in self.context.repo_path.glob(scope_glob):
                if not file_path.is_file():
                    continue

                # Check against exclusions
                if any(file_path.match(ex) for ex in exclusions):
                    continue

                if not compiled_pattern.match(file_path.name):
                    findings.append(
                        AuditFinding(
                            check_id=f"naming.{rule.get('id', 'unnamed_rule')}",
                            severity=AuditSeverity.ERROR,
                            message=f"File name '{file_path.name}' violates naming convention. Expected pattern: {pattern}",
                            file_path=str(
                                file_path.relative_to(self.context.repo_path)
                            ),
                        )
                    )
        return findings

--- END OF FILE ./src/features/governance/checks/naming_conventions.py ---

--- START OF FILE ./src/features/governance/checks/orphaned_logic.py ---
# src/features/governance/checks/orphaned_logic.py
"""
A constitutional audit check to find "orphaned logic" - public symbols
that have not been assigned a capability ID in the database.
"""

from __future__ import annotations

import re
from typing import Any, Dict, List

from shared.config import settings
from shared.models import AuditFinding, AuditSeverity

from features.governance.audit_context import AuditorContext


# ID: f7064ae9-8396-4e53-b550-f85b482fb2a5
class OrphanedLogicCheck:
    """
    Ensures that all public symbols are assigned a capability, preventing
    undocumented or untracked functionality. This check respects the
    `audit_ignore_policy.yaml` and the new `entry_point_patterns.yaml`.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        self.symbols = self.context.symbols_map

        ignore_policy = self.context.policies.get("audit_ignore_policy", {})
        self.ignored_symbol_keys = {
            item["key"]
            for item in ignore_policy.get("symbol_ignores", [])
            if "key" in item
        }
        self.entry_point_patterns = settings.load(
            "mind.knowledge.entry_point_patterns"
        ).get("patterns", [])

    def _is_entry_point(self, symbol_data: Dict[str, Any]) -> bool:
        """Checks if a symbol matches any of the defined entry point patterns."""
        for pattern in self.entry_point_patterns:
            match_rules = pattern.get("match", {})
            is_match = True
            for rule_key, rule_value in match_rules.items():
                # NOTE: This implementation is resilient. It only checks for keys
                # that are guaranteed to be in the symbol_data from the DB view.
                # If the DB schema is extended later (e.g., with 'base_classes'),
                # this check will automatically start using it without code changes.
                symbol_value = symbol_data.get(rule_key)

                if rule_key == "type":
                    is_class = symbol_data.get("is_class", False)
                    if (rule_value == "class" and not is_class) or (
                        rule_value == "function" and is_class
                    ):
                        is_match = False
                        break
                elif rule_key == "name_regex":
                    if not re.search(rule_value, symbol_data.get("name", "")):
                        is_match = False
                        break
                elif rule_key == "module_path_contains":
                    if rule_value not in symbol_data.get("file_path", ""):
                        is_match = False
                        break
                elif rule_key == "has_capability_tag":
                    if rule_value and not symbol_data.get("capability"):
                        is_match = False
                        break
                elif rule_key == "is_public_function":
                    if rule_value and symbol_data.get("name", "").startswith("_"):
                        is_match = False
                        break
                # Safely ignore rules we can't check, like 'base_class_includes' for now
                elif symbol_value is None:
                    is_match = False
                    break
            if is_match:
                return True
        return False

    # ID: 92129e3b-c392-41a2-a836-d3e2af32e011
    def find_unassigned_public_symbols(self) -> List[Dict[str, Any]]:
        """Finds all public symbols with a null capability key that are not ignored."""
        unassigned = []
        for symbol_key, symbol_data in self.symbols.items():
            is_public = symbol_data.get("is_public", False)
            is_unassigned = symbol_data.get("capability") is None
            is_ignored = symbol_key in self.ignored_symbol_keys
            is_entry_point = self._is_entry_point(symbol_data)

            if is_public and is_unassigned and not is_ignored and not is_entry_point:
                symbol_data["key"] = symbol_key
                unassigned.append(symbol_data)
        return unassigned

    # ID: f7903b52-27f9-44e2-b3b5-5d0d90c5e949
    def execute(self) -> List[AuditFinding]:
        """
        Runs the check and returns a list of findings for any orphaned symbols.
        """
        findings = []
        orphaned_symbols = self.find_unassigned_public_symbols()

        for symbol in orphaned_symbols:
            symbol_key = symbol.get("key", "unknown")
            short_name = symbol_key.split("::")[-1]

            findings.append(
                AuditFinding(
                    check_id="linkage.capability.unassigned",
                    severity=AuditSeverity.ERROR,
                    message=f"Public symbol '{short_name}' is not assigned to a capability in the database.",
                    file_path=symbol.get("file_path"),
                    line_number=symbol.get("line_number"),
                    context={"symbol_key": symbol_key},
                )
            )

        return findings

--- END OF FILE ./src/features/governance/checks/orphaned_logic.py ---

--- START OF FILE ./src/features/governance/checks/security_checks.py ---
# src/features/governance/checks/security_checks.py
"""
Scans source code for hardcoded secrets and other security vulnerabilities
based on configurable detection patterns and exclusion rules.
"""

from __future__ import annotations

import ast
import fnmatch
import re
from pathlib import Path

from shared.models import AuditFinding, AuditSeverity

from features.governance.checks.base_check import BaseCheck


# ID: e5596ce5-1529-4670-864a-5bd8adfc160d
class SecurityChecks(BaseCheck):
    """Container for security-related constitutional checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        super().__init__(context)
        self.secrets_policy = self.context.policies.get("secrets_management_policy", {})
        self.safety_policy = self.context.policies.get("safety_policy", {})

    # ID: 7c0ecd2a-1bc2-45c9-8da9-48a8b6c35876
    def execute(self) -> list[AuditFinding]:
        """Scans source code for hardcoded secrets and other security vulnerabilities."""
        findings = []
        findings.extend(self._check_for_hardcoded_secrets())
        findings.extend(self._check_dangerous_calls())
        findings.extend(self._check_unsafe_imports())
        return findings

    def _get_files_to_scan(self, rule: dict) -> list[Path]:
        """Gets a list of Python files to scan, respecting rule exclusions."""
        exclude_globs = rule.get("scope", {}).get("exclude", [])
        exclude_paths = [exc.get("path") for exc in exclude_globs if exc.get("path")]

        files_to_scan = []
        for file_path in self.context.python_files:
            if not file_path.is_file():
                continue
            rel_path_str = str(file_path.relative_to(self.repo_root))
            if any(fnmatch.fnmatch(rel_path_str, glob) for glob in exclude_paths):
                continue
            files_to_scan.append(file_path)
        return files_to_scan

    def _check_for_hardcoded_secrets(self) -> list[AuditFinding]:
        """Scans for hardcoded secrets."""
        rule = next(
            (
                r
                for r in self.secrets_policy.get("rules", [])
                if r.get("id") == "no_hardcoded_secrets"
            ),
            None,
        )
        if not rule:
            return []

        findings = []
        patterns = rule.get("detection", {}).get("patterns", [])
        exclude_globs = rule.get("detection", {}).get("exclude", [])
        compiled_patterns = [re.compile(p) for p in patterns]

        for file_path in self.context.python_files:
            # --- THIS IS THE FIX ---
            # Use fnmatch here as well for consistency and correctness.
            rel_path_str = str(file_path.relative_to(self.repo_root))
            if any(fnmatch.fnmatch(rel_path_str, glob) for glob in exclude_globs):
                continue
            # --- END OF FIX ---

            try:
                content = file_path.read_text(encoding="utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    for pattern in compiled_patterns:
                        if pattern.search(line):
                            findings.append(
                                AuditFinding(
                                    check_id="security.secrets.hardcoded",
                                    severity=AuditSeverity.ERROR,
                                    message=f"Potential hardcoded secret found on line {i}.",
                                    file_path=str(
                                        file_path.relative_to(self.repo_root)
                                    ),
                                    line_number=i,
                                )
                            )
            except Exception:
                continue
        return findings

    def _check_dangerous_calls(self) -> list[AuditFinding]:
        """Scans for dangerous function calls based on the safety policy."""
        rule = next(
            (
                r
                for r in self.safety_policy.get("rules", [])
                if r.get("id") == "no_dangerous_execution"
            ),
            None,
        )
        if not rule:
            return []

        findings = []
        patterns = [
            re.compile(p) for p in rule.get("detection", {}).get("patterns", [])
        ]
        files_to_scan = self._get_files_to_scan(rule)

        for file_path in files_to_scan:
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))
                for node in ast.walk(tree):
                    if isinstance(node, ast.Call):
                        call_str = ast.unparse(node.func)
                        for pattern in patterns:
                            if pattern.search(call_str):
                                findings.append(
                                    AuditFinding(
                                        check_id="security.dangerous.call",
                                        severity=AuditSeverity.ERROR,
                                        message=f"Use of dangerous call pattern: '{call_str}'",
                                        file_path=str(
                                            file_path.relative_to(self.repo_root)
                                        ),
                                        line_number=node.lineno,
                                    )
                                )
            except Exception:
                continue
        return findings

    def _check_unsafe_imports(self) -> list[AuditFinding]:
        """Scans for forbidden imports based on the safety policy."""
        rule = next(
            (
                r
                for r in self.safety_policy.get("rules", [])
                if r.get("id") == "no_unsafe_imports"
            ),
            None,
        )
        if not rule:
            return []

        findings = []
        forbidden_imports = set(rule.get("detection", {}).get("forbidden", []))
        files_to_scan = self._get_files_to_scan(rule)

        for file_path in files_to_scan:
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            if alias.name in forbidden_imports:
                                findings.append(
                                    AuditFinding(
                                        check_id="security.dangerous.import",
                                        severity=AuditSeverity.ERROR,
                                        message=f"Import of forbidden module: '{alias.name}'",
                                        file_path=str(
                                            file_path.relative_to(self.repo_root)
                                        ),
                                        line_number=node.lineno,
                                    )
                                )
                    elif (
                        isinstance(node, ast.ImportFrom)
                        and node.module in forbidden_imports
                    ):
                        findings.append(
                            AuditFinding(
                                check_id="security.dangerous.import",
                                severity=AuditSeverity.ERROR,
                                message=f"Import from forbidden module: '{node.module}'",
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=node.lineno,
                            )
                        )
            except Exception:
                continue
        return findings

--- END OF FILE ./src/features/governance/checks/security_checks.py ---

--- START OF FILE ./src/features/governance/checks/style_checks.py ---
# src/features/governance/checks/style_checks.py
"""
Auditor checks for code style and convention compliance, as defined in
.intent/charter/policies/code_style_policy.yaml.
"""

from __future__ import annotations

import ast

from shared.models import AuditFinding, AuditSeverity

from features.governance.checks.base_check import BaseCheck


# ID: fd4ffac0-217f-4b9c-9a70-3a0106779421
class StyleChecks(BaseCheck):
    """Container for code style and convention constitutional checks."""

    def __init__(self, context):
        super().__init__(context)
        self.style_policy = self.context.policies.get("code_style_policy", {})

    # ID: 017a0a53-b5c2-4c50-adf9-5c407fa6eb55
    def execute(self) -> list[AuditFinding]:
        """Verifies that Python modules adhere to documented style conventions."""
        findings = []
        rules = {rule.get("id"): rule for rule in self.style_policy.get("rules", [])}
        files_to_check = {
            s["file_path"]
            for s in self.context.symbols_list
            if s.get("file_path", "").endswith(".py")
        }
        for file_rel_path in sorted(list(files_to_check)):
            file_abs_path = self.repo_root / file_rel_path
            try:
                source_code = file_abs_path.read_text(encoding="utf-8")
                tree = ast.parse(source_code)
                if "style.docstrings_public_apis" in rules:
                    has_docstring = (
                        tree.body
                        and isinstance(tree.body[0], ast.Expr)
                        and isinstance(tree.body[0].value, ast.Constant)
                    )
                    if not has_docstring:
                        findings.append(
                            AuditFinding(
                                check_id="code.style.missing-module-docstring",
                                severity=AuditSeverity.WARNING,
                                message="Missing required module-level docstring.",
                                file_path=file_rel_path,
                            )
                        )
            except Exception as e:
                findings.append(
                    AuditFinding(
                        check_id="code.parser.error",
                        severity=AuditSeverity.ERROR,
                        message=f"Could not parse file: {e}",
                        file_path=file_rel_path,
                    )
                )
        return findings

--- END OF FILE ./src/features/governance/checks/style_checks.py ---

--- START OF FILE ./src/features/governance/constitutional_auditor.py ---
# src/features/governance/constitutional_auditor.py
"""
Constitutional Auditor — The primary orchestration engine for all governance checks.

This refactored version implements a dynamic discovery and execution pipeline for
all audit checks defined in the `src/features/governance/checks/` directory.

Pipeline:
  1. Discover all check classes that inherit from BaseCheck.
  2. Instantiate and execute each check, collecting all raw findings.
  3. Apply entry-point-aware post-processing to downgrade severities for
     valid entry points (e.g., CLI commands, data models).
  4. Persist all raw and processed artifacts to the /reports directory.
"""

from __future__ import annotations

import asyncio
import importlib
import inspect
import json
import pkgutil
from typing import (
    Any,
    Dict,
    List,
    MutableMapping,
    Optional,
    Tuple,
)

from shared.models import AuditFinding
from shared.path_utils import get_repo_root

from features.governance import checks
from features.governance.audit_context import AuditorContext
from features.governance.audit_postprocessor import (
    EntryPointAllowList,
    apply_entry_point_downgrade_and_report,
)
from features.governance.checks.base_check import BaseCheck

# --- Configuration for the Auditor ---
REPORTS_DIR = get_repo_root() / "reports"
FINDINGS_FILENAME = "audit_findings.json"
PROCESSED_FINDINGS_FILENAME = "audit_findings.processed.json"
SYMBOL_INDEX_FILENAME = "symbol_index.json"
DOWNGRADE_SEVERITY_TO = "info"
DEAD_SYMBOL_RULE_IDS = {"linkage.capability.unassigned"}


# ID: 12c75fea-ee99-4d3d-ade6-8b9d086f6e89
class ConstitutionalAuditor:
    """
    Orchestrates the constitutional audit by discovering and running all checks.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    def _discover_checks(self) -> List[type[BaseCheck]]:
        """Dynamically discovers all BaseCheck subclasses in the checks package."""
        check_classes = []
        for _, name, _ in pkgutil.iter_modules(checks.__path__):
            module = importlib.import_module(f"features.governance.checks.{name}")
            for item_name, item in inspect.getmembers(module, inspect.isclass):
                if (
                    issubclass(item, BaseCheck)
                    and item is not BaseCheck
                    and not inspect.isabstract(item)
                ):
                    check_classes.append(item)
        return check_classes

    async def _run_all_checks(self) -> Tuple[List[AuditFinding], int]:
        """Instantiates and runs all discovered checks, collecting their findings."""
        all_findings: List[AuditFinding] = []
        check_classes = self._discover_checks()

        for check_class in check_classes:
            check_instance = check_class(self.context)

            # --- THIS IS THE FIX ---
            # Intelligently handle both sync and async execute methods.
            if inspect.iscoroutinefunction(check_instance.execute):
                # If it's an `async def`, await it directly.
                findings = await check_instance.execute()
            else:
                # If it's a regular `def`, run it in a thread to avoid blocking.
                findings = await asyncio.to_thread(check_instance.execute)
            # --- END OF FIX ---

            all_findings.extend(findings)

        unassigned_count = len(
            [f for f in all_findings if f.check_id == "linkage.capability.unassigned"]
        )

        return all_findings, unassigned_count

    # ID: 32160d41-0c9c-4e43-ba55-9994b857eb76
    async def run_full_audit_async(self) -> List[MutableMapping[str, Any]]:
        """
        The main entry point for running a full, orchestrated constitutional audit.
        """
        # Ensure the knowledge graph is loaded, as all checks depend on it.
        await self.context.load_knowledge_graph()

        # Run all individual check classes to get raw findings.
        raw_findings_objects, unassigned_count = await self._run_all_checks()
        raw_findings = [f.as_dict() for f in raw_findings_objects]

        # Build the symbol index for the post-processor.
        symbol_index = {
            key: {
                "entry_point_type": data.get("entry_point_type"),
                "pattern_name": data.get("pattern_name"),
                "entry_point_justification": data.get("entry_point_justification"),
            }
            for key, data in self.context.symbols_map.items()
        }

        # Persist the raw artifacts for debugging and traceability.
        (REPORTS_DIR / FINDINGS_FILENAME).write_text(json.dumps(raw_findings, indent=2))
        (REPORTS_DIR / SYMBOL_INDEX_FILENAME).write_text(
            json.dumps(symbol_index, indent=2)
        )

        # Apply the intelligent post-processing to downgrade valid entry points.
        processed_findings = apply_entry_point_downgrade_and_report(
            findings=raw_findings,
            symbol_index=symbol_index,
            reports_dir=REPORTS_DIR,
            allow_list=EntryPointAllowList.default(),
            dead_rule_ids=DEAD_SYMBOL_RULE_IDS,
            downgrade_to=DOWNGRADE_SEVERITY_TO,
            write_reports=True,
        )

        # Persist the final, processed findings.
        (REPORTS_DIR / PROCESSED_FINDINGS_FILENAME).write_text(
            json.dumps(processed_findings, indent=2)
        )

        return processed_findings


# For backward compatibility with the CLI layer that expects a function
# This part of the file is left unchanged
# ID: 115bf540-765e-4620-9b4e-7cb9efae908e
def run_full_audit(
    context: Any, *, config: Optional[Dict] = None
) -> List[MutableMapping[str, Any]]:
    auditor = ConstitutionalAuditor(context)
    return asyncio.run(auditor.run_full_audit_async())


# ID: 531a4409-4ae2-4dbd-af30-33215d28f311
async def run_full_audit_async(
    context: Any, *, config: Optional[Dict] = None
) -> List[MutableMapping[str, Any]]:
    auditor = ConstitutionalAuditor(context)
    return await auditor.run_full_audit_async()

--- END OF FILE ./src/features/governance/constitutional_auditor.py ---

--- START OF FILE ./src/features/governance/key_management_service.py ---
# src/features/governance/key_management_service.py
"""
Intent: Key management commands for the CORE Admin CLI.
Provides Ed25519 key generation and helper output for approver configuration.
"""

from __future__ import annotations

import os
from datetime import datetime, timezone

import typer
import yaml
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ed25519
from shared.config import settings
from shared.logger import getLogger

log = getLogger("core_admin.keys")


# The @typer.run decorator has been removed. The function is now a standard function.
# ID: b02176d9-c38f-4447-9ad5-ef17d6648263
def keygen(
    identity: str = typer.Argument(
        ..., help="Identity for the key pair (e.g., 'your.name@example.com')."
    ),
) -> None:
    """Intent: Generate a new Ed25519 key pair and print an approver YAML block."""
    log.info(f"🔑 Generating new key pair for identity: {identity}")

    key_storage_dir = settings.REPO_PATH / settings.KEY_STORAGE_DIR
    key_storage_dir.mkdir(parents=True, exist_ok=True)
    private_key_path = key_storage_dir / "private.key"

    if private_key_path.exists():
        typer.confirm(
            "⚠️ A private key already exists. Overwriting it will invalidate your "
            "old identity. Continue?",
            abort=True,
        )

    private_key = ed25519.Ed25519PrivateKey.generate()
    public_key = private_key.public_key()

    pem_private = private_key.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption(),
    )
    private_key_path.write_bytes(pem_private)
    os.chmod(private_key_path, 0o600)

    pem_public = public_key.public_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PublicFormat.SubjectPublicKeyInfo,
    )

    log.info(f"\n✅ Private key saved securely to: {private_key_path}")
    log.info(
        "\n📋 Add the following YAML block to "
        "'.intent/constitution/approvers.yaml' under 'approvers':\n"
    )

    approver_data = {
        "identity": identity,
        "public_key": pem_public.decode("utf-8"),
        "created_at": datetime.now(timezone.utc).isoformat(),
        "role": "maintainer",
        "description": "Primary maintainer",
    }
    print(yaml.dump([approver_data], indent=2, sort_keys=False))

--- END OF FILE ./src/features/governance/key_management_service.py ---

--- START OF FILE ./src/features/governance/micro_proposal_validator.py ---
# src/features/governance/micro_proposal_validator.py
from __future__ import annotations

from fnmatch import fnmatch
from typing import Any, Dict, List, Tuple

from shared.logger import getLogger

log = getLogger(__name__)


def _default_policy() -> Dict[str, Any]:
    """
    Safe defaults:
      - allow typical repo paths
      - forbid anything under .intent/**
    """
    return {
        "rules": [
            {
                "id": "safe_paths",
                "allowed_paths": [
                    "src/**",
                    "tests/**",
                    "docs/**",
                    "**/*.md",
                    "**/*.py",
                ],
                "forbidden_paths": [".intent/**"],
            }
        ]
    }


# ID: 25d8ae10-c6d2-4da4-8220-04ba7c01e6cd
class MicroProposalValidator:
    """
    Minimal, deterministic validator:
      - no file I/O
      - enforces allowed/forbidden paths
      - wording matches test expectations
    """

    def __init__(self):
        self.policy: Dict[str, Any] = _default_policy()
        rule = next(
            (r for r in self.policy.get("rules", []) if r.get("id") == "safe_paths"), {}
        )
        self._allowed: List[str] = list(rule.get("allowed_paths", []) or [])
        self._forbidden: List[str] = list(rule.get("forbidden_paths", []) or [])

    def _path_ok(self, file_path: str) -> Tuple[bool, str]:
        # Forbid first
        for pat in self._forbidden:
            if fnmatch(file_path, pat):
                # Exact phrasing expected by the tests
                return False, f"Path '{file_path}' is explicitly forbidden by policy"
        # Then allowlist
        if self._allowed and not any(fnmatch(file_path, pat) for pat in self._allowed):
            return False, f"Path '{file_path}' not in allowed paths"
        return True, "ok"

    # ID: cf03b817-ac77-43a3-a1b0-6826283885e0
    def validate(self, plan: List[Any]) -> Tuple[bool, str]:
        """
        Lightweight validation used before execution.
        Accepts Pydantic objects (with .model_dump()) or plain dicts.
        """
        if not isinstance(plan, list) or not plan:
            return False, "Plan is empty"

        for idx, step in enumerate(plan, 1):
            step_dict = step.model_dump() if hasattr(step, "model_dump") else dict(step)  # type: ignore
            action = step_dict.get("action") or step_dict.get("name")
            if not action:
                return False, f"Step {idx} missing action"

            params = step_dict.get("parameters") or step_dict.get("params") or {}
            file_path = params.get("file_path")
            if isinstance(file_path, str):
                ok, msg = self._path_ok(file_path)
                if not ok:
                    return False, msg

        return True, ""

--- END OF FILE ./src/features/governance/micro_proposal_validator.py ---

--- START OF FILE ./src/features/governance/policy_coverage_service.py ---
# src/features/governance/policy_coverage_service.py
"""
Provides a service to perform a meta-audit on the constitution itself,
checking for policy coverage and structural integrity.
"""

from __future__ import annotations

import hashlib
import json
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from pydantic import BaseModel
from shared.config import settings
from shared.logger import getLogger

log = getLogger("policy_coverage_service")


# ID: 01a2975a-5754-435d-9e5a-78fc10648abc
class PolicyCoverageReport(BaseModel):
    report_id: str
    generated_at_utc: str
    repo_root: str
    summary: Dict[str, int]
    records: List[Dict[str, Any]]
    exit_code: int


@dataclass
class _PolicyRef:
    """Internal helper to track discovered policies."""

    id: str
    path: Path
    status: str = "active"
    title: Optional[str] = None


# ID: 78d662f3-f672-4f51-b73e-fb411c106728
class PolicyCoverageService:
    """
    Runs a meta-audit on the constitution to ensure all active policies
    are well-formed and covered by the governance model.
    """

    def __init__(self, repo_root: Optional[Path] = None):
        self.repo_root: Path = repo_root or settings.REPO_PATH
        # --- THIS IS THE REFACTOR ---
        # The service now loads its governing policy via the settings object
        self.enforcement_model_policy = settings.load(
            "charter.policies.governance.enforcement_model_policy"
        )
        self.enforcement_model = self._load_enforcement_model()
        # --- END OF REFACTOR ---

    def _load_enforcement_model(self) -> Dict[str, int]:
        """Loads and parses the enforcement model from the pre-loaded policy content."""
        levels = self.enforcement_model_policy.get("levels", {})
        # Note: exit_code is not a standard part of the model, so we default to standard behavior
        return {
            "error": (
                1 if (levels.get("error") or {}).get("ci_behavior") == "fail" else 0
            ),
            "warn": 0,
            "info": 0,
        }

    def _discover_active_policies(self) -> List[_PolicyRef]:
        """Discovers all active policies by reading the meta.yaml index via settings."""
        refs = []
        # settings._meta_config is a private but convenient accessor here
        policies_in_meta = settings._meta_config.get("charter", {}).get("policies", {})

        # ID: 1fead1e3-077b-4243-92dc-5b151d6fc690
        def find_policies_recursive(data: Any, prefix: str):
            if isinstance(data, dict):
                for key, value in data.items():
                    find_policies_recursive(value, f"{prefix}.{key}" if prefix else key)
            elif isinstance(data, str) and data.endswith("_policy.yaml"):
                logical_path = prefix.replace("charter.policies.", "", 1)
                full_path = settings.get_path(prefix)
                if full_path.exists():
                    refs.append(_PolicyRef(id=logical_path, path=full_path))

        find_policies_recursive(policies_in_meta, "charter.policies")
        return refs

    @staticmethod
    def _extract_rules(policy_data: Dict[str, Any]) -> List[Dict[str, str]]:
        """Extracts and normalizes rule definitions from a policy file."""
        rules = policy_data.get("rules", [])
        if not isinstance(rules, list):
            return [{"id": "__policy_present__", "enforcement": "warn"}]

        extracted = []
        for r in rules:
            if isinstance(r, dict):
                extracted.append(
                    {
                        "id": str(r.get("id", "__missing_id__")),
                        "enforcement": str(r.get("enforcement", "warn")).lower(),
                    }
                )
        return extracted or [{"id": "__policy_present__", "enforcement": "warn"}]

    # ID: 07977c2f-e3df-4c79-a7eb-f7761d4a6487
    def run(self) -> PolicyCoverageReport:
        """
        Executes the policy coverage audit and returns a structured report.
        """
        policies = self._discover_active_policies()
        records: List[Dict[str, Any]] = []
        failures: List[Tuple[str, str]] = []

        for policy_ref in policies:
            policy_data = settings.load(f"charter.policies.{policy_ref.id}")
            rules = self._extract_rules(policy_data)

            for rule in rules:
                level = rule["enforcement"]
                is_covered = bool(rule["id"] != "__missing_id__") and level in [
                    "error",
                    "warn",
                    "info",
                ]

                records.append(
                    {
                        "policy_id": policy_ref.id,
                        "policy_path": str(policy_ref.path.relative_to(self.repo_root)),
                        "rule_id": rule["id"],
                        "enforcement": level,
                        "covered": is_covered,
                    }
                )
                if not is_covered:
                    failures.append((policy_ref.id, level))

        exit_code = 0
        for _, level in failures:
            exit_code = max(exit_code, self.enforcement_model.get(level, 0))

        report_dict = {
            "generated_at_utc": datetime.now(timezone.utc).isoformat(),
            "repo_root": str(self.repo_root),
            "summary": {
                "policies_seen": len(policies),
                "rules_found": len(records),
                "uncovered_rules": len(failures),
            },
            "records": records,
            "exit_code": exit_code,
        }

        report_json = json.dumps(report_dict, sort_keys=True, separators=(",", ":"))
        report_id = hashlib.sha256(report_json.encode("utf-8")).hexdigest()
        report_dict["report_id"] = report_id

        return PolicyCoverageReport(**report_dict)

--- END OF FILE ./src/features/governance/policy_coverage_service.py ---

--- START OF FILE ./src/features/governance/policy_loader.py ---
# src/features/governance/policy_loader.py
"""
Centralized loaders for constitution-backed policies used by agents and services.
- Avoids hardcoding actions/params in code.
- Keeps a single source of truth for Planner/ExecutionAgent validation.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict

import yaml
from shared.logger import getLogger

log = getLogger(__name__)

# Define base paths relative to the assumed .intent directory structure
CONSTITUTION_DIR = Path(".intent/charter")
GOVERNANCE_DIR = CONSTITUTION_DIR / "policies" / "governance"
AGENT_DIR = CONSTITUTION_DIR / "policies" / "agent"


def _load_policy_yaml(path: Path) -> Dict[str, Any]:
    """
    Loads and performs basic validation on a policy YAML file.
    This is now the single source of truth for loading these policies.
    """
    if not path.exists():
        msg = f"Policy file not found: {path}"
        log.error(msg)
        raise ValueError(msg)
    try:
        with path.open("r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
        if not isinstance(data, dict):
            msg = f"Policy file must be a dictionary: {path}"
            log.error(msg)
            raise ValueError(msg)
        return data
    except Exception as e:
        msg = f"Failed to load policy YAML: {path} ({e})"
        log.error(msg)
        raise ValueError(msg) from e


# ID: b843e5d2-401f-4271-8a47-6d722de9b8ce
def load_available_actions() -> Dict[str, Any]:
    """
    Load the canonical list of available actions for the PlannerAgent.
    """
    policy = _load_policy_yaml(GOVERNANCE_DIR / "available_actions_policy.yaml")
    actions = policy.get("actions")
    if not isinstance(actions, list) or not actions:
        raise ValueError("'actions' must be a non-empty list in the policy.")
    return policy


# ID: 29d61bb4-8fdc-42e9-9d1c-30cae93a9e10
def load_micro_proposal_policy() -> Dict[str, Any]:
    """
    Load the Micro-Proposal Policy for autonomous path guardrails.
    """
    policy = _load_policy_yaml(AGENT_DIR / "micro_proposal_policy.yaml")
    rules = policy.get("rules")
    if not isinstance(rules, list) or not rules:
        raise ValueError("'rules' must be a non-empty list in the policy.")
    return policy


__all__ = [
    "load_available_actions",
    "load_micro_proposal_policy",
]

--- END OF FILE ./src/features/governance/policy_loader.py ---

--- START OF FILE ./src/features/governance/runtime_validator.py ---
# src/features/governance/runtime_validator.py
"""
Provides a service to run the project's test suite against proposed code changes
in a safe, isolated "canary" environment.
"""

from __future__ import annotations

import asyncio
import shutil
import tempfile
from pathlib import Path

from rich.console import Console
from shared.logger import getLogger

log = getLogger("runtime_validator")
console = Console()


# ID: c1a2b3d4-e5f6-7a8b-9c0d-1f2a3b4c5d6e
class RuntimeValidatorService:
    """A service to test code changes in an isolated environment."""

    def __init__(self, repo_root: Path):
        self.repo_root = repo_root
        self.ignore_patterns = shutil.ignore_patterns(
            ".git",
            ".venv",
            "venv",
            "__pycache__",
            ".pytest_cache",
            ".ruff_cache",
            "work",
        )

    # ID: d2b3c4d5-e6f7-a8b9-c0d1-e2f3a4b5c6d7
    async def run_tests_in_canary(
        self, file_path_str: str, new_code_content: str
    ) -> tuple[bool, str]:
        """
        Creates a temporary copy of the project, applies the new code, and runs pytest.

        Returns:
            A tuple of (passed: bool, details: str).
        """
        with tempfile.TemporaryDirectory() as tmpdir:
            canary_path = Path(tmpdir) / "canary_repo"
            log.info(f"Creating canary test environment at {canary_path}...")

            try:
                # 1. Create the isolated environment
                shutil.copytree(
                    self.repo_root, canary_path, ignore=self.ignore_patterns
                )

                # 2. Apply the proposed change
                target_file = canary_path / file_path_str
                target_file.parent.mkdir(parents=True, exist_ok=True)
                target_file.write_text(new_code_content, encoding="utf-8")

                # 3. Run the test suite inside the canary environment
                log.info("Running test suite in canary environment...")
                proc = await asyncio.create_subprocess_exec(
                    "poetry",
                    "run",
                    "pytest",
                    cwd=canary_path,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                stdout, stderr = await proc.communicate()

                if proc.returncode == 0:
                    log.info("✅ Canary tests PASSED.")
                    return True, "All tests passed in the isolated environment."
                else:
                    log.warning("❌ Canary tests FAILED.")
                    error_details = (
                        f"Pytest failed with exit code {proc.returncode}.\n\n"
                        f"STDOUT:\n{stdout.decode()}\n\n"
                        f"STDERR:\n{stderr.decode()}"
                    )
                    return False, error_details

            except Exception as e:
                log.error(f"Error during canary test run: {e}", exc_info=True)
                return False, f"An unexpected exception occurred: {str(e)}"

--- END OF FILE ./src/features/governance/runtime_validator.py ---

--- START OF FILE ./src/features/introspection/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/features/introspection/__init__.py ---

--- START OF FILE ./src/features/introspection/audit_unassigned_capabilities.py ---
# src/features/introspection/audit_unassigned_capabilities.py
"""
Provides a utility to find and report on symbols in the knowledge graph
that have not been assigned a capability ID.
"""

from __future__ import annotations

import asyncio
from typing import Any, Dict, List

from core.knowledge_service import KnowledgeService
from shared.config import settings
from shared.logger import getLogger

log = getLogger("audit_unassigned_caps")


# ID: d93e7a47-27c1-4fa5-bf39-0a44bef8bf59
def get_unassigned_symbols() -> List[Dict[str, Any]]:
    """
    Scans the knowledge graph for governable symbols with a capability of
    'unassigned' and returns them.
    """

    async def _async_get():
        knowledge_service = KnowledgeService(settings.REPO_PATH)
        graph = await knowledge_service.get_graph()
        symbols = graph.get("symbols", {})
        unassigned = []

        for key, symbol_data in symbols.items():
            is_public = not symbol_data.get("name", "").startswith("_")
            is_unassigned = symbol_data.get("capability") == "unassigned"

            if is_public and is_unassigned:
                symbol_data["key"] = key
                unassigned.append(symbol_data)
        return unassigned

    try:
        return asyncio.run(_async_get())
    except Exception as e:
        log.error(f"Error processing knowledge graph: {e}")
        return []

--- END OF FILE ./src/features/introspection/audit_unassigned_capabilities.py ---

--- START OF FILE ./src/features/introspection/capability_discovery_service.py ---
# src/features/introspection/capability_discovery_service.py
"""
Refactored under dry_by_design.
Pattern: move_function.
Removed local _load_yaml in favor of the canonical implementation from shared.config_loader.
"""

from __future__ import annotations

from pathlib import Path
from typing import Dict, Iterable, List, Optional, Set

from shared.config_loader import load_yaml_file
from shared.logger import getLogger
from shared.models import CapabilityMeta

log = getLogger("capability_discovery")


# ID: 0a3c2441-928c-47e6-9f9d-3663b31245af
class CapabilityRegistry:
    """
    Holds the canonical capability keys and alias mapping.
    Provides simple resolution (canonical → itself, alias → canonical).
    """

    def __init__(self, canonical: Set[str], aliases: Dict[str, str]):
        """Initializes the registry with canonical tags and an alias map."""
        self.canonical: Set[str] = set(canonical)
        self.aliases: Dict[str, str] = dict(aliases)

    # ID: 6d38d34c-a0da-4bce-a961-c3ff9c0f093e
    def resolve(self, tag: str) -> Optional[str]:
        """
        Return canonical capability if `tag` is known, otherwise None.
        Resolution is single-hop (alias -> canonical).
        """
        if tag in self.canonical:
            return tag
        return self.aliases.get(tag)


def _iter_capability_files(base: Path) -> Iterable[Path]:
    """
    Yields YAML files under capability_tags/, ignoring schema and non-yaml files.
    """
    if not base.exists():
        return []
    for p in sorted(base.glob("**/*")):
        if p.is_dir():
            if p.name in {"schemas"}:
                continue
            continue
        if p.suffix.lower() in {".yaml", ".yml"}:
            yield p


def _extract_canonical_from_doc(doc: dict) -> Set[str]:
    """
    Extracts canonical capability keys from a domain manifest file.
    """
    canonical: Set[str] = set()
    tags = doc.get("tags", [])
    if isinstance(tags, list):
        for item in tags:
            if (
                isinstance(item, dict)
                and "key" in item
                and isinstance(item["key"], str)
            ):
                canonical.add(item["key"])
    return canonical


def _extract_aliases_from_doc(doc: dict) -> Dict[str, str]:
    """
    Extracts aliases from a manifest file.
    """
    aliases: Dict[str, str] = {}
    raw = doc.get("aliases")
    if isinstance(raw, dict):
        for k, v in raw.items():
            if isinstance(k, str) and isinstance(v, str) and k and v:
                aliases[k] = v
    return aliases


def _merge_sets(*sets: Iterable[str]) -> Set[str]:
    """Merges multiple iterables into a single set."""
    acc: Set[str] = set()
    for s in sets:
        acc.update(s)
    return acc


def _detect_alias_cycles(aliases: Dict[str, str]) -> List[List[str]]:
    """Detects simple cycles in the alias graph."""
    visited: Set[str] = set()
    stack: Set[str] = set()
    cycles: List[List[str]] = []

    # ID: 208ce23e-ee4f-4e52-90e8-f2a8949fc284
    def dfs(node: str, path: List[str]):
        visited.add(node)
        stack.add(node)
        nxt = aliases.get(node)
        if nxt:
            if nxt not in visited:
                dfs(nxt, path + [nxt])
            elif nxt in stack:
                if nxt in path:
                    idx = path.index(nxt)
                    cycles.append(path[idx:] + [nxt])
        stack.remove(node)

    for a in aliases:
        if a not in visited:
            dfs(a, [a])

    return cycles


# ID: 2779fe54-cfaf-4b3b-8df5-156347d53166
def load_and_validate_capabilities(intent_dir: Path) -> CapabilityRegistry:
    """
    Loads and validates all canonical capabilities and aliases.
    """
    base = intent_dir / "knowledge" / "capability_tags"
    canonical_tags: Set[str] = set()
    alias_map: Dict[str, str] = {}

    if not base.exists():
        raise FileNotFoundError(f"Capability tags directory not found: {base}")

    for path in _iter_capability_files(base):
        try:
            doc = load_yaml_file(path)
        except Exception as e:
            raise ValueError(f"Failed to load capability YAML: {path} ({e})") from e

        canonical_tags |= _extract_canonical_from_doc(doc)
        alias_map.update(_extract_aliases_from_doc(doc))

    cycles = _detect_alias_cycles(alias_map)
    if cycles:
        formatted = "; ".join(" -> ".join(c) for c in cycles)
        raise ValueError(f"Alias cycle(s) detected: {formatted}")

    unresolved = [(a, t) for a, t in alias_map.items() if t not in canonical_tags]
    if unresolved:
        lines = "\n - ".join(f"'{a}' → '{t}'" for a, t in unresolved)
        raise ValueError(
            "Alias targets that do not map to a canonical capability:\n - " + lines
        )

    return CapabilityRegistry(canonical=canonical_tags, aliases=alias_map)


# ID: 8bd2e3d4-f273-4d7d-bf6d-a47b7f0fefce
def validate_agent_roles(agent_roles: dict, registry: CapabilityRegistry) -> None:
    """Validates agent role configurations against the capability registry."""
    errors: List[str] = []
    roles = agent_roles.get("roles", {})
    if not isinstance(roles, dict):
        raise ValueError("agent_roles must contain a 'roles' mapping")

    for role, cfg in roles.items():
        allowed = cfg.get("allowed_tags", [])
        for tag in allowed:
            if not registry.resolve(tag):
                errors.append(
                    f"Role '{role}' references unknown capability tag '{tag}'"
                )
    if errors:
        joined = "\n - ".join(errors)
        raise ValueError(
            "Agent role configuration contains unresolved/invalid capability tags:\n - "
            + joined
        )


# ID: 650d3944-b37d-4aaf-8f7f-d0c08530cb86
def collect_code_capabilities(
    root: Path, include_globs: List[str], exclude_globs: List[str], require_kgb: bool
) -> Dict[str, CapabilityMeta]:
    """Unified discovery entrypoint."""
    from features.introspection.discovery.from_kgb import collect_from_kgb
    from features.introspection.discovery.from_source_scan import (
        collect_from_source_scan,
    )

    try:
        if require_kgb:
            return collect_from_kgb(root)
        return collect_from_source_scan(root, include_globs, exclude_globs)
    except Exception as e:
        log.warning(
            f"Capability discovery failed: {e}. Returning empty.", exc_info=True
        )
        return {}

--- END OF FILE ./src/features/introspection/capability_discovery_service.py ---

--- START OF FILE ./src/features/introspection/discovery/from_kgb.py ---
# src/features/introspection/discovery/from_kgb.py
"""
Discovers implemented capabilities by leveraging the KnowledgeGraphBuilder.
"""

from __future__ import annotations

from pathlib import Path
from typing import Dict

from features.introspection.knowledge_graph_service import KnowledgeGraphBuilder
from shared.models import CapabilityMeta


# ID: 12a7fddd-fa62-4dd8-8e1b-54208392a078
def collect_from_kgb(root: Path) -> Dict[str, CapabilityMeta]:
    """
    Uses the KnowledgeGraphBuilder to find all capabilities.
    """
    builder = KnowledgeGraphBuilder(root_path=root)
    graph = builder.build()

    capabilities: Dict[str, CapabilityMeta] = {}
    for symbol in graph.get("symbols", {}).values():
        cap_key = symbol.get("capability")
        if cap_key and cap_key != "unassigned":
            capabilities[cap_key] = CapabilityMeta(
                key=cap_key,
                domain=symbol.get("domain"),
                owner=symbol.get("owner"),
            )
    return capabilities

--- END OF FILE ./src/features/introspection/discovery/from_kgb.py ---

--- START OF FILE ./src/features/introspection/discovery/from_manifest.py ---
# src/features/introspection/discovery/from_manifest.py
"""
Discovers capability definitions by parsing constitutional manifest files.
"""

from __future__ import annotations

from pathlib import Path
from typing import Dict

import yaml
from shared.logger import getLogger
from shared.models import CapabilityMeta

log = getLogger("discovery.from_manifest")


# ID: 67f5324b-5dbd-4250-a216-bbd557d3c8e9
def load_manifest_capabilities(
    root: Path, explicit_path: Path | None = None
) -> Dict[str, CapabilityMeta]:
    """
    Scans for manifest files and aggregates all declared capabilities.
    The primary source of truth is now .intent/mind/project_manifest.yaml.
    """
    capabilities: Dict[str, CapabilityMeta] = {}

    manifest_path = root / ".intent" / "mind" / "project_manifest.yaml"

    if manifest_path.exists():
        try:
            content = yaml.safe_load(manifest_path.read_text("utf-8")) or {}
            caps = content.get("capabilities", [])

            if isinstance(caps, list):
                for key in caps:
                    if isinstance(key, str):
                        # --- THIS IS THE FIX ---
                        # Instead of storing None, we store an actual instance
                        # of the CapabilityMeta dataclass, as the consumer expects.
                        capabilities[key] = CapabilityMeta(key=key)
                        # --- END OF FIX ---

        except (yaml.YAMLError, IOError) as e:
            log.warning(f"Could not parse manifest at {manifest_path}: {e}")

    return capabilities

--- END OF FILE ./src/features/introspection/discovery/from_manifest.py ---

--- START OF FILE ./src/features/introspection/discovery/from_source_scan.py ---
# src/features/introspection/discovery/from_source_scan.py
"""
Discovers implemented capabilities by performing a direct source code scan.
This is a fallback for when the knowledge graph is not available.
"""

from __future__ import annotations

import re
from pathlib import Path
from typing import Dict, List

from shared.models import CapabilityMeta

CAPABILITY_PATTERN = re.compile(r"#\s*CAPABILITY:\s*(\S+)")


# ID: 3fb50751-54f5-4282-9b52-fcc5eb6c23d2
def collect_from_source_scan(
    root: Path, include_globs: List[str], exclude_globs: List[str]
) -> Dict[str, CapabilityMeta]:
    """
    Scans Python files for # CAPABILITY tags.
    """
    capabilities: Dict[str, CapabilityMeta] = {}
    search_path = root / "src"

    files_to_scan = list(search_path.rglob("*.py"))

    for py_file in files_to_scan:
        try:
            content = py_file.read_text("utf-8")
            matches = CAPABILITY_PATTERN.findall(content)
            for cap_key in matches:
                if cap_key not in capabilities:
                    capabilities[cap_key] = CapabilityMeta(key=cap_key)
        except (IOError, UnicodeDecodeError):
            continue

    return capabilities

--- END OF FILE ./src/features/introspection/discovery/from_source_scan.py ---

--- START OF FILE ./src/features/introspection/drift_detector.py ---
# src/features/introspection/drift_detector.py
"""
Detects drift between declared capabilities in manifests and implemented
capabilities in the source code.
"""

from __future__ import annotations

import json
from dataclasses import asdict
from pathlib import Path
from typing import Dict, Set

from shared.models import CapabilityMeta, DriftReport


# ID: 6cc5efdf-037e-4862-b13e-0a569d889a97
def detect_capability_drift(
    manifest_caps: Dict[str, CapabilityMeta], code_caps: Dict[str, CapabilityMeta]
) -> DriftReport:
    """
    Compares two dictionaries of capabilities and returns a drift report.
    """
    manifest_keys: Set[str] = set(manifest_caps.keys())
    code_keys: Set[str] = set(code_caps.keys())

    missing_in_code = sorted(list(manifest_keys - code_keys))
    undeclared_in_manifest = sorted(list(code_keys - manifest_keys))

    mismatched = []
    for key in manifest_keys.intersection(code_keys):
        manifest_cap = manifest_caps[key]
        code_cap = code_caps[key]
        if manifest_cap != code_cap:
            mismatched.append(
                {
                    "capability": key,
                    "manifest": asdict(manifest_cap),
                    "code": asdict(code_cap),
                }
            )

    return DriftReport(
        missing_in_code=missing_in_code,
        undeclared_in_manifest=undeclared_in_manifest,
        mismatched_mappings=mismatched,
    )


# ID: db10bc9b-b4b3-41f2-8d81-b32731540d95
def write_report(path: Path, report: DriftReport) -> None:
    """Writes the drift report to a JSON file."""
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(report.to_dict(), indent=2), encoding="utf-8")

--- END OF FILE ./src/features/introspection/drift_detector.py ---

--- START OF FILE ./src/features/introspection/drift_service.py ---
# src/features/introspection/drift_service.py
"""
Provides a dedicated service for detecting drift between the declared constitution
and the implemented reality of the codebase.
"""

from __future__ import annotations

from pathlib import Path
from typing import Dict

from core.knowledge_service import KnowledgeService
from shared.models import CapabilityMeta, DriftReport

from features.introspection.discovery.from_manifest import (
    load_manifest_capabilities,
)
from features.introspection.drift_detector import detect_capability_drift


# ID: 58d789bd-6dc5-440d-ad53-efb8a204b4d3
async def run_drift_analysis_async(root: Path) -> DriftReport:
    """
    Performs a full drift analysis by comparing manifest capabilities
    against the capabilities discovered in the codebase via the KnowledgeService.
    """
    manifest_caps = load_manifest_capabilities(root, explicit_path=None)

    knowledge_service = KnowledgeService(root)
    graph = await knowledge_service.get_graph()

    code_caps: Dict[str, CapabilityMeta] = {}
    for symbol in graph.get("symbols", {}).values():
        key = symbol.get("key")
        if key and key != "unassigned":
            code_caps[key] = CapabilityMeta(
                key=key,
                domain=symbol.get("domain"),
                owner=symbol.get("owner"),
            )

    return detect_capability_drift(manifest_caps, code_caps)

--- END OF FILE ./src/features/introspection/drift_service.py ---

--- START OF FILE ./src/features/introspection/export_vectors.py ---
# src/features/introspection/export_vectors.py
"""
A utility to export all vectors and their payloads from the Qdrant database
to a local JSONL file for analysis, clustering, or backup.
"""

from __future__ import annotations

import asyncio
import json
from pathlib import Path

import typer
from rich.console import Console
from rich.progress import track
from services.clients.qdrant_client import QdrantService
from shared.logger import getLogger

log = getLogger("export_vectors")
console = Console()


async def _async_export(output_path: Path):
    """The core async logic for exporting vectors."""
    console.print(
        f"🚀 Exporting all vectors to [bold cyan]{output_path}[/bold cyan]..."
    )
    output_path.parent.mkdir(parents=True, exist_ok=True)

    qdrant_service = QdrantService()

    try:
        all_vectors = await qdrant_service.get_all_vectors()

        if not all_vectors:
            console.print(
                "[yellow]No vectors found in the database to export.[/yellow]"
            )
            return

        count = 0
        with output_path.open("w", encoding="utf-8") as f:
            for record in track(all_vectors, description="Writing vectors..."):
                line_data = {
                    "id": record.id,
                    "payload": record.payload,
                    "vector": record.vector,
                }
                f.write(json.dumps(line_data) + "\n")
                count += 1

        console.print(
            f"[bold green]✅ Successfully exported {count} vectors.[/bold green]"
        )

    except Exception as e:
        log.error(f"Failed to export vectors: {e}", exc_info=True)
        console.print(f"[bold red]❌ An error occurred during export: {e}[/bold red]")
        raise typer.Exit(code=1)


# ID: 51a560a2-7304-49d9-9b31-364cc68ae0c3
def export_vectors(
    output: Path = typer.Option(
        "reports/vectors_export.jsonl",
        "--output",
        "-o",
        help="The path to save the exported JSONL file.",
    ),
):
    """Exports all vectors from Qdrant to a JSONL file."""
    asyncio.run(_async_export(output))


if __name__ == "__main__":
    typer.run(export_vectors)

--- END OF FILE ./src/features/introspection/export_vectors.py ---

--- START OF FILE ./src/features/introspection/generate_capability_docs.py ---
# src/features/introspection/generate_capability_docs.py
"""
Generates the canonical capability reference documentation from the database.
"""

from __future__ import annotations

import asyncio

from rich.console import Console
from services.repositories.db.engine import get_session
from shared.config import settings
from sqlalchemy import text

console = Console()

# --- Configuration ---
OUTPUT_PATH = settings.REPO_PATH / "docs" / "10_CAPABILITY_REFERENCE.md"
GITHUB_URL_BASE = "https://github.com/DariuszNewecki/CORE/blob/main/"

HEADER = """
# 10. Capability Reference

This document is the canonical, auto-generated reference for all capabilities recognized by the CORE constitution.
It is generated from the `core.knowledge_graph` database view and should not be edited manually.
"""


async def _fetch_capabilities() -> list[dict]:
    """Fetches all public capabilities from the database knowledge graph view."""
    console.print("[cyan]Fetching capabilities from the database...[/cyan]")
    async with get_session() as session:
        stmt = text(
            """
            SELECT capability, intent, file, line_number
            FROM core.knowledge_graph
            WHERE is_public = TRUE AND capability IS NOT NULL
            ORDER BY capability;
            """
        )
        result = await session.execute(stmt)
        return [dict(row._mapping) for row in result]


def _group_by_domain(capabilities: list[dict]) -> dict[str, list[dict]]:
    """Groups capabilities by their domain prefix."""
    domains = {}
    for cap in capabilities:
        key = cap["capability"]
        # Infer domain from the key, e.g., 'autonomy.self_healing.fix_headers' -> 'autonomy.self_healing'
        domain_key = ".".join(key.split(".")[:-1]) if "." in key else "general"
        if domain_key not in domains:
            domains[domain_key] = []
        domains[domain_key].append(cap)
    return domains


# ID: 2ea63de3-081d-40b3-9386-0d372487aabd
def main():
    """The main entry point for the documentation generation script."""

    async def _async_main():
        capabilities = await _fetch_capabilities()
        if not capabilities:
            console.print(
                "[yellow]Warning: No capabilities found in the database. Documentation will be empty.[/yellow]"
            )
            return

        domains = _group_by_domain(capabilities)

        console.print(
            f"[cyan]Generating documentation for {len(capabilities)} capabilities across {len(domains)} domains...[/cyan]"
        )

        md_content = [HEADER.strip(), ""]

        for domain_name in sorted(domains.keys()):
            md_content.append(f"## Domain: `{domain_name}`")
            md_content.append("")

            for cap in sorted(domains[domain_name], key=lambda x: x["capability"]):
                md_content.append(f"- **`{cap['capability']}`**")

                description = cap.get("intent") or "No description provided."
                md_content.append(f"  - **Description:** {description.strip()}")

                file_path = cap.get("file")
                # Use a default line number if it's missing to avoid errors
                line_number = cap.get("line_number") or 0
                github_link = f"{GITHUB_URL_BASE}{file_path}#L{line_number + 1}"
                md_content.append(f"  - **Source:** [{file_path}]({github_link})")
            md_content.append("")

        final_text = "\n".join(md_content)

        OUTPUT_PATH.write_text(final_text, encoding="utf-8")
        console.print(
            f"[bold green]✅ Capability reference documentation successfully written to {OUTPUT_PATH}[/bold green]"
        )

    asyncio.run(_async_main())


if __name__ == "__main__":
    main()

--- END OF FILE ./src/features/introspection/generate_capability_docs.py ---

--- START OF FILE ./src/features/introspection/generate_correction_map.py ---
# src/features/introspection/generate_correction_map.py
"""
A utility to generate alias maps from semantic clustering results.
It takes the proposed domain mappings and creates a YAML file that can be used
by the AliasResolver to standardize capability keys.
"""

from __future__ import annotations

import json
from pathlib import Path

import typer
import yaml
from rich.console import Console
from shared.logger import getLogger

log = getLogger("generate_correction_map")
console = Console()


# ID: b6657e93-2382-43ef-b9fb-71104aecee1f
def generate_maps(
    input_path: Path = typer.Option(
        "reports/proposed_domains.json",
        "--input",
        "-i",
        help="Path to the JSON file with proposed domains from clustering.",
        exists=True,
    ),
    output: Path = typer.Option(
        "reports/aliases.yaml",
        "--output",
        "-o",
        help="Path to save the generated aliases YAML file.",
    ),
):
    """
    Generates an alias map from clustering results to a YAML file.
    """
    console.print(
        f"🗺️  Generating alias map from [bold cyan]{input_path}[/bold cyan]..."
    )

    try:
        proposed_domains = json.loads(input_path.read_text("utf-8"))
    except (json.JSONDecodeError, FileNotFoundError) as e:
        log.error(f"Failed to load or parse input file: {e}")
        raise typer.Exit(code=1)

    # In this simplified model, we might just be creating a map of old_key -> new_key
    # For now, let's assume the clustering output is a simple dictionary.
    # A more complex implementation might rationalize domains.

    alias_map = {"aliases": proposed_domains}

    output.parent.mkdir(parents=True, exist_ok=True)
    output.write_text(yaml.dump(alias_map, indent=2, sort_keys=True), "utf-8")

    console.print(
        f"✅ Successfully generated alias map with {len(proposed_domains)} entries."
    )
    console.print(f"   -> Saved to: [bold green]{output}[/bold green]")


if __name__ == "__main__":
    typer.run(generate_maps)

--- END OF FILE ./src/features/introspection/generate_correction_map.py ---

--- START OF FILE ./src/features/introspection/graph_analysis_service.py ---
# src/features/introspection/graph_analysis_service.py
"""
Provides a service for finding semantic clusters of symbols in the codebase
using K-Means clustering on their vector embeddings.
"""

from __future__ import annotations

from typing import List

import numpy as np
from rich.console import Console

# --- START OF FIX: Corrected imports ---
from services.clients.qdrant_client import QdrantService
from shared.logger import getLogger

try:
    from sklearn.cluster import KMeans
except ImportError:
    KMeans = None
# --- END OF FIX ---

log = getLogger("graph_analysis_service")
console = Console()


# ID: 1a2b3c4d-5e6f-7a8b-9c0d-1e2f3a4b5c6d
async def find_semantic_clusters(
    n_clusters: int = 15,
) -> List[List[str]]:
    """
    Finds clusters of semantically similar code symbols using K-Means clustering.
    """
    if KMeans is None:
        log.error(
            "scikit-learn is not installed. Cannot perform clustering. "
            "Please run 'poetry install --with dev'."
        )
        return []

    log.info(f"Finding {n_clusters} semantic clusters using K-Means...")
    qdrant_service = QdrantService()

    try:
        all_points = await qdrant_service.get_all_vectors()
        if not all_points:
            log.warning("No vectors found in Qdrant. Cannot perform clustering.")
            return []

        # --- START OF FIX: Prepare data for K-Means ---
        vectors = []
        symbol_keys = []
        for point in all_points:
            if point.payload and "symbol" in point.payload and point.vector:
                symbol_keys.append(point.payload["symbol"])
                vectors.append(point.vector)

        if not vectors:
            log.warning("No valid vectors with symbol payloads found.")
            return []

        log.info(f"Clustering {len(vectors)} vectors into {n_clusters} domains...")
        vector_array = np.array(vectors, dtype=np.float32)

        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init="auto")
        labels = kmeans.fit_predict(vector_array)

        # Group symbol keys by their predicted cluster label
        clusters: List[List[str]] = [[] for _ in range(n_clusters)]
        for i, label in enumerate(labels):
            clusters[label].append(symbol_keys[i])
        # --- END OF FIX ---

        log.info(f"Found {len(clusters)} semantic clusters.")

        # Sort clusters by size, largest first, and remove empty ones
        clusters.sort(key=len, reverse=True)
        non_empty_clusters = [c for c in clusters if c]

        return non_empty_clusters

    except Exception as e:
        log.error(f"Failed to find semantic clusters: {e}", exc_info=True)
        return []

--- END OF FILE ./src/features/introspection/graph_analysis_service.py ---

--- START OF FILE ./src/features/introspection/knowledge_graph_service.py ---
# src/features/introspection/knowledge_graph_service.py
"""
Provides the KnowledgeGraphBuilder, the primary tool for introspecting the
codebase and creating an in-memory representation of its symbols.
"""

from __future__ import annotations

import ast
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List

import yaml
from shared.ast_utility import (
    FunctionCallVisitor,
    calculate_structural_hash,
    extract_base_classes,
    extract_docstring,
    extract_parameters,
    parse_metadata_comment,
)
from shared.config import settings
from shared.logger import getLogger

log = getLogger("knowledge_graph_builder")


# ID: efd11315-be84-44cd-ace1-f688d85a9d86
class KnowledgeGraphBuilder:
    """
    Scans the source code to build a comprehensive in-memory knowledge graph.
    It does not interact with the database; that is handled by the sync_service.
    """

    def __init__(self, root_path: Path):
        self.root_path = root_path
        self.intent_dir = self.root_path / ".intent"
        self.src_dir = self.root_path / "src"
        self.symbols: Dict[str, Dict[str, Any]] = {}
        self.domain_map = self._load_domain_map()
        self.entry_point_patterns = self._load_entry_point_patterns()

    def _load_domain_map(self) -> Dict[str, str]:
        """Loads the architectural domain map from the constitution."""
        try:
            structure_path = (
                self.intent_dir / "mind" / "knowledge" / "source_structure.yaml"
            )
            structure = yaml.safe_load(structure_path.read_text("utf-8"))
            return {
                str(self.src_dir / d.get("path", "").replace("src/", "")): d["domain"]
                for d in structure.get("structure", [])
            }
        except (FileNotFoundError, yaml.YAMLError, KeyError):
            return {}

    def _load_entry_point_patterns(self) -> List[Dict[str, Any]]:
        """Loads the declarative patterns for identifying system entry points."""
        try:
            patterns_path = (
                self.intent_dir / "mind" / "knowledge" / "entry_point_patterns.yaml"
            )
            patterns = yaml.safe_load(patterns_path.read_text("utf-8"))
            return patterns.get("patterns", [])
        except (FileNotFoundError, yaml.YAMLError):
            return []

    # ID: f5689b89-8060-4328-a9f4-0d4e2ad77175
    def build(self) -> Dict[str, Any]:
        """
        Executes the full build process for the knowledge graph and returns it.
        """
        log.info(f"Building knowledge graph for repository at: {self.root_path}")
        for py_file in self.src_dir.rglob("*.py"):
            self._scan_file(py_file)

        knowledge_graph = {
            "metadata": {
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "repo_root": str(self.root_path),
            },
            "symbols": self.symbols,
        }

        output_path = settings.REPO_PATH / "reports" / "knowledge_graph.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(json.dumps(knowledge_graph, indent=2))
        log.info(
            f"Knowledge graph artifact with {len(self.symbols)} symbols saved to {output_path}"
        )

        return knowledge_graph

    def _scan_file(self, file_path: Path):
        """Scans a single Python file and adds its symbols to the graph."""
        try:
            content = file_path.read_text(encoding="utf-8")
            tree = ast.parse(content, filename=str(file_path))
            source_lines = content.splitlines()

            for node in ast.walk(tree):
                if isinstance(
                    node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                ):
                    self._process_symbol(node, file_path, source_lines)
        except Exception as e:
            log.error(f"Failed to process file {file_path}: {e}")

    def _determine_domain(self, file_path: Path) -> str:
        """Determines the architectural domain of a file."""
        abs_file_path = file_path.resolve()
        for domain_path, domain_name in self.domain_map.items():
            if str(abs_file_path).startswith(str(Path(domain_path).resolve())):
                return domain_name
        return "unknown"

    def _process_symbol(self, node: ast.AST, file_path: Path, source_lines: List[str]):
        """Extracts all relevant data from a symbol AST node."""
        if not hasattr(node, "name"):
            return

        rel_path = file_path.relative_to(self.root_path)
        symbol_path_key = f"{rel_path}::{node.name}"
        metadata = parse_metadata_comment(node, source_lines)
        docstring = (extract_docstring(node) or "").strip()

        call_visitor = FunctionCallVisitor()
        call_visitor.visit(node)

        symbol_data = {
            "uuid": symbol_path_key,
            "key": metadata.get("capability"),
            "symbol_path": symbol_path_key,
            "name": node.name,
            "type": type(node).__name__,
            "file_path": str(rel_path),
            "is_public": not node.name.startswith("_"),
            "title": node.name.replace("_", " ").title(),
            "description": docstring.split("\n")[0] if docstring else None,
            "docstring": docstring,
            "calls": sorted(list(set(call_visitor.calls))),
            "line_number": node.lineno,
            "end_line_number": getattr(node, "end_lineno", node.lineno),
            "is_async": isinstance(node, ast.AsyncFunctionDef),
            "parameters": extract_parameters(node) if hasattr(node, "args") else [],
            "is_class": isinstance(node, ast.ClassDef),
            "base_classes": (
                extract_base_classes(node) if isinstance(node, ast.ClassDef) else []
            ),
            "structural_hash": calculate_structural_hash(node),
        }
        self.symbols[symbol_path_key] = symbol_data

--- END OF FILE ./src/features/introspection/knowledge_graph_service.py ---

--- START OF FILE ./src/features/introspection/knowledge_helpers.py ---
# src/features/introspection/knowledge_helpers.py
"""
Helper utilities for knowledge graph vectorization:
- extract_source_code
- reporting helpers (log_failure)
"""

from __future__ import annotations

import ast
from pathlib import Path
from typing import Any, Dict

from shared.logger import getLogger

log = getLogger("core_admin.knowledge.helpers")


# ID: 225abffb-ec3e-4798-a75d-1be1697b0e27
def extract_source_code(repo_root: Path, symbol_data: Dict[str, Any]) -> str | None:
    """
    Extracts the source code for a symbol using its database record.
    This is the single, canonical implementation for reading symbol source.
    """
    module_path = symbol_data.get("module")
    symbol_path_str = symbol_data.get("symbol_path")

    if not module_path or not symbol_path_str:
        log.warning(
            "Cannot extract source code: symbol data is missing 'module' or 'symbol_path'."
        )
        return None

    # Convert module path (e.g., 'core.agents.planner') to file system path
    file_system_path_str = "src/" + module_path.replace(".", "/") + ".py"
    file_path = repo_root / file_system_path_str

    if not file_path.exists():
        log.warning(
            f"Source file not found for symbol {symbol_path_str} at expected path {file_path}"
        )
        return None

    symbol_name = symbol_path_str.split("::")[-1]

    try:
        content = file_path.read_text("utf-8")
        tree = ast.parse(content, filename=str(file_path))
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                current_symbol_name = getattr(node, "name", None)
                if current_symbol_name == symbol_name:
                    return ast.get_source_segment(content, node)
    except Exception as e:
        log.warning(
            f"AST parsing failed for {file_path} while seeking {symbol_name}: {e}"
        )
        return None

    return None


# ID: a42a6659-2ee3-4400-815d-d60280165229
def log_failure(failure_log_path: Path, key: str, message: str, category: str) -> None:
    """Append a failure line to the given log file path. Ensures parent exists."""
    failure_log_path.parent.mkdir(parents=True, exist_ok=True)
    with failure_log_path.open("a", encoding="utf-8") as f:
        f.write(f"{category}\t{key}\t{message}\n")

--- END OF FILE ./src/features/introspection/knowledge_helpers.py ---

--- START OF FILE ./src/features/introspection/knowledge_vectorizer.py ---
# src/system/admin/knowledge_vectorizer.py
"""
Handles the vectorization of individual capabilities (per-chunk), including interaction with Qdrant.
Idempotency is enforced at the chunk (symbol_key) level via `chunk_id` stored in the payload.
"""

from __future__ import annotations

from datetime import datetime, timezone
from pathlib import Path
from typing import Dict

from core.cognitive_service import CognitiveService
from services.clients.qdrant_client import QdrantService
from shared.config import settings
from shared.logger import getLogger
from shared.services.embedding_utils import normalize_text, sha256_hex

from .knowledge_helpers import extract_source_code, log_failure

log = getLogger("core_admin.knowledge")

DEFAULT_PAGE_SIZE = 250
MAX_SCROLL_LIMIT = 10000


# ID: 81ddb9e8-60c9-4564-bd08-b5e6c2843381
async def get_stored_chunks(qdrant_service: QdrantService) -> Dict[str, dict]:
    """
    Return mapping: chunk_id (symbol_key) -> {hash, rev, point_id, capability}
    """
    log.info("Checking Qdrant for already vectorized chunks...")
    chunks: Dict[str, dict] = {}
    next_offset = None
    try:
        while True:
            stored_points, next_offset = await qdrant_service.client.scroll(
                collection_name=qdrant_service.collection_name,
                limit=DEFAULT_PAGE_SIZE,
                offset=next_offset,
                with_payload=[
                    "chunk_id",
                    "content_sha256",
                    "model_rev",
                    "capability_tags",
                ],
                with_vectors=False,
            )
            for point in stored_points:
                payload = point.payload or {}
                cid = payload.get("chunk_id")
                if not cid:
                    continue
                chunks[cid] = {
                    "hash": payload.get("content_sha256"),
                    "rev": payload.get("model_rev"),
                    "point_id": str(point.id),
                    "capability": (payload.get("capability_tags") or [None])[0],
                }
            if not next_offset:
                break
        log.info(f"Found {len(chunks)} chunks already in Qdrant")
        return chunks
    except Exception as e:
        log.warning(f"Could not retrieve stored chunks from Qdrant: {e}")
        return {}


# ID: 9e54c111-0ffc-4a99-b243-8b89569335e1
async def sync_existing_vector_ids(
    qdrant_service: QdrantService, symbols_map: dict
) -> int:
    """
    Sync vector IDs from Qdrant for chunks (symbols) that already exist
    but don't have vector_id in knowledge graph.
    """
    log.info("Syncing existing vector IDs from Qdrant...")
    try:
        stored_points, _ = await qdrant_service.client.scroll(
            collection_name=qdrant_service.collection_name,
            limit=MAX_SCROLL_LIMIT,
            with_payload=["chunk_id"],
            with_vectors=False,
        )
        chunk_to_point_id: Dict[str, str] = {
            p.payload["chunk_id"]: str(p.id)
            for p in stored_points
            if p.payload and "chunk_id" in p.payload
        }
        synced_count = 0
        for symbol_key, symbol_data in symbols_map.items():
            if not symbol_data.get("vector_id") and symbol_key in chunk_to_point_id:
                symbol_data["vector_id"] = chunk_to_point_id[symbol_key]
                synced_count += 1
        if synced_count > 0:
            log.info(f"Synced {synced_count} existing vector IDs from Qdrant")
        return synced_count
    except Exception as e:
        log.warning(f"Could not sync existing vector IDs from Qdrant: {e}")
        return 0


# ID: 5140843f-a6d0-44e1-a592-2b82c33d7fa9
async def process_vectorization_task(
    task: dict,
    repo_root: Path,
    symbols_map: dict,
    cognitive_service: CognitiveService,
    qdrant_service: QdrantService,
    dry_run: bool,
    failure_log_path: Path,
    verbose: bool,
    stored_chunks: Dict[str, dict] | None = None,
) -> bool:
    """
    Process a single vectorization task. It assumes the decision to process has already been made.
    """
    cap_key = task["cap_key"]
    symbol_key = task["symbol_key"]

    try:
        source_code = extract_source_code(repo_root, symbols_map[symbol_key])
        if source_code is None:
            raise ValueError("Source code could not be extracted.")

        normalized_code = normalize_text(source_code)
        content_hash = sha256_hex(normalized_code)

        # The redundant skipping logic has been REMOVED.
        # This function now unconditionally processes the task it is given.

        log.debug(f"Processing chunk '{symbol_key}' (cap: {cap_key})")
        vector = await cognitive_service.get_embedding_for_code(normalized_code)

        payload_data = {
            "source_path": symbols_map[symbol_key].get("file"),
            "source_type": "code",
            "chunk_id": symbol_key,
            "content_sha256": content_hash,
            "language": "python",
            "symbol": symbol_key,
            "capability_tags": [cap_key],
            "model_rev": settings.EMBED_MODEL_REVISION,
        }

        if dry_run:
            symbols_map[symbol_key]["vector_id"] = f"dry_run_{symbol_key}"
            log.info(f"[DRY RUN] Would vectorize '{cap_key}' (chunk: {symbol_key})")
            return True

        point_id = await qdrant_service.upsert_capability_vector(
            vector=vector,
            payload_data=payload_data,
        )
        symbols_map[symbol_key].update(
            {
                "vector_id": str(point_id),
                "vectorized_at": datetime.now(timezone.utc).isoformat(),
                "embedding_model": settings.LOCAL_EMBEDDING_MODEL_NAME,
                "model_revision": settings.EMBED_MODEL_REVISION,
                "content_hash": content_hash,
            }
        )
        log.debug(
            f"Successfully vectorized '{cap_key}' (chunk: {symbol_key}) with ID: {point_id}"
        )
        return True

    except Exception as e:
        log.error(f"Failed to process capability '{cap_key}': {e}")
        if not dry_run:
            log_failure(failure_log_path, cap_key, str(e), "knowledge_vectorize")
        if verbose:
            log.exception(f"Detailed error for '{cap_key}':")
        return False

--- END OF FILE ./src/features/introspection/knowledge_vectorizer.py ---

--- START OF FILE ./src/features/introspection/semantic_clusterer.py ---
# src/system/tools/semantic_clusterer.py
"""
Performs semantic clustering on exported capability vectors to discover data-driven domains.
"""

from __future__ import annotations

import json
from pathlib import Path

import numpy as np
import typer
from dotenv import load_dotenv
from shared.logger import getLogger

try:
    from sklearn.cluster import KMeans
except ImportError:
    KMeans = None

log = getLogger("core_tools.semantic_clusterer")
app = typer.Typer(
    help="Export vector data from Qdrant for semantic analysis.",
    add_completion=False,
)


# ID: 5350324a-ea70-4235-b220-d2a227a30b0a
def run_clustering(
    input_path: Path,
    output: Path,
    n_clusters: int,
):
    """
    Loads exported vectors, runs K-Means clustering, and saves the proposed
    capability-to-domain mappings to a JSON file.
    """
    if KMeans is None:
        log.error("scikit-learn is not installed. Aborting.")
        raise RuntimeError("scikit-learn is not installed for clustering.")

    log.info("🚀 Starting semantic clustering process...")
    output.parent.mkdir(parents=True, exist_ok=True)

    log.info(f"   -> Loading vectors from {input_path}...")
    vectors = []
    capability_keys = []
    with input_path.open("r", encoding="utf-8") as f:
        for line in f:
            record = json.loads(line)
            # --- START: THE DEFINITIVE FIX ---
            # We now correctly look for the 'symbol' key, which is the unique ID.
            if "vector" in record and "payload" in record:
                if "symbol" in record["payload"]:
                    vectors.append(record["vector"])
                    capability_keys.append(record["payload"]["symbol"])
            # --- END: THE DEFINITIVE FIX ---

    if not vectors:
        log.error(f"❌ No valid vector data found in {input_path}.")
        raise ValueError(f"No valid vector data found in {input_path}.")

    log.info(
        f"   -> Loaded {len(vectors)} vectors for clustering into {n_clusters} domains."
    )
    X = np.array(vectors)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init="auto")
    kmeans.fit(X)
    labels = kmeans.labels_
    proposed_domains = {
        key: f"domain_{label}" for key, label in zip(capability_keys, labels)
    }

    with output.open("w", encoding="utf-8") as f:
        json.dump(proposed_domains, f, indent=2, sort_keys=True)
    log.info(
        f"✅ Successfully generated domain proposals for {len(proposed_domains)} capabilities and saved to {output}"
    )


if __name__ == "__main__":
    load_dotenv()
    typer.run(run_clustering)

--- END OF FILE ./src/features/introspection/semantic_clusterer.py ---

--- START OF FILE ./src/features/introspection/symbol_index_builder.py ---
# src/features/introspection/symbol_index_builder.py
# Build a minimal symbol_index the auditor can consume, without relying on
# project-specific services. It scans Python files under `src/`, loads
# .intent/mind/knowledge/entry_point_patterns.yaml, and classifies symbols.

from __future__ import annotations

import ast
import json
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional

# Optional dependency (PyYAML). If missing, we fall back to a tiny default set.
try:
    import yaml  # type: ignore
except Exception:  # pragma: no cover
    yaml = None  # type: ignore


@dataclass
# ID: 39b26f28-006c-487b-ba6b-648c2a0942ca
class Pattern:
    name: str
    description: str
    match: Dict[str, Any]
    entry_point_type: str


@dataclass
# ID: 7f417874-2248-4eb1-9b33-65eaf7abf457
class SymbolMeta:
    key: str
    filepath: str
    name: str
    type: str  # "function" | "class" | "method"
    base_classes: List[str]
    decorators: List[str]
    is_public_function: bool
    module_path: str


def _load_patterns(patterns_path: Path) -> List[Pattern]:
    if yaml is None:
        # Minimal safe fallback if PyYAML is not present
        default_patterns = [
            {
                "name": "typer_cli_command",
                "description": "Public functions in src/cli/ are CLI commands.",
                "match": {
                    "type": "function",
                    "is_public_function": True,
                    "module_path_contains": "src/cli/",
                },
                "entry_point_type": "cli_command",
            },
            {
                "name": "sqlalchemy_orm_model",
                "description": "ORM models count as data models.",
                "match": {
                    "type": "class",
                    "module_path_contains": "src/services/database/models",
                },
                "entry_point_type": "data_model",
            },
        ]
        return [Pattern(**p) for p in default_patterns]

    data = yaml.safe_load(patterns_path.read_text(encoding="utf-8"))
    items = data.get("patterns", []) if isinstance(data, dict) else []
    out: List[Pattern] = []
    for p in items:
        out.append(
            Pattern(
                name=p.get("name", ""),
                description=p.get("description", ""),
                match=p.get("match", {}) or {},
                entry_point_type=p.get("entry_point_type", ""),
            )
        )
    return out


def _iter_py_files(root: Path) -> Iterable[Path]:
    for p in root.rglob("*.py"):
        # Skip venvs and reports etc.
        s = str(p.as_posix())
        if "/.venv/" in s or "/venv/" in s or "/.git/" in s or s.startswith("reports/"):
            continue
        yield p


class _Visitor(ast.NodeVisitor):
    def __init__(self, filepath: Path) -> None:
        self.filepath = filepath
        self.module_path = filepath.as_posix()
        self.symbols: List[SymbolMeta] = []
        self._class_stack: List[ast.ClassDef] = []

    # ID: 88f6a80e-1874-4c28-8240-f80c53509d16
    def visit_ClassDef(self, node: ast.ClassDef) -> Any:
        bases = [self._name_of(b) for b in node.bases]
        decorators = [self._name_of(d) for d in node.decorator_list]
        meta = SymbolMeta(
            key=f"{self.module_path}::{node.name}",
            filepath=self.module_path,
            name=node.name,
            type="class",
            base_classes=bases,
            decorators=decorators,
            is_public_function=not node.name.startswith("_"),
            module_path=self.module_path,
        )
        self.symbols.append(meta)

        self._class_stack.append(node)
        self.generic_visit(node)
        self._class_stack.pop()
        return None

    # ID: 28ae72fd-9693-4cf8-91a8-c5e857f717c3
    def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:
        self._handle_function_like(node)
        return None

    # ID: 806bb60a-67a0-4b27-bbe4-f0960c19da1d
    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:
        self._handle_function_like(node)
        return None

    def _handle_function_like(self, node: ast.AST) -> None:
        name = getattr(node, "name", "<unknown>")
        decorators = [self._name_of(d) for d in getattr(node, "decorator_list", [])]
        bases: List[str] = []
        sym_type = "method" if self._class_stack else "function"
        if self._class_stack:
            # include base classes of the owning class (helpful for ActionHandler match)
            owner = self._class_stack[-1]
            bases = [self._name_of(b) for b in owner.bases]

        meta = SymbolMeta(
            key=f"{self.module_path}::{self._qualified_name(name)}",
            filepath=self.module_path,
            name=name,
            type=sym_type,
            base_classes=bases,
            decorators=decorators,
            is_public_function=not name.startswith("_"),
            module_path=self.module_path,
        )
        self.symbols.append(meta)

    def _qualified_name(self, name: str) -> str:
        if self._class_stack:
            return f"{self._class_stack[-1].name}.{name}"
        return name

    @staticmethod
    def _name_of(node: ast.AST) -> str:
        if isinstance(node, ast.Name):
            return node.id
        if isinstance(node, ast.Attribute):
            return _Visitor._name_of(node.value) + "." + node.attr
        if isinstance(node, ast.Subscript):
            return _Visitor._name_of(node.value)
        try:
            return ast.unparse(node)  # py3.9+
        except Exception:
            return node.__class__.__name__


def _match_pattern(sym: SymbolMeta, pat: Pattern) -> bool:
    m = pat.match
    # type match
    if "type" in m:
        if m["type"] == "function" and sym.type not in {"function", "method"}:
            return False
        if m["type"] == "class" and sym.type != "class":
            return False

    # module path contains
    if "module_path_contains" in m:
        if m["module_path_contains"] not in sym.module_path:
            return False

    # name regex
    if "name_regex" in m:
        if not re.search(m["name_regex"], sym.name):
            # also allow Class.method part if present
            qn = sym.key.split("::", 1)[-1]
            if not re.search(m["name_regex"], qn):
                return False

    # is_public_function
    if "is_public_function" in m:
        want_pub = bool(m["is_public_function"])
        if sym.type in {"function", "method"}:
            if sym.is_public_function != want_pub:
                return False

    # base_class_includes
    if "base_class_includes" in m:
        needed = str(m["base_class_includes"])
        if not any(needed in b for b in sym.base_classes):
            return False

    # has_decorator
    if "has_decorator" in m:
        need = str(m["has_decorator"])
        if not any(need in d for d in sym.decorators):
            return False

    # has_capability_tag (best-effort: look for '# ID:' above def/class)
    if m.get("has_capability_tag"):
        # We will scan the file quickly: if '# ID:' appears on the same line
        # as the def/class or just above it, consider it tagged.
        try:
            source = Path(sym.filepath).read_text(encoding="utf-8").splitlines()
            # find line number by searching name; best-effort
            for i, line in enumerate(source, 1):
                if f"def {sym.name}" in line or f"class {sym.name}" in line:
                    window = "\n".join(source[max(0, i - 4) : i + 1])
                    if "# ID" in window or "#ID" in window:
                        break
            else:
                return False
        except Exception:
            return False

    return True


def _classify(
    symbols: List[SymbolMeta], patterns: List[Pattern]
) -> Dict[str, Dict[str, Any]]:
    index: Dict[str, Dict[str, Any]] = {}
    for s in symbols:
        ep_type = None
        pat_name = None
        just = None
        for p in patterns:
            if _match_pattern(s, p):
                ep_type = p.entry_point_type or None
                pat_name = p.name or None
                just = p.description or None
                break

        index[s.key] = {
            "entry_point_type": ep_type,
            "pattern_name": pat_name,
            "entry_point_justification": just,
        }
    return index


# ID: 47559b4a-19e6-4ef4-ba52-4951fe0346ec
def build_symbol_index(
    project_root: str | Path = ".",
    patterns_path: str | Path = ".intent/mind/knowledge/entry_point_patterns.yaml",
    src_dir: str | Path = "src",
) -> Dict[str, Dict[str, Any]]:
    root = Path(project_root).resolve()
    src = (root / src_dir).resolve()
    patterns_file = (root / patterns_path).resolve()

    if not patterns_file.exists():
        raise FileNotFoundError(f"Entry point patterns not found: {patterns_file}")

    patterns = _load_patterns(patterns_file)
    all_symbols: List[SymbolMeta] = []

    for py in _iter_py_files(src):
        try:
            text = py.read_text(encoding="utf-8")
        except Exception:
            continue
        try:
            tree = ast.parse(text)
        except Exception:
            continue
        v = _Visitor(py)
        v.visit(tree)
        all_symbols.extend(v.symbols)

    return _classify(all_symbols, patterns)


# ID: 04b011a8-a32a-42b9-a42b-3f27b5226db0
def main(argv: Optional[List[str]] = None) -> int:
    import argparse

    parser = argparse.ArgumentParser(
        description="Build symbol_index.json from AST + patterns."
    )
    parser.add_argument("--project-root", default=".", help="Project root (default: .)")
    parser.add_argument(
        "--patterns",
        default=".intent/mind/knowledge/entry_point_patterns.yaml",
        help="Patterns YAML path",
    )
    parser.add_argument("--src", default="src", help="Source directory (default: src)")
    parser.add_argument(
        "--out", default="reports/symbol_index.json", help="Output JSON path"
    )
    args = parser.parse_args(argv or sys.argv[1:])

    index = build_symbol_index(args.project_root, args.patterns, args.src)
    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(
        json.dumps(index, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    print(f"Wrote {out_path.as_posix()} with {len(index)} symbols.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

--- END OF FILE ./src/features/introspection/symbol_index_builder.py ---

--- START OF FILE ./src/features/introspection/sync_service.py ---
# src/features/introspection/sync_service.py
from __future__ import annotations

import ast
import uuid
from typing import Any, Dict, List

from rich.console import Console
from services.database.session_manager import get_session
from shared.ast_utility import calculate_structural_hash
from shared.config import settings
from sqlalchemy import text

console = Console()


# ID: 2fc08ba1-31ee-42cd-84cf-f68f81013acf
class SymbolVisitor(ast.NodeVisitor):
    """
    An AST visitor that discovers top-level public symbols and their immediate methods,
    while correctly ignoring nested functions and classes as implementation details.
    """

    def __init__(self, file_path: str):
        self.file_path = file_path
        self.symbols: List[Dict[str, Any]] = []
        self.class_stack: List[str] = []

    # ID: 0b1d3e2c-5f6a-7b8c-9d0e-1f2a3b4c5d6e
    def visit_ClassDef(self, node: ast.ClassDef):
        # Only process top-level classes. Nested classes are implementation details.
        if not self.class_stack:
            self._process_symbol(node)
            self.class_stack.append(node.name)
            # Visit children to find methods of this class.
            self.generic_visit(node)
            self.class_stack.pop()

    # ID: 2d3e4f5a-6b7c-8d9e-0f1a2b3c4d5e
    # ID: 4a14b3db-a724-487f-bcb4-fa020583ae73
    def visit_FunctionDef(self, node: ast.FunctionDef):
        # Process the function only if it's top-level or a direct method of a class.
        if len(self.class_stack) <= 1:
            self._process_symbol(node)
        # Do NOT call generic_visit here to prevent descending into nested helper functions.

    # ID: 4e5f6a7b-8c9d-0e1f-2a3b4c5d6e7f
    # ID: 6e4cfc45-18a3-4d82-b554-eb4615eefea8
    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
        # Process the async function only if it's top-level or a direct method of a class.
        if len(self.class_stack) <= 1:
            self._process_symbol(node)
        # Do NOT call generic_visit here to prevent descending into nested helper functions.

    def _process_symbol(
        self, node: ast.ClassDef | ast.FunctionDef | ast.AsyncFunctionDef
    ):
        """Extracts metadata for a single symbol, respecting its context."""
        is_public = not node.name.startswith("_")
        is_dunder = node.name.startswith("__") and node.name.endswith("__")
        if not (is_public and not is_dunder):
            return

        path_components = self.class_stack + [node.name]
        symbol_path = f"{self.file_path}::{'.'.join(path_components)}"
        qualname = ".".join(path_components)

        module_name = (
            self.file_path.replace("src/", "").replace(".py", "").replace("/", ".")
        )
        kind_map = {
            "ClassDef": "class",
            "FunctionDef": "function",
            "AsyncFunctionDef": "function",
        }

        self.symbols.append(
            {
                "id": uuid.uuid5(uuid.NAMESPACE_DNS, symbol_path),
                "symbol_path": symbol_path,
                "module": module_name,
                "qualname": qualname,
                "kind": kind_map.get(type(node).__name__, "function"),
                "ast_signature": "TBD",
                "fingerprint": calculate_structural_hash(node),
                "state": "discovered",
                "is_public": True,
            }
        )


# ID: da739a48-f3c2-4c27-b870-51ddb224bc32
class SymbolScanner:
    """Scans the codebase to extract symbol information."""

    # ID: 1c60168e-3d83-4c72-b4be-390554f51b18
    def scan(self) -> List[Dict[str, Any]]:
        """Scans all Python files in src/ and extracts symbols."""
        src_dir = settings.REPO_PATH / "src"
        all_symbols = []
        for file_path in src_dir.rglob("*.py"):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))
                rel_path_str = str(file_path.relative_to(settings.REPO_PATH))
                visitor = SymbolVisitor(rel_path_str)
                visitor.visit(tree)
                all_symbols.extend(visitor.symbols)
            except Exception as e:
                console.print(f"[bold red]Error scanning {file_path}: {e}[/bold red]")
        unique_symbols = {s["symbol_path"]: s for s in all_symbols}
        return list(unique_symbols.values())


# ID: 5ca33e91-947b-435c-9756-c74a22f37a2b
async def run_sync_with_db() -> Dict[str, int]:
    """
    Executes the full, database-centric sync logic using the "smart merge" strategy.
    This is the single source of truth for updating the symbols table from the codebase.
    """
    scanner = SymbolScanner()
    code_state = scanner.scan()
    stats = {"scanned": len(code_state), "inserted": 0, "updated": 0, "deleted": 0}

    async with get_session() as session:
        async with session.begin():
            # 1. Create a temporary table
            await session.execute(
                text(
                    "CREATE TEMPORARY TABLE core_symbols_staging (LIKE core.symbols INCLUDING DEFAULTS) ON COMMIT DROP;"
                )
            )

            # 2. Populate the temporary table
            if code_state:
                await session.execute(
                    text(
                        """
                        INSERT INTO core_symbols_staging (id, symbol_path, module, qualname, kind, ast_signature, fingerprint, state, is_public)
                        VALUES (:id, :symbol_path, :module, :qualname, :kind, :ast_signature, :fingerprint, :state, :is_public)
                        """
                    ),
                    code_state,
                )

            # 3. Calculate stats
            deleted_result = await session.execute(
                text(
                    "SELECT COUNT(*) FROM core.symbols WHERE symbol_path NOT IN (SELECT symbol_path FROM core_symbols_staging)"
                )
            )
            stats["deleted"] = deleted_result.scalar_one()

            inserted_result = await session.execute(
                text(
                    "SELECT COUNT(*) FROM core_symbols_staging WHERE symbol_path NOT IN (SELECT symbol_path FROM core.symbols)"
                )
            )
            stats["inserted"] = inserted_result.scalar_one()

            updated_result = await session.execute(
                text(
                    """
                    SELECT COUNT(*) FROM core.symbols s
                    JOIN core_symbols_staging st ON s.symbol_path = st.symbol_path
                    WHERE s.fingerprint != st.fingerprint
                """
                )
            )
            stats["updated"] = updated_result.scalar_one()

            # 4. Delete obsolete symbols
            await session.execute(
                text(
                    "DELETE FROM core.symbols WHERE symbol_path NOT IN (SELECT symbol_path FROM core_symbols_staging)"
                )
            )

            # 5. Update changed symbols
            await session.execute(
                text(
                    """
                    UPDATE core.symbols
                    SET
                        fingerprint = st.fingerprint,
                        last_modified = NOW(),
                        last_embedded = NULL,
                        updated_at = NOW()
                    FROM core_symbols_staging st
                    WHERE core.symbols.symbol_path = st.symbol_path
                    AND core.symbols.fingerprint != st.fingerprint;
                """
                )
            )

            # 6. Insert new symbols
            await session.execute(
                text(
                    """
                    INSERT INTO core.symbols (id, symbol_path, module, qualname, kind, ast_signature, fingerprint, state, is_public, created_at, updated_at, last_modified, first_seen, last_seen)
                    SELECT id, symbol_path, module, qualname, kind, ast_signature, fingerprint, state, is_public, NOW(), NOW(), NOW(), NOW(), NOW()
                    FROM core_symbols_staging
                    ON CONFLICT (symbol_path) DO NOTHING;
                """
                )
            )
    return stats

--- END OF FILE ./src/features/introspection/sync_service.py ---

--- START OF FILE ./src/features/introspection/vectorization_service.py ---
# src/features/introspection/vectorization_service.py
"""
High-performance orchestrator for capability vectorization.
This version reads its work queue directly from the database, treating it as the
single source of truth for the symbol catalog. It intelligently re-vectorizes
symbols when their source code has been modified by comparing structural hashes.
"""

from __future__ import annotations

import ast
import hashlib
from pathlib import Path
from typing import Dict, List, Optional

from core.cognitive_service import CognitiveService
from rich.console import Console
from rich.progress import track
from services.clients.qdrant_client import QdrantService
from services.database.session_manager import get_session
from shared.config import settings
from shared.logger import getLogger
from shared.utils.embedding_utils import normalize_text
from sqlalchemy import text

log = getLogger("core_admin.knowledge.orchestrator")
console = Console()


async def _fetch_all_public_symbols_from_db() -> List[Dict]:
    """Queries the database for all public symbols."""
    async with get_session() as session:
        stmt = text(
            """
            SELECT id, symbol_path, module, fingerprint AS structural_hash
            FROM core.symbols
            WHERE is_public = TRUE
            """
        )
        result = await session.execute(stmt)
        return [dict(row._mapping) for row in result]


async def _get_stored_vector_hashes(qdrant_service: QdrantService) -> Dict[str, str]:
    """Fetches all point IDs and their content hashes from Qdrant."""
    hashes = {}
    offset = None
    try:
        while True:
            points, next_offset = await qdrant_service.client.scroll(
                collection_name=qdrant_service.collection_name,
                limit=1000,
                offset=offset,
                with_payload=["content_sha256"],
                with_vectors=False,
            )
            for point in points:
                if point.payload and "content_sha256" in point.payload:
                    hashes[str(point.id)] = point.payload.get("content_sha256")
            if not next_offset:
                break
            offset = next_offset
    except Exception as e:
        log.warning(
            f"Could not retrieve hashes from Qdrant, will re-vectorize all. Error: {e}"
        )
    return hashes


def _get_source_code(file_path: Path, symbol_path: str) -> Optional[str]:
    """Extracts the source code of a specific symbol from a file using AST."""
    if not file_path.exists():
        log.warning(
            f"Source file not found for symbol {symbol_path} at path {file_path}"
        )
        return None

    content = file_path.read_text("utf-8", errors="ignore")
    try:
        tree = ast.parse(content)
        target_name = symbol_path.split("::")[-1]

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                if hasattr(node, "name") and node.name == target_name:
                    return ast.get_source_segment(content, node)
    except Exception:
        return None
    return None


async def _process_vectorization_task(
    task: Dict,
    cognitive_service: CognitiveService,
    qdrant_service: QdrantService,
    failure_log_path: Path,
) -> Optional[str]:
    """Processes a single symbol: gets embedding and upserts to Qdrant. Returns Qdrant point ID on success."""
    try:
        vector = await cognitive_service.get_embedding_for_code(task["source_code"])
        if not vector:
            raise ValueError("Embedding service returned None")

        point_id = str(task["id"])

        payload_data = {
            "source_path": task["file_path_str"],
            "source_type": "code",
            "chunk_id": task["symbol_path"],
            "content_sha256": task["code_hash"],
            "language": "python",
            "symbol": task["symbol_path"],
            "capability_tags": [point_id],
        }
        await qdrant_service.upsert_capability_vector(
            point_id_str=point_id,
            vector=vector,
            payload_data=payload_data,
        )

        return point_id
    except Exception as e:
        log.error(f"Failed to process symbol '{task['symbol_path']}': {e}")
        failure_log_path.parent.mkdir(parents=True, exist_ok=True)
        with failure_log_path.open("a", encoding="utf-8") as f:
            f.write(f"vectorization_error\t{task['symbol_path']}\t{e}\n")
        return None


async def _update_db_after_vectorization(updates: List[Dict]):
    """Creates links in symbol_vector_links and updates the last_embedded timestamp."""
    if not updates:
        return
    async with get_session() as session:
        async with session.begin():
            await session.execute(
                text(
                    """
                    INSERT INTO core.symbol_vector_links (symbol_id, vector_id, embedding_model, embedding_version, created_at)
                    VALUES (:symbol_id, :vector_id, :embedding_model, :embedding_version, NOW())
                    ON CONFLICT (symbol_id) DO UPDATE SET
                        vector_id = EXCLUDED.vector_id,
                        embedding_model = EXCLUDED.embedding_model,
                        embedding_version = EXCLUDED.embedding_version,
                        created_at = NOW();
                """
                ),
                updates,
            )
            await session.execute(
                text(
                    "UPDATE core.symbols SET last_embedded = NOW() WHERE id = ANY(:symbol_ids)"
                ),
                {"symbol_ids": [u["symbol_id"] for u in updates]},
            )
    console.print(f"   -> Updated {len(updates)} records in the database.")


# ID: 4bcad5fa-c30b-4c24-bf6c-5b692ecbbf67
async def run_vectorize(
    cognitive_service: CognitiveService,
    dry_run: bool = False,
    force: bool = False,
):
    """
    The main orchestration logic for vectorizing capabilities based on the database.
    """
    console.print("[bold cyan]🚀 Starting Database-Driven Vectorization...[/bold cyan]")
    failure_log_path = settings.REPO_PATH / "logs" / "vectorization_failures.log"

    all_symbols = await _fetch_all_public_symbols_from_db()

    qdrant_service = QdrantService()
    await qdrant_service.ensure_collection()
    stored_vector_hashes = await _get_stored_vector_hashes(qdrant_service)

    tasks = []
    for symbol in all_symbols:
        symbol_id_str = str(symbol["id"])

        module_path = symbol["module"]
        file_path_str = "src/" + module_path.replace(".", "/") + ".py"
        file_path = settings.REPO_PATH / file_path_str

        source_code = _get_source_code(file_path, symbol["symbol_path"])
        if not source_code:
            continue

        normalized_code = normalize_text(source_code)
        current_code_hash = hashlib.sha256(normalized_code.encode("utf-8")).hexdigest()

        stored_hash = stored_vector_hashes.get(symbol_id_str)

        if force or current_code_hash != stored_hash:
            task_data = {
                **symbol,
                "source_code": normalized_code,
                "code_hash": current_code_hash,
                "file_path_str": str(file_path.relative_to(settings.REPO_PATH)),
            }
            tasks.append(task_data)

    if not tasks:
        console.print(
            "[bold green]✅ Vector knowledge base is already up-to-date.[/bold green]"
        )
        return

    console.print(f"   -> Found {len(tasks)} symbols needing vectorization.")

    if dry_run:
        console.print(
            "\n[bold yellow]💧 Dry Run: No embeddings will be generated or stored.[/bold yellow]"
        )
        for task in tasks[:5]:
            console.print(f"   -> Would vectorize: {task['symbol_path']}")
        if len(tasks) > 5:
            console.print(f"   -> ... and {len(tasks) - 5} more.")
        return

    updates_to_db = []
    for task in track(tasks, description="Vectorizing symbols..."):
        point_id = await _process_vectorization_task(
            task, cognitive_service, qdrant_service, failure_log_path
        )
        if point_id:
            updates_to_db.append(
                {
                    "symbol_id": task["id"],
                    "vector_id": point_id,
                    "embedding_model": settings.LOCAL_EMBEDDING_MODEL_NAME,
                    "embedding_version": 1,  # This should come from a constitutional setting eventually
                }
            )

    await _update_db_after_vectorization(updates_to_db)

    console.print(
        f"\n[bold green]✅ Vectorization complete. Processed {len(updates_to_db)}/{len(tasks)} symbols.[/bold green]"
    )
    if len(updates_to_db) < len(tasks):
        console.print(
            f"[bold red]   -> {len(tasks) - len(updates_to_db)} failures logged to {failure_log_path}[/bold red]"
        )

--- END OF FILE ./src/features/introspection/vectorization_service.py ---

--- START OF FILE ./src/features/maintenance/command_sync_service.py ---
# src/features/maintenance/command_sync_service.py
"""
Provides a service to introspect the live Typer CLI application and synchronize
the discovered commands with the `core.cli_commands` database table.
"""

from __future__ import annotations

from typing import Any, Dict, List

import typer
from rich.console import Console
from services.database.models import CliCommand
from services.database.session_manager import get_session
from sqlalchemy import delete
from sqlalchemy.dialects.postgresql import insert as pg_insert

console = Console()


def _introspect_typer_app(app: typer.Typer, prefix: str = "") -> List[Dict[str, Any]]:
    """Recursively scans a Typer app to discover all commands and their metadata."""
    commands = []

    for cmd_info in app.registered_commands:
        if not cmd_info.name:
            continue

        full_name = f"{prefix}{cmd_info.name}"
        callback = cmd_info.callback
        module_name = callback.__module__ if callback else "unknown"

        commands.append(
            {
                "name": full_name,
                "module": module_name,
                "entrypoint": callback.__name__ if callback else "unknown",
                "summary": (cmd_info.help or "").split("\n")[0],
                "category": prefix.replace(".", " ").strip() or "general",
            }
        )

    for group_info in app.registered_groups:
        if group_info.name:
            new_prefix = f"{prefix}{group_info.name}."
            commands.extend(
                _introspect_typer_app(group_info.typer_instance, new_prefix)
            )

    return commands


# ID: fbbc9eaa-df52-48e5-95ea-998c027002d9
async def sync_commands_to_db(main_app: typer.Typer):
    """
    Introspects the main CLI application, discovers all commands, and upserts them
    into the database, making the database the single source of truth.
    """
    console.print(
        "[bold cyan]🚀 Synchronizing CLI command registry with the database...[/bold cyan]"
    )

    discovered_commands = _introspect_typer_app(main_app)

    if not discovered_commands:
        console.print(
            "[bold yellow]⚠️ No commands discovered. Nothing to sync.[/bold yellow]"
        )
        return

    console.print(
        f"   -> Discovered {len(discovered_commands)} commands from the application code."
    )

    async with get_session() as session:
        async with session.begin():
            # Clear the table to ensure a clean sync from the code's source of truth
            await session.execute(delete(CliCommand))

            # Use PostgreSQL's ON CONFLICT DO UPDATE for an upsert operation
            stmt = pg_insert(CliCommand).values(discovered_commands)
            update_dict = {c.name: c for c in stmt.excluded if not c.primary_key}
            upsert_stmt = stmt.on_conflict_do_update(
                index_elements=["name"],
                set_=update_dict,
            )

            await session.execute(upsert_stmt)

    console.print(
        f"[bold green]✅ Successfully synchronized {len(discovered_commands)} commands to the database.[/bold green]"
    )

--- END OF FILE ./src/features/maintenance/command_sync_service.py ---

--- START OF FILE ./src/features/maintenance/dotenv_sync_service.py ---
# src/features/maintenance/dotenv_sync_service.py
"""
Provides a service to synchronize runtime configuration from the .env file
into the database, governed by the runtime_requirements.yaml policy.
"""

from __future__ import annotations

from typing import Any, Dict, List

from rich.console import Console
from rich.table import Table
from services.database.models import RuntimeService as RuntimeSetting
from services.database.session_manager import get_session
from shared.config import settings
from shared.logger import getLogger
from sqlalchemy import func
from sqlalchemy.dialects.postgresql import insert as pg_insert

log = getLogger("dotenv_sync_service")
console = Console()


# The local, duplicated ORM model has been removed.
# We now import the canonical model from `services.database.models` and alias it for compatibility.


# ID: 46c39446-1163-4d8d-ad63-5956a248260f
async def run_dotenv_sync(dry_run: bool):
    """
    Reads variables defined in runtime_requirements.yaml from the environment/.env
    and upserts them into the core.runtime_settings table.
    """
    console.print(
        "[bold cyan]🚀 Synchronizing .env configuration to database...[/bold cyan]"
    )

    try:
        runtime_reqs = settings.load("mind.config.runtime_requirements")
        variables_to_sync = runtime_reqs.get("variables", {})
    except FileNotFoundError as e:
        console.print(
            f"[bold red]❌ Error: Cannot find runtime_requirements policy: {e}[/bold red]"
        )
        return

    settings_to_upsert: List[Dict[str, Any]] = []
    for key, config in variables_to_sync.items():
        value = getattr(settings, key, None)

        if value is None:
            value_str = None
        elif isinstance(value, bool):
            value_str = str(value).lower()
        else:
            value_str = str(value)

        is_secret = config.get("source") == "secret" or "_KEY" in key or "_TOKEN" in key
        settings_to_upsert.append(
            {
                "key": key,
                "value": value_str,
                "description": config.get("description"),
                "is_secret": is_secret,
            }
        )

    if dry_run:
        console.print(
            "[bold yellow]-- DRY RUN: The following settings would be synced --[/bold yellow]"
        )
        table = Table(title="Configuration Sync Plan")
        table.add_column("Key", style="cyan")
        table.add_column("Value", style="magenta")
        table.add_column("Is Secret?", style="red")

        for setting in settings_to_upsert:
            display_value = (
                "********"
                if setting["is_secret"] and setting["value"]
                else str(setting["value"])
            )
            table.add_row(setting["key"], display_value, str(setting["is_secret"]))
        console.print(table)
        return

    try:
        async with get_session() as session:
            async with session.begin():
                stmt = pg_insert(RuntimeSetting).values(settings_to_upsert)
                update_dict = {
                    "value": stmt.excluded.value,
                    "description": stmt.excluded.description,
                    "is_secret": stmt.excluded.is_secret,
                    "last_updated": func.now(),
                }
                upsert_stmt = stmt.on_conflict_do_update(
                    index_elements=["key"],
                    set_=update_dict,
                )
                await session.execute(upsert_stmt)

        console.print(
            f"[bold green]✅ Successfully synchronized {len(settings_to_upsert)} settings to the database.[/bold green]"
        )
    except Exception as e:
        log.error(f"Database sync failed: {e}", exc_info=True)
        console.print(
            f"[bold red]❌ Error: Failed to write to the database: {e}[/bold red]"
        )

--- END OF FILE ./src/features/maintenance/dotenv_sync_service.py ---

--- START OF FILE ./src/features/maintenance/maintenance_service.py ---
# src/features/maintenance/maintenance_service.py
"""
Provides centralized services for repository maintenance tasks that were
previously handled by standalone scripts.
"""

from __future__ import annotations

import re

from rich.console import Console
from shared.config import settings

console = Console()

# This map defines the OLD python import paths to the NEW python import paths.
REWIRE_MAP = {
    # Legacy system.admin -> new cli.commands
    "system.admin": "cli.commands",
    "system.admin_cli": "cli.admin_cli",
    # Legacy agents -> new core.agents
    "agents": "core.agents",
    # Legacy system.tools -> new features
    "system.tools.codegraph_builder": "features.introspection.knowledge_graph_service",
    "system.tools.scaffolder": "features.project_lifecycle.scaffolding_service",
    # Legacy shared locations
    "shared.services.qdrant_service": "services.clients.qdrant_client",
    "shared.services.embedding_service": "services.adapters.embedding_provider",
    "shared.services.repositories.db.engine": "services.repositories.db.engine",
    "system.governance.models": "shared.models",
}


# ID: 76ae8501-8f82-4a13-9648-bf1af142aae3
def rewire_imports(dry_run: bool = True) -> int:
    """
    Scans the entire 'src' directory and corrects Python import statements
    based on the architectural REWIRE_MAP. This is a critical tool for use
    after major refactoring.

    Args:
        dry_run: If True, only prints changes without writing them.

    Returns:
        The number of import changes made or proposed.
    """
    src_dir = settings.REPO_PATH / "src"
    all_python_files = list(src_dir.rglob("*.py"))
    total_changes = 0
    import_re = re.compile(r"^(from\s+([a-zA-Z0-9_.]+)|import\s+([a-zA-Z0-9_.]+))")

    # Sort keys by length, longest first, to handle nested paths correctly
    sorted_rewire_keys = sorted(REWIRE_MAP.keys(), key=len, reverse=True)

    for file_path in all_python_files:
        try:
            content = file_path.read_text(encoding="utf-8")
            lines = content.splitlines()
            new_lines = []
            file_was_changed = False

            for line in lines:
                match = import_re.match(line)
                if not match:
                    new_lines.append(line)
                    continue

                original_import_path = match.group(2) or match.group(3)
                modified_line = line

                for old_prefix in sorted_rewire_keys:
                    if original_import_path.startswith(old_prefix):
                        new_prefix = REWIRE_MAP[old_prefix]
                        new_import_path = original_import_path.replace(
                            old_prefix, new_prefix, 1
                        )
                        modified_line = line.replace(
                            original_import_path, new_import_path
                        )
                        break  # Stop after the first (longest) match

                if modified_line != line:
                    console.print(
                        f"\n📝 Change detected in: [yellow]{file_path.relative_to(settings.REPO_PATH)}[/yellow]"
                    )
                    console.print(f"  - {line}")
                    console.print(f"  + [green]{modified_line}[/green]")
                    new_lines.append(modified_line)
                    file_was_changed = True
                    total_changes += 1
                else:
                    new_lines.append(line)

            if file_was_changed and not dry_run:
                file_path.write_text("\n".join(new_lines) + "\n", encoding="utf-8")

        except Exception as e:
            console.print(f"❌ Error processing {file_path}: {e}")

    return total_changes

--- END OF FILE ./src/features/maintenance/maintenance_service.py ---

--- START OF FILE ./src/features/maintenance/migration_service.py ---
# src/features/maintenance/migration_service.py
"""
Provides a one-time migration service to populate the SSOT database from legacy
file-based sources (.intent/mind/project_manifest.yaml and AST scan).
"""

from __future__ import annotations

import asyncio
import json
import uuid
from typing import Any, Dict, List

import yaml
from rich.console import Console
from services.database.session_manager import get_session
from shared.config import settings
from sqlalchemy import text

console = Console()


async def _migrate_capabilities_from_manifest() -> List[Dict[str, Any]]:
    """Loads capabilities from the legacy project_manifest.yaml file, ensuring uniqueness."""
    manifest_path = settings.get_path("mind.project_manifest")
    if not manifest_path.exists():
        console.print(
            "[yellow]Warning: project_manifest.yaml not found. No capabilities to migrate.[/yellow]"
        )
        return []

    content = yaml.safe_load(manifest_path.read_text("utf-8")) or {}
    capability_keys = content.get("capabilities", [])

    unique_clean_keys = set()
    for key in capability_keys:
        clean_key = key.replace("`", "").strip()
        if clean_key:
            unique_clean_keys.add(clean_key)

    migrated_caps = []
    for clean_key in sorted(list(unique_clean_keys)):
        domain = clean_key.split(".")[0] if "." in clean_key else "general"
        title = clean_key.split(".")[-1].replace("_", " ").capitalize()

        migrated_caps.append(
            {
                "id": uuid.uuid5(uuid.NAMESPACE_DNS, clean_key),
                "name": clean_key,
                "title": title,
                "objective": "Migrated from legacy project_manifest.yaml.",
                "owner": "system",
                "domain": domain,
                "tags": json.dumps([]),
                "status": "Active",
            }
        )
    return migrated_caps


async def _migrate_symbols_from_ast() -> List[Dict[str, Any]]:
    """Scans the codebase using SymbolScanner to populate the symbols table."""
    from features.introspection.sync_service import SymbolScanner

    scanner = SymbolScanner()
    code_symbols = await asyncio.to_thread(scanner.scan)

    migrated_syms = []
    for symbol_data in code_symbols:
        migrated_syms.append(
            {
                "id": uuid.uuid5(uuid.NAMESPACE_DNS, symbol_data["symbol_path"]),
                "uuid": symbol_data["uuid"],
                "module": symbol_data["file_path"],
                "qualname": symbol_data["symbol_path"].split("::")[-1],
                "kind": (
                    "function" if "Function" in symbol_data.get("type", "") else "class"
                ),
                "ast_signature": "TBD",
                "fingerprint": symbol_data["structural_hash"],
                "state": "discovered",
                "symbol_path": symbol_data[
                    "symbol_path"
                ],  # Ensure the true identifier is present
            }
        )
    return migrated_syms


# ID: cd2c3cf5-54ec-493c-b11f-d8bb6eae7a0f
async def run_ssot_migration(dry_run: bool):
    """Orchestrates the full one-time migration from files to the SSOT database."""
    console.print(
        "🚀 Starting one-time migration of knowledge from files to database..."
    )

    capabilities = await _migrate_capabilities_from_manifest()
    symbols = await _migrate_symbols_from_ast()

    if dry_run:
        console.print(
            "[bold yellow]-- DRY RUN: The following actions would be taken --[/bold yellow]"
        )
        console.print(
            f"  - Insert {len(capabilities)} unique capabilities from project_manifest.yaml."
        )
        console.print(f"  - Insert {len(symbols)} symbols from source code scan.")
        return

    async with get_session() as session:
        async with session.begin():
            console.print("  -> Deleting existing data from tables...")
            await session.execute(text("DELETE FROM core.symbol_capability_links;"))
            await session.execute(text("DELETE FROM core.symbols;"))
            await session.execute(text("DELETE FROM core.capabilities;"))

            console.print(f"  -> Inserting {len(capabilities)} capabilities...")
            if capabilities:
                await session.execute(
                    text(
                        """
                    INSERT INTO core.capabilities (id, name, title, objective, owner, domain, tags, status)
                    VALUES (:id, :name, :title, :objective, :owner, :domain, :tags, :status)
                """
                    ),
                    capabilities,
                )

            console.print(f"  -> Inserting {len(symbols)} symbols...")
            if symbols:
                # Insert symbols one by one to handle potential duplicates gracefully if any slip through
                insert_stmt = text(
                    """
                    INSERT INTO core.symbols (id, uuid, module, qualname, kind, ast_signature, fingerprint, state, symbol_path)
                    VALUES (:id, :uuid, :module, :qualname, :kind, :ast_signature, :fingerprint, :state, :symbol_path)
                    ON CONFLICT (symbol_path) DO NOTHING;
                """
                )
                for symbol in symbols:
                    await session.execute(insert_stmt, symbol)

    console.print("[bold green]✅ One-time migration complete.[/bold green]")
    console.print(
        "Run 'core-admin mind snapshot' to create the first export from the database."
    )

--- END OF FILE ./src/features/maintenance/migration_service.py ---

--- START OF FILE ./src/features/project_lifecycle/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/features/project_lifecycle/__init__.py ---

--- START OF FILE ./src/features/project_lifecycle/bootstrap_service.py ---
# src/features/project_lifecycle/bootstrap_service.py
"""
Provides CLI commands for bootstrapping the project with initial setup tasks,
such as creating a default set of GitHub issues for a new repository.
"""

from __future__ import annotations

import shutil
import subprocess
from typing import Optional

import typer
from rich.console import Console
from shared.logger import getLogger

log = getLogger("core_admin.bootstrap")
console = Console()

bootstrap_app = typer.Typer(
    help="Commands for project bootstrapping and initial setup."
)

ISSUES_TO_CREATE = [
    {
        "title": "Add JSON logging & request IDs",
        "body": "**Goal**: Switch logger to support LOG_FORMAT=json and add request id middleware in FastAPI.\n\n**Acceptance**\n- LOG_FORMAT=json writes structured logs\n- x-request-id is set/propagated\n- Docs updated in docs/CONVENTIONS.md",
        "labels": "roadmap,organizational,ci",
    },
    {
        "title": "Pre-commit hooks (Black, Ruff)",
        "body": "**Goal**: Add .pre-commit-config.yaml and wire to Make.\n\n**Acceptance**\n- pre-commit runs Black/Ruff locally\n- CI stays green",
        "labels": "roadmap,organizational,ci",
    },
    {
        "title": "Docs: CONVENTIONS.md & DEPENDENCIES.md",
        "body": "**Goal**: Codify folder map, import rules, capability tags, dependency policy.\n\n**Acceptance**\n- New contributors can place files w/o asking\n- Import discipline matrix documented",
        "labels": "roadmap,organizational,docs",
    },
    {
        "title": "Governance: proposal.schema.json + proposal_checks",
        "body": "**Goal**: Enforce schema & drift checks for .intent/proposals.\n\n**Acceptance**\n- Auditor shows schema pass/fail\n- Drift (token mismatch) → warning\n- Example proposal present",
        "labels": "roadmap,organizational,audit",
    },
]

LABELS_TO_ENSURE = [
    {"name": "roadmap", "color": "0366d6", "desc": "Roadmap item"},
    {"name": "organizational", "color": "a2eeef", "desc": "Project organization"},
    {"name": "ci", "color": "7057ff", "desc": "CI/CD"},
    {"name": "audit", "color": "d73a4a", "desc": "Constitutional audit & governance"},
    {"name": "docs", "color": "0e8a16", "desc": "Documentation"},
]


def _run_gh_command(command: list[str], ignore_errors: bool = False):
    """Helper to run a 'gh' command and handle errors."""
    if not shutil.which("gh"):
        console.print(
            "[bold red]❌ 'gh' (GitHub CLI) command not found in your PATH.[/bold red]"
        )
        console.print("   -> Please install it to use this feature.")
        raise typer.Exit(code=1)
    try:
        subprocess.run(command, check=True, capture_output=True, text=True)
    except subprocess.CalledProcessError as e:
        if not ignore_errors:
            console.print(f"[bold red]Error running gh command: {e.stderr}[/bold red]")
            raise typer.Exit(code=1)


@bootstrap_app.command("issues")
# ID: 695834ae-f6a1-49ed-baa8-7e99276df2ac
def bootstrap_issues(
    repo: Optional[str] = typer.Option(
        None, "--repo", help="The GitHub repository in 'owner/repo' format."
    ),
):
    """Creates a standard set of starter issues for the project on GitHub."""
    console.print("[bold cyan]🚀 Bootstrapping standard GitHub issues...[/bold cyan]")

    console.print("   -> Ensuring required labels exist...")
    for label in LABELS_TO_ENSURE:
        cmd = [
            "gh",
            "label",
            "create",
            label["name"],
            "--color",
            label["color"],
            "--description",
            label["desc"],
        ]
        if repo:
            cmd.extend(["--repo", repo])
        _run_gh_command(cmd, ignore_errors=True)

    console.print(f"   -> Creating {len(ISSUES_TO_CREATE)} starter issues...")
    for issue in ISSUES_TO_CREATE:
        cmd = [
            "gh",
            "issue",
            "create",
            "--title",
            issue["title"],
            "--body",
            issue["body"],
            "--label",
            issue["labels"],
        ]
        if repo:
            cmd.extend(["--repo", repo])
        _run_gh_command(cmd)

    console.print(
        "[bold green]✅ Successfully created starter issues on GitHub.[/bold green]"
    )


# The obsolete `register` function has been removed.

--- END OF FILE ./src/features/project_lifecycle/bootstrap_service.py ---

--- START OF FILE ./src/features/project_lifecycle/definition_service.py ---
# src/features/project_lifecycle/definition_service.py
from __future__ import annotations

import asyncio
from functools import partial
from typing import Any, Dict, List, Set

from core.cognitive_service import CognitiveService
from rich.console import Console
from services.clients.qdrant_client import QdrantService
from services.database.session_manager import get_session
from shared.config import settings
from shared.logger import getLogger
from shared.utils.parallel_processor import ThrottledParallelProcessor
from sqlalchemy import text

from features.introspection.knowledge_helpers import extract_source_code

console = Console()
log = getLogger("definition_service")


# ID: b095628d-b3d0-4fad-bfb6-483a217ea42c
async def get_undefined_symbols() -> List[Dict[str, Any]]:
    """
    Fetches symbols that are ready for definition (have a vector link but no key).
    """
    async with get_session() as session:
        result = await session.execute(
            text(
                """
                SELECT s.id, s.symbol_path, s.module, vl.vector_id
                FROM core.symbols s
                JOIN core.symbol_vector_links vl ON s.id = vl.symbol_id
                WHERE s.key IS NULL
                """
            )
        )
        return [dict(row._mapping) for row in result]


# ID: ec330970-c4ad-4bfd-87de-9e43fdaffaf0
async def define_single_symbol(
    symbol: Dict[str, Any],
    cognitive_service: CognitiveService,
    qdrant_service: QdrantService,
    existing_keys: Set[str],
) -> Dict[str, Any]:
    """Uses an AI to generate a definition for a single symbol, using semantic context."""
    log.info(f"Defining symbol: {symbol.get('symbol_path')}")
    source_code = extract_source_code(settings.REPO_PATH, symbol)
    if not source_code:
        log.warning(
            f"Cannot extract source code for {symbol.get('symbol_path')}: symbol data is likely missing 'module' or 'symbol_path'."
        )
        return {"id": symbol["id"], "key": "error.code_not_found"}

    similar_capabilities_str = "No similar capabilities found."
    vector_id = symbol.get("vector_id")
    if vector_id:
        try:
            vector = await qdrant_service.get_vector_by_id(vector_id)
            if vector:
                similar_hits = await qdrant_service.search_similar(vector, limit=3)
                similar_keys = [
                    hit["payload"]["chunk_id"]
                    for hit in similar_hits
                    if hit.get("payload")
                ]
                if similar_keys:
                    similar_capabilities_str = (
                        "Found similar existing capabilities: "
                        + ", ".join(f"`{k}`" for k in similar_keys)
                    )
        except Exception as e:
            log.warning(
                f"Semantic search failed during definition for {symbol['symbol_path']}: {e}"
            )

    prompt_template_path = settings.get_path("mind.prompts.capability_definer")
    prompt_template = prompt_template_path.read_text(encoding="utf-8")

    final_prompt = prompt_template.format(
        code=source_code, similar_capabilities=similar_capabilities_str
    )

    definer_agent = await cognitive_service.aget_client_for_role("CodeReviewer")
    raw_suggested_key = await definer_agent.make_request_async(
        final_prompt, user_id="definer_agent"
    )

    cleaned_key = (
        raw_suggested_key.strip().replace("`", "").replace("'", "").replace('"', "")
    )

    if cleaned_key in existing_keys:
        console.print(
            f"[yellow]Warning: AI suggested existing key '{cleaned_key}' for a new symbol. Skipping to avoid conflict.[/yellow]"
        )
        return {"id": symbol["id"], "key": "error.duplicate_key"}

    try:
        delay_str = settings.model_extra.get("LLM_SECONDS_BETWEEN_REQUESTS", "1")
        delay = int(delay_str)
    except (ValueError, TypeError):
        delay = 1
    await asyncio.sleep(delay)

    return {"id": symbol["id"], "key": cleaned_key}


# ID: 2d5b3476-74be-46f5-b173-1a909327bb85
async def update_definitions_in_db(definitions: List[Dict[str, Any]]):
    """Updates the 'key' column for symbols in the database."""
    if not definitions:
        return

    log.info(f"Attempting to update {len(definitions)} definitions in the database...")

    serializable_definitions = [
        {"id": str(d["id"]), "key": d["key"]} for d in definitions
    ]

    async with get_session() as session:
        async with session.begin():
            await session.execute(
                text("UPDATE core.symbols SET key = :key WHERE id = :id"),
                serializable_definitions,
            )
    log.info("Database update transaction completed.")


# ID: 3409dc17-cc09-4564-bfa6-7e83c8a32468
async def define_new_symbols(cognitive_service: CognitiveService):
    """The main orchestrator for the autonomous definition process."""
    undefined_symbols = await get_undefined_symbols()
    if not undefined_symbols:
        console.print("   -> No new symbols to define.")
        return

    async with get_session() as session:
        result = await session.execute(
            text("SELECT key FROM core.symbols WHERE key IS NOT NULL")
        )
        existing_keys = {row[0] for row in result}

    console.print(f"   -> Found {len(undefined_symbols)} new symbols to define...")

    qdrant_service = QdrantService()
    processor = ThrottledParallelProcessor(description="Defining symbols...")
    worker_fn = partial(
        define_single_symbol,
        cognitive_service=cognitive_service,
        qdrant_service=qdrant_service,
        existing_keys=existing_keys,
    )
    definitions = await processor.run_async(undefined_symbols, worker_fn)

    valid_definitions = [
        d for d in definitions if d.get("key") and not d["key"].startswith("error.")
    ]

    unique_definitions = []
    seen_keys = set()
    for d in valid_definitions:
        key = d["key"]
        if key not in seen_keys:
            unique_definitions.append(d)
            seen_keys.add(key)
        else:
            console.print(
                f"[yellow]Warning: AI generated duplicate key '{key}'. Skipping redundant assignment.[/yellow]"
            )

    await update_definitions_in_db(unique_definitions)
    console.print(
        f"   -> Successfully defined {len(unique_definitions)} new capabilities."
    )

--- END OF FILE ./src/features/project_lifecycle/definition_service.py ---

--- START OF FILE ./src/features/project_lifecycle/integration_service.py ---
# src/features/project_lifecycle/integration_service.py
from __future__ import annotations

import subprocess

import typer
from rich.console import Console
from shared.config import settings
from shared.context import CoreContext
from shared.logger import getLogger

log = getLogger("integration_service")
console = Console()


# ID: a6ace728-0c7f-48b8-b7a0-52ff9b24d99d
async def integrate_changes(context: CoreContext, commit_message: str):
    """
    Orchestrates the full, non-destructive, and intelligent integration of code changes
    by executing the constitutionally-defined `integration_workflow`.

    This workflow is designed to be safe and developer-friendly. If it fails,
    it halts and leaves the working directory in its current state for the
    developer to fix. It will never destroy uncommitted work.
    """
    git_service = context.git_service
    workflow_failed = False

    try:
        # Step 1: Stage all current work. This captures the developer's full intent.
        console.print("[bold]Step 1: Staging all current changes...[/bold]")
        git_service.add_all()
        staged_files = git_service.get_staged_files()
        if not staged_files:
            console.print(
                "[yellow]No changes found to integrate. Working directory is clean.[/yellow]"
            )
            return

        console.print(f"   -> Staged {len(staged_files)} file(s) for integration.")

        # Step 2: Load and execute the constitutional workflow.
        workflow_policy = settings.load("charter.policies.operations.workflows_policy")
        integration_steps = workflow_policy.get("integration_workflow", [])

        for i, step in enumerate(integration_steps, 1):
            console.print(
                f"\n[bold]Step {i + 1}/{len(integration_steps) + 2}: {step['description']}[/bold]"
            )
            command_parts = step["command"].split()

            # --- THIS IS THE FIX ---
            # Run the command without `check=True` so we can handle the failure gracefully.
            process = subprocess.run(
                command_parts,
                capture_output=True,
                text=True,
                cwd=settings.REPO_PATH,
            )

            # Print output regardless of success
            if process.stdout:
                console.print(process.stdout)
            if process.stderr:
                console.print(f"[yellow]{process.stderr}[/yellow]")

            # Now, manually check the return code.
            if process.returncode != 0:
                console.print(f"[bold red]❌ Step '{step['id']}' failed.[/bold red]")

                if not step.get("continues_on_failure", False):
                    console.print(
                        "\n[bold red]Integration halted. Please fix the error above, then re-run the command.[/bold red]"
                    )
                    workflow_failed = True
                    break  # Exit the loop immediately
                else:
                    console.print(
                        "   -> [yellow]Continuing because step is marked as non-blocking.[/yellow]"
                    )
            # --- END OF FIX ---

        if workflow_failed:
            raise Exception("Workflow halted due to a failed step.")

        # Step 3: All checks passed. Stage any new changes and commit.
        console.print(
            f"\n[bold]Step {len(integration_steps) + 2}/{len(integration_steps) + 2}: Committing all changes...[/bold]"
        )
        git_service.commit(commit_message)
        console.print(
            "[bold green]✅ Successfully integrated and committed changes.[/bold green]"
        )

    except Exception as e:
        log.error(f"Integration process failed: {e}")
        # The user-friendly message is now printed inside the loop.
        # We can exit gracefully here.
        raise typer.Exit(code=1)

--- END OF FILE ./src/features/project_lifecycle/integration_service.py ---

--- START OF FILE ./src/features/project_lifecycle/scaffolding_service.py ---
# src/features/project_lifecycle/scaffolding_service.py
"""
Provides a reusable service for scaffolding new CORE-governed projects with constitutional compliance.
"""

from __future__ import annotations

from pathlib import Path

import typer
import yaml
from shared.config import settings
from shared.logger import getLogger
from shared.path_utils import get_repo_root

log = getLogger("core_admin.scaffolder")
CORE_ROOT = get_repo_root()

STARTER_KITS_DIR = CORE_ROOT / "src" / "features" / "project_lifecycle" / "starter_kits"


# ID: 356e7222-34ea-443d-8e17-2ab64b3f9c8b
class Scaffolder:
    """A reusable service for creating new, constitutionally-governed projects."""

    def __init__(
        self,
        project_name: str,
        profile: str = "default",
        workspace_dir: Path | None = None,
    ):
        """Initializes the Scaffolder with project name, profile, and workspace directory."""
        self.name = project_name
        self.profile = profile
        source_structure = settings.load("mind.knowledge.source_structure")
        workspace_path_str = source_structure.get("paths", {}).get("workspace", "work")
        self.workspace = workspace_dir or (CORE_ROOT / workspace_path_str)
        self.project_root = self.workspace / self.name
        self.starter_kit_path = STARTER_KITS_DIR / self.profile

        if not self.starter_kit_path.is_dir():
            raise FileNotFoundError(
                f"Starter kit profile '{self.profile}' not found at "
                f"{self.starter_kit_path}."
            )

    # ID: c4ca3239-7e79-48d8-8a6a-dddf3323cf66
    def scaffold_base_structure(self):
        """Creates the base project structure, including tests and CI directories."""
        log.info(f"💾 Creating project structure at {self.project_root}...")
        if self.project_root.exists():
            raise FileExistsError(f"Directory '{self.project_root}' already exists.")

        self.project_root.mkdir(parents=True, exist_ok=True)
        (self.project_root / "src").mkdir()
        (self.project_root / "tests").mkdir()
        (self.project_root / ".github" / "workflows").mkdir(parents=True, exist_ok=True)
        (self.project_root / "reports").mkdir()

        intent_dir = self.project_root / ".intent"
        intent_dir.mkdir()

        constitutional_files_to_copy = [
            "principles.yaml",
            "project_manifest.yaml",
            "safety_policies.yaml",
            "source_structure.yaml",
        ]

        for filename in constitutional_files_to_copy:
            source_path = self.starter_kit_path / filename
            if source_path.exists():
                # --- THIS IS THE FIX: Replace shutil.copy ---
                target_path = intent_dir / filename
                target_path.write_bytes(source_path.read_bytes())
                # --- END OF FIX ---

        readme_template = self.starter_kit_path / "README.md"
        if readme_template.exists():
            # --- THIS IS THE FIX: Replace shutil.copy ---
            target_path = intent_dir / "README.md"
            target_path.write_bytes(readme_template.read_bytes())
            # --- END OF FIX ---

        for template_path in self.starter_kit_path.glob("*.template"):
            content = template_path.read_text(encoding="utf-8").format(
                project_name=self.name
            )
            target_name = (
                ".gitignore"
                if template_path.name == "gitignore.template"
                else template_path.name.replace(".template", "")
            )
            (self.project_root / target_name).write_text(content, encoding="utf-8")

        manifest_path = intent_dir / "project_manifest.yaml"
        if manifest_path.exists():
            manifest_data = yaml.safe_load(manifest_path.read_text(encoding="utf-8"))
            if manifest_data:
                manifest_data["name"] = self.name
                manifest_path.write_text(
                    yaml.dump(manifest_data, indent=2), encoding="utf-8"
                )

        log.info(f"   -> ✅ Base structure for '{self.name}' created successfully.")

    # ID: 167f91ce-7b9f-4d07-9722-a5283af11019
    def write_file(self, relative_path: str, content: str):
        """Writes content to a file within the new project's directory, creating parent directories as needed."""
        target_file = self.project_root / relative_path
        target_file.parent.mkdir(parents=True, exist_ok=True)
        target_file.write_text(content, encoding="utf-8")
        log.info(f"   -> 📄 Wrote agent-generated file: {relative_path}")


# ID: c38bc7ce-2f6f-447b-9919-b6f7c2e6cf64
def new_project(
    name: str = typer.Argument(
        ...,
        help="The name of the new CORE-governed application to create.",
    ),
    profile: str = typer.Option(
        "default",
        "--profile",
        help="The starter kit profile to use for the new project's constitution.",
    ),
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--write",
        help="Show what will be created without writing files. Use --write to apply.",
    ),
):
    """Scaffolds a new CORE-governed application with the given name, profile, and dry-run option, including base structure and README generation."""
    scaffolder = Scaffolder(project_name=name, profile=profile)
    log.info(
        f"🚀 Scaffolding new CORE application: '{name}' using '{profile}' profile."
    )
    if dry_run:
        log.info("\n💧 Dry Run Mode: No files will be written.")
        typer.secho(
            f"Would create project '{name}' in '{scaffolder.workspace}/' with the "
            f"'{profile}' starter kit.",
            fg=typer.colors.YELLOW,
        )
    else:
        try:
            scaffolder.scaffold_base_structure()
            readme_template_path = scaffolder.starter_kit_path / "README.md.template"
            if readme_template_path.exists():
                readme_content = readme_template_path.read_text(
                    encoding="utf-8"
                ).format(project_name=name)
                scaffolder.write_file("README.md", readme_content)

        except FileExistsError as e:
            log.error(f"❌ {e}")
            raise typer.Exit(code=1)
        except Exception as e:
            log.error(f"❌ An unexpected error occurred: {e}", exc_info=True)
            raise typer.Exit(code=1)

--- END OF FILE ./src/features/project_lifecycle/scaffolding_service.py ---

--- START OF FILE ./src/features/self_healing/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/features/self_healing/__init__.py ---

--- START OF FILE ./src/features/self_healing/capability_tagging_service.py ---
# src/features/self_healing/capability_tagging_service.py
"""
Provides the service logic for using an AI agent to suggest and apply
capability tags to untagged public symbols in the codebase.
"""

from __future__ import annotations

import asyncio
from pathlib import Path
from typing import Optional

from core.agents.tagger_agent import CapabilityTaggerAgent
from core.cognitive_service import CognitiveService
from core.knowledge_service import KnowledgeService
from rich.console import Console
from services.database.session_manager import get_session
from shared.config import settings
from shared.logger import getLogger

from features.introspection.knowledge_graph_service import KnowledgeGraphBuilder

log = getLogger("capability_tagging_service")
console = Console()
REPO_ROOT = settings.REPO_PATH


async def _async_tag_capabilities(
    cognitive_service: CognitiveService,
    knowledge_service: KnowledgeService,
    file_path: Optional[Path],
    write: bool,
):
    """The core async logic for the capability tagging process."""
    agent = CapabilityTaggerAgent(cognitive_service, knowledge_service)

    suggestions = await agent.suggest_and_apply_tags(
        file_path=file_path.as_posix() if file_path else None
    )

    if not suggestions:
        console.print(
            "[bold green]✅ No new public capabilities to register.[/bold green]"
        )
        return

    if not write:
        console.print(
            "[bold yellow]💧 Dry Run: Run with --write to apply suggested capability tags.[/bold yellow]"
        )
        return

    console.print(
        f"\n[bold green]✅ Applying {len(suggestions)} new capability tags to source code...[/bold green]"
    )

    async with get_session() as session:
        async with session.begin():
            for key, new_info in suggestions.items():
                suggested_name = new_info["suggestion"]
                graph = await knowledge_service.get_graph()
                source_file_path = REPO_ROOT / new_info["file"]
                lines = source_file_path.read_text("utf-8").splitlines()
                symbol_data = graph["symbols"][new_info["key"]]
                line_to_tag = symbol_data["line_number"] - 1

                original_line = lines[line_to_tag]
                indentation = len(original_line) - len(original_line.lstrip(" "))
                tag_line = f"{' ' * indentation}# ID: {suggested_name}"

                lines.insert(line_to_tag, tag_line)
                source_file_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
                console.print(f"   -> Tagged '{suggested_name}' in {new_info['file']}")

    log.info("🧠 Rebuilding knowledge graph to reflect changes...")
    builder = KnowledgeGraphBuilder(REPO_ROOT)
    await builder.build_and_sync()
    log.info("✅ Knowledge graph successfully updated.")


# ID: 1651d1d3-f58c-4fce-8662-c9591c70edf7
def tag_unassigned_capabilities(
    cognitive_service: CognitiveService,
    knowledge_service: KnowledgeService,
    file_path: Optional[Path],
    write: bool,
):
    """Synchronous wrapper for the capability tagging service."""
    asyncio.run(
        _async_tag_capabilities(cognitive_service, knowledge_service, file_path, write)
    )

--- END OF FILE ./src/features/self_healing/capability_tagging_service.py ---

--- START OF FILE ./src/features/self_healing/clarity_service.py ---
# src/system/admin/fixer_clarity.py
"""
Implements the 'fix clarity' command, using an AI agent to perform
principled refactoring of Python code for improved readability and simplicity.
"""

from __future__ import annotations

import asyncio
from pathlib import Path

import typer
from core.cognitive_service import CognitiveService
from rich.console import Console
from shared.config import settings
from shared.logger import getLogger

log = getLogger("core_admin.fixer_clarity")
console = Console()


async def _async_fix_clarity(file_path: Path, dry_run: bool):
    """Async core logic for clarity-focused refactoring."""
    log.info(f"🔬 Analyzing '{file_path.name}' for clarity improvements...")

    cognitive_service = CognitiveService(settings.REPO_PATH)
    prompt_template = (
        settings.MIND / "prompts" / "refactor_for_clarity.prompt"
    ).read_text()

    original_code = file_path.read_text("utf-8")
    final_prompt = prompt_template.replace("{source_code}", original_code)

    refactor_client = cognitive_service.get_client_for_role("RefactoringArchitect")

    with console.status(
        "[bold green]Asking AI Architect to refactor for clarity...[/bold green]"
    ):
        refactored_code = await refactor_client.make_request_async(
            final_prompt, user_id="clarity_fixer_agent"
        )

    if not refactored_code.strip() or refactored_code.strip() == original_code.strip():
        console.print(
            "[bold green]✅ AI Architect found no clarity improvements to make.[/bold green]"
        )
        return

    if dry_run:
        console.print(
            f"\n[bold yellow]-- DRY RUN: Would refactor {file_path.name} --[/bold yellow]"
        )
        # You can add a diff view here if desired in the future
    else:
        file_path.write_text(refactored_code, "utf-8")
        console.print(
            f"\n[bold green]✅ Successfully refactored '{file_path.name}' for clarity.[/bold green]"
        )


# ID: 90f74d6c-6ee1-4174-b231-1813d97b1562
def fix_clarity(
    file_path: Path = typer.Argument(
        ..., help="Path to the Python file to refactor.", exists=True, dir_okay=False
    ),
    write: bool = typer.Option(
        False, "--write", help="Apply the refactoring to the file."
    ),
):
    """Uses an AI agent to refactor a Python file for improved clarity and simplicity."""
    asyncio.run(_async_fix_clarity(file_path, dry_run=not write))

--- END OF FILE ./src/features/self_healing/clarity_service.py ---

--- START OF FILE ./src/features/self_healing/code_style_service.py ---
# src/features/self_healing/code_style_service.py
"""
Provides the service logic for formatting code according to constitutional style rules.
"""

from __future__ import annotations

from typing import Optional

from shared.utils.subprocess_utils import run_poetry_command


# ID: 5c5890b0-8c2f-4d9a-a4e2-0f7b6a5c4e3b
def format_code(path: Optional[str] = None):
    """
    Format code using Black and Ruff, optionally targeting a specific file or directory.
    If no path is provided, it defaults to formatting 'src' and 'tests'.
    """
    targets = [path] if path else ["src", "tests"]

    run_poetry_command(
        f"✨ Formatting {' '.join(targets)} with Black...", ["black", *targets]
    )
    run_poetry_command(
        f"✨ Fixing {' '.join(targets)} with Ruff...",
        ["ruff", "check", "--fix", *targets],
    )

--- END OF FILE ./src/features/self_healing/code_style_service.py ---

--- START OF FILE ./src/features/self_healing/complexity_service.py ---
# src/features/self_healing/complexity_service.py
"""
Administrative tool for identifying and refactoring code complexity outliers.
This version includes a "Semantic Capability Reconciliation" step to ensure
that refactoring not only improves the code but also proposes necessary
amendments to the system's constitution.
"""

from __future__ import annotations

import asyncio
import json
import re
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional

import typer
import yaml
from rich.console import Console
from rich.panel import Panel

from core.cognitive_service import CognitiveService

# --- START OF AMENDMENT: Import the new async validator ---
from core.validation_pipeline import validate_code_async

# --- END OF AMENDMENT ---
from features.governance.audit_context import AuditorContext
from shared.config import settings
from shared.logger import getLogger
from shared.utils.parsing import extract_json_from_response, parse_write_blocks

log = getLogger("core_admin.fixer_complexity")
console = Console()
REPO_ROOT = settings.REPO_PATH


def _get_capabilities_from_code(code: str) -> List[str]:
    """A simple parser to extract # CAPABILITY tags from a string of code."""
    return re.findall(r"#\s*CAPABILITY:\s*(\S+)", code)


def _propose_constitutional_amendment(proposal_plan: Dict[str, Any]):
    """Creates a formal proposal file for a constitutional amendment."""
    proposal_dir = REPO_ROOT / ".intent" / "proposals"
    proposal_dir.mkdir(exist_ok=True)

    target_file_name = Path(proposal_plan["target_path"]).stem
    proposal_id = str(uuid.uuid4())[:8]
    proposal_filename = f"cr-refactor-{target_file_name}-{proposal_id}.yaml"
    proposal_path = proposal_dir / proposal_filename

    proposal_content = {
        "target_path": proposal_plan["target_path"],
        "action": "replace_file",
        "justification": proposal_plan["justification"],
        "content": yaml.dump(
            proposal_plan["content"], indent=2, default_flow_style=False
        ),
    }

    proposal_path.write_text(
        yaml.dump(proposal_content, indent=2, sort_keys=False), encoding="utf-8"
    )
    log.info(
        f"📄 Constitutional amendment proposed at: {proposal_path.relative_to(REPO_ROOT)}"
    )
    return True


async def _run_capability_reconciliation(
    cognitive_service: CognitiveService,
    original_code: str,
    original_capabilities: List[str],
    refactoring_plan: Dict[str, str],
) -> Dict[str, Any]:
    """
    Asks an AI Constitutionalist to analyze the refactoring, re-tag capabilities,
    and propose manifest changes.
    """
    log.info("🏛️  Asking AI Constitutionalist to reconcile capabilities...")
    refactored_code_json = json.dumps(refactoring_plan, indent=2)

    prompt = f"""
You are an expert CORE Constitutionalist. You understand that a good refactoring not only improves code but also clarifies purpose.
The original file provided these capabilities: {original_capabilities}
A refactoring has occurred, resulting in these new files:
{refactored_code_json}
Your task is to perform a semantic analysis and produce a JSON object with two keys: "code_modifications" and "constitutional_amendment_proposal".
1.  **code_modifications**: This should be a JSON object where keys are file paths and values are the complete, final source code WITH the original capabilities correctly re-tagged onto the new functions that now hold that responsibility.
2.  **constitutional_amendment_proposal**: If the refactoring has clarified purpose and new, more atomic capabilities should exist, define a manifest change proposal. If no change is needed, this key should be null. The proposal should have 'target_path', 'justification', and 'content' for the new manifest.
Your entire output must be a single, valid JSON object.
"""

    constitutionalist = cognitive_service.get_client_for_role("Planner")
    response = await constitutionalist.make_request_async(
        prompt, user_id="constitutionalist_agent"
    )

    try:
        reconciliation_result = extract_json_from_response(response)
        if not reconciliation_result:
            raise ValueError("No valid JSON object found in the AI's response.")
        log.info("   -> ✅ AI Constitutionalist provided a valid reconciliation plan.")
        return reconciliation_result
    except (json.JSONDecodeError, ValueError) as e:
        log.error(f"❌ Failed to parse reconciliation plan from AI: {e}")
        log.error(f"   -> AI Raw Response: {response}")
        return {
            "code_modifications": refactoring_plan,
            "constitutional_amendment_proposal": None,
        }


async def _async_complexity_outliers(
    file_path: Optional[Path],
    dry_run: bool,
):
    """Async core logic for identifying and refactoring complexity outliers."""
    log.info("🩺 Starting complexity outlier analysis and refactoring cycle...")
    outlier_files: list[str] = (
        [str(file_path.relative_to(REPO_ROOT))] if file_path else []
    )
    if not outlier_files:
        log.error("❌ Please provide a specific file path to refactor.")
        return

    cognitive_service = CognitiveService(REPO_ROOT)

    for file_rel_path in outlier_files:
        try:
            log.info(f"--- Processing: {file_rel_path} ---")
            source_code = (REPO_ROOT / file_rel_path).read_text(encoding="utf-8")

            log.info("🧠 Asking RefactoringArchitect for a plan...")
            prompt_template = (
                (settings.MIND / "prompts" / "refactor_outlier.prompt")
                .read_text(encoding="utf-8")
                .replace("{source_code}", source_code)
            )
            refactor_client = cognitive_service.get_client_for_role(
                "RefactoringArchitect"
            )
            response = await refactor_client.make_request_async(
                prompt_template, user_id="refactoring_agent"
            )

            refactoring_plan = parse_write_blocks(response)
            if not refactoring_plan:
                raise ValueError(
                    "No valid [[write:]] blocks found in the refactoring plan response."
                )

            log.info("🔬 Validating generated code for constitutional compliance...")
            auditor_context = AuditorContext(REPO_ROOT)
            validated_code_plan = {}
            for path, code in refactoring_plan.items():
                # --- START OF AMENDMENT: Call the async validator and await it ---
                result = await validate_code_async(
                    path, str(code), auditor_context=auditor_context
                )
                # --- END OF AMENDMENT ---
                if result["status"] == "dirty":
                    raise Exception(f"Validation FAILED for proposed file '{path}'")
                validated_code_plan[path] = result["code"]
            log.info("   -> ✅ Plan is valid and formatted.")

            final_code_to_write = validated_code_plan

            if dry_run:
                console.print(
                    Panel(
                        f"Refactoring Plan for [bold cyan]{file_rel_path}[/bold cyan]",
                        expand=False,
                    )
                )
                for path in final_code_to_write:
                    console.print(
                        f"  📄 [yellow]Action:[/yellow] Write to [bold]{path}[/bold]"
                    )
                log.warning("💧 Dry Run: Skipping write. Plan is valid.")
                continue

            log.info("💾 Applying validated and formatted refactoring...")
            (REPO_ROOT / file_rel_path).unlink()
            for path, code in final_code_to_write.items():
                (REPO_ROOT / path).write_text(code, encoding="utf-8")

            log.info(
                "✅ Refactoring applied. Run 'make check' to validate the new code state and fix any manifest drift."
            )

        except Exception as e:
            log.error(f"❌ Failed to process '{file_rel_path}': {e}", exc_info=True)
            continue


# ID: 6e802493-3d72-40e4-b80e-89c1518fdabb
def complexity_outliers(
    file_path: Optional[Path] = typer.Argument(
        None,
        help="Optional: The path to a specific file to refactor. If omitted, outliers are detected automatically.",
        exists=True,
        dir_okay=False,
        resolve_path=True,
    ),
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--write",
        help="Show what refactoring would be applied. Use --write to apply.",
    ),
):
    """Identifies and refactors complexity outliers to improve separation of concerns."""
    asyncio.run(_async_complexity_outliers(file_path, dry_run))

--- END OF FILE ./src/features/self_healing/complexity_service.py ---

--- START OF FILE ./src/features/self_healing/docstring_service.py ---
# src/features/self_healing/docstring_service.py
"""
Implements the 'fix docstrings' command, an AI-powered tool to add
missing docstrings to functions and methods.
"""

from __future__ import annotations

import asyncio

import typer
from core.cognitive_service import CognitiveService
from core.knowledge_service import KnowledgeService
from rich.progress import track
from shared.config import settings
from shared.logger import getLogger

from features.introspection.knowledge_helpers import extract_source_code

log = getLogger("core_admin.fixer_docstrings")
REPO_ROOT = settings.REPO_PATH


async def _async_fix_docstrings(dry_run: bool):
    """Async core logic for finding and fixing missing docstrings."""
    log.info("🔍 Searching for symbols missing docstrings...")

    knowledge_service = KnowledgeService(REPO_ROOT)
    graph = await knowledge_service.get_graph()
    symbols = graph.get("symbols", {})

    symbols_to_fix = [
        s
        for s in symbols.values()
        if not s.get("docstring")
        and s.get("type") in ["FunctionDef", "AsyncFunctionDef"]
    ]

    if not symbols_to_fix:
        log.info("✅ No symbols are missing docstrings. Excellent!")
        return

    log.info(f"Found {len(symbols_to_fix)} symbol(s) missing docstrings. Fixing...")

    cognitive_service = CognitiveService(REPO_ROOT)
    prompt_template = (
        settings.MIND / "prompts" / "fix_function_docstring.prompt"
    ).read_text(encoding="utf-8")
    writer_client = cognitive_service.get_client_for_role("DocstringWriter")

    modification_plan = {}

    for symbol in track(symbols_to_fix, description="Generating docstrings..."):
        try:
            source_code = extract_source_code(REPO_ROOT, symbol)
            final_prompt = prompt_template.format(source_code=source_code)

            new_docstring_content = await writer_client.make_request_async(
                final_prompt, user_id="docstring_writer_agent"
            )

            if new_docstring_content:
                file_path = REPO_ROOT / symbol["file"]
                if file_path not in modification_plan:
                    modification_plan[file_path] = []

                modification_plan[file_path].append(
                    {
                        "line_number": symbol["line_number"],
                        "indent": len(symbol.get("name", ""))
                        - len(symbol.get("name", "").lstrip()),
                        "docstring": new_docstring_content.strip().replace('"', '\\"'),
                    }
                )

        except Exception as e:
            log.error(f"Could not process {symbol['symbol_path']}: {e}")

    if dry_run:
        typer.secho("\n💧 Dry Run Summary:", bold=True)
        for file_path, patches in modification_plan.items():
            typer.secho(
                f"  - Would add {len(patches)} docstring(s) to: "
                f"{file_path.relative_to(REPO_ROOT)}",
                fg=typer.colors.YELLOW,
            )
    else:
        log.info("\n💾 Writing changes to disk...")
        for file_path, patches in modification_plan.items():
            try:
                lines = file_path.read_text(encoding="utf-8").splitlines()
                patches.sort(key=lambda p: p["line_number"], reverse=True)

                for patch in patches:
                    indent_space = " " * (patch["indent"] + 4)
                    docstring = f'{indent_space}"""{patch["docstring"]}"""'
                    lines.insert(patch["line_number"], docstring)

                file_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
                log.info(
                    f"   -> ✅ Wrote {len(patches)} docstring(s) to "
                    f"{file_path.relative_to(REPO_ROOT)}"
                )
            except Exception as e:
                log.error(f"Failed to write to {file_path}: {e}")


# ID: 974fbc4d-da2e-4f45-8199-30972715c284
def fix_docstrings(
    write: bool = typer.Option(
        False, "--write", help="Apply the suggested docstrings directly to the files."
    ),
):
    """Uses an AI agent to find and add missing docstrings to functions and methods."""
    asyncio.run(_async_fix_docstrings(dry_run=not write))

--- END OF FILE ./src/features/self_healing/docstring_service.py ---

--- START OF FILE ./src/features/self_healing/duplicate_id_service.py ---
# src/features/self_healing/duplicate_id_service.py
"""
Provides a service to intelligently find and resolve duplicate UUIDs in the codebase.
"""

from __future__ import annotations

import uuid
from collections import defaultdict
from typing import Dict, List, Tuple

from rich.console import Console
from services.database.session_manager import get_session
from shared.config import settings
from sqlalchemy import text

from features.governance.checks.id_uniqueness_check import IdUniquenessCheck

console = Console()


async def _get_symbol_creation_dates() -> Dict[str, str]:
    """Queries the database to get the creation timestamp for each symbol UUID."""
    async with get_session() as session:
        # --- MODIFIED: Select the correct 'id' column instead of 'uuid' ---
        result = await session.execute(text("SELECT id, created_at FROM core.symbols"))
        # --- MODIFIED: Access the result using 'row.id' instead of 'row.uuid' ---
        return {str(row.id): row.created_at.isoformat() for row in result}


# ID: 5891cbbe-ae62-4743-92fa-2e204ca5fa13
async def resolve_duplicate_ids(dry_run: bool = True) -> int:
    """
    Finds all duplicate IDs and fixes them by assigning new UUIDs to all but the oldest symbol.

    Returns:
        The number of files that were (or would be) modified.
    """
    console.print("🕵️  Scanning for duplicate UUIDs...")

    # 1. Discover duplicates using the existing auditor check
    context = __import__(
        "features.governance.audit_context"
    ).governance.audit_context.AuditorContext(settings.REPO_PATH)
    uniqueness_check = IdUniquenessCheck(context)
    findings = uniqueness_check.execute()

    duplicates = [f for f in findings if f.check_id == "linkage.id.duplicate"]

    if not duplicates:
        console.print("[bold green]✅ No duplicate UUIDs found.[/bold green]")
        return 0

    console.print(
        f"[bold yellow]Found {len(duplicates)} duplicate UUID(s). Resolving...[/bold yellow]"
    )

    # 2. Get creation dates from the database to find the "original"
    symbol_creation_dates = await _get_symbol_creation_dates()

    files_to_modify: Dict[str, List[Tuple[int, str]]] = defaultdict(list)

    for finding in duplicates:
        locations_str = finding.context.get("locations", "")
        # The UUID is in the message: "Duplicate ID tag found: {uuid}"
        duplicate_uuid = finding.message.split(": ")[-1]

        locations = []
        for loc in locations_str.split(", "):
            path, line = loc.rsplit(":", 1)
            locations.append((path, int(line)))

        # Find the original symbol (the one created first)
        original_location = None

        # Check if we have creation date info for this UUID
        if duplicate_uuid in symbol_creation_dates:
            # Assume the first location for a given UUID is the original if we have DB info
            original_location = locations[0]
        else:
            # Fallback for symbols not yet in DB: assume first found is original
            original_location = locations[0]

        console.print(f"  -> Duplicate UUID: [cyan]{duplicate_uuid}[/cyan]")
        console.print(
            f"     - Original determined to be at: [green]{original_location[0]}:{original_location[1]}[/green]"
        )

        # Mark all other locations for change
        for path, line_num in locations:
            if (path, line_num) != original_location:
                console.print(
                    f"     - Copy found at: [yellow]{path}:{line_num}[/yellow]"
                )
                files_to_modify[path].append((line_num, duplicate_uuid))

    if not files_to_modify:
        console.print(
            "[bold green]✅ All duplicates seem to be resolved or are new. No changes needed.[/bold green]"
        )
        return 0

    if dry_run:
        console.print(
            "\n[bold yellow]-- DRY RUN: No files will be changed. --[/bold yellow]"
        )
        for path, changes in files_to_modify.items():
            console.print(
                f"  - Would modify [cyan]{path}[/cyan] to fix {len(changes)} duplicate ID(s)."
            )
        return len(files_to_modify)

    # Apply the changes
    console.print("\n[bold]Applying fixes...[/bold]")
    for file_str, changes in files_to_modify.items():
        file_path = settings.REPO_PATH / file_str
        content = file_path.read_text("utf-8")
        lines = content.splitlines()

        for line_num, old_uuid in changes:
            new_uuid = str(uuid.uuid4())
            line_index = line_num - 1
            if old_uuid in lines[line_index]:
                lines[line_index] = lines[line_index].replace(old_uuid, new_uuid)
                console.print(
                    f"  - Replaced ID in [green]{file_str}:{line_num}[/green]"
                )

        file_path.write_text("\n".join(lines) + "\n", "utf-8")

    return len(files_to_modify)

--- END OF FILE ./src/features/self_healing/duplicate_id_service.py ---

--- START OF FILE ./src/features/self_healing/enrichment_service.py ---
# src/features/self_healing/enrichment_service.py
from __future__ import annotations

import asyncio
from functools import partial
from typing import Any, Dict, List

from core.cognitive_service import CognitiveService
from rich.console import Console
from services.clients.qdrant_client import QdrantService
from services.database.session_manager import get_session
from shared.config import settings
from shared.logger import getLogger
from shared.utils.parallel_processor import ThrottledParallelProcessor
from shared.utils.parsing import extract_json_from_response
from sqlalchemy import text

from features.introspection.knowledge_helpers import extract_source_code

console = Console()
log = getLogger("enrichment_service")
REPO_ROOT = settings.REPO_PATH


async def _get_symbols_to_enrich() -> List[Dict[str, Any]]:
    """Fetches symbols that are ready for enrichment (have a null or placeholder description)."""
    async with get_session() as session:
        # --- FIX #1: Query the correct table (core.symbols) and look for NULL/TBD intents ---
        result = await session.execute(
            text(
                """
                SELECT uuid, symbol_path, module AS file_path, vector_id
                FROM core.symbols
                WHERE intent IS NULL OR intent = 'TBD'
            """
            )
        )
        return [dict(row._mapping) for row in result]


async def _enrich_single_symbol(
    symbol: Dict[str, Any],
    cognitive_service: CognitiveService,
    qdrant_service: QdrantService,
) -> Dict[str, str]:
    """Uses an AI to generate a description for a single symbol."""
    try:
        log.debug(f"Enriching symbol: {symbol.get('symbol_path')}")
        source_code = extract_source_code(REPO_ROOT, symbol)
        if not source_code:
            return {"uuid": symbol["uuid"], "description": "error.code_not_found"}

        prompt_template = (
            REPO_ROOT / ".intent/mind/prompts/enrich_symbol.prompt"
        ).read_text("utf-8")
        final_prompt = prompt_template.format(
            symbol_path=symbol["symbol_path"],
            file_path=symbol["file_path"],
            similar_capabilities="Context from similar capabilities is disabled for this operation.",
            source_code=source_code,
        )

        log.debug(
            f"FINAL PROMPT for {symbol['symbol_path']}:\n---\n{final_prompt}\n---"
        )

        # --- FIX #2: Use the 'Coder' role, which is assigned to the code-aware model ---
        enricher_agent = await cognitive_service.aget_client_for_role("Coder")

        raw_response = await enricher_agent.make_request_async(
            final_prompt, user_id="enricher_agent"
        )

        parsed_response = extract_json_from_response(raw_response)
        if parsed_response and isinstance(parsed_response, dict):
            description = parsed_response.get(
                "description", "error.parsing_failed"
            ).strip()
        else:
            description = "error.parsing_failed"

        try:
            delay_str = settings.model_extra.get("LLM_SECONDS_BETWEEN_REQUESTS", "1")
            delay = int(delay_str)
        except (ValueError, TypeError):
            delay = 1
        await asyncio.sleep(delay)

        return {"uuid": symbol["uuid"], "description": description}
    except Exception as e:
        log.error(f"Failed to enrich symbol '{symbol.get('symbol_path')}': {e}")
        return {"uuid": symbol["uuid"], "description": "error.processing_failed"}


async def _update_descriptions_in_db(descriptions: List[Dict[str, str]]):
    """Updates the 'intent' column for symbols in the database."""
    if not descriptions:
        return

    log.info(
        f"Attempting to update {len(descriptions)} descriptions in the database..."
    )
    async with get_session() as session:
        async with session.begin():
            await session.execute(
                text(
                    "UPDATE core.symbols SET intent = :description WHERE uuid = :uuid"
                ),
                descriptions,
            )
    log.info("Database update transaction completed.")


# ID: cfbe1b7c-18cc-4d71-8910-0acb6696f119
async def enrich_symbols(cognitive_service: CognitiveService, dry_run: bool):
    """The main orchestrator for the autonomous symbol enrichment process."""
    symbols_to_enrich = await _get_symbols_to_enrich()
    if not symbols_to_enrich:
        console.print(
            "[bold green]✅ No symbols with placeholder descriptions found.[/bold green]"
        )
        return

    console.print(f"   -> Found {len(symbols_to_enrich)} symbols to enrich...")

    qdrant_service = QdrantService()
    processor = ThrottledParallelProcessor(description="Enriching symbols...")
    worker_fn = partial(
        _enrich_single_symbol,
        cognitive_service=cognitive_service,
        qdrant_service=qdrant_service,
    )
    descriptions = await processor.run_async(symbols_to_enrich, worker_fn)

    valid_descriptions = [
        d
        for d in descriptions
        if d.get("description") and not d["description"].startswith("error.")
    ]

    if dry_run:
        console.print(
            "[bold yellow]-- DRY RUN: The following descriptions would be written --[/bold yellow]"
        )
        for d in valid_descriptions[:10]:
            console.print(
                f"  - Symbol UUID [dim]{d['uuid']}[/dim] -> '{d['description']}'"
            )
        if len(valid_descriptions) > 10:
            console.print(f"  - ... and {len(valid_descriptions) - 10} more.")
        return

    await _update_descriptions_in_db(valid_descriptions)
    console.print(
        f"   -> Successfully enriched {len(valid_descriptions)} symbols in the database."
    )

--- END OF FILE ./src/features/self_healing/enrichment_service.py ---

--- START OF FILE ./src/features/self_healing/fix_manifest_hygiene.py ---
# src/features/self_healing/fix_manifest_hygiene.py
"""
A self-healing tool that scans domain manifests for misplaced capability
declarations and moves them to the correct manifest file.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict

import typer
import yaml
from rich.console import Console
from shared.config import settings
from shared.logger import getLogger

log = getLogger("fix_manifest_hygiene")
console = Console()
REPO_ROOT = settings.REPO_PATH
DOMAINS_DIR = REPO_ROOT / ".intent" / "mind" / "knowledge" / "domains"


# ID: 104d24d1-119d-42ef-88c5-197eb75e0b81
def run_fix_manifest_hygiene(
    write: bool = typer.Option(
        False, "--write", help="Apply fixes to the manifest files."
    ),
):
    """
    Scans for and corrects misplaced capability declarations in domain manifests.
    """
    dry_run = not write
    log.info("🧼 Starting manifest hygiene check for misplaced capabilities...")
    if not DOMAINS_DIR.is_dir():
        log.error(f"Domains directory not found at: {DOMAINS_DIR}")
        raise typer.Exit(code=1)

    all_domain_files = {p.stem: p for p in DOMAINS_DIR.glob("*.yaml")}
    changes_to_make: Dict[str, Dict[str, Any]] = {}

    for domain_name, file_path in all_domain_files.items():
        try:
            content = yaml.safe_load(file_path.read_text("utf-8")) or {}
            capabilities = content.get("tags", [])

            misplaced_caps = [
                cap
                for cap in capabilities
                if isinstance(cap, dict)
                and "key" in cap
                and not cap["key"].startswith(f"{domain_name}.")
            ]

            if misplaced_caps:
                # Keep only the correctly placed capabilities
                content["tags"] = [
                    cap for cap in capabilities if cap not in misplaced_caps
                ]
                changes_to_make[str(file_path)] = {
                    "action": "update",
                    "content": content,
                }

                # Move the misplaced capabilities to their correct files
                for cap in misplaced_caps:
                    correct_domain = cap["key"].split(".")[0]
                    correct_file_path = all_domain_files.get(correct_domain)

                    if correct_file_path:
                        correct_path_str = str(correct_file_path)
                        if correct_path_str not in changes_to_make:
                            changes_to_make[correct_path_str] = {
                                "action": "update",
                                "content": yaml.safe_load(
                                    correct_file_path.read_text("utf-8")
                                )
                                or {"tags": []},
                            }

                        changes_to_make[correct_path_str]["content"].setdefault(
                            "tags", []
                        ).append(cap)
                        log.info(
                            f"   -> Planning to move '{cap['key']}' from '{file_path.name}' to '{correct_file_path.name}'"
                        )
                    else:
                        log.warning(
                            f"   -> Could not find a manifest file for domain '{correct_domain}' to move '{cap['key']}'."
                        )

        except Exception as e:
            log.error(f"Error processing {file_path.name}: {e}")

    if not changes_to_make:
        console.print(
            "[bold green]✅ Manifest hygiene is perfect. No misplaced capabilities found.[/bold green]"
        )
        return

    if dry_run:
        console.print(
            "\n[bold yellow]-- DRY RUN: The following manifest changes would be applied --[/bold yellow]"
        )
        for path_str, change in changes_to_make.items():
            console.print(
                f"  - File to {change['action']}: {Path(path_str).relative_to(REPO_ROOT)}"
            )
        return

    console.print("\n[bold]Applying manifest hygiene fixes...[/bold]")
    for path_str, change in changes_to_make.items():
        try:
            Path(path_str).write_text(
                yaml.dump(change["content"], indent=2, sort_keys=False), "utf-8"
            )
            console.print(f"  - ✅ Updated {Path(path_str).name}")
        except Exception as e:
            console.print(f"  - ❌ Failed to update {Path(path_str).name}: {e}")


if __name__ == "__main__":
    typer.run(run_fix_manifest_hygiene)

--- END OF FILE ./src/features/self_healing/fix_manifest_hygiene.py ---

--- START OF FILE ./src/features/self_healing/header_service.py ---
# src/features/self_healing/header_service.py
"""
The orchestration logic for the unified header fixer, which uses a deterministic
tool to enforce constitutional style rules on Python file headers.
"""

from __future__ import annotations

import asyncio

from rich.progress import track
from shared.config import settings
from shared.logger import getLogger
from shared.utils.header_tools import HeaderTools

from features.introspection.knowledge_graph_service import KnowledgeGraphBuilder

log = getLogger("core_admin.fixer")
REPO_ROOT = settings.REPO_PATH


def _run_header_fix_cycle(dry_run: bool, all_py_files: list[str]):
    """The core logic for finding and fixing all header style violations."""
    log.info(f"Scanning {len(all_py_files)} files for header compliance...")

    files_to_fix = {}
    for file_path_str in track(all_py_files, description="Analyzing headers..."):
        file_path = REPO_ROOT / file_path_str
        try:
            original_content = file_path.read_text(encoding="utf-8")
            header = HeaderTools.parse(original_content)

            # Check for violations that need fixing
            correct_location_comment = f"# {file_path_str}"
            is_compliant = (
                header.location == correct_location_comment
                and header.module_description is not None
                and header.has_future_import
            )

            if not is_compliant:
                header.location = correct_location_comment
                if not header.module_description:
                    # Provide a default, high-quality docstring
                    header.module_description = (
                        f'"""Provides functionality for the {file_path.stem} module."""'
                    )
                header.has_future_import = True

                corrected_code = HeaderTools.reconstruct(header)
                if corrected_code != original_content:
                    files_to_fix[file_path_str] = corrected_code

        except Exception as e:
            log.warning(f"Could not process {file_path_str}: {e}")

    if not files_to_fix:
        log.info("✅ All file headers are constitutionally compliant.")
        return

    log.info(f"Found {len(files_to_fix)} file(s) requiring header fixes.")

    if dry_run:
        for file_path in sorted(files_to_fix.keys()):
            log.info(f"   -> [DRY RUN] Would fix header in: {file_path}")
    else:
        log.info("💾 Writing changes to disk...")
        for file_path_str, new_code in files_to_fix.items():
            (REPO_ROOT / file_path_str).write_text(new_code, "utf-8")
        log.info("   -> ✅ All header fixes have been applied.")

        # Rebuild the knowledge graph after making changes
        log.info("🧠 Rebuilding knowledge graph to reflect all changes...")
        builder = KnowledgeGraphBuilder(REPO_ROOT)
        asyncio.run(builder.build_and_sync())
        log.info("✅ Knowledge graph successfully updated.")

--- END OF FILE ./src/features/self_healing/header_service.py ---

--- START OF FILE ./src/features/self_healing/id_tagging_service.py ---
# src/features/self_healing/id_tagging_service.py
from __future__ import annotations

import ast
import uuid
from collections import defaultdict

from rich.console import Console
from shared.ast_utility import find_symbol_id_and_def_line
from shared.config import settings

console = Console()


def _is_public(node: ast.FunctionDef | ast.AsyncFunctionDef | ast.ClassDef) -> bool:
    """Determines if a symbol is public (not starting with _ or a dunder)."""
    is_dunder = node.name.startswith("__") and node.name.endswith("__")
    return not node.name.startswith("_") and not is_dunder


# ID: 38f29597-95bb-4e6c-aabb-72baaf841522
def assign_missing_ids(dry_run: bool = True) -> int:
    """
    Scans all Python files in the 'src/' directory, finds public symbols
    missing an '# ID:' tag, and adds a new UUID tag to them. Returns the count.
    """
    src_dir = settings.REPO_PATH / "src"
    files_to_process = list(src_dir.rglob("*.py"))
    total_ids_assigned = 0
    files_to_fix = defaultdict(list)

    for file_path in files_to_process:
        try:
            content = file_path.read_text("utf-8")
            source_lines = content.splitlines()
            tree = ast.parse(content, filename=str(file_path))

            for node in ast.walk(tree):
                if isinstance(
                    node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                ):
                    if not _is_public(node):
                        continue

                    id_result = find_symbol_id_and_def_line(node, source_lines)

                    if not id_result.has_id:
                        files_to_fix[file_path].append(
                            {
                                "line_number": id_result.definition_line_num,
                                "name": node.name,
                            }
                        )
        except Exception as e:
            console.print(
                f"   -> [bold red]❌ Error processing {file_path}: {e}[/bold red]"
            )

    if not files_to_fix:
        return 0

    for file_path, fixes in files_to_fix.items():
        fixes.sort(key=lambda x: x["line_number"], reverse=True)

        if dry_run:
            total_ids_assigned += len(fixes)
            continue

        try:
            lines = file_path.read_text("utf-8").splitlines()
            for fix in fixes:
                line_index = fix["line_number"] - 1
                original_line = lines[line_index]
                indentation = len(original_line) - len(original_line.lstrip(" "))

                new_id = str(uuid.uuid4())
                tag_line = f"{' ' * indentation}# ID: {new_id}"

                lines.insert(line_index, tag_line)
                total_ids_assigned += 1

            file_path.write_text("\n".join(lines) + "\n", "utf-8")
        except Exception as e:
            console.print(
                f"   -> [bold red]❌ Error writing to {file_path}: {e}[/bold red]"
            )

    return total_ids_assigned

--- END OF FILE ./src/features/self_healing/id_tagging_service.py ---

--- START OF FILE ./src/features/self_healing/knowledge_consolidation_service.py ---
# src/features/self_healing/knowledge_consolidation_service.py
"""
Provides services for identifying and consolidating duplicated or common knowledge
across the codebase, serving the 'dry_by_design' principle.
"""

from __future__ import annotations

import ast
import hashlib
from typing import Dict, List, Tuple

# --- THIS IS THE FIX ---
from shared.ast_utility import normalize_ast
from shared.config import settings

# --- END OF FIX ---


# ID: e9a1b8c3-d7f4-4b1e-a9d5-f8c3d7f4b1e9
def find_structurally_similar_helpers(
    min_occurrences: int = 3,
    max_lines: int = 10,
) -> Dict[str, List[Tuple[str, int]]]:
    """
    Scans the 'src/' directory for small, structurally identical public functions.

    It works by creating a normalized Abstract Syntax Tree (AST) for each function,
    hashing it, and grouping functions by their hash. This allows it to find
    functionally identical helpers even if variable names and docstrings differ.

    Args:
        min_occurrences: The minimum number of times a function must appear to be considered a duplicate.
        max_lines: The maximum number of lines a function can have to be considered a small helper.

    Returns:
        A dictionary where keys are the structural hash of duplicated functions
        and values are a list of tuples containing (file_path, line_number).
    """
    src_root = settings.REPO_PATH / "src"
    duplicates: Dict[str, List[Tuple[str, int]]] = {}

    for py_file in src_root.rglob("*.py"):
        # Exclude tests and other non-source directories
        if "test" in py_file.parts or "venv" in py_file.parts:
            continue
        try:
            content = py_file.read_text(encoding="utf-8")
            tree = ast.parse(content)
        except (SyntaxError, UnicodeDecodeError):
            continue

        for node in ast.walk(tree):
            if (
                isinstance(node, ast.FunctionDef)
                and len(node.body) <= max_lines
                and not node.name.startswith("_")
                and not node.decorator_list
            ):
                try:
                    # Normalize the AST to make the hash independent of var names and docstrings
                    norm_ast_str = normalize_ast(node)
                    h = hashlib.sha256(norm_ast_str.encode()).hexdigest()
                    rel_path = str(py_file.relative_to(settings.REPO_PATH))
                    duplicates.setdefault(h, []).append((rel_path, node.lineno))
                except Exception:
                    continue  # Skip nodes that fail normalization

    # Filter for groups that meet the minimum occurrence threshold
    return {
        h: places for h, places in duplicates.items() if len(places) >= min_occurrences
    }

--- END OF FILE ./src/features/self_healing/knowledge_consolidation_service.py ---

--- START OF FILE ./src/features/self_healing/linelength_service.py ---
# src/features/self_healing/linelength_service.py
"""
Implements the 'fix line-lengths' command, an AI-powered tool to
refactor code for better readability by adhering to line length policies.
"""

from __future__ import annotations

import asyncio
from pathlib import Path
from typing import List, Optional

import typer
from core.cognitive_service import CognitiveService

# --- START OF AMENDMENT: Import the new async validator ---
from core.validation_pipeline import validate_code_async
from rich.progress import track
from shared.config import settings
from shared.logger import getLogger

# --- END OF AMENDMENT ---
from features.governance.audit_context import AuditorContext

log = getLogger("core_admin.fixer_linelength")
REPO_ROOT = settings.REPO_PATH


async def _async_fix_line_lengths(files_to_process: List[Path], dry_run: bool):
    """Async core logic for finding and fixing all line length violations."""
    log.info(
        f"Scanning {len(files_to_process)} files for lines longer than 100 characters..."
    )

    cognitive_service = CognitiveService(REPO_ROOT)
    prompt_template_path = settings.MIND / "prompts" / "fix_line_length.prompt"
    if not prompt_template_path.exists():
        log.error(f"Prompt not found at {prompt_template_path}. Cannot proceed.")
        raise typer.Exit(code=1)
    prompt_template = prompt_template_path.read_text(encoding="utf-8")
    fixer_client = cognitive_service.get_client_for_role("CodeStyleFixer")

    auditor_context = AuditorContext(REPO_ROOT)
    await auditor_context.load_knowledge_graph()  # Pre-load the graph for the validator

    files_with_long_lines = []
    for file_path in files_to_process:
        try:
            for line in file_path.read_text(encoding="utf-8").splitlines():
                if len(line) > 100:
                    files_with_long_lines.append(file_path)
                    break
        except Exception:
            continue

    if not files_with_long_lines:
        log.info("✅ No files with long lines found. Nothing to do.")
        return

    log.info(f"Found {len(files_with_long_lines)} file(s) with long lines to fix.")

    modification_plan = {}

    for file_path in track(
        files_with_long_lines, description="Asking AI to refactor files..."
    ):
        try:
            original_content = file_path.read_text(encoding="utf-8")
            final_prompt = prompt_template.replace("{source_code}", original_content)

            corrected_code = await fixer_client.make_request_async(
                final_prompt, user_id="line_length_fixer_agent"
            )

            if corrected_code and corrected_code.strip() != original_content.strip():
                # --- START OF AMENDMENT: Call the async validator and await it ---
                validation_result = await validate_code_async(
                    str(file_path),
                    corrected_code,
                    quiet=True,
                    auditor_context=auditor_context,
                )
                # --- END OF AMENDMENT ---
                if validation_result["status"] == "clean":
                    modification_plan[file_path] = validation_result["code"]
                else:
                    log.warning(
                        f"Skipping {file_path.name}: AI-generated code failed validation."
                    )
        except Exception as e:
            log.error(f"Could not process {file_path.name}: {e}")

    if dry_run:
        typer.secho("\n💧 Dry Run Summary:", bold=True)
        for file_path in sorted(modification_plan.keys()):
            typer.secho(
                f"  - Would fix line lengths in: {file_path.relative_to(REPO_ROOT)}",
                fg=typer.colors.YELLOW,
            )
    else:
        log.info("\n💾 Writing changes to disk...")
        for file_path, new_code in modification_plan.items():
            file_path.write_text(new_code, "utf-8")
            log.info(
                f"   -> ✅ Fixed line lengths in {file_path.relative_to(REPO_ROOT)}"
            )


# ID: 1655a2ca-f71f-470b-8f43-a33ee28d64dd
def fix_line_lengths(
    file_path: Optional[Path] = typer.Argument(
        None,
        help="Optional: A specific file to fix. If omitted, all project files are scanned.",
        exists=True,
        dir_okay=False,
        resolve_path=True,
    ),
    write: bool = typer.Option(
        False, "--write", help="Apply the changes directly to the files."
    ),
):
    """Uses an AI agent to refactor files with lines longer than 100 characters."""
    files_to_scan = []
    if file_path:
        files_to_scan.append(file_path)
    else:
        # Scan all Python files in the src directory
        src_dir = REPO_ROOT / "src"
        files_to_scan.extend(src_dir.rglob("*.py"))

    asyncio.run(_async_fix_line_lengths(files_to_scan, dry_run=not write))

--- END OF FILE ./src/features/self_healing/linelength_service.py ---

--- START OF FILE ./src/features/self_healing/policy_id_service.py ---
# src/features/self_healing/policy_id_service.py
"""
Provides the service logic for the one-time constitutional migration to add
UUIDs to all policy files, bringing them into compliance with the updated policy_schema.
"""

from __future__ import annotations

import uuid

import yaml
from rich.console import Console
from shared.config import settings

console = Console()


# ID: c1a2b3d4-e5f6-7a8b-9c0d-1e2f3a4b5c6d
def add_missing_policy_ids(dry_run: bool = True) -> int:
    """
    Scans all constitutional policy files and adds a `policy_id` UUID if it's missing.

    Args:
        dry_run: If True, only reports on the changes that would be made.

    Returns:
        The total number of policies that were (or would be) updated.
    """
    policies_dir = settings.REPO_PATH / ".intent" / "charter" / "policies"
    if not policies_dir.is_dir():
        console.print(
            f"[bold red]Policies directory not found at: {policies_dir}[/bold red]"
        )
        return 0

    files_to_process = list(policies_dir.rglob("*_policy.yaml"))
    policies_updated = 0

    console.print(
        f"🔍 Scanning {len(files_to_process)} policy files for missing IDs..."
    )

    for file_path in files_to_process:
        try:
            content = file_path.read_text("utf-8")
            # Use safe_load to check for the key's existence
            data = yaml.safe_load(content) or {}

            if "policy_id" in data:
                continue

            # If the key is missing, add it
            policies_updated += 1
            new_id = str(uuid.uuid4())

            # Prepend the new ID to the raw file content to preserve comments and structure
            new_content = f"policy_id: {new_id}\n" + content

            if dry_run:
                console.print(
                    f"  -> [DRY RUN] Would add `policy_id: {new_id}` to [cyan]{file_path.name}[/cyan]"
                )
            else:
                file_path.write_text(new_content, "utf-8")
                console.print(
                    f"  -> ✅ Added `policy_id` to [green]{file_path.name}[/green]"
                )

        except Exception as e:
            console.print(
                f"  -> [bold red]❌ Error processing {file_path.name}: {e}[/bold red]"
            )

    return policies_updated

--- END OF FILE ./src/features/self_healing/policy_id_service.py ---

--- START OF FILE ./src/features/self_healing/prune_orphaned_vectors.py ---
# src/features/self_healing/prune_orphaned_vectors.py
"""
A self-healing tool to find and delete orphaned vectors from the Qdrant database.
An orphan is a vector whose corresponding symbol no longer exists in the main database.
"""

from __future__ import annotations

import asyncio

import typer
from qdrant_client import AsyncQdrantClient
from qdrant_client.http.models import PointIdsList
from rich.console import Console
from services.database.session_manager import get_session
from shared.config import settings
from shared.logger import getLogger
from sqlalchemy import text

log = getLogger("prune_orphaned_vectors")
console = Console()


async def _async_prune_orphans(dry_run: bool):
    """The core async logic for finding and pruning orphaned vectors."""
    console.print("[bold cyan]🌿 Starting orphan vector pruning process...[/bold cyan]")

    valid_vector_ids = set()
    try:
        # 1. Get the ground truth: all valid vector IDs from the link table.
        console.print("   -> Fetching valid vector IDs from PostgreSQL link table...")
        async with get_session() as session:
            result = await session.execute(
                text("SELECT vector_id FROM core.symbol_vector_links")
            )
            valid_vector_ids = {row[0] for row in result}
        console.print(
            f"      - Found {len(valid_vector_ids)} valid vector links in the main database."
        )

    except Exception as e:
        console.print(f"[bold red]❌ Database query failed: {e}[/bold red]")
        raise typer.Exit(code=1)

    # 2. Get the current state: all vector IDs from the vector store
    qdrant_service = AsyncQdrantClient(url=settings.QDRANT_URL)
    vector_point_ids = set()
    try:
        console.print("   -> Fetching all vector point IDs from Qdrant...")
        all_points, _ = await qdrant_service.scroll(
            collection_name=settings.QDRANT_COLLECTION_NAME,
            limit=10000,
            with_payload=False,
            with_vectors=False,
        )
        vector_point_ids = {str(point.id) for point in all_points}
        console.print(f"      - Found {len(vector_point_ids)} vectors in Qdrant.")

    except Exception as e:
        console.print(
            f"[bold red]❌ Failed to connect to or query Qdrant: {e}[/bold red]"
        )
        raise typer.Exit(code=1)

    # 3. Compare the two sets to find the orphans
    orphaned_ids = list(vector_point_ids - valid_vector_ids)

    if not orphaned_ids:
        console.print(
            "\n[bold green]✅ No orphaned vectors found. The vector store is clean.[/bold green]"
        )
        return

    console.print(
        f"\n[bold yellow]Found {len(orphaned_ids)} orphaned vectors to prune.[/bold yellow]"
    )

    if dry_run:
        console.print(
            "\n[bold yellow]-- DRY RUN: The following vector point IDs would be deleted --[/bold yellow]"
        )
        for point_id in orphaned_ids[:20]:
            console.print(f"  - {point_id}")
        if len(orphaned_ids) > 20:
            console.print(f"  - ... and {len(orphaned_ids) - 20} more.")
        return

    # 4. Execute the deletion
    console.print("\n[bold]Pruning orphaned vectors from Qdrant...[/bold]")
    await qdrant_service.delete(
        collection_name=settings.QDRANT_COLLECTION_NAME,
        points_selector=PointIdsList(points=orphaned_ids),
    )

    console.print(
        f"[bold green]✅ Successfully pruned {len(orphaned_ids)} orphaned vectors.[/bold green]"
    )


# ID: 47ae55f7-19bb-4bd9-9361-e33733a64ba9
def main_sync(
    write: bool = typer.Option(
        False, "--write", help="Permanently delete orphaned vectors from Qdrant."
    ),
):
    """Entry point for the Typer command."""
    asyncio.run(_async_prune_orphans(dry_run=not write))

--- END OF FILE ./src/features/self_healing/prune_orphaned_vectors.py ---

--- START OF FILE ./src/features/self_healing/prune_private_capabilities.py ---
# src/features/self_healing/prune_private_capabilities.py
"""
A self-healing tool that scans the codebase and removes # CAPABILITY tags
from private symbols (those starting with an underscore), enforcing the
'caps.ignore_private' constitutional policy.
"""

from __future__ import annotations

import asyncio
import re

import typer
from core.knowledge_service import KnowledgeService
from rich.console import Console
from shared.config import settings
from shared.logger import getLogger

log = getLogger("prune_private_caps")
console = Console()
REPO_ROOT = settings.REPO_PATH


# ID: 85bc7272-2a3e-4833-80a6-fd3f27e5df9c
def main(
    write: bool = typer.Option(
        False, "--write", help="Apply fixes and remove tags from source files."
    ),
):
    """
    Finds and removes capability tags from private symbols (_ or __).
    """
    dry_run = not write
    log.info("🐍 Pruning capability tags from private symbols...")

    async def _async_main():
        knowledge_service = KnowledgeService(REPO_ROOT)
        graph = await knowledge_service.get_graph()
        symbols = graph.get("symbols", {})

        private_symbols_with_tags = [
            s
            for s in symbols.values()
            if s.get("name", "").startswith("_")
            and s.get("capability") != "unassigned"
            and s.get("capability") is not None
        ]

        if not private_symbols_with_tags:
            console.print(
                "[bold green]✅ No private symbols with capability tags found. Compliance is perfect.[/bold green]"
            )
            return

        console.print(
            f"[yellow]Found {len(private_symbols_with_tags)} private symbol(s) with capability tags.[/yellow]"
        )

        files_to_modify = {}
        tag_pattern = re.compile(r"^\s*#\s*CAPABILITY:\s*\S+\s*$", re.IGNORECASE)

        for symbol in private_symbols_with_tags:
            file_path_str = symbol.get("file")
            if not file_path_str:
                continue

            file_path = REPO_ROOT / file_path_str
            line_num = symbol.get("line_number", 0)

            if file_path not in files_to_modify:
                if file_path.exists():
                    files_to_modify[file_path] = file_path.read_text(
                        "utf-8"
                    ).splitlines()
                else:
                    log.warning(
                        f"File not found for symbol {symbol['symbol_path']}: {file_path}"
                    )
                    continue

            tag_line_index = line_num - 2
            if 0 <= tag_line_index < len(files_to_modify[file_path]):
                line_to_check = files_to_modify[file_path][tag_line_index]
                if tag_pattern.match(line_to_check):
                    log.info(
                        f"   -> Planning to remove tag for '{symbol['name']}' in {file_path_str}"
                    )
                    files_to_modify[file_path][tag_line_index] = "__DELETE_THIS_LINE__"

        if dry_run:
            console.print(
                "\n[bold yellow]-- DRY RUN: No files will be changed --[/bold yellow]"
            )
            return

        console.print("\n[bold]Applying fixes to source files...[/bold]")
        for file_path, lines in files_to_modify.items():
            new_content = (
                "\n".join([line for line in lines if line != "__DELETE_THIS_LINE__"])
                + "\n"
            )
            file_path.write_text(new_content, "utf-8")
            console.print(f"  - ✅ Pruned tags from {file_path.relative_to(REPO_ROOT)}")

    asyncio.run(_async_main())


if __name__ == "__main__":
    typer.run(main)

--- END OF FILE ./src/features/self_healing/prune_private_capabilities.py ---

--- START OF FILE ./src/features/self_healing/purge_legacy_tags_service.py ---
# src/features/self_healing/purge_legacy_tags_service.py
from __future__ import annotations

from collections import defaultdict

from rich.console import Console

from features.governance.audit_context import AuditorContext
from features.governance.checks.legacy_tag_check import LegacyTagCheck
from shared.config import settings

console = Console()


# ID: 5b7a5950-e534-4fb8-ad13-f9e6ad555643
def purge_legacy_tags(dry_run: bool = True) -> int:
    """
    removes them from the source files. This function is constitutionally

    Args:
        dry_run: If True, only prints the actions that would be taken.

    Returns:
        The total number of lines that were (or would be) removed.
    """
    context = AuditorContext(settings.REPO_PATH)
    check = LegacyTagCheck(context)
    all_findings = check.execute()

    if not all_findings:
        console.print(
            "[bold green]✅ No legacy tags found anywhere in the project.[/bold green]"
        )
        return 0

    # --- THIS IS THE CRITICAL AMENDMENT ---
    # Filter the findings to only include those within the 'src/' directory.
    src_findings = [
        finding
        for finding in all_findings
        if finding.file_path and finding.file_path.startswith("src/")
    ]
    # --- END OF AMENDMENT ---

    if not src_findings:
        console.print(
            f"[bold yellow]🔍 Found {len(all_findings)} total legacy tag(s) in non-code files, but none in 'src/'. No automated action taken.[/bold yellow]"
        )
        return 0

    console.print(
        f"[bold]🔍 Found {len(all_findings)} total legacy tag(s). Purging the {len(src_findings)} found in 'src/'...[/bold]"
    )

    # Group findings by file path to process one file at a time
    files_to_fix = defaultdict(list)
    for finding in src_findings:
        files_to_fix[finding.file_path].append(finding.line_number)

    total_lines_removed = 0
    for file_path_str, line_numbers_to_delete in files_to_fix.items():
        console.print(f"🔧 Processing file: [cyan]{file_path_str}[/cyan]")
        file_path = settings.REPO_PATH / file_path_str

        # Your critical insight: sort line numbers in reverse to avoid index shifting
        sorted_line_numbers = sorted(line_numbers_to_delete, reverse=True)

        if dry_run:
            for line_num in sorted_line_numbers:
                console.print(f"   -> [DRY RUN] Would delete line {line_num}")
                total_lines_removed += 1
            continue

        try:
            lines = file_path.read_text("utf-8").splitlines()
            for line_num in sorted_line_numbers:
                # Convert 1-based line number to 0-based index
                index_to_delete = line_num - 1
                if 0 <= index_to_delete < len(lines):
                    del lines[index_to_delete]
                    total_lines_removed += 1

            file_path.write_text("\n".join(lines) + "\n", "utf-8")
            console.print(f"   -> ✅ Purged {len(sorted_line_numbers)} legacy tag(s).")
        except Exception as e:
            console.print(
                f"   -> [bold red]❌ Error processing {file_path_str}: {e}[/bold red]"
            )

    return total_lines_removed

--- END OF FILE ./src/features/self_healing/purge_legacy_tags_service.py ---

--- START OF FILE ./src/services/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/services/__init__.py ---

--- START OF FILE ./src/services/adapters/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/services/adapters/__init__.py ---

--- START OF FILE ./src/services/adapters/embedding_provider.py ---
# src/shared/services/embedding_service.py
"""
EmbeddingService (quality-first, single-file)

This is now a pure, low-level client. It has no knowledge of the constitution
and receives all configuration during initialization.
"""

from __future__ import annotations

import asyncio
import os
import random
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

import requests

from shared.logger import getLogger

log = getLogger("embedding_service")


# ID: 2593a4dc-adff-4d0c-aec9-09cc2a73cf97
class EmbeddingService:
    """
    Minimal, robust client for OpenAI-compatible or Ollama-compatible embeddings endpoint.
    Keeps the interface tiny and predictable.
    """

    def __init__(
        self,
        model: str,
        base_url: str,
        api_key: Optional[str],
        expected_dim: int,
        request_timeout_sec: float = 120.0,
        connect_timeout_sec: float = 10.0,
        max_retries: int = 4,
    ) -> None:
        """Initializes the EmbeddingService with explicit configuration."""
        self.model = model
        self.expected_dim = expected_dim
        self.base_url = base_url
        self.api_key = api_key
        self.request_timeout_sec = request_timeout_sec
        self.connect_timeout_sec = connect_timeout_sec
        self.max_retries = max_retries

        self._validate_configuration()
        self._detect_api_type_and_endpoint()
        self._log_initialization_info()

        if os.getenv("PYTEST_CURRENT_TEST") is None:
            self._check_server_health()

    def _validate_configuration(self) -> None:
        """Validates that required configuration parameters are present."""
        if not self.base_url or not self.model:
            raise ValueError("base_url and model are required for EmbeddingService.")

        parsed_url = urlparse(self.base_url)
        if not parsed_url.scheme or not parsed_url.netloc:
            raise ValueError(f"Invalid base_url: {self.base_url}")

    def _detect_api_type_and_endpoint(self) -> None:
        """Detects the API type and sets the appropriate endpoint path."""
        parsed_url = urlparse(self.base_url)

        if "11434" in self.base_url or "ollama" in parsed_url.netloc.lower():
            self.api_type = "ollama_compatible"
            self.endpoint_path = "/api/embeddings"
        else:
            self.api_type = "openai"
            self.endpoint_path = "/v1/embeddings"

    def _log_initialization_info(self) -> None:
        """Logs initialization information."""
        log.info(
            "EmbeddingService: model=%s dim=%s url=%s",
            self.model,
            self.expected_dim,
            self.base_url,
        )

    def _check_server_health(self) -> None:
        """Checks if the embedding server is responsive and model is available."""
        try:
            health_endpoint = self._get_health_check_endpoint()
            response = requests.get(health_endpoint, timeout=self.connect_timeout_sec)

            if response.status_code != 200:
                self._handle_health_check_failure(response)

            if self.api_type == "ollama_compatible":
                self._validate_ollama_model_availability(response)

        except Exception as e:
            log.error(f"Failed to check embedding server health: {e}", exc_info=True)
            raise RuntimeError(f"Embedding server health check failed: {e}") from e

    def _get_health_check_endpoint(self) -> str:
        """Returns the appropriate health check endpoint based on API type."""
        if self.api_type == "ollama_compatible":
            return f"{self.base_url}/api/tags"
        else:
            return f"{self.base_url}/v1/models"

    def _handle_health_check_failure(self, response: requests.Response) -> None:
        """Handles failed health check responses."""
        log.error(
            "Embedding server health check failed: HTTP %s: %s",
            response.status_code,
            response.text[:200],
        )
        raise RuntimeError("Embedding server is not responsive")

    def _validate_ollama_model_availability(self, response: requests.Response) -> None:
        """Validates that the specified model is available on the Ollama server."""
        models = response.json().get("models", [])
        available_model_names = [model.get("name", "") for model in models]

        if self.model not in available_model_names:
            log.error(
                "Model %s not found on server. Available: %s",
                self.model,
                available_model_names,
            )
            raise RuntimeError(f"Model {self.model} not available on server")

    # ID: 8543c877-b51c-4e97-bf5a-3e97f173be48
    async def get_embedding(self, text: str) -> List[float]:
        """
        Return a single embedding vector for the given text.
        Raises:
            ValueError if empty input or wrong dimension is returned.
            RuntimeError for non-retryable HTTP failures or server issues.
        """
        text = (text or "").strip()
        if not text:
            raise ValueError("EmbeddingService.get_embedding: empty text")

        payload = self._build_request_payload(text)
        headers = self._build_headers()
        response_data = await self._post_with_retries(json=payload, headers=headers)

        embedding = self._extract_embedding_from_response(response_data)
        self._validate_embedding_dimensions(embedding)

        return embedding

    def _build_request_payload(self, text: str) -> Dict[str, str]:
        """Builds the request payload based on API type."""
        if self.api_type == "ollama_compatible":
            return {"model": self.model, "prompt": text}
        else:
            return {"model": self.model, "input": text}

    def _build_headers(self) -> Dict[str, str]:
        """Builds request headers, including Authorization if an API key is present."""
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        return headers

    def _extract_embedding_from_response(
        self, response_data: Dict[str, Any]
    ) -> List[float]:
        """Extracts the embedding vector from the API response."""
        try:
            embedding = response_data.get("embedding") or response_data.get(
                "data",
                [{}],
            )[0].get("embedding", [])
        except Exception as e:
            raise RuntimeError(f"EmbeddingService: invalid response format: {e}") from e

        if not isinstance(embedding, list) or not embedding:
            raise RuntimeError("EmbeddingService: empty embedding returned")

        return embedding

    def _validate_embedding_dimensions(self, embedding: List[float]) -> None:
        """Validates that the embedding has the expected dimensions."""
        if len(embedding) != self.expected_dim:
            raise ValueError(
                f"Unexpected embedding dimension {len(embedding)} != "
                f"expected {self.expected_dim}"
            )

    async def _post_with_retries(
        self, *, json: Dict[str, Any], headers: Dict[str, str]
    ) -> Dict[str, Any]:
        """
        Execute POST in a thread (to keep async),
        with exponential backoff and jitter for transient errors.
        """
        attempt = 0
        last_error: Optional[Exception] = None
        backoff_base_sec = 0.6
        endpoint_url = f"{self.base_url.rstrip('/')}{self.endpoint_path}"

        while attempt <= self.max_retries:
            try:
                response = await self._execute_http_request(endpoint_url, headers, json)
                self._validate_http_response(response)
                return response.json()

            except Exception as e:
                last_error = e
                attempt += 1

                if self._should_stop_retrying(e, attempt):
                    break

                await self._wait_before_retry(
                    attempt,
                    endpoint_url,
                    e,
                    backoff_base_sec,
                )

        raise RuntimeError(
            f"EmbeddingService: request to {endpoint_url} failed after "
            f"{self.max_retries} retries: {last_error}"
        ) from last_error

    async def _execute_http_request(
        self,
        endpoint_url: str,
        headers: Dict[str, str],
        json_data: Dict[str, Any],
    ) -> requests.Response:
        """Executes the HTTP request in a thread."""
        return await asyncio.to_thread(
            requests.post,
            endpoint_url,
            headers=headers,
            json=json_data,
            timeout=(self.connect_timeout_sec, self.request_timeout_sec),
        )

    def _validate_http_response(self, response: requests.Response) -> None:
        """Validates HTTP response status codes and raises appropriate errors."""
        status_code = response.status_code
        response_text = response.text[:200]

        if status_code in (408, 429, 500, 502, 503, 504):
            raise RuntimeError(f"Transient HTTP {status_code}: {response_text}")
        if status_code == 400:
            raise RuntimeError(f"Bad request: {response_text}")
        if status_code == 401:
            raise RuntimeError(f"Unauthorized: {response_text}")
        if status_code < 200 or status_code >= 300:
            raise RuntimeError(f"HTTP {status_code}: {response_text}")

    def _should_stop_retrying(self, error: Exception, attempt: int) -> bool:
        """Determines whether to stop retrying based on the error and attempt count."""
        if attempt > self.max_retries:
            return True
        if isinstance(error, RuntimeError) and "Transient" not in str(error):
            return True
        return False

    async def _wait_before_retry(
        self, attempt: int, endpoint_url: str, error: Exception, backoff_base_sec: float
    ) -> None:
        """Waits before retrying with exponential backoff and jitter."""
        backoff_time = backoff_base_sec * (2 ** (attempt - 1)) + random.uniform(0, 0.1)

        log.warning(
            "Embedding POST to %s failed (attempt %s/%s): %s; retrying in %.1fs",
            endpoint_url,
            attempt,
            self.max_retries,
            error,
            backoff_time,
        )

        await asyncio.sleep(backoff_time)

--- END OF FILE ./src/services/adapters/embedding_provider.py ---

--- START OF FILE ./src/services/clients/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/services/clients/__init__.py ---

--- START OF FILE ./src/services/clients/llm_api_client.py ---
# src/services/clients/llm_api_client.py
"""
Provides a base client for asynchronous and synchronous communication with
Chat Completions and Embedding APIs for LLM interactions.
"""

from __future__ import annotations

import asyncio
import random
import time
from typing import Any, List

import httpx
from shared.config import settings
from shared.logger import getLogger

log = getLogger(__name__)


# ID: ccbed73e-3e71-4ede-ac2a-3069ee9abc0f
class BaseLLMClient:
    """
    Base class for LLM clients, handling common request logic for Chat and Embedding APIs.
    """

    def __init__(self, api_url: str, model_name: str, api_key: str | None = None):
        """Initializes the LLM client with API credentials and endpoint."""
        if not api_url or not model_name:
            raise ValueError(
                f"{self.__class__.__name__} requires both API_URL and MODEL_NAME."
            )

        self.base_url = api_url.rstrip("/")
        self.api_key = api_key
        self.model_name = model_name
        self.api_type = self._determine_api_type(self.base_url)
        self.headers = self._get_headers()

        try:
            connect_timeout = int(settings.model_extra.get("LLM_CONNECT_TIMEOUT", 10))
            request_timeout = int(settings.model_extra.get("LLM_REQUEST_TIMEOUT", 180))
        except (ValueError, TypeError):
            connect_timeout = 10
            request_timeout = 180

        self.timeout_config = httpx.Timeout(
            connect=connect_timeout, read=request_timeout, write=30.0, pool=None
        )
        self.async_client = httpx.AsyncClient(timeout=self.timeout_config, http2=True)
        self.sync_client = httpx.Client(timeout=self.timeout_config, http2=True)

    def _determine_api_type(self, base_url: str) -> str:
        """Determines the API type based on the URL."""
        if "anthropic" in base_url:
            return "anthropic"
        if "localhost" in base_url or "127.0.0.1" in base_url or "192.168" in base_url:
            return "ollama_compatible"
        return "openai"

    def _get_headers(self) -> dict:
        """Determines the correct headers based on the API type."""
        if self.api_type == "anthropic":
            if not self.api_key:
                raise ValueError("Anthropic API requires an API key.")
            return {
                "x-api-key": self.api_key,
                "anthropic-version": "2023-06-01",
                "Content-Type": "application/json",
            }
        elif self.api_type == "openai":
            headers = {"Content-Type": "application/json"}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"
            return headers
        return {"Content-Type": "application/json"}

    def _get_api_url(self, task_type: str) -> str:
        """Gets the correct API endpoint URL based on the task type."""
        if task_type == "embedding":
            if self.api_type == "ollama_compatible":
                return f"{self.base_url}/api/embeddings"
            return f"{self.base_url}/v1/embeddings"
        if self.api_type == "anthropic":
            return f"{self.base_url}/v1/messages"
        return f"{self.base_url}/v1/chat/completions"

    def _prepare_payload(self, prompt: str, user_id: str, task_type: str) -> dict:
        """Prepares the request payload based on the API and task type."""
        if task_type == "embedding":
            if self.api_type == "ollama_compatible":
                return {"model": self.model_name, "prompt": prompt}
            return {"model": self.model_name, "input": [prompt]}
        if self.api_type == "anthropic":
            return {
                "model": self.model_name,
                "max_tokens": 4096,
                "messages": [{"role": "user", "content": prompt}],
            }
        else:
            return {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "user": user_id,
            }

    def _parse_response(self, response_data: dict, task_type: str) -> Any:
        """Parses the response to extract the content based on API and task type."""
        try:
            if task_type == "embedding":
                embedding = response_data.get("embedding") or response_data.get(
                    "data", [{}]
                )[0].get("embedding", [])
                if not embedding:
                    raise ValueError("Invalid embedding format in API response.")
                return embedding
            if self.api_type == "anthropic":
                return response_data.get("content", [{}])[0].get("text", "")
            else:
                return response_data["choices"][0]["message"]["content"]
        except (KeyError, IndexError, ValueError) as e:
            log.error(
                f"Could not parse response for task '{task_type}': {response_data}"
            )
            raise ValueError(f"Invalid API response structure: {e}") from e

    # ID: aded16ca-2a27-4690-a69a-7c5aec0153e9
    async def make_request_async(
        self, prompt: str, user_id: str = "core_system", task_type: str = "chat"
    ) -> Any:
        api_url = self._get_api_url(task_type)
        payload = self._prepare_payload(prompt, user_id, task_type)
        backoff_delays = [1.0, 2.0, 4.0]

        for attempt in range(len(backoff_delays) + 1):
            try:
                response = await self.async_client.post(
                    api_url, headers=self.headers, json=payload
                )
                response.raise_for_status()
                return self._parse_response(response.json(), task_type)
            except Exception as e:
                error_message = f"Request failed (attempt {attempt + 1}/{len(backoff_delays) + 1}) for {api_url}: {type(e).__name__} - {e}"
                if attempt < len(backoff_delays):
                    wait_time = backoff_delays[attempt] + random.uniform(0, 0.5)
                    log.warning(f"{error_message}. Retrying in {wait_time:.1f}s...")
                    await asyncio.sleep(wait_time)
                    continue
                log.error(f"Final attempt failed: {error_message}", exc_info=True)
                raise

    # ID: 6f1354ee-09ee-49d1-8eeb-a4fcc7c1bc58
    async def get_embedding(self, text: str) -> List[float]:
        return await self.make_request_async(
            prompt=text, user_id="embedding_service", task_type="embedding"
        )

    # ID: cfe08d4d-f3d5-475f-87ab-849846e97886
    def make_request_sync(
        self, prompt: str, user_id: str = "core_system", task_type: str = "chat"
    ) -> Any:
        api_url = self._get_api_url(task_type)
        payload = self._prepare_payload(prompt, user_id, task_type)
        backoff_delays = [1.0, 2.0, 4.0]

        for attempt in range(len(backoff_delays) + 1):
            try:
                response = self.sync_client.post(
                    api_url, headers=self.headers, json=payload
                )
                response.raise_for_status()
                return self._parse_response(response.json(), task_type)
            except Exception as e:
                # --- THIS IS THE FIX: ADD DETAILED LOGGING ---
                error_message = f"Sync request failed (attempt {attempt + 1}/{len(backoff_delays) + 1}) for {api_url}: {type(e).__name__} - {e}"
                if isinstance(e, httpx.HTTPStatusError):
                    error_message += f"\nResponse body: {e.response.text}"
                # --- END OF FIX ---
                if attempt < len(backoff_delays):
                    wait_time = backoff_delays[attempt] + random.uniform(0, 0.5)
                    log.warning(f"{error_message}. Retrying in {wait_time:.1f}s...")
                    time.sleep(wait_time)
                    continue
                log.error(f"Final sync attempt failed: {error_message}", exc_info=True)
                raise

--- END OF FILE ./src/services/clients/llm_api_client.py ---

--- START OF FILE ./src/services/clients/qdrant_client.py ---
# src/services/clients/qdrant_client.py
"""
QdrantService (quality-first, single-file)

This service now enforces the EmbeddingPayload schema for all upserts,
ensuring every vector is stored with complete, traceable provenance.
"""

from __future__ import annotations

import uuid
from typing import Any, List, Optional, Sequence

from shared.config import settings
from shared.models import EmbeddingPayload
from shared.time import now_iso as _now_iso

try:
    from qdrant_client import AsyncQdrantClient
    from qdrant_client.http import models as qm
except Exception as e:
    raise RuntimeError(
        "qdrant-client is required. Install with: pip install qdrant-client"
    ) from e

try:
    from shared.logger import getLogger

    log = getLogger("qdrant_service")
except Exception:
    import logging

    logging.basicConfig(level=logging.INFO)
    log = logging.getLogger("qdrant_service")


def _uuid5_from_text(text: str) -> str:
    """
    Deterministic UUID from text (stable across runs).
    Uses UUID5 with URL namespace to avoid collisions.
    """
    return str(uuid.uuid5(uuid.NAMESPACE_URL, text))


# ID: 53349105-1b11-4917-9e24-ce9dc6f9a128
class QdrantService:
    """Handles all interactions with the Qdrant vector database."""

    def __init__(
        self,
        url: Optional[str] = None,
        api_key: Optional[str] = None,
        collection_name: Optional[str] = None,
        vector_size: Optional[int] = None,
    ) -> None:
        """Initializes the Qdrant client from constitutional settings."""
        self.url = url or settings.QDRANT_URL
        self.api_key = (
            api_key
            if api_key is not None
            else settings.model_extra.get("QDRANT_API_KEY")
        )
        self.collection_name = collection_name or settings.QDRANT_COLLECTION_NAME
        self.vector_size = int(vector_size or settings.LOCAL_EMBEDDING_DIM)

        # Optional support for named vectors if your collection is later migrated.
        # If set, we'll prefer this key inside record.vectors.
        self.vector_name: Optional[str] = settings.model_extra.get("QDRANT_VECTOR_NAME")

        if not self.url:
            raise ValueError("QDRANT_URL is not configured.")

        self.client = AsyncQdrantClient(
            url=self.url,
            api_key=self.api_key or None,
        )

        log.info(
            "QdrantService: url=%s collection=%s dim=%s",
            self.url,
            self.collection_name,
            self.vector_size,
        )

    # ID: 299a4be1-32fe-4c3f-aad4-f8d15065111e
    async def ensure_collection(self) -> None:
        """Idempotently create the collection if it is missing."""
        try:
            collections_response = await self.client.get_collections()
            existing_collections = [c.name for c in collections_response.collections]
            if self.collection_name in existing_collections:
                return

            log.info(
                "Creating Qdrant collection '%s' (dim=%s, distance=cosine).",
                self.collection_name,
                self.vector_size,
            )

            await self.client.recreate_collection(
                collection_name=self.collection_name,
                vectors_config=qm.VectorParams(
                    size=self.vector_size, distance=qm.Distance.COSINE
                ),
                on_disk_payload=True,
            )
        except Exception as e:
            log.error(f"Failed to ensure Qdrant collection exists: {e}", exc_info=True)
            raise

    # ID: 1aa3971e-527b-481a-8029-c8ad01b5e670
    async def upsert_capability_vector(
        self,
        point_id_str: str,
        vector: List[float],
        payload_data: dict,
    ) -> str:
        """
        Validates the payload against the EmbeddingPayload schema and upserts the vector.
        Uses the provided point ID. Returns the point ID.
        """
        if len(vector) != self.vector_size:
            raise ValueError(f"Vector dim {len(vector)} != expected {self.vector_size}")

        try:
            payload_data["model"] = settings.LOCAL_EMBEDDING_MODEL_NAME
            payload_data["model_rev"] = settings.EMBED_MODEL_REVISION
            payload_data["dim"] = self.vector_size
            payload_data["created_at"] = _now_iso()
            payload = EmbeddingPayload(**payload_data)
        except Exception as e:
            log.error(f"Invalid embedding payload: {e}")
            raise ValueError(f"Invalid embedding payload: {e}") from e

        pid = point_id_str

        await self.client.upsert(
            collection_name=self.collection_name,
            points=[
                qm.PointStruct(
                    id=pid,
                    vector=vector,
                    payload=payload.model_dump(mode="json"),
                )
            ],
            wait=True,
        )

        log.debug(f"Upserted vector for chunk '{payload.chunk_id}' with ID: {pid}")
        return pid

    # ID: 400e23e3-0911-4419-86be-9b06ba5b3fb5
    async def get_all_vectors(self) -> List[qm.Record]:
        """Fetches all points with their vectors and payloads from the collection."""
        try:
            records, _ = await self.client.scroll(
                collection_name=self.collection_name,
                limit=10000,
                with_payload=True,
                with_vectors=True,
            )
            return records
        except Exception as e:
            log.error(f"❌ Failed to retrieve all vectors from Qdrant: {e}")
            return []

    # ID: 19e184b6-3b4e-483f-902d-c8ac35d3e8d4 (updated)
    async def get_vector_by_id(self, point_id: str) -> Optional[List[float]]:
        """
        Retrieves a single vector by its point ID.
        """
        try:
            records = await self.client.retrieve(
                collection_name=self.collection_name,
                ids=[str(point_id)],  # ensure it's a string
                with_vectors=True,
                with_payload=False,
            )
        # --- THIS IS THE DIAGNOSTIC FIX ---
        except Exception as e:
            # This new, more specific logging will tell us the real error.
            log.warning(
                "Could not retrieve vector for point ID %s (collection=%s): An unexpected exception occurred during the API call. Type: %s, Error: %s",
                point_id,
                self.collection_name,
                type(e).__name__,
                e,
            )
            return None
        # --- END OF FIX ---

        if not records:
            log.warning(
                "Could not retrieve vector for point ID %s (collection=%s): point not found",
                point_id,
                self.collection_name,
            )
            return None

        rec = records[0]

        # Case 1: classic single vector
        vec = getattr(rec, "vector", None)
        if isinstance(vec, (list, tuple)):
            return list(map(float, vec))

        # Case 2: newer clients / named vectors
        vectors_obj = getattr(rec, "vectors", None)
        if isinstance(vectors_obj, dict):
            # Prefer configured name if provided
            if self.vector_name and self.vector_name in vectors_obj:
                chosen = vectors_obj[self.vector_name]
                if isinstance(chosen, (list, tuple)):
                    return list(map(float, chosen))

            # Fallback: pick first key deterministically
            if vectors_obj:
                first_key = sorted(vectors_obj.keys())[0]
                chosen = vectors_obj[first_key]
                if isinstance(chosen, (list, tuple)):
                    log.debug(
                        "Using named vector '%s' for point %s (collection=%s) "
                        "because QDRANT_VECTOR_NAME is not set.",
                        first_key,
                        point_id,
                        self.collection_name,
                    )
                    return list(map(float, chosen))

            log.warning(
                "Could not retrieve vector for point ID %s (collection=%s): vectors dict present but empty or invalid. keys=%s",
                point_id,
                self.collection_name,
                list(vectors_obj.keys()) if isinstance(vectors_obj, dict) else None,
            )
            return None

        log.warning(
            "Could not retrieve vector for point ID %s (collection=%s): "
            "no usable 'vector' or 'vectors' on record. attrs(vector=%s, vectors_type=%s)",
            point_id,
            self.collection_name,
            type(getattr(rec, "vector", None)).__name__,
            type(getattr(rec, "vectors", None)).__name__,
        )
        return None

    # (unchanged) simple search helper
    # ID: b969f68c-ab5b-473b-8a9c-c53cffd38199
    async def search_similar(
        self,
        query_vector: Sequence[float],
        limit: int = 5,
        with_payload: bool = True,
        filter_: Optional[qm.Filter] = None,
    ) -> list[dict[str, Any]]:
        """
        Simple nearest-neighbor search.
        """
        search_result = await self.client.search(
            collection_name=self.collection_name,
            query_vector=list(map(float, query_vector)),
            limit=limit,
            with_payload=with_payload,
            query_filter=filter_,
        )
        return [{"score": hit.score, "payload": hit.payload} for hit in search_result]

--- END OF FILE ./src/services/clients/qdrant_client.py ---

--- START OF FILE ./src/services/config_service.py ---
# src/services/config_service.py
"""
Provides a centralized service for accessing runtime configuration.

This service is the single source of truth for configuration for the running application.
It prioritizes values from the database and falls back to the environment (.env file)
if a value is not found in the database.

ENHANCED: Now supports encrypted secrets via SecretsService.
"""

from __future__ import annotations

import asyncio
import os
from typing import Any, Dict, Optional

from cryptography.fernet import Fernet
from shared.config import settings
from shared.logger import getLogger
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from services.database.session_manager import get_session

log = getLogger("config_service")


# ID: secrets-manager-001
class SecretsManager:
    """
    Handles encryption/decryption of secrets.

    This is a lightweight version integrated into ConfigurationService.
    """

    def __init__(self):
        self._cipher: Optional[Fernet] = None
        self._initialized = False

    def _ensure_initialized(self):
        """Lazy-load the cipher when first needed."""
        if self._initialized:
            return

        master_key = os.getenv("CORE_MASTER_KEY")
        if not master_key:
            log.warning(
                "CORE_MASTER_KEY not found in environment. "
                "Encrypted secrets will not be available."
            )
            self._cipher = None
        else:
            try:
                self._cipher = Fernet(master_key.encode())
                log.info("SecretsManager initialized with master key")
            except Exception as e:
                log.error(f"Failed to initialize secrets cipher: {e}")
                self._cipher = None

        self._initialized = True

    def decrypt(self, ciphertext: str) -> str:
        """Decrypt a secret value."""
        self._ensure_initialized()

        if not self._cipher:
            raise RuntimeError(
                "SecretsManager not initialized. Set CORE_MASTER_KEY in .env"
            )

        try:
            return self._cipher.decrypt(ciphertext.encode()).decode()
        except Exception as e:
            raise ValueError(f"Decryption failed: {e}")

    def encrypt(self, plaintext: str) -> str:
        """Encrypt a secret value."""
        self._ensure_initialized()

        if not self._cipher:
            raise RuntimeError(
                "SecretsManager not initialized. Set CORE_MASTER_KEY in .env"
            )

        return self._cipher.encrypt(plaintext.encode()).decode()


# ID: 266a4a72-e1e5-4c7a-911f-ede92726c323
class ConfigurationService:
    """
    A service that provides configuration from the DB with a .env fallback.

    ENHANCED FEATURES:
    - Supports encrypted secrets via get_secret()
    - Cache invalidation via reload()
    - Audit logging for secret access
    - Better error handling
    """

    def __init__(self):
        self._cache: Dict[str, Any] = {}
        self._initialized: bool = False
        self._lock = asyncio.Lock()
        self._secrets_manager = SecretsManager()

    async def _load_settings_from_db(self):
        """Loads all non-secret settings from the database into the cache."""
        async with self._lock:
            if self._initialized:
                return

            log.info(
                "Initializing ConfigurationService: loading settings from database..."
            )
            try:
                async with get_session() as session:
                    stmt = text(
                        "SELECT key, value FROM core.runtime_settings WHERE is_secret = FALSE"
                    )
                    result = await session.execute(stmt)
                    self._cache.clear()  # Clear cache before reload
                    for row in result:
                        self._cache[row.key] = row.value

                self._initialized = True
                log.info(
                    f"Loaded {len(self._cache)} configuration settings from the database."
                )
            except Exception as e:
                log.error(
                    f"Failed to load configuration from database: {e}. "
                    f"Will rely on .env fallback."
                )
                # DON'T mark as initialized on failure - allow retry
                # But wait a bit before next attempt to avoid spam
                await asyncio.sleep(5)

    # ID: 4c588ad7-2835-475c-8b69-0d122aebedcb
    async def get(self, key: str, default: Any = None) -> Any:
        """
        Gets a configuration value.

        It checks the database-backed cache first, then falls back to the
        environment-loaded settings object, and finally returns the default.

        Args:
            key: Configuration key
            default: Default value if not found

        Returns:
            Configuration value
        """
        if not self._initialized:
            await self._load_settings_from_db()

        # 1. Try to get from the database cache
        value = self._cache.get(key)
        if value is not None:
            return value

        # 2. Fallback: try to get from the .env-backed settings object
        value = getattr(settings, key, None)
        if value is not None:
            return value

        # 3. Return the default
        return default

    async def get_int(self, key: str, default: Optional[int] = None) -> Optional[int]:
        """Get configuration value as integer."""
        value = await self.get(key, default=default)
        if value is None:
            return None
        return int(value)

    async def get_float(self, key: str, default: Optional[float] = None) -> Optional[float]:
        """Get configuration value as float."""
        value = await self.get(key, default=default)
        if value is None:
            return None
        return float(value)

    async def get_bool(self, key: str, default: bool = False) -> bool:
        """Get configuration value as boolean."""
        value = await self.get(key, default=default)
        if value is None:
            return default
        if isinstance(value, bool):
            return value
        return str(value).lower() in ("true", "1", "yes", "on")

    # ID: config-service-secrets-001
    async def get_secret(
        self,
        key: str,
        audit_context: Optional[str] = None,
    ) -> str:
        """
        Get an encrypted secret from the database.

        Args:
            key: Secret key (e.g., "anthropic.api_key")
            audit_context: Context for audit log (e.g., "planner_agent")

        Returns:
            Decrypted secret value

        Raises:
            KeyError: If secret not found
            RuntimeError: If CORE_MASTER_KEY not set

        Example:
            api_key = await config_service.get_secret("anthropic.api_key", "planner")
        """
        try:
            async with get_session() as session:
                stmt = text(
                    "SELECT value FROM core.runtime_settings "
                    "WHERE key = :key AND is_secret = TRUE"
                )
                result = await session.execute(stmt, {"key": key})
                row = result.fetchone()

                if not row:
                    raise KeyError(f"Secret '{key}' not found in database")

                # Decrypt the value
                decrypted_value = self._secrets_manager.decrypt(row.value)

                # Audit the access
                await self._audit_secret_access(session, key, audit_context)

                return decrypted_value

        except KeyError:
            raise
        except Exception as e:
            log.error(f"Failed to retrieve secret '{key}': {e}")
            raise

    async def set_secret(
        self,
        key: str,
        value: str,
        description: Optional[str] = None,
    ) -> None:
        """
        Store an encrypted secret in the database.

        Args:
            key: Secret key
            value: Plaintext secret value (will be encrypted)
            description: Optional description
        """
        encrypted_value = self._secrets_manager.encrypt(value)

        async with get_session() as session:
            stmt = text("""
                INSERT INTO core.runtime_settings (key, value, description, is_secret, last_updated)
                VALUES (:key, :value, :description, TRUE, NOW())
                ON CONFLICT (key)
                DO UPDATE SET
                    value = EXCLUDED.value,
                    description = EXCLUDED.description,
                    last_updated = NOW()
            """)

            await session.execute(
                stmt,
                {
                    "key": key,
                    "value": encrypted_value,
                    "description": description or f"Encrypted secret: {key}",
                },
            )
            await session.commit()

        log.info(f"Secret '{key}' stored successfully (encrypted)")

    async def _audit_secret_access(
        self,
        session: AsyncSession,
        key: str,
        context: Optional[str],
    ) -> None:
        """Log secret access for audit trail."""
        try:
            stmt = text("""
                INSERT INTO core.agent_memory (
                    cognitive_role,
                    memory_type,
                    content,
                    relevance_score,
                    created_at
                ) VALUES (
                    :role,
                    'fact',
                    :content,
                    1.0,
                    NOW()
                )
            """)

            await session.execute(
                stmt,
                {
                    "role": context or "system",
                    "content": f"Accessed secret: {key}",
                },
            )
            # Let the caller handle commit
        except Exception as e:
            # Don't fail secret retrieval if audit fails
            log.error(f"Failed to audit secret access: {e}")

    async def reload(self) -> None:
        """
        Force reload of configuration from database.

        Use this if config changes in DB and you need to pick up changes
        without restarting the application.
        """
        async with self._lock:
            log.info("Reloading configuration from database...")
            self._initialized = False
            await self._load_settings_from_db()

    async def set(
        self,
        key: str,
        value: str,
        description: Optional[str] = None,
    ) -> None:
        """
        Set a non-secret configuration value in the database.

        Note: This bypasses constitutional governance. Use with caution!
        For production changes, use the governance workflow.
        """
        async with get_session() as session:
            stmt = text("""
                INSERT INTO core.runtime_settings (key, value, description, is_secret, last_updated)
                VALUES (:key, :value, :description, FALSE, NOW())
                ON CONFLICT (key)
                DO UPDATE SET
                    value = EXCLUDED.value,
                    description = COALESCE(EXCLUDED.description, core.runtime_settings.description),
                    last_updated = NOW()
            """)

            await session.execute(
                stmt,
                {"key": key, "value": value, "description": description},
            )
            await session.commit()

        # Update cache
        self._cache[key] = value
        log.info(f"Config '{key}' set to '{value}'")


# Create a single, global instance for easy access
config_service = ConfigurationService()


# ID: e9bf3e46-3eba-4e6d-88bc-40eda491a2bd
def get_config_service() -> ConfigurationService:
    """Factory function to get the global instance of the ConfigurationService."""
    return config_service


# ID: config-service-llm-helper-001
class LLMResourceConfig:
    """
    Convenience helper for accessing LLM resource configuration.

    Usage:
        llm_config = LLMResourceConfig(config_service, "anthropic")
        api_key = await llm_config.get_api_key()
        model = await llm_config.get_model_name()
        max_concurrent = await llm_config.get_max_concurrent()
    """

    def __init__(self, config_service: ConfigurationService, resource_name: str):
        self.config = config_service
        self.resource_name = resource_name
        # Normalize: anthropic_claude_sonnet -> anthropic
        self._prefix = resource_name.lower().replace("-", "_").split("_")[0]

    async def get_api_key(self, audit_context: Optional[str] = None) -> str:
        """Get API key for this resource (decrypted)."""
        key = f"{self._prefix}.api_key"
        return await self.config.get_secret(key, audit_context=audit_context)

    async def get_model_name(self) -> str:
        """Get model name for this resource."""
        # Try specific key first, fall back to resource_name key
        key = f"{self._prefix}.model_name"
        value = await self.config.get(key)
        if not value:
            # Fallback: try with full resource name
            key = f"{self.resource_name}.model_name"
            value = await self.config.get(key)
        return value or "unknown"

    async def get_max_concurrent(self) -> int:
        """Get max concurrent requests for this resource."""
        key = f"{self._prefix}.max_concurrent"
        value = await self.config.get(key)
        if not value:
            value = await self.config.get("llm.default_max_concurrent", default="2")
        return int(value)

    async def get_rate_limit(self) -> float:
        """Get rate limit (seconds between requests)."""
        key = f"{self._prefix}.rate_limit"
        value = await self.config.get(key)
        if not value:
            value = await self.config.get("llm.default_rate_limit", default="2.0")
        return float(value)

    async def get_timeout(self) -> int:
        """Get request timeout in seconds."""
        key = f"{self._prefix}.timeout"
        value = await self.config.get(key)
        if not value:
            value = await self.config.get("llm.default_timeout", default="300")
        return int(value)
--- END OF FILE ./src/services/config_service.py ---

--- START OF FILE ./src/services/database/models.py ---
# src/services/database/models.py
"""
SQLAlchemy ORM models for CORE's v2.1 operational database schema.
This file is the Python representation of the database's structure and is
aligned with the v2.1 SQL schema.
"""

from __future__ import annotations

from sqlalchemy import (
    JSON,
    BigInteger,
    Boolean,
    Column,
    DateTime,
    ForeignKey,
    Integer,
    Numeric,
    Text,
    func,
)
from sqlalchemy.dialects.postgresql import UUID as pgUUID
from sqlalchemy.orm import declarative_base

Base = declarative_base()


# =============================================================================
# SECTION 1: KNOWLEDGE LAYER
# =============================================================================


# ID: 2cc67cc1-1eed-4bde-a5d0-718898c13e7d
class Symbol(Base):
    __tablename__ = "symbols"
    __table_args__ = {"schema": "core"}
    id = Column(
        pgUUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid()
    )
    symbol_path = Column(Text, nullable=False, unique=True)
    module = Column(Text, nullable=False)
    qualname = Column(Text, nullable=False)
    kind = Column(Text, nullable=False)
    ast_signature = Column(Text, nullable=False)
    fingerprint = Column(Text, nullable=False)
    state = Column(Text, nullable=False, server_default="discovered")
    health_status = Column(Text, server_default="unknown")
    is_public = Column(Boolean, nullable=False, server_default="true")
    previous_paths = Column(JSON)  # Using JSON for text[]
    key = Column(Text)
    intent = Column(Text)
    embedding_model = Column(Text, server_default="text-embedding-3-small")
    last_embedded = Column(DateTime(timezone=True))
    first_seen = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )
    last_seen = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )
    last_modified = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )
    updated_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )


# ID: 4932bee2-69e1-4849-aa68-b8f99493da6f
class Capability(Base):
    __tablename__ = "capabilities"
    __table_args__ = {"schema": "core"}
    id = Column(
        pgUUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid()
    )
    name = Column(Text, nullable=False)
    domain = Column(Text, nullable=False, server_default="general")
    title = Column(Text, nullable=False)
    objective = Column(Text)
    owner = Column(Text, nullable=False)
    entry_points = Column(JSON, server_default="[]")
    dependencies = Column(JSON, server_default="[]")
    test_coverage = Column(Numeric(5, 2))
    tags = Column(JSON, nullable=False, server_default="[]")
    status = Column(Text, server_default="Active")
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )
    updated_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )


# ID: b1b4c985-b551-41f6-8f54-805c9baea7ef
class SymbolCapabilityLink(Base):
    __tablename__ = "symbol_capability_links"
    __table_args__ = {"schema": "core"}
    symbol_id = Column(
        pgUUID(as_uuid=True), ForeignKey("core.symbols.id"), primary_key=True
    )
    capability_id = Column(
        pgUUID(as_uuid=True), ForeignKey("core.capabilities.id"), primary_key=True
    )
    source = Column(Text, primary_key=True)
    confidence = Column(Numeric, nullable=False)
    verified = Column(Boolean, nullable=False, server_default="false")
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )


# ID: 7f6f266f-0134-4f49-96f0-74221fffb235
class Domain(Base):
    __tablename__ = "domains"
    __table_args__ = {"schema": "core"}
    key = Column(Text, primary_key=True)
    title = Column(Text, nullable=False)
    description = Column(Text)
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )


# =============================================================================
# SECTION 2: GOVERNANCE LAYER (Constitutional compliance)
# =============================================================================


# ID: 9467fb58-3f84-472a-9f59-ef5444bcebb9
class Proposal(Base):
    __tablename__ = "proposals"
    __table_args__ = {"schema": "core"}

    id = Column(BigInteger, primary_key=True)
    target_path = Column(Text, nullable=False)
    content_sha256 = Column(Text, nullable=False)
    justification = Column(Text, nullable=False)
    risk_tier = Column(Text, server_default="low")
    is_critical = Column(Boolean, nullable=False, server_default="false")
    status = Column(Text, nullable=False, server_default="open")
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )
    created_by = Column(Text, nullable=False)


# ID: e8f6ca99-95a9-4097-b133-f1d9103dcc57
class ProposalSignature(Base):
    __tablename__ = "proposal_signatures"
    __table_args__ = {"schema": "core"}

    proposal_id = Column(BigInteger, ForeignKey("core.proposals.id"), primary_key=True)
    approver_identity = Column(Text, primary_key=True)
    signature_base64 = Column(Text, nullable=False)
    signed_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )
    is_valid = Column(Boolean, nullable=False, server_default="true")


# =============================================================================
# SECTION 3: OPERATIONAL LAYER
# =============================================================================


# ID: b00a4201-294c-4fbf-8ae5-d6a5e42d7db7
class LlmResource(Base):
    __tablename__ = "llm_resources"
    __table_args__ = {"schema": "core"}
    name = Column(Text, primary_key=True)
    env_prefix = Column(Text, nullable=False, unique=True)
    provided_capabilities = Column(JSON, server_default="[]")
    performance_metadata = Column(JSON)
    is_available = Column(Boolean, server_default="true")
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )


# ID: 869fa796-6848-4e73-aa02-e9f1f16dd2b2
class CognitiveRole(Base):
    __tablename__ = "cognitive_roles"
    __table_args__ = {"schema": "core"}
    role = Column(Text, primary_key=True)
    description = Column(Text)
    assigned_resource = Column(Text, ForeignKey("core.llm_resources.name"))
    required_capabilities = Column(JSON, server_default="[]")
    max_concurrent_tasks = Column(Integer, server_default="1")
    specialization = Column(JSON)
    is_active = Column(Boolean, server_default="true")
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )


# ID: 30c36fed-1175-4352-b585-4822f68f9eb9
class Task(Base):
    __tablename__ = "tasks"
    __table_args__ = {"schema": "core"}
    id = Column(
        pgUUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid()
    )
    intent = Column(Text, nullable=False)
    assigned_role = Column(Text, ForeignKey("core.cognitive_roles.role"))
    parent_task_id = Column(pgUUID(as_uuid=True), ForeignKey("core.tasks.id"))
    status = Column(Text, nullable=False, server_default="pending")
    plan = Column(JSON)
    context = Column(JSON, server_default="{}")
    error_message = Column(Text)
    failure_reason = Column(Text)
    relevant_symbols = Column(JSON)
    context_retrieval_query = Column(Text)
    context_retrieved_at = Column(DateTime(timezone=True))
    context_tokens_used = Column(Integer)
    requires_approval = Column(Boolean, server_default="false")
    proposal_id = Column(BigInteger, ForeignKey("core.proposals.id"))
    estimated_complexity = Column(Integer)
    actual_duration_seconds = Column(Integer)
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )
    started_at = Column(DateTime(timezone=True))
    completed_at = Column(DateTime(timezone=True))


# ID: 0b4c145d-c20d-4a6e-ad90-0dd172ab9d1e
class Action(Base):
    __tablename__ = "actions"
    __table_args__ = {"schema": "core"}
    id = Column(
        pgUUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid()
    )
    task_id = Column(pgUUID(as_uuid=True), ForeignKey("core.tasks.id"), nullable=False)
    action_type = Column(Text, nullable=False)
    target = Column(Text)
    payload = Column(JSON)
    result = Column(JSON)
    success = Column(Boolean, nullable=False)
    cognitive_role = Column(Text, nullable=False)
    reasoning = Column(Text)
    duration_ms = Column(Integer)
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )


# =============================================================================
# SECTION 4: VECTOR INTEGRATION LAYER
# =============================================================================


# ID: 200ed659-c322-4c49-b098-0ecc877d0276
class SymbolVectorLink(Base):
    __tablename__ = "symbol_vector_links"
    __table_args__ = {"schema": "core"}
    symbol_id = Column(
        pgUUID(as_uuid=True), ForeignKey("core.symbols.id"), primary_key=True
    )
    vector_id = Column(Text, nullable=False)
    embedding_model = Column(Text, nullable=False)
    embedding_version = Column(Integer, nullable=False)
    created_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )


# =============================================================================
# SECTION 6: SYSTEM METADATA
# =============================================================================


# ID: 7cb97df4-dc18-49e3-8ef1-6dcc752efebd
class CliCommand(Base):
    __tablename__ = "cli_commands"
    __table_args__ = {"schema": "core"}
    name = Column(Text, primary_key=True)
    module = Column(Text, nullable=False)
    entrypoint = Column(Text, nullable=False)
    summary = Column(Text)
    category = Column(Text)


# ID: c9d097a0-89dd-4769-97a5-4d213720d120
class RuntimeService(Base):
    __tablename__ = "runtime_services"
    __table_args__ = {"schema": "core"}
    name = Column(Text, primary_key=True)
    implementation = Column(Text, nullable=False, unique=True)
    is_active = Column(Boolean, server_default="true")


# ID: 4b0d98d6-23a0-4c9f-8389-0d40e7ef2105
class Migration(Base):
    __tablename__ = "_migrations"
    __table_args__ = {"schema": "core"}
    id = Column(Text, primary_key=True)
    applied_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )


# ID: 15a8b3cf-3c2d-4504-9890-85a386700323
class Northstar(Base):
    __tablename__ = "northstar"
    __table_args__ = {"schema": "core"}
    id = Column(
        pgUUID(as_uuid=True), primary_key=True, server_default=func.gen_random_uuid()
    )
    mission = Column(Text, nullable=False)
    updated_at = Column(
        DateTime(timezone=True), nullable=False, server_default=func.now()
    )

--- END OF FILE ./src/services/database/models.py ---

--- START OF FILE ./src/services/database/session_manager.py ---
# src/services/database/session_manager.py
"""
The single source of truth for creating and managing database sessions.
"""

from __future__ import annotations

from contextlib import asynccontextmanager
from typing import AsyncGenerator

from shared.config import settings
from sqlalchemy.ext.asyncio import (
    AsyncEngine,
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
)

_ENGINE: AsyncEngine = create_async_engine(
    settings.DATABASE_URL,
    echo=str(getattr(settings, "DATABASE_ECHO", "false")).lower() == "true",
    future=True,
)

AsyncSessionFactory = async_sessionmaker(
    bind=_ENGINE,
    class_=AsyncSession,
    expire_on_commit=False,
)


@asynccontextmanager
# ID: b35cd62e-6ada-4eee-b70b-ea20606e9d12
async def get_session() -> AsyncGenerator[AsyncSession, None]:
    """
    Primary entry point for services that need a session with a 'with' block.
    """
    session: AsyncSession = AsyncSessionFactory()
    try:
        yield session
    finally:
        await session.close()


# --- START MODIFICATION: Add FastAPI Dependency Provider ---
# ID: a5020e20-0b41-4790-b810-8b2354cad751
async def get_db_session() -> AsyncGenerator[AsyncSession, None]:
    """
    A dedicated dependency provider for FastAPI routes.
    This yields the session and ensures it's closed after the request.
    """
    async with get_session() as session:
        yield session


# --- END MODIFICATION ---

--- END OF FILE ./src/services/database/session_manager.py ---

--- START OF FILE ./src/services/llm/client.py ---
# src/services/llm/client.py (UPDATED VERSION)
"""
A simplified LLM Client that acts as a facade over a specific AI provider.

NOW USES: Database-backed configuration instead of environment variables.
"""

from __future__ import annotations

import asyncio
import random
from typing import Any, List, Optional

from sqlalchemy.ext.asyncio import AsyncSession

from services.config_service import ConfigService, LLMResourceConfig
from shared.logger import getLogger

from .providers.base import AIProvider

log = getLogger(__name__)


# ID: 8a9f272d-4f69-48f0-bda3-b485446bfc37
class LLMClient:
    """
    A client that uses a provider strategy to interact with an LLM API.

    UPDATED: Now reads configuration from database instead of environment variables.
    """

    def __init__(
        self,
        provider: AIProvider,
        resource_config: LLMResourceConfig,
    ):
        self.provider = provider
        self.resource_config = resource_config
        self.model_name = provider.model_name

        # Rate limiting state
        self._semaphore: Optional[asyncio.Semaphore] = None
        self._last_request_time: float = 0

    @classmethod
    async def create(
        cls,
        db: AsyncSession,
        provider: AIProvider,
        resource_name: str,
    ) -> LLMClient:
        """
        Factory method to create LLMClient with database configuration.

        Args:
            db: Database session
            provider: Configured AI provider instance
            resource_name: Name of the LLM resource (e.g., "anthropic", "deepseek_chat")

        Returns:
            Configured LLMClient instance

        Usage:
            config = await ConfigService.create(db)
            resource_config = await LLMResourceConfig.for_resource(config, "anthropic")

            provider = AnthropicProvider(
                api_key=await resource_config.get_api_key(),
                model_name=await resource_config.get_model_name(),
            )

            client = await LLMClient.create(db, provider, "anthropic")
        """
        config = await ConfigService.create(db)
        resource_config = await LLMResourceConfig.for_resource(config, resource_name)

        instance = cls(provider, resource_config)

        # Initialize rate limiting based on DB config
        max_concurrent = await resource_config.get_max_concurrent()
        instance._semaphore = asyncio.Semaphore(max_concurrent)

        log.info(
            f"Initialized LLMClient for {resource_name} "
            f"(model={provider.model_name}, max_concurrent={max_concurrent})"
        )

        return instance

    async def _enforce_rate_limit(self):
        """Enforce rate limiting based on database configuration."""
        rate_limit = await self.resource_config.get_rate_limit()

        if rate_limit > 0:
            now = asyncio.get_event_loop().time()
            time_since_last = now - self._last_request_time

            if time_since_last < rate_limit:
                wait_time = rate_limit - time_since_last
                log.debug(f"Rate limiting: waiting {wait_time:.2f}s")
                await asyncio.sleep(wait_time)

            self._last_request_time = asyncio.get_event_loop().time()

    async def _request_with_retry(self, method, *args, **kwargs) -> Any:
        """
        Generic retry logic with concurrency control.

        Enforces:
        - Max concurrent requests (via semaphore)
        - Rate limiting (via delay between requests)
        - Exponential backoff on failures
        """
        if not self._semaphore:
            raise RuntimeError("LLMClient not properly initialized - use create() factory method")

        backoff_delays = [1.0, 2.0, 4.0]

        async with self._semaphore:  # Enforce max concurrent
            await self._enforce_rate_limit()  # Enforce rate limit

            for attempt in range(len(backoff_delays) + 1):
                try:
                    return await method(*args, **kwargs)
                except Exception as e:
                    error_message = (
                        f"Request failed (attempt {attempt + 1}/{len(backoff_delays) + 1}): "
                        f"{type(e).__name__} - {e}"
                    )

                    if attempt < len(backoff_delays):
                        wait_time = backoff_delays[attempt] + random.uniform(0, 0.5)
                        log.warning(f"{error_message}. Retrying in {wait_time:.1f}s...")
                        await asyncio.sleep(wait_time)
                        continue

                    log.error(f"Final attempt failed: {error_message}", exc_info=True)
                    raise

    # ID: 1c0b0c26-46a8-4559-9b73-0b8a429a1303
    async def make_request_async(
        self, prompt: str, user_id: str = "core_system"
    ) -> str:
        """Makes a chat completion request using the configured provider with retries."""
        return await self._request_with_retry(
            self.provider.chat_completion, prompt, user_id
        )

    # ID: 262ea6eb-241e-444d-8388-aab25b9b5fa8
    async def get_embedding(self, text: str) -> List[float]:
        """Gets an embedding using the configured provider with retries."""
        return await self._request_with_retry(self.provider.get_embedding, text)


# ID: llm-client-factory-001
async def create_llm_client_for_role(
    db: AsyncSession,
    cognitive_role: str,
) -> LLMClient:
    """
    Factory function to create an LLM client for a specific cognitive role.

    This reads the role's assigned LLM resource from the database and
    creates an appropriately configured client.

    Args:
        db: Database session
        cognitive_role: Role name (e.g., "planner", "coder")

    Returns:
        Configured LLMClient instance

    Raises:
        ValueError: If role not found or not assigned to a resource

    Usage:
        client = await create_llm_client_for_role(db, "planner")
        response = await client.make_request_async("Plan this task...")
    """
    from sqlalchemy import text

    # Get the assigned resource for this role
    query = text("""
        SELECT assigned_resource
        FROM core.cognitive_roles
        WHERE role = :role AND is_active = true
    """)

    result = await db.execute(query, {"role": cognitive_role})
    row = result.fetchone()

    if not row or not row[0]:
        raise ValueError(f"Cognitive role '{cognitive_role}' not found or not assigned to a resource")

    resource_name = row[0]

    # Get resource configuration
    config = await ConfigService.create(db)
    resource_config = await LLMResourceConfig.for_resource(config, resource_name)

    # Determine provider type and create appropriate provider instance
    api_url = await resource_config.get_api_url()
    api_key = await resource_config.get_api_key(audit_context=cognitive_role)
    model_name = await resource_config.get_model_name()

    # Import appropriate provider based on API URL
    if "anthropic" in api_url:
        from .providers.anthropic import AnthropicProvider
        provider = AnthropicProvider(api_key=api_key, model_name=model_name)
    elif "deepseek" in api_url:
        from .providers.openai import OpenAIProvider  # DeepSeek uses OpenAI-compatible API
        provider = OpenAIProvider(api_url=api_url, api_key=api_key, model_name=model_name)
    elif "ollama" in api_url or "11434" in api_url:
        from .providers.ollama import OllamaProvider
        provider = OllamaProvider(api_url=api_url, model_name=model_name)
    else:
        # Default to OpenAI-compatible
        from .providers.openai import OpenAIProvider
        provider = OpenAIProvider(api_url=api_url, api_key=api_key, model_name=model_name)

    # Create and return client
    return await LLMClient.create(db, provider, resource_name)
--- END OF FILE ./src/services/llm/client.py ---

--- START OF FILE ./src/services/llm/providers/base.py ---
# src/services/llm/providers/base.py
"""
Defines the abstract base class for all AI provider strategies.
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from typing import List

import httpx


# ID: 32b9740b-010f-4fd0-8886-f17093aa855f
class AIProvider(ABC):
    """
    Abstract base class defining the interface for an AI service provider.
    """

    def __init__(
        self,
        api_url: str,
        model_name: str,
        api_key: str | None = None,
        timeout: int = 180,
    ):
        self.api_url = api_url.rstrip("/")
        self.model_name = model_name
        self.api_key = api_key
        self.timeout = httpx.Timeout(timeout)
        self.headers = self._prepare_headers()

    @abstractmethod
    def _prepare_headers(self) -> dict:
        """Prepare the specific headers for this provider."""
        pass

    @abstractmethod
    # ID: af87b72f-3b74-419d-b6c1-635c4185c033
    async def chat_completion(self, prompt: str, user_id: str) -> str:
        """Generate a text completion for a given prompt."""
        pass

    @abstractmethod
    # ID: bf6da823-1185-4a93-98bb-da095eb92f4f
    async def get_embedding(self, text: str) -> List[float]:
        """Generate an embedding vector for a given text."""
        pass

--- END OF FILE ./src/services/llm/providers/base.py ---

--- START OF FILE ./src/services/llm/providers/ollama.py ---
# src/services/llm/providers/ollama.py
"""
Provides an AIProvider implementation for Ollama APIs.
"""

from __future__ import annotations

from typing import List

import httpx

from .base import AIProvider


# ID: 0e708721-68c7-4252-b819-2c1827646b5e
class OllamaProvider(AIProvider):
    """Provider for Ollama-compatible chat and embedding APIs."""

    def _prepare_headers(self) -> dict:
        return {"Content-Type": "application/json"}

    # ID: 434c2772-9daf-4886-9a27-66004814fcff
    async def chat_completion(self, prompt: str, user_id: str) -> str:
        """Generates a chat completion using the Ollama format."""
        # Note: Ollama also supports /v1/chat/completions, but we use the native one for clarity
        endpoint = f"{self.api_url}/api/chat"
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,  # Ensure we get a single response
        }
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(endpoint, headers=self.headers, json=payload)
            response.raise_for_status()
            data = response.json()
            return data["message"]["content"]

    # ID: b74a1365-3b5b-4080-8433-dfe0d4243390
    async def get_embedding(self, text: str) -> List[float]:
        """Generates an embedding using the Ollama format."""
        endpoint = f"{self.api_url}/api/embeddings"
        payload = {"model": self.model_name, "prompt": text}
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(endpoint, headers=self.headers, json=payload)
            response.raise_for_status()
            data = response.json()
            return data["embedding"]

--- END OF FILE ./src/services/llm/providers/ollama.py ---

--- START OF FILE ./src/services/llm/providers/openai.py ---
# src/services/llm/providers/openai.py
"""
Provides an AIProvider implementation for OpenAI-compatible APIs (e.g., DeepSeek).
"""

from __future__ import annotations

from typing import List

import httpx

from .base import AIProvider


# ID: d73fe343-cad0-459e-9850-a9365a2be942
class OpenAIProvider(AIProvider):
    """Provider for OpenAI-compatible chat and embedding APIs."""

    def _prepare_headers(self) -> dict:
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        return headers

    # ID: 14948fa2-8ab2-4e16-addf-de5c1d24a807
    async def chat_completion(self, prompt: str, user_id: str) -> str:
        """Generates a chat completion using the OpenAI format."""
        endpoint = f"{self.api_url}/v1/chat/completions"
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "user": user_id,
        }
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(endpoint, headers=self.headers, json=payload)
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]

    # ID: bd55279d-308d-4483-890f-05835055b54e
    async def get_embedding(self, text: str) -> List[float]:
        """Generates an embedding using the OpenAI format."""
        endpoint = f"{self.api_url}/v1/embeddings"
        payload = {"model": self.model_name, "input": [text]}
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(endpoint, headers=self.headers, json=payload)
            response.raise_for_status()
            data = response.json()
            return data["data"][0]["embedding"]

--- END OF FILE ./src/services/llm/providers/openai.py ---

--- START OF FILE ./src/services/llm/resource_selector.py ---
# src/services/llm/resource_selector.py
"""
Provides a dedicated service for selecting the optimal LLM resource for a given cognitive role.
"""

from __future__ import annotations

import json  # <-- ADD THIS IMPORT
from typing import List, Optional

from services.database.models import CognitiveRole, LlmResource
from shared.logger import getLogger

log = getLogger("resource_selector")


# ID: b2f33e2c-4c68-46a5-9e89-a9ac80e3e0e5
class ResourceSelector:
    """
    Selects the best LLM resource based on capabilities and performance metadata.
    This service serves the 'separation_of_concerns' principle by decoupling resource
    selection from the main CognitiveService orchestration.
    """

    def __init__(self, resources: List[LlmResource], roles: List[CognitiveRole]):
        """
        Initializes the selector with the full list of available resources and roles from the database.
        """
        self.resources = resources
        self.roles = roles
        self.resources_by_name = {r.name: r for r in self.resources}
        self.roles_by_name = {r.role: r for r in self.roles}
        log.info(
            f"ResourceSelector initialized with {len(self.resources)} resources and {len(self.roles)} roles."
        )

    def _is_resource_qualified(
        self, resource: LlmResource, role: CognitiveRole
    ) -> bool:
        """Checks if a resource has the capabilities required by a role."""
        # --- THIS IS THE FIX: Handle string-encoded JSON ---
        res_caps_raw = resource.provided_capabilities
        req_caps_raw = role.required_capabilities

        res_caps = set(
            json.loads(res_caps_raw)
            if isinstance(res_caps_raw, str)
            else res_caps_raw or []
        )
        req_caps = set(
            json.loads(req_caps_raw)
            if isinstance(req_caps_raw, str)
            else req_caps_raw or []
        )
        # --- END OF FIX ---
        return req_caps.issubset(res_caps)

    def _score_resource(self, resource: LlmResource) -> int:
        """Returns the cost rating of a resource, defaulting to a medium cost."""
        # --- THIS IS THE FIX: Handle string-encoded JSON ---
        md_raw = resource.performance_metadata
        md = json.loads(md_raw) if isinstance(md_raw, str) else md_raw or {}
        # --- END OF FIX ---
        cost = md.get("cost_rating")
        return int(cost) if isinstance(cost, (int, float)) else 3

    # ID: 8636a4a6-7c58-4bb0-8372-e48e9184884d
    def select_resource_for_role(self, role_name: str) -> Optional[LlmResource]:
        """
        Selects the best resource for a role, prioritizing the explicitly assigned one.
        """
        role = self.roles_by_name.get(role_name)
        if not role:
            log.error(f"Cannot select resource: Role '{role_name}' not found.")
            return None

        assigned_resource_name = role.assigned_resource
        if assigned_resource_name:
            assigned_resource = self.resources_by_name.get(assigned_resource_name)
            if assigned_resource and self._is_resource_qualified(
                assigned_resource, role
            ):
                log.info(
                    f"Selected explicitly assigned resource '{assigned_resource.name}' for role '{role_name}'."
                )
                return assigned_resource
            else:
                log.warning(
                    f"Assigned resource '{assigned_resource_name}' for role '{role_name}' is not available or not qualified. Searching for an alternative."
                )

        candidates = []
        for resource in self.resources:
            if self._is_resource_qualified(resource, role):
                score = self._score_resource(resource)
                candidates.append((score, resource))

        if not candidates:
            log.warning(f"No suitable resource found for role '{role_name}'.")
            return None

        candidates.sort(key=lambda x: x[0])
        best_alternative = candidates[0][1]

        log.info(
            f"Selected best alternative resource '{best_alternative.name}' for role '{role_name}'."
        )
        return best_alternative

--- END OF FILE ./src/services/llm/resource_selector.py ---

--- START OF FILE ./src/services/mind_service.py ---
# src/services/mind_service.py
"""
Provides a constitutionally-governed, read-only interface to the Mind (.intent).
This service is the single, authoritative broker for accessing constitutional
knowledge, ensuring that the Body does not have arbitrary access to the filesystem
of the Mind, thus upholding the `separation_of_concerns` principle.
"""

from __future__ import annotations

from typing import Any

from shared.config import settings
from shared.utils.yaml_processor import strict_yaml_processor


# ID: d8390520-9c5b-4af3-881f-78f79601c7ff
class MindService:
    """A read-only API for accessing constitutional files from the .intent directory."""

    # ID: dda66271-6df0-4f6e-9a24-fe4ece6bafeb
    def load_policy(self, logical_path: str) -> dict[str, Any]:
        """
        Loads and parses a policy file using its logical path from meta.yaml.
        """
        policy_path = settings.get_path(logical_path)
        # DELEGATE to the canonical processor
        return strict_yaml_processor.load_strict(policy_path)


# ID: 8fd3d8eb-f721-4628-8263-94c6dd6d5171
def get_mind_service() -> MindService:
    """Factory function to get an instance of the MindService."""
    return MindService()

--- END OF FILE ./src/services/mind_service.py ---

--- START OF FILE ./src/services/repositories/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/services/repositories/__init__.py ---

--- START OF FILE ./src/services/repositories/db/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/services/repositories/db/__init__.py ---

--- START OF FILE ./src/services/repositories/db/common.py ---
# src/services/repositories/db/common.py
"""
Provides common utilities for database-related CLI commands.
"""

from __future__ import annotations

import os
import pathlib
import subprocess
from datetime import datetime, timezone
from typing import List, Set

import sqlparse
import yaml
from sqlalchemy import text

# CORRECTED IMPORT: Now points to the single source of truth for sessions.
from services.database.session_manager import get_session


# This robust function finds the project root without relying on the global settings object.
def _get_repo_root_for_migration() -> pathlib.Path:
    """Finds the repo root by searching upwards for a known marker file."""
    current_path = pathlib.Path(__file__).resolve()
    for parent in [current_path, *current_path.parents]:
        if (parent / "pyproject.toml").exists():
            return parent
    raise RuntimeError("Could not determine the repository root for migration.")


REPO_ROOT = _get_repo_root_for_migration()
META_YAML_PATH = REPO_ROOT / ".intent" / "meta.yaml"


# ID: 80ae5adf-d9cc-432e-b962-369b8992c700
def load_policy() -> dict:
    """Load the database_policy.yaml using a minimal, self-contained pathfinder."""
    try:
        with META_YAML_PATH.open("r", encoding="utf-8") as f:
            meta_config = yaml.safe_load(f)

        db_policy_path_str = meta_config["charter"]["policies"]["data"][
            "database_policy"
        ]
        db_policy_path = REPO_ROOT / ".intent" / db_policy_path_str

        with db_policy_path.open("r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    except (FileNotFoundError, KeyError) as e:
        raise FileNotFoundError(
            f"Could not locate database policy via meta.yaml. Ensure it's correctly indexed. Original error: {e}"
        ) from e
    except yaml.YAMLError as e:
        raise ValueError(
            f"Failed to parse a required YAML file for DB migration: {e}"
        ) from e


# ID: a5ec72d4-d489-434f-ad69-a36a39229d92
async def ensure_ledger() -> None:
    """Ensure core schema and the migrations ledger table exist."""
    async with get_session() as session:
        async with session.begin():
            await session.execute(text("create schema if not exists core"))
            await session.execute(
                text(
                    """
                    create table if not exists core._migrations (
                      id text primary key,
                      applied_at timestamptz not null default now()
                    )
                    """
                )
            )


# ID: ec3e6b37-b4e8-4870-80f5-10d652ac5902
async def get_applied() -> Set[str]:
    """Return set of applied migration IDs."""
    async with get_session() as session:
        result = await session.execute(text("select id from core._migrations"))
        return {r[0] for r in result}


# ID: 27163ec0-f952-4ed7-938b-080473bee2eb
async def apply_sql_file(path: pathlib.Path) -> None:
    """Apply a .sql file by splitting into single statements (asyncpg-safe)."""
    sql_text = path.read_text(encoding="utf-8")
    statements: List[str] = [s.strip() for s in sqlparse.split(sql_text) if s.strip()]
    async with get_session() as session:
        async with session.begin():
            for stmt in statements:
                await session.execute(text(stmt))


# ID: e3cbb291-e852-4ad5-bcc3-8b4046c1def0
async def record_applied(mig_id: str) -> None:
    """Record a migration as applied."""
    async with get_session() as session:
        async with session.begin():
            await session.execute(
                text(
                    "insert into core._migrations (id, applied_at) values (:id, :ts)"
                ).bindparams(id=mig_id, ts=datetime.now(tz=timezone.utc))
            )


# ID: c0a84f36-7546-405b-8de4-eba8548ff56b
def git_commit_sha() -> str:
    """Best-effort: get current commit SHA, or fallback to env, max 40 chars."""
    try:
        res = subprocess.run(
            ["git", "rev-parse", "--verify", "HEAD"],
            capture_output=True,
            text=True,
            check=False,
        )
        if res.returncode == 0:
            return res.stdout.strip()[:40]
    except Exception:
        pass
    return (os.getenv("GIT_COMMIT", "") or "").strip()[:40]

--- END OF FILE ./src/services/repositories/db/common.py ---

--- START OF FILE ./src/services/repositories/db/engine.py ---
# src/services/repositories/db/engine.py
"""
Refactored under dry_by_design.
Pattern: extract_module. Source of truth for DB engine logic is now session_manager.
Merged from: src/services/repositories/db/engine.py::_initialize_db
"""

from __future__ import annotations

from sqlalchemy import text

# The single source of truth for DB sessions is now imported.
from services.database.session_manager import get_session

# The get_session and _initialize_db functions previously here are now removed.


# ID: 4ec8bd10-ae74-4b30-b60c-799fb7d9f9bb
async def ping() -> dict:
    """Lightweight connectivity check, using the canonical session manager."""
    # _initialize_db is removed; get_session handles all engine/session logic.
    async with get_session() as session:
        async with session.begin():
            v = await session.execute(text("select version()"))
            return {"ok": True, "version": v.scalar_one()}

--- END OF FILE ./src/services/repositories/db/engine.py ---

--- START OF FILE ./src/services/repositories/db/migration_service.py ---
# src/services/repositories/db/migration_service.py
"""
Provides the canonical, single-source-of-truth service for applying database schema migrations.
"""

from __future__ import annotations

import asyncio
import pathlib

import typer
from rich.console import Console

from .common import (
    apply_sql_file,
    ensure_ledger,
    get_applied,
    load_policy,
    record_applied,
)

console = Console()


async def _run_migrations(apply: bool):
    """The core async logic for running migrations."""
    try:
        pol = load_policy()
        migrations_config = pol.get("migrations", {})
        order = migrations_config.get("order", [])
        migration_dir = migrations_config.get("directory", "sql")
    except Exception as e:
        console.print(f"[bold red]❌ Error loading database policy: {e}[/bold red]")
        raise typer.Exit(code=1)

    await ensure_ledger()
    applied = await get_applied()
    pending = [m for m in order if m not in applied]

    if not pending:
        console.print("[bold green]✅ DB schema is up to date.[/bold green]")
        return

    console.print(f"[yellow]Pending migrations found: {pending}[/yellow]")
    if not apply:
        console.print("   -> Run with '--apply' to execute them.")
        return

    for mig in pending:
        console.print(f"   -> Applying migration: {mig}...")
        try:
            await apply_sql_file(pathlib.Path(migration_dir) / mig)
            await record_applied(mig)
            console.print("      [green]...success.[/green]")
        except Exception as e:
            console.print(f"[bold red]      ❌ FAILED to apply {mig}: {e}[/bold red]")
            raise typer.Exit(code=1)

    console.print(
        "[bold green]✅ All pending migrations applied successfully.[/bold green]"
    )


# ID: 7bb0c5ee-480b-4d14-9147-853c9f9b25c5
def migrate_db(
    apply: bool = typer.Option(False, "--apply", help="Apply pending migrations."),
):
    """Initialize DB schema and apply pending migrations."""
    asyncio.run(_run_migrations(apply))

--- END OF FILE ./src/services/repositories/db/migration_service.py ---

--- START OF FILE ./src/services/repositories/db/status_service.py ---
# src/services/repositories/db/status_service.py
"""
Refactored under dry_by_design.
Pattern: extract_module. Source of truth for database status logic.
Merged from: src/cli/logic/status.py::status
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Set

from services.repositories.db.common import (
    ensure_ledger,
    get_applied,
    load_policy,
)
from services.repositories.db.engine import ping


@dataclass
# ID: c4fbc704-9f97-48df-bc55-63fb1b850838
class StatusReport:
    """A data structure holding the results of a database status check."""

    is_connected: bool
    db_version: str | None
    applied_migrations: Set[str]
    pending_migrations: List[str]


# ID: 75fac84c-5818-47c0-9d50-c0670d065c8c
async def status() -> StatusReport:
    """Checks DB connectivity and migration status, returning a structured report."""
    # 1) connection/ping
    try:
        info = await ping()
        is_connected = info.get("ok", False)
        db_version = info.get("version")
    except Exception:
        return StatusReport(
            is_connected=False,
            db_version=None,
            applied_migrations=set(),
            pending_migrations=[],
        )

    # 2) policy & migrations
    pol = load_policy()
    order = pol.get("migrations", {}).get("order", [])

    await ensure_ledger()
    applied = await get_applied()
    pending = [m for m in order if m not in applied]

    return StatusReport(
        is_connected=is_connected,
        db_version=db_version,
        applied_migrations=applied,
        pending_migrations=pending,
    )

--- END OF FILE ./src/services/repositories/db/status_service.py ---

--- START OF FILE ./src/shared/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/shared/__init__.py ---

--- START OF FILE ./src/shared/action_logger.py ---
# src/shared/action_logger.py
"""
Provides a dedicated service for writing structured, auditable events to the system's action log.
"""

from __future__ import annotations

import json
from datetime import datetime, timezone
from typing import Any, Dict

from shared.config import settings
from shared.logger import getLogger

log = getLogger("action_logger")


# ID: 7a8b9c0d-1e2f-3a4b-5c6d-7e8f9a0b1c2d
class ActionLogger:
    """Handles writing structured JSON events to the CORE_ACTION_LOG_PATH."""

    def __init__(self):
        """Initializes the logger, ensuring the log file's parent directory exists."""
        try:
            log_path_str = settings.CORE_ACTION_LOG_PATH
            if not log_path_str:
                raise ValueError("CORE_ACTION_LOG_PATH is not set in the environment.")
            self.log_path = settings.REPO_PATH / log_path_str
            self.log_path.parent.mkdir(parents=True, exist_ok=True)
        except (ValueError, AttributeError) as e:
            log.error(
                f"ActionLogger failed to initialize: {e}. Logging will be disabled."
            )
            self.log_path = None

    # ID: 5d7a8b9c-0d1e-2f3a-4b5c-6d7e8f9a0b1c
    def log_event(self, event_type: str, details: Dict[str, Any]):
        """
        Writes a single, timestamped event to the action log file.

        Args:
            event_type: A dot-notation string identifying the event (e.g., 'crate.processing.started').
            details: A dictionary of context-specific information about the event.
        """
        if not self.log_path:
            return  # Fail silently if the logger could not be initialized.

        log_entry = {
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "event_type": event_type,
            "details": details,
        }
        try:
            with self.log_path.open("a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry) + "\n")
        except Exception as e:
            log.error(f"Failed to write to action log at {self.log_path}: {e}")


# A singleton instance for easy access across the application
action_logger = ActionLogger()

--- END OF FILE ./src/shared/action_logger.py ---

--- START OF FILE ./src/shared/ast_utility.py ---
# src/shared/ast_utility.py
"""
Utility functions for working with Python AST (Abstract Syntax Trees).

Provides helpers to parse, inspect, and analyze Python source code at the
AST level. Includes visitors for extracting function calls, base classes,
docstrings, parameters, metadata tags, and a robust structural hash that is
insensitive to docstrings and whitespace.
"""

from __future__ import annotations

import ast
import copy
import hashlib
import logging
import re
import uuid
from dataclasses import dataclass
from typing import Dict, List, Optional

log = logging.getLogger(__name__)


# --- THIS IS THE NEW, ROBUST HELPER FUNCTION ---
# ID: a1b2c3d4-e5f6-7a8b-9c0d-1e2f3a4b5c6d
def find_definition_line(
    node: ast.FunctionDef | ast.AsyncFunctionDef | ast.ClassDef, source_lines: List[str]
) -> int:
    """
    Finds the actual line number of the 'def' or 'class' keyword,
    skipping over any decorators.
    """
    if not node.decorator_list:
        return node.lineno

    # The line number of the last decorator
    last_decorator_line = (
        node.decorator_list[-1].end_lineno or node.decorator_list[-1].lineno
    )

    # Search for "def" or "class" from the last decorator onwards
    for i in range(last_decorator_line - 1, len(source_lines)):
        line = source_lines[i].strip()
        if (
            line.startswith(f"def {node.name}")
            or line.startswith(f"async def {node.name}")
            or line.startswith(f"class {node.name}")
        ):
            return i + 1  # Return 1-based line number

    return node.lineno  # Fallback


@dataclass
# ID: aae372c1-f0db-43e3-a048-89940a5fd108
class SymbolIdResult:
    """Holds the result of finding a symbol's ID and definition line."""

    has_id: bool
    uuid: Optional[str] = None
    id_tag_line_num: Optional[int] = None
    definition_line_num: int = 0


# ID: 6a3b9d5c-1f8e-4b2a-9c7d-8e5f4a3b2c1d
def find_symbol_id_and_def_line(
    node: ast.FunctionDef | ast.AsyncFunctionDef | ast.ClassDef, source_lines: List[str]
) -> SymbolIdResult:
    """
    Finds the actual definition line and ID tag for a symbol, correctly skipping decorators.
    """
    definition_line = find_definition_line(node, source_lines)

    # The ID tag should be on the line immediately preceding the definition line
    tag_line_index = definition_line - 2

    if 0 <= tag_line_index < len(source_lines):
        line_above = source_lines[tag_line_index].strip()
        match = re.search(r"#\s*ID:\s*([0-9a-fA-F\-]+)", line_above)
        if match:
            found_uuid = match.group(1)
            try:
                # Validate it's a proper UUID
                uuid.UUID(found_uuid)
                return SymbolIdResult(
                    has_id=True,
                    uuid=found_uuid,
                    id_tag_line_num=tag_line_index + 1,
                    definition_line_num=definition_line,
                )
            except ValueError:
                pass  # Invalid UUID format, treat as no ID

    return SymbolIdResult(has_id=False, definition_line_num=definition_line)


# --- END OF NEW HELPER FUNCTION ---


# ---------------------------------------------------------------------------
# Basic extractors
# ---------------------------------------------------------------------------


# ID: 79ccf26e-3710-4802-9ccb-29423f545e45
def extract_docstring(node: ast.AST) -> Optional[str]:
    """Extract the docstring from the given AST node if it exists."""
    return ast.get_docstring(node)


# ID: 79024211-279d-40af-91c3-679d5afdcf9f
def extract_base_classes(node: ast.ClassDef) -> List[str]:
    """Return a list of base class names for the given class node."""
    bases: List[str] = []
    for base in node.bases:
        if isinstance(base, ast.Name):
            bases.append(base.id)
        elif isinstance(base, ast.Attribute):
            # e.g. module.Class — capture best-effort dotted path
            left = None
            if isinstance(base.value, ast.Name):
                left = base.value.id
            elif isinstance(base.value, ast.Attribute):
                # fallback: last attribute segment
                left = base.value.attr
            bases.append(f"{left}.{base.attr}" if left else base.attr)
    return bases


# ID: 502f4096-53ca-49d8-b3e4-ec7a075b0881
def extract_parameters(node: ast.FunctionDef | ast.AsyncFunctionDef) -> List[str]:
    """Extract parameter names from a function (or async function) definition node."""
    if not hasattr(node, "args") or node.args is None:
        return []
    return [arg.arg for arg in getattr(node.args, "args", [])]


# ID: d73a2936-68f4-4dc4-b6ef-db6188740683
class FunctionCallVisitor(ast.NodeVisitor):
    """Visitor that collects function call names within a node."""

    def __init__(self) -> None:
        """Initialize an empty collection of function call names."""
        self.calls: List[str] = []

    # ID: 2eec3148-6aeb-4d74-9dd3-b73be105ee02
    def visit_Call(self, node: ast.Call) -> None:
        """Record the called function/method name, then continue traversal."""
        if isinstance(node.func, ast.Name):
            self.calls.append(node.func.id)
        elif isinstance(node.func, ast.Attribute):
            self.calls.append(node.func.attr)
        self.generic_visit(node)


# ---------------------------------------------------------------------------
# Metadata parsing (used by knowledge discovery)
# ---------------------------------------------------------------------------


# ID: 5f4a3e52-b52a-49ac-aa37-a5201376979f
def parse_metadata_comment(node: ast.AST, source_lines: List[str]) -> Dict[str, str]:
    """Returns a dict like {'capability': 'domain.key'} when present; otherwise empty dict."""
    if getattr(node, "lineno", None) and node.lineno > 1:
        line = source_lines[node.lineno - 2].strip()
        if line.startswith("#") and "CAPABILITY:" in line.upper():
            try:
                # split on the first colon to preserve values containing colons
                prefix, value = line.split(":", 1)
                return {"capability": value.strip()}
            except ValueError:
                pass
    return {}


# ---------------------------------------------------------------------------
# Structural hashing (canonical implementation lives here)
# ---------------------------------------------------------------------------


def _strip_docstrings(node: ast.AST) -> ast.AST:
    """Remove leading docstring expressions from modules/classes/functions."""
    if isinstance(
        node, (ast.Module, ast.ClassDef, ast.FunctionDef, ast.AsyncFunctionDef)
    ):
        if (
            getattr(node, "body", None)
            and len(node.body) > 0
            and isinstance(node.body[0], ast.Expr)
            and isinstance(getattr(node.body[0], "value", None), ast.Constant)
            and isinstance(node.body[0].value.value, str)
        ):
            node.body = node.body[1:]

    for child in ast.iter_child_nodes(node):
        _strip_docstrings(child)

    return node


# ID: 1b0ec762-579f-4b3d-93eb-c88e42253c54
def calculate_structural_hash(node: ast.AST) -> str:
    """Calculate a stable structural hash for an AST node.

    The hash is:
      - insensitive to docstrings (they are stripped)
      - insensitive to whitespace and newlines
    """
    try:
        normalized = ast.parse(ast.unparse(node))
        normalized = _strip_docstrings(normalized)
        structural = ast.unparse(normalized).replace("\n", "").replace(" ", "")
        return hashlib.sha256(structural.encode("utf-8")).hexdigest()
    except Exception:
        # Fallback: never block callers on hashing
        try:
            fallback = ast.unparse(node)
        except Exception:
            fallback = repr(node)
        log.exception("Structural hash computation failed; using fallback hash.")
        return hashlib.sha256(fallback.encode("utf-8")).hexdigest()


# ADD these lines
# ID: 6ca3e58a-deda-4cd8-b9fa-d9909235e218
def normalize_ast(node: ast.AST) -> str:
    """
    Return a deterministic string representation of an AST node.
    Docstrings are erased, variable names replaced with v0, v1...
    Used to detect structural duplicates.
    """

    # ID: 3b00bddc-d6d8-4e55-b2fa-aadb989ebcc1
    class Normalizer(ast.NodeTransformer):
        def __init__(self):
            self._var_counter = 0
            self._var_map = {}

        # ID: ba8ec44b-1eb5-4e95-a44b-6d507f4f539d
        def visit_Name(self, node: ast.Name) -> ast.Name:
            if isinstance(node.ctx, ast.Store):
                new_name = f"v{self._var_counter}"
                self._var_map[node.id] = new_name
                self._var_counter += 1
                node.id = new_name
            elif node.id in self._var_map:
                node.id = self._var_map[node.id]
            return self.generic_visit(node)

        # ID: 86ecbe35-65c3-4338-bfbd-1c55c3ca53fb
        def visit_Constant(self, node: ast.Constant) -> ast.Constant:
            # erase string literals (docstrings)
            if isinstance(node.value, str):
                node.value = ""
            return node

    normalized = Normalizer().visit(copy.deepcopy(node))
    return ast.dump(normalized, indent=0)

--- END OF FILE ./src/shared/ast_utility.py ---

--- START OF FILE ./src/shared/config.py ---
# src/shared/config.py
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Optional

import yaml

# --- THIS IS THE FIX ---
# We now explicitly load the .env file right here, ensuring it's always available.
from dotenv import load_dotenv
from pydantic import PrivateAttr
from pydantic_settings import BaseSettings, SettingsConfigDict

from shared.logger import getLogger

log = getLogger("core.config")

REPO_ROOT = Path(__file__).resolve().parents[2]
# This line proactively loads the .env file from the project root.
load_dotenv(REPO_ROOT / ".env")
# --- END OF FIX ---


# ID: fffe6c00-5587-4951-a7c2-2dd83d1adb5f
class Settings(BaseSettings):
    """
    The single, canonical source of truth for all CORE configuration.
    It loads from environment variables and provides "Pathfinder" methods
    to access constitutional files via the .intent/meta.yaml index.
    """

    model_config = SettingsConfigDict(
        # We still keep this for pydantic's native features, but our load_dotenv is more robust.
        env_file=".env",
        env_file_encoding="utf-8",
        extra="allow",
        case_sensitive=True,
    )

    _meta_config: Dict[str, Any] = PrivateAttr(default_factory=dict)

    REPO_PATH: Path = REPO_ROOT
    MIND: Path = REPO_PATH / ".intent"
    BODY: Path = REPO_PATH / "src"
    LLM_ENABLED: bool = True
    LOG_LEVEL: str = "INFO"
    CORE_MAX_CONCURRENT_REQUESTS: int = 5

    DATABASE_URL: str
    QDRANT_URL: str
    QDRANT_COLLECTION_NAME: str = "core_capabilities"

    LOCAL_EMBEDDING_API_URL: str
    LOCAL_EMBEDDING_MODEL_NAME: str
    LOCAL_EMBEDDING_DIM: int
    LOCAL_EMBEDDING_API_KEY: Optional[str] = None
    EMBED_MODEL_REVISION: str = "2025-09-15"

    KEY_STORAGE_DIR: Path = REPO_PATH / ".intent" / "keys"

    def __init__(self, **values: Any):
        super().__init__(**values)
        if (self.REPO_PATH / ".intent" / "meta.yaml").exists():
            self._load_meta_config()

    # ID: ea16ad9b-bf16-4e44-bc48-dd8734f79f73
    def initialize_for_test(self, repo_path: Path):
        self.REPO_PATH = repo_path
        self.MIND = repo_path / ".intent"
        self.BODY = repo_path / "src"
        self._load_meta_config()

    def _load_meta_config(self):
        meta_path = self.REPO_PATH / ".intent" / "meta.yaml"
        if not meta_path.exists():
            self._meta_config = {}
            return
        try:
            self._meta_config = self._load_file_content(meta_path)
        except (IOError, ValueError) as e:
            raise RuntimeError(f"FATAL: Could not parse .intent/meta.yaml: {e}")

    def _load_file_content(self, file_path: Path) -> Dict[str, Any]:
        content = file_path.read_text("utf-8")
        if file_path.suffix in (".yaml", ".yml"):
            return yaml.safe_load(content) or {}
        if file_path.suffix == ".json":
            return json.loads(content) or {}
        raise ValueError(f"Unsupported config file type: {file_path}")

    # ID: 84038773-3d4c-4f59-8cf7-db6b68e0fd37
    def get_path(self, logical_path: str) -> Path:
        keys = logical_path.split(".")
        value: Any = self._meta_config
        try:
            for key in keys:
                value = value[key]
            if not isinstance(value, str):
                raise TypeError
            if value.startswith("charter/") or value.startswith("mind/"):
                return self.REPO_PATH / ".intent" / value
            return self.REPO_PATH / value
        except (KeyError, TypeError):
            raise FileNotFoundError(
                f"Logical path '{logical_path}' not found or invalid in meta.yaml."
            )

    # ID: ab774e44-9edf-4309-af2a-84a20f9e86bd
    def find_logical_path_for_file(self, filename: str) -> str:
        def _search(d: Any) -> Optional[str]:
            if isinstance(d, dict):
                for _, v in d.items():
                    if isinstance(v, str) and v.endswith(filename):
                        return v
                    found = _search(v)
                    if found:
                        return found
            return None

        found_path = _search(self._meta_config)
        if found_path:
            return found_path
        raise ValueError(f"Filename '{filename}' not found in meta.yaml index.")

    # ID: 73a12ea2-7924-482f-a6c7-b0c67c56b486
    def load(self, logical_path: str) -> Dict[str, Any]:
        file_path = self.get_path(logical_path)
        try:
            return self._load_file_content(file_path)
        except FileNotFoundError:
            raise
        except (IOError, ValueError) as e:
            raise IOError(f"Failed to load or parse file for '{logical_path}': {e}")


try:
    settings = Settings()
except (RuntimeError, FileNotFoundError) as e:
    log.critical(f"FATAL ERROR during settings initialization: {e}")


# ID: a40df97d-3f3f-4ab9-9fca-7986ca5d5b25
def get_path_or_none(logical_path: str) -> Optional[Path]:
    try:
        if "settings" not in globals() or settings is None:
            return None
        return settings.get_path(logical_path)
    except Exception:
        return None

--- END OF FILE ./src/shared/config.py ---

--- START OF FILE ./src/shared/config_loader.py ---
# src/shared/config_loader.py
"""
Utility for loading configuration files (YAML or JSON) safely.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict

import yaml

from shared.logger import getLogger

log = getLogger(__name__)


# ID: 85467d87-762e-461c-8c08-af2b982e2387
def load_yaml_file(file_path: Path) -> Dict[str, Any]:
    """
    Loads a YAML or JSON config file safely, with consistent error handling.
    This is the single source of truth for YAML loading.

    Args:
        file_path: Path to the configuration file.

    Returns:
        A dictionary containing the parsed configuration data.

    Raises:
        FileNotFoundError: If the file does not exist.
        ValueError: If the file format is unsupported or parsing fails.
    """
    if not file_path.exists():
        log.error(f"Config file not found: {file_path}")
        raise FileNotFoundError(f"Config file not found: {file_path}")

    try:
        content = file_path.read_text(encoding="utf-8")
        if file_path.suffix in (".yaml", ".yml"):
            return yaml.safe_load(content) or {}
        elif file_path.suffix == ".json":
            return json.loads(content) or {}
        else:
            log.error(f"Unsupported file type: {file_path.suffix}")
            raise ValueError(f"Unsupported config file type: {file_path}")
    except (yaml.YAMLError, json.JSONDecodeError) as e:
        log.error(f"Error parsing config {file_path}: {e}")
        raise ValueError(f"Invalid config format in {file_path}") from e
    except UnicodeDecodeError as e:
        log.error(f"Encoding error in {file_path}: {e}")
        raise ValueError(f"Encoding error in config {file_path}") from e

--- END OF FILE ./src/shared/config_loader.py ---

--- START OF FILE ./src/shared/constants.py ---
# src/shared/constants.py
"""
Centralized location for system-wide constant values.
"""

# Maximum allowed file size for system operations (1MB)
MAX_FILE_SIZE_BYTES = 1 * 1024 * 1024

--- END OF FILE ./src/shared/constants.py ---

--- START OF FILE ./src/shared/context.py ---
# src/shared/context.py
"""
Defines the CoreContext, a dataclass that holds singleton instances of all major
services, enabling explicit dependency injection throughout the application.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any


@dataclass
# ID: 9f1dd7c7-1cb2-435d-bd07-b7d436c9459f
class CoreContext:
    """
    A container for shared services, passed explicitly to commands.

    NOTE: Fields are typed as 'Any' to avoid cross-domain imports from here.
    Concrete types are created/wired in the CLI layer.
    """

    git_service: Any
    cognitive_service: Any
    knowledge_service: Any
    qdrant_service: Any
    auditor_context: Any
    file_handler: Any
    planner_config: Any
    _is_test_mode: bool = False  # <-- ADD THIS LINE

--- END OF FILE ./src/shared/context.py ---

--- START OF FILE ./src/shared/legacy_models.py ---
# src/shared/legacy_models.py
"""
Pydantic models for parsing legacy YAML configuration files during migration.
"""

from __future__ import annotations

from pydantic import BaseModel, Field


# ID: 54bbf6eb-5417-4d45-8aea-04f1932cae87
class LegacyCliCommand(BaseModel):
    """Represents a single command from the legacy cli_registry.yaml."""

    name: str
    module: str
    entrypoint: str
    summary: str | None = None
    category: str | None = None


# ID: 6686610f-46bc-4eee-9cb1-5301b16276d7
class LegacyCliRegistry(BaseModel):
    """Represents the top-level structure of the legacy cli_registry.yaml."""

    commands: list[LegacyCliCommand]


# ID: 644ea3cb-f501-4017-919f-23270e114839
class LegacyLlmResource(BaseModel):
    """Represents a single resource from the legacy resource_manifest.yaml."""

    name: str
    provided_capabilities: list[str] = Field(default_factory=list)
    env_prefix: str
    performance_metadata: dict | None = None


# ID: 41b53390-8b31-4ed7-a01d-769b9e669308
class LegacyResourceManifest(BaseModel):
    """Represents the top-level structure of the legacy resource_manifest.yaml."""

    llm_resources: list[LegacyLlmResource]


# ID: 13914243-a1b0-47fd-bbfc-b415540d5cbe
class LegacyCognitiveRole(BaseModel):
    """Represents a single role from the legacy cognitive_roles.yaml."""

    role: str
    description: str | None = None
    assigned_resource: str | None = None
    required_capabilities: list[str] = Field(default_factory=list)


# ID: 9bf273ce-d632-4f7d-ac3a-833c51d4cda7
class LegacyCognitiveRoles(BaseModel):
    """Represents the top-level structure of the legacy cognitive_roles.yaml."""

    cognitive_roles: list[LegacyCognitiveRole]

--- END OF FILE ./src/shared/legacy_models.py ---

--- START OF FILE ./src/shared/logger.py ---
# src/shared/logger.py

"""
CORE's Unified Logging System.

This module provides a single, pre-configured logger instance for the entire
application. It uses the 'rich' library to ensure all output is consistent,
beautifully formatted, and informative.

All other modules should import `getLogger` from this file instead of using
print() or configuring their own loggers.
"""

from __future__ import annotations

import logging
import os

from rich.logging import RichHandler

# --- Configuration ---
# Get the log level from the environment, defaulting to INFO
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
LOG_FORMAT = "%(message)s"
LOG_DATE_FORMAT = "[%X]"  # e.g., [14:30:55]

# --- Prevent duplicate handlers if this module is reloaded ---
logging.getLogger().handlers = []

# --- Create and configure the handler ---
handler = RichHandler(
    rich_tracebacks=True,
    show_time=True,
    show_level=True,
    show_path=False,
    log_time_format=LOG_DATE_FORMAT,
)

# --- Configure the root logger ---
logging.basicConfig(
    level=LOG_LEVEL,
    format=LOG_FORMAT,
    handlers=[handler],
    force=True,  # Ensure our configuration overwrites any defaults
)

# --- THIS IS THE FIX: Set quieter log levels for noisy libraries ---
# This tells the http client library used by Qdrant to only log warnings and errors.
logging.getLogger("httpx").setLevel(logging.WARNING)
# We can also make our own verbose services quieter by default.
logging.getLogger("qdrant_service").setLevel(logging.WARNING)
# --- END OF FIX ---


# ID: b6a332e8-17ca-4a0a-8699-4eaff466aafe
def getLogger(name: str) -> logging.Logger:
    """
    Returns a pre-configured logger instance.

    Args:
        name (str): The name of the logger, typically __name__ of the calling module.

    Returns:
        logging.Logger: The configured logger.
    """
    return logging.getLogger(name)


# Example of a root-level logger if needed directly
log = getLogger("core_root")

# Set the log level for the root logger from the environment variable
log.setLevel(LOG_LEVEL)

--- END OF FILE ./src/shared/logger.py ---

--- START OF FILE ./src/shared/models/__init__.py ---
# src/shared/models/__init__.py
"""
Makes all Pydantic models in this directory available for easy import.
"""

from __future__ import annotations

from .audit_models import AuditFinding, AuditSeverity
from .capability_models import CapabilityMeta
from .drift_models import DriftReport  # <-- ADD THIS LINE
from .embedding_payload import EmbeddingPayload
from .execution_models import (
    ExecutionTask,
    PlanExecutionError,
    PlannerConfig,
    TaskParams,
)

__all__ = [
    "DriftReport",  # <-- AND ADD THIS LINE
    "EmbeddingPayload",
    "AuditFinding",
    "AuditSeverity",
    "ExecutionTask",
    "PlanExecutionError",
    "PlannerConfig",
    "TaskParams",
    "CapabilityMeta",
]

--- END OF FILE ./src/shared/models/__init__.py ---

--- START OF FILE ./src/shared/models/audit_models.py ---
# src/shared/models/audit_models.py
"""
Defines the Pydantic models for representing the results of a constitutional audit.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from enum import IntEnum  # <-- CHANGED from Enum to IntEnum
from typing import Any, Dict, Optional


# ID: 5ccdae76-2214-413d-8551-13d4b224b694
class AuditSeverity(IntEnum):  # <-- CHANGED from Enum to IntEnum
    """Enumeration for the severity of an audit finding."""

    INFO = 1
    WARNING = 2
    ERROR = 3

    # This allows us to use severity.name in lowercase, e.g., 'info'
    def __str__(self):
        return self.name.lower()

    @property
    # ID: bad8d002-de4c-4b09-900f-0cd784c60242
    def is_blocking(self) -> bool:
        """Returns True if the severity level should block a CI/CD pipeline."""
        return self == AuditSeverity.ERROR


@dataclass
# ID: 1bc3d2f1-466b-49b9-aacd-6fac9e03a068
class AuditFinding:
    """Represents a single finding from a constitutional audit check."""

    check_id: str
    severity: AuditSeverity
    message: str
    file_path: Optional[str] = None
    line_number: Optional[int] = None
    context: Dict[str, Any] = field(default_factory=dict)

    # ID: d638215e-ceb0-421e-b33b-a0b191876530
    def as_dict(self) -> Dict[str, Any]:
        """Serializes the finding to a dictionary for reporting."""
        return {
            "check_id": self.check_id,
            "severity": str(self.severity),
            "message": self.message,
            "file_path": self.file_path,
            "line_number": self.line_number,
            "context": self.context,
        }

--- END OF FILE ./src/shared/models/audit_models.py ---

--- START OF FILE ./src/shared/models/capability_models.py ---
# src/shared/models/capability_models.py
"""
Defines the Pydantic/dataclass models for representing capabilities and
their metadata throughout the system.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional


@dataclass
# ID: 6c0a8c58-e1f0-4182-9857-1eb3dfa0410e
class CapabilityMeta:
    """
    A dataclass to hold the metadata for a single capability, discovered
    either from manifest files or source code tags.
    """

    key: str
    domain: Optional[str] = None
    owner: Optional[str] = None

--- END OF FILE ./src/shared/models/capability_models.py ---

--- START OF FILE ./src/shared/models/drift_models.py ---
# src/shared/models/drift_models.py
"""
Defines the Pydantic/dataclass models for representing capability drift.
"""

from __future__ import annotations

from dataclasses import asdict, dataclass
from typing import Any, Dict, List


@dataclass
# ID: a8f4575c-a899-4dde-9d8f-c2825eaa7259
class DriftReport:
    """A structured report of the drift between manifest and code."""

    missing_in_code: List[str]
    undeclared_in_manifest: List[str]
    mismatched_mappings: List[Dict]

    # ID: 9db89268-07cb-4bf7-9abe-14df2f0aae8a
    def to_dict(self) -> Dict[str, Any]:
        """Serializes the report to a dictionary."""
        return asdict(self)

--- END OF FILE ./src/shared/models/drift_models.py ---

--- START OF FILE ./src/shared/models/embedding_payload.py ---
# src/shared/models/embedding_payload.py
"""
Defines the Pydantic model for the data payload associated with each
vector stored in the Qdrant database.
"""

from __future__ import annotations

from typing import List, Optional

from pydantic import BaseModel, Field


# ID: 103f4a4c-a895-4de7-b5bf-ce230bcda4aa
class EmbeddingPayload(BaseModel):
    """
    Strict schema for the payload of every vector stored in Qdrant.
    This ensures all stored knowledge is traceable to its origin.
    """

    source_path: str = Field(..., description="Repo-relative path of the source file.")
    source_type: str = Field(
        ..., description="Type of content (e.g., 'code', 'intent')."
    )
    chunk_id: str = Field(
        ..., description="Stable locator for the text chunk (e.g., symbol key)."
    )
    content_sha256: str = Field(
        ..., description="Fingerprint of the normalized chunk text."
    )
    model: str = Field(..., description="Name of the embedding model used.")
    model_rev: str = Field(..., description="Pinned revision of the embedding model.")
    dim: int = Field(..., description="Dimensionality of the vector.")
    created_at: str = Field(..., description="ISO 8601 timestamp of vector creation.")

    # Optional fields for richer context
    language: Optional[str] = Field(None, description="Programming or markup language.")
    symbol: Optional[str] = Field(
        None, description="For code: fully qualified function/class name."
    )
    capability_tags: Optional[List[str]] = Field(
        None, description="Associated capability tags."
    )

--- END OF FILE ./src/shared/models/embedding_payload.py ---

--- START OF FILE ./src/shared/models/execution_models.py ---
# src/shared/models/execution_models.py
"""
Defines the Pydantic models for representing autonomous execution plans and tasks.
"""

from __future__ import annotations

from typing import List, Optional

from pydantic import BaseModel, Field


# ID: 1a71c89f-73f0-436b-ad58-f24cfbdec162
class TaskParams(BaseModel):
    """Parameters for a single task in an execution plan."""

    # --- THIS IS THE FIX ---
    # The file_path is now optional to allow for tasks that don't operate on a single file.
    file_path: Optional[str] = None
    # --- END OF FIX ---

    code: Optional[str] = None
    symbol_name: Optional[str] = None
    justification: Optional[str] = None
    tag: Optional[str] = None


# ID: 3173b37e-a64f-4227-92c5-84e444b68dc1
class ExecutionTask(BaseModel):
    """A single, validated step in an execution plan."""

    step: str
    action: str
    params: TaskParams


# ID: 73684d31-61e0-4f28-bb94-7134f296371b
class PlannerConfig(BaseModel):
    """Configuration for the Planner and Execution agents."""

    task_timeout: int = Field(default=300, description="Timeout for a single task.")
    rollback_on_failure: bool = Field(default=True, description="Rollback on failure.")
    auto_commit: bool = Field(default=True, description="Auto-commit changes.")


# ID: 1ccf34ef-9cea-4411-91b1-d93457a2b43a
class PlanExecutionError(Exception):
    """Custom exception for errors during plan execution."""

    def __init__(self, message: str, violations: List[dict] | None = None):
        super().__init__(message)
        self.violations = violations or []

--- END OF FILE ./src/shared/models/execution_models.py ---

--- START OF FILE ./src/shared/path_utils.py ---
# src/shared/path_utils.py
"""
Provides utility functions for working with file system paths within the repository structure.
"""

from __future__ import annotations

from pathlib import Path
from typing import Optional


# ID: d302f037-094f-4573-92d0-39dc29c012f6
def get_repo_root(start_path: Optional[Path] = None) -> Path:
    """Find and return the repository root by locating the .git directory, starting from the current directory or provided path."""
    """
    Find and return the repository root by locating the .git directory.
    Starts from current directory or provided path.

    Returns:
        Path: Absolute path to repo root.

    Raises:
        RuntimeError: If no .git directory is found.
    """
    current = Path(start_path or Path.cwd()).resolve()

    # Traverse upward until .git is found
    for parent in [current, *current.parents]:
        if (parent / ".git").exists():
            return parent

    raise RuntimeError("Not a git repository: could not find .git directory")

--- END OF FILE ./src/shared/path_utils.py ---

--- START OF FILE ./src/shared/schemas/manifest_validator.py ---
# src/shared/schemas/manifest_validator.py
"""
Provides utilities for validating manifest entries against JSON schemas using jsonschema.
"""

from __future__ import annotations

import json
from typing import Any, Dict, List, Tuple

import jsonschema

from shared.path_utils import get_repo_root

# --- THIS IS THE FIX ---
# The single source of truth for the location of constitutional schemas.
SCHEMA_DIR = get_repo_root() / ".intent" / "charter" / "schemas"
# --- END OF FIX ---


# ID: cfab52b8-8fed-4536-bc75-ed81a1161331
def load_schema(schema_name: str) -> Dict[str, Any]:
    """
    Load a JSON schema from the .intent/schemas/ directory.

    Args:
        schema_name (str): The filename of the schema (e.g., 'knowledge_graph_entry.schema.json').

    Returns:
        Dict[str, Any]: The loaded JSON schema.

    Raises:
        FileNotFoundError: If the schema file is not found.
        json.JSONDecodeError: If the schema file is not valid JSON.
    """
    schema_path = SCHEMA_DIR / schema_name

    if not schema_path.exists():
        raise FileNotFoundError(f"Schema file not found: {schema_path}")

    try:
        with open(schema_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        raise json.JSONDecodeError(
            f"Invalid JSON in schema file {schema_path}: {e.msg}", e.doc, e.pos
        )


# ID: 047e2cb8-1e18-4175-9be2-1017a2fba3d7
def validate_manifest_entry(
    entry: Dict[str, Any], schema_name: str = "knowledge_graph_entry.schema.json"
) -> Tuple[bool, List[str]]:
    """
    Validate a single manifest entry against a schema.

    Args:
        entry: The dictionary representing a single function/class entry.
        schema_name: The filename of the schema to validate against.

    Returns:
        A tuple of (is_valid: bool, list_of_error_messages: List[str]).
    """
    try:
        schema = load_schema(schema_name)
    except Exception as e:
        return False, [f"Failed to load schema '{schema_name}': {e}"]

    # Use Draft7Validator for compatibility with our schema definition.
    validator = jsonschema.Draft7Validator(schema)
    errors = []

    for error in validator.iter_errors(entry):
        # Create a user-friendly error message
        path = ".".join(str(p) for p in error.absolute_path) or "<root>"
        errors.append(f"Validation error at '{path}': {error.message}")

    is_valid = not errors
    return is_valid, errors

--- END OF FILE ./src/shared/schemas/manifest_validator.py ---

--- START OF FILE ./src/shared/time.py ---
# src/shared/time.py
"""
Lightweight time utilities shared across services.
Implements the canonical capability for a UTC ISO timestamp function.
"""

from __future__ import annotations

from datetime import datetime, timezone


# ID: 4f686bb3-7252-4f74-8e7c-d38a6ec85dc6
def now_iso() -> str:
    """Return current UTC timestamp in ISO 8601 format."""
    return datetime.now(timezone.utc).isoformat()


# A trivial change for testing.

--- END OF FILE ./src/shared/time.py ---

--- START OF FILE ./src/shared/utils/__init__.py ---
[EMPTY FILE]
--- END OF FILE ./src/shared/utils/__init__.py ---

--- START OF FILE ./src/shared/utils/alias_resolver.py ---
# src/system/guard/alias_resolver.py
"""
Provides a utility for loading and resolving capability aliases from the
constitutionally-defined alias map.

If the alias file is missing or unreadable, this resolver degrades gracefully:
- it logs at DEBUG (not WARNING/ERROR), and
- it returns the identity (no aliasing).
"""

from __future__ import annotations

from pathlib import Path
from typing import Dict, Optional

from shared.config import settings
from shared.config_loader import load_yaml_file
from shared.logger import getLogger

log = getLogger("core_admin.alias_resolver")

__all__ = ["AliasResolver"]


# ID: b0a5e5d3-c7fa-4502-b457-d38addc0e922
class AliasResolver:
    """Loads and resolves capability aliases."""

    def __init__(self, alias_file_path: Optional[Path] = None):
        """
        Initializes the resolver by loading the alias map from the constitution.
        Defaults to reports/aliases.yaml.
        """
        self.alias_map: Dict[str, str] = {}
        path = alias_file_path or (settings.REPO_PATH / "reports" / "aliases.yaml")

        if path.exists():
            try:
                data = load_yaml_file(path)
                self.alias_map = (
                    data.get("aliases", {}) if isinstance(data, dict) else {}
                )
                log.info(
                    "Loaded %d capability aliases from %s.",
                    len(self.alias_map),
                    path,
                )
            except Exception as e:
                # Degrade silently to identity behavior
                self.alias_map = {}
                log.debug(
                    "Failed to load alias map from %s (%s). Proceeding without aliases.",
                    path,
                    e,
                )
        else:
            # No file present -> identity behavior without noise
            self.alias_map = {}
            log.debug(
                "Alias map not found at %s; proceeding without aliases.",
                path,
            )

    # ID: ebad6cf5-b36f-4c50-8e3b-eb2d1c33f289
    def resolve(self, key: str) -> str:
        """
        Resolves a capability key to its canonical name using the alias map.
        If the key is not an alias, it returns the original key.
        """
        return self.alias_map.get(key, key)

--- END OF FILE ./src/shared/utils/alias_resolver.py ---

--- START OF FILE ./src/shared/utils/common_knowledge.py ---
# src/shared/utils/common_knowledge.py
# ID: 7c05b6a1-0b1d-5c9a-9b5f-1cce01b9f8a7
"""
Constitutional “common knowledge” – ultra-reusable micro-functions.
Import from here *before* inventing yet another helper.
"""

import re


# ID: c494b539-a653-4d46-b627-94a90698a832
def action_name() -> str:
    """Return the canonical action name string for handlers."""
    return "action_name"


# ID: 15ad7ea3-9219-4e6e-8b4c-644ac781ea1b
def sanitize_key(raw: str) -> str:
    """Lower-case, underscore, alnum only."""
    return re.sub(r"[^0-9a-zA-Z]+", "_", raw).lower()


# ID: 78f9e52e-43e0-4942-bbc4-c4d3784553ee
def normalize_text(text: str) -> str:
    """Collapse whitespace and strip."""
    return re.sub(r"\s+", " ", text).strip()

--- END OF FILE ./src/shared/utils/common_knowledge.py ---

--- START OF FILE ./src/shared/utils/constitutional_parser.py ---
# src/shared/utils/constitutional_parser.py
"""
Parses the constitutional structure definition from meta.yaml to discover all declared file paths.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Set


# ID: ae492732-1dab-4982-a129-1f7f9af67439
def get_all_constitutional_paths(meta_content: dict, intent_dir: Path) -> Set[str]:
    """
    Recursively discovers all declared constitutional file paths from the parsed
    content of meta.yaml.

    Args:
        meta_content: The dictionary parsed from meta.yaml.
        intent_dir: The path to the .intent directory.

    Returns:
        A set of repo-relative paths (e.g., '.intent/charter/policies/safety_policy.yaml').
    """
    repo_root = intent_dir.parent
    # The path to meta.yaml is known relative to the intent_dir
    known_paths: Set[str] = {
        str((intent_dir / "meta.yaml").relative_to(repo_root)).replace("\\", "/")
    }

    def _recursive_find(data: Any):
        if isinstance(data, dict):
            for value in data.values():
                _recursive_find(value)
        elif isinstance(data, list):
            for item in data:
                _recursive_find(item)
        elif (
            isinstance(data, str)
            and (intent_dir.name not in data)
            and ("/" in data or "\\" in data)
        ):
            # --- THIS IS THE DEFINITIVE FIX ---
            # All paths are constructed relative to the provided intent_dir,
            # removing the hardcoded ".intent".
            full_path = intent_dir / data
            known_paths.add(str(full_path.relative_to(repo_root)).replace("\\", "/"))
            # --- END OF FIX ---

    _recursive_find(meta_content)
    return known_paths

--- END OF FILE ./src/shared/utils/constitutional_parser.py ---

--- START OF FILE ./src/shared/utils/crypto.py ---
# src/shared/utils/crypto.py
"""
Provides shared, constitutionally-governed cryptographic utilities for
tasks like signing and token generation.
"""

from __future__ import annotations

import json
from typing import Any, Dict

from cryptography.hazmat.primitives import hashes


def _get_canonical_payload(proposal: Dict[str, Any]) -> str:
    """
    Creates a stable, sorted JSON string of the proposal's core intent,
    ignoring all other metadata like signatures. This is the single source
    of truth for what gets signed.
    """
    signable_data = {
        "target_path": proposal.get("target_path"),
        "action": proposal.get("action"),
        "justification": proposal.get("justification"),
        "content": proposal.get("content", ""),
    }
    return json.dumps(signable_data, sort_keys=True)


# ID: 38528901-21cb-4bbb-9f77-524beefdf990
def generate_approval_token(proposal: Dict[str, Any]) -> str:
    """
    Produces a deterministic token based on a canonical representation
    of the proposal's intent.
    """
    canonical_string = _get_canonical_payload(proposal)
    digest = hashes.Hash(hashes.SHA256())
    digest.update(canonical_string.encode("utf-8"))

    return f"core-proposal-v6:{digest.finalize().hex()}"

--- END OF FILE ./src/shared/utils/crypto.py ---

--- START OF FILE ./src/shared/utils/embedding_utils.py ---
# src/shared/utils/embedding_utils.py
"""
Provides utilities for handling text embeddings, including chunking and aggregation.
This module ensures that large documents can be processed reliably by embedding models.
"""

from __future__ import annotations

import asyncio
import hashlib
import os
from typing import List, Optional, Protocol

import httpx
import numpy as np

from shared.logger import getLogger
from shared.utils.common_knowledge import normalize_text

log = getLogger("embedding_utils")

# A reasonable chunk size to avoid overwhelming the embedding model
DEFAULT_CHUNK_SIZE = 512
DEFAULT_CHUNK_OVERLAP = 50


# ID: f48d93d3-7ddf-4df2-8c10-dc63311b9485
class Embeddable(Protocol):
    """Defines the interface for any service that can create embeddings."""

    # ID: ac2f3e7e-34f5-44b6-80b1-dce2e7160c2e
    async def get_embedding(self, text: str) -> List[float]: ...


class _Adapter:
    """Internal adapter to make EmbeddingService conform to the Embeddable protocol."""

    def __init__(self, service):
        self._service = service

    # ID: 5a628ba4-df9b-4e8e-9ecc-9e74dc125b1f
    async def get_embedding(self, text: str) -> List[float]:
        return await self._service.get_embedding(text)


def _chunk_text(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    """Splits text into overlapping chunks."""
    if not text:
        return []

    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += max(1, chunk_size - chunk_overlap)
    return chunks


# The duplicated normalize_text function has been removed. It is now imported.


# ID: 46703a51-3079-42fe-9bf7-e9724b009949
def sha256_hex(text: str) -> str:
    """Computes the SHA256 hex digest for a string."""
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


# =========================
# Provider-aware embedder
# =========================


# ID: 9b4a4f28-1e0a-4a2e-9c3f-9a1a5e3b3c1a
class EmbeddingService:
    """
    Provider-aware embedding client that conforms to the Embeddable protocol.

    - Ollama:   POST {base}/api/embeddings with {model, prompt}
    - OpenAI/DeepSeek: POST {base}/v1/embeddings with {model, input} (+ Authorization)
    """

    def __init__(
        self,
        provider: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        timeout: float = 30.0,
        api_key: Optional[str] = None,
    ) -> None:
        # Resolve provider + base/model from env with sensible defaults
        self.provider = (
            provider or os.getenv("EMBEDDINGS_PROVIDER") or "ollama"
        ).lower()

        if self.provider == "ollama":
            self.base = (
                base_url
                or os.getenv("EMBEDDINGS_API_BASE")
                or os.getenv("LOCAL_EMBEDDING_API_URL")
                or "http://localhost:11434"
            ).rstrip("/")
            self.model = (
                model
                or os.getenv("EMBEDDINGS_MODEL")
                or os.getenv("LOCAL_EMBEDDING_MODEL_NAME")
                or "nomic-embed-text"
            )
            self.endpoint = "/api/embeddings"
            self.headers = {"Content-Type": "application/json"}
            self._payload = lambda text: {"model": self.model, "prompt": text}
            self._extract = lambda data: data.get("embedding")
        else:
            # Treat anything else as OpenAI-compatible (DeepSeek/API keys supported)
            self.base = (
                base_url
                or os.getenv("DEEPSEEK_EMBEDDING_API_URL")
                or os.getenv("OPENAI_API_BASE")
                or "https://api.openai.com"
            ).rstrip("/")
            self.model = (
                model
                or os.getenv("DEEPSEEK_EMBEDDING_MODEL_NAME")
                or os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
            )
            key = (
                api_key
                or os.getenv("DEEPSEEK_EMBEDDING_API_KEY")
                or os.getenv("OPENAI_API_KEY")
            )
            self.endpoint = "/v1/embeddings"
            self.headers = {"Content-Type": "application/json"}
            if key:
                self.headers["Authorization"] = f"Bearer {key}"
            self._payload = lambda text: {"model": self.model, "input": text}
            self._extract = lambda data: (data.get("data") or [{}])[0].get("embedding")

        self.timeout = timeout
        log.info(f"EmbeddingService initialized for API at {self.base}")

    # ID: 8f5d2d61-1a1a-4e21-9c69-7a9e1d0ec0ab
    async def get_embedding(self, text: str) -> List[float]:
        """Return a single embedding vector for the given text."""
        url = f"{self.base}{self.endpoint}"
        payload = self._payload(text)

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            resp = await client.post(url, json=payload, headers=self.headers)

        if resp.status_code != 200:
            # Keep this error text shape (it matches what you saw in logs)
            log.error(
                f"HTTP error from embedding API: {resp.status_code} - {resp.text}"
            )
            raise RuntimeError(f"Embedding API HTTP {resp.status_code}")

        data = resp.json()
        vec = self._extract(data)
        if not vec:
            log.error("Embedding service returned no vector.")
            raise RuntimeError("No vector returned from embedding service")
        return vec  # type: ignore[return-value]


# ID: 6a1b6cde-0d0a-4e7c-8c4d-4f9a4fe0d6d1
def build_embedder_from_env() -> Embeddable:
    """
    Factory: builds an Embeddable using environment variables.
    This avoids a module-level `get_embedding` symbol (which caused duplication warnings).
    """
    return _Adapter(EmbeddingService())


# ID: 95c51c61-f288-483c-b76e-2915765004da
async def chunk_and_embed(
    embedder: Embeddable,
    text: str,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,
) -> np.ndarray:
    """
    Chunks text, gets embeddings for each chunk in parallel, and returns the
    averaged embedding vector for the entire text.
    """
    text = normalize_text(text)
    chunks = _chunk_text(text, chunk_size, chunk_overlap)
    if not chunks:
        # Should not happen with valid text, but as a safeguard
        raise ValueError("Cannot generate embedding for empty text.")

    embedding_tasks = [embedder.get_embedding(chunk) for chunk in chunks]
    chunk_vectors = await asyncio.gather(*embedding_tasks)

    # Convert list of lists to a 2D numpy array for easy averaging
    vector_array = np.array(chunk_vectors, dtype=np.float32)

    # Calculate the mean vector across the chunk dimension (axis=0)
    mean_vector = np.mean(vector_array, axis=0)

    # Normalize the final vector to unit length
    norm = np.linalg.norm(mean_vector)
    if norm == 0:
        return mean_vector  # Avoid division by zero

    normalized_vector = mean_vector / norm
    return normalized_vector

--- END OF FILE ./src/shared/utils/embedding_utils.py ---

--- START OF FILE ./src/shared/utils/header_tools.py ---
# src/shared/utils/header_tools.py
"""
Provides a deterministic tool for parsing and reconstructing Python file headers
according to CORE's constitutional style guide.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Optional


@dataclass
# ID: 4a498b02-ef0b-4ce2-bd66-d8289669cd8f
class HeaderComponents:
    """A data class to hold the parsed components of a Python file header."""

    location: Optional[str] = None
    module_description: Optional[str] = None
    has_future_import: bool = False
    other_imports: List[str] = field(default_factory=list)
    body: List[str] = field(default_factory=list)


# ID: 3f524d93-83cd-41bd-b5e2-38a7703d39d4
class HeaderTools:
    """A stateless utility class for parsing and reconstructing file headers."""

    @staticmethod
    # ID: 8f8fa33d-1ab8-4ee8-8dc7-a71355167611
    def parse(source_code: str) -> HeaderComponents:
        """Parses the source code and extracts header components."""
        components = HeaderComponents()
        lines = source_code.splitlines()
        state = "start"
        docstring_lines = []

        for line in lines:
            if state == "start":
                if line.strip().startswith("#") and ("/" in line or "\\" in line):
                    components.location = line
                    state = "location_found"
                else:  # No header found, treat everything as body
                    components.body.append(line)
                    state = "body_started"

            elif state == "location_found":
                if not line.strip():
                    continue  # Skip blank lines
                if '"""' in line or "'''" in line:
                    docstring_lines.append(line)
                    if line.count('"""') == 2 or line.count("'''") == 2:
                        state = "docstring_done"
                    else:
                        state = "in_docstring"
                else:
                    components.body.append(line)
                    state = "body_started"

            elif state == "in_docstring":
                docstring_lines.append(line)
                if '"""' in line or "'''" in line:
                    state = "docstring_done"

            elif state == "docstring_done":
                if not line.strip():
                    continue
                if "from __future__ import annotations" in line:
                    components.has_future_import = True
                    state = "future_import_found"
                else:
                    components.body.append(line)
                    state = "body_started"

            elif state == "future_import_found":
                if line.strip().startswith("from") or line.strip().startswith("import"):
                    components.other_imports.append(line)
                else:
                    components.body.append(line)
                    state = "body_started"

            elif state == "body_started":
                components.body.append(line)

        if docstring_lines:
            components.module_description = "\n".join(docstring_lines)

        # --- START OF FIX ---
        # Strip future import from body if it was misplaced and rename variable 'l' to 'line'.
        body_without_future = [
            line
            for line in components.body
            if "from __future__ import annotations" not in line
        ]
        # --- END OF FIX ---
        if len(body_without_future) < len(components.body):
            components.has_future_import = True
            components.body = body_without_future

        return components

    @staticmethod
    # ID: e85d9dde-b46f-43f7-b83f-106a63103c48
    def reconstruct(components: HeaderComponents) -> str:
        """Reconstructs the source code from its parsed components."""
        parts = []
        if components.location:
            parts.append(components.location)

        if components.module_description:
            if parts:
                parts.append("")
            parts.append(components.module_description)

        if components.has_future_import:
            if parts:
                parts.append("")
            parts.append("from __future__ import annotations")

        if components.other_imports:
            parts.extend(components.other_imports)

        if components.body:
            if parts and (parts[-1] != "" or components.body[0] != ""):
                parts.append("")
            parts.extend(components.body)

        return "\n".join(parts) + "\n"

--- END OF FILE ./src/shared/utils/header_tools.py ---

--- START OF FILE ./src/shared/utils/import_scanner.py ---
# src/shared/utils/import_scanner.py
"""
Scans Python files to extract top-level import statements.
"""

from __future__ import annotations

import ast
from pathlib import Path
from typing import List

from shared.logger import getLogger

log = getLogger(__name__)


# ID: fd3f890e-c234-4942-afb0-3b7551d393b9
def scan_imports_for_file(file_path: Path) -> List[str]:
    """
    Parse a Python file and extract all imported module paths.

    Args:
        file_path (Path): Path to the file.

    Returns:
        List[str]: List of imported module paths.
    """
    imports = []
    try:
        source = file_path.read_text(encoding="utf-8")
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)

    except Exception as e:
        log.warning(f"Failed to scan imports for {file_path}: {e}", exc_info=True)

    return imports

--- END OF FILE ./src/shared/utils/import_scanner.py ---

--- START OF FILE ./src/shared/utils/manifest_aggregator.py ---
# src/shared/utils/manifest_aggregator.py

"""
Aggregates domain-specific capability definitions from the constitution into a unified view.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict

import yaml

from shared.logger import getLogger

log = getLogger("manifest_aggregator")


# ID: 5f0549df-0ce5-468a-a232-c61663724a77
def aggregate_manifests(repo_root: Path) -> Dict[str, Any]:
    """
    Finds all domain-specific capability definition YAML files and merges them.
    This is "canary-aware": if a 'reports/proposed_manifests' directory
    exists, it will be used as the source of truth instead of the live
    '.intent/knowledge/domains' manifests.

    Args:
        repo_root (Path): The absolute path to the repository root.

    Returns:
        A dictionary representing the aggregated manifest.
    """
    # --- THIS IS THE FIX: Changed from log.info to log.debug ---
    log.debug(
        "🔍 Starting manifest aggregation by searching all constitutional sources..."
    )
    all_capabilities = []
    manifests_found = 0

    proposed_manifests_dir = repo_root / "reports" / "proposed_manifests"
    live_manifests_dir = repo_root / ".intent" / "knowledge" / "domains"

    if proposed_manifests_dir.is_dir() and any(proposed_manifests_dir.iterdir()):
        search_dir = proposed_manifests_dir
        log.warning(
            "   -> ⚠️ Found proposed manifests. Auditor will use these for validation."
        )
    else:
        search_dir = live_manifests_dir

    if search_dir.is_dir():
        for domain_file in sorted(search_dir.glob("*.yaml")):
            manifests_found += 1
            log.debug(f"   -> Loading capabilities from: {domain_file.name}")
            try:
                domain_manifest = yaml.safe_load(domain_file.read_text()) or {}
                if "tags" in domain_manifest and isinstance(
                    domain_manifest["tags"], list
                ):
                    all_capabilities.extend(domain_manifest["tags"])
            except yaml.YAMLError as e:
                log.error(
                    f"   -> ❌ Skipping invalid YAML file: {domain_file.name} - {e}"
                )
                continue

    log.debug(f"   -> Aggregated capabilities from {manifests_found} domain manifests.")

    monolith_path = repo_root / ".intent" / "project_manifest.yaml"
    monolith_data = {}
    if monolith_path.exists():
        monolith_data = yaml.safe_load(monolith_path.read_text())

    unique_caps = set()
    for item in all_capabilities:
        if isinstance(item, str):
            unique_caps.add(item)
        elif isinstance(item, dict) and "key" in item:
            unique_caps.add(item["key"])

    unique_caps.update(monolith_data.get("required_capabilities", []))

    return {
        "name": monolith_data.get("name", "CORE"),
        "intent": monolith_data.get("intent", "No intent provided."),
        "active_agents": monolith_data.get("active_agents", []),
        "required_capabilities": sorted(list(unique_caps)),
    }

--- END OF FILE ./src/shared/utils/manifest_aggregator.py ---

--- START OF FILE ./src/shared/utils/parallel_processor.py ---
# src/shared/utils/parallel_processor.py
"""
Provides a reusable, throttled parallel processor for running async tasks
concurrently with a progress bar, governed by a constitutional limit.
"""

from __future__ import annotations

import asyncio
from typing import Awaitable, Callable, List, TypeVar

from rich.progress import track

from shared.config import settings
from shared.logger import getLogger

log = getLogger("parallel_processor")
T = TypeVar("T")
R = TypeVar("R")


# ID: c88b0e64-3e38-4fef-983e-cd59281e53e0
class ThrottledParallelProcessor:
    """
    A dedicated executor for running a worker function over a list of items
    in parallel, with concurrency limited by the constitution.
    """

    def __init__(self, description: str = "Processing items..."):
        """
        Initializes the processor.
        """
        self.concurrency_limit = settings.CORE_MAX_CONCURRENT_REQUESTS
        self.description = description
        log.info(
            f"ThrottledParallelProcessor initialized with concurrency limit: "
            f"{self.concurrency_limit}"
        )

    async def _process_items_async(
        self, items: List[T], worker_fn: Callable[[T], Awaitable[R]]
    ) -> List[R]:
        """The core async logic for processing items in parallel."""
        semaphore = asyncio.Semaphore(self.concurrency_limit)
        results = []

        async def _worker(item: T) -> R:
            async with semaphore:
                return await worker_fn(item)

        tasks = [asyncio.create_task(_worker(item)) for item in items]

        # Use track for a visual progress bar in the console
        for task in track(
            asyncio.as_completed(tasks), description=self.description, total=len(items)
        ):
            results.append(await task)

        return results

    # --- START: THE DEFINITIVE FIX ---
    # ID: dee1af19-41c8-49c6-ba11-a109746795b7
    async def run_async(
        self, items: List[T], worker_fn: Callable[[T], Awaitable[R]]
    ) -> List[R]:
        """
        Asynchronous entry point to run the worker over all items.
        To be used when called from an already-running async function.
        """
        return await self._process_items_async(items, worker_fn)

    # ID: 466317ce-4caa-4c49-a466-5389d9c25874
    def run_sync(
        self, items: List[T], worker_fn: Callable[[T], Awaitable[R]]
    ) -> List[R]:
        """
        Synchronous entry point to run the async worker over all items.
        This will start and manage its own asyncio event loop.
        """
        return asyncio.run(self._process_items_async(items, worker_fn))

    # --- END: THE DEFINITIVE FIX ---

--- END OF FILE ./src/shared/utils/parallel_processor.py ---

--- START OF FILE ./src/shared/utils/parsing.py ---
# src/shared/utils/parsing.py
"""
Shared utilities for parsing structured data from unstructured text,
primarily from Large Language Model (LLM) outputs.
"""

from __future__ import annotations

import json
import re
from typing import Dict, List, Optional, Union


# ID: f2bd2480-f310-4090-ac1a-58ce05bfc4d3
def extract_json_from_response(text: str) -> Optional[Union[Dict, List]]:
    """
    Extracts a JSON object or array from a raw text response, making it robust
    against common LLM formatting issues like introductory text.

    Args:
        text: Raw text response that may contain JSON.

    Returns:
        Parsed JSON as a dictionary or list, or None if no valid JSON found.
    """
    # 1. Try to extract JSON from markdown code blocks
    json_data = _extract_from_markdown(text)
    if json_data is not None:
        return json_data

    # 2. Fallback: Find raw JSON by matching braces/brackets
    return _extract_raw_json(text)


def _extract_from_markdown(text: str) -> Optional[Union[Dict, List]]:
    """
    Attempts to extract JSON from a markdown code block.

    Args:
        text: Text that may contain a markdown JSON block.

    Returns:
        Parsed JSON or None if extraction fails.
    """
    pattern = r"```(?:json)?\s*(\{[\s\S]*?\}|\[[\s\S]*?\])\s*```"
    match = re.search(pattern, text, re.DOTALL)

    if not match:
        return None

    try:
        return json.loads(match.group(1))
    except json.JSONDecodeError:
        return None


def _extract_raw_json(text: str) -> Optional[Union[Dict, List]]:
    """
    Extracts JSON by finding the outermost braces or brackets.
    Robust against extra text before or after the JSON.

    Args:
        text: Text that may contain raw JSON.

    Returns:
        Parsed JSON or None if extraction fails.
    """
    first_brace = text.find("{")
    first_bracket = text.find("[")

    # No JSON markers found
    if first_brace == -1 and first_bracket == -1:
        return None

    # Determine which comes first: object or array
    if first_brace != -1 and (first_bracket == -1 or first_brace < first_bracket):
        end_char = "}"
        start_index = first_brace
    else:
        end_char = "]"
        start_index = first_bracket

    # Find the matching closing character
    last_index = text.rfind(end_char)
    if last_index <= start_index:
        return None

    try:
        json_str = text[start_index : last_index + 1]
        return json.loads(json_str)
    except (json.JSONDecodeError, ValueError):
        return None


# ID: 853be68b-f2d4-4494-bf4c-98200bc08026
def parse_write_blocks(text: str) -> Dict[str, str]:
    """
    Parses a string for one or more [[write:file_path]]...[[/write]] blocks.

    Args:
        text: The raw string output from an LLM.

    Returns:
        A dictionary where keys are file paths and values are the code blocks.

    Example:
        >>> text = "[[write:test.py]]\\nprint('hello')\\n[[/write]]"
        >>> parse_write_blocks(text)
        {'test.py': "print('hello')"}
    """
    pattern = r"\[\[write:(.+?)\]\]\s*\n(.*?)\n\s*\[\[/write\]\]"
    matches = re.findall(pattern, text, re.DOTALL)
    return {path.strip(): content.strip() for path, content in matches}

--- END OF FILE ./src/shared/utils/parsing.py ---

--- START OF FILE ./src/shared/utils/subprocess_utils.py ---
# src/shared/utils/subprocess_utils.py
"""
Provides shared utilities for running external commands as subprocesses.
"""

from __future__ import annotations

import shutil
import subprocess

import typer
from rich.console import Console

from shared.logger import getLogger

log = getLogger("subprocess_utils")
console = Console()


# ID: 71034daa-0b93-4c24-910b-d1413b470795
def run_poetry_command(description: str, command: list[str]):
    """Helper to run a command via Poetry, log it, and handle errors."""
    POETRY_EXECUTABLE = shutil.which("poetry")
    if not POETRY_EXECUTABLE:
        log.error("❌ Could not find 'poetry' executable in your PATH.")
        raise typer.Exit(code=1)

    typer.secho(f"\n{description}", bold=True)
    full_command = [POETRY_EXECUTABLE, "run", *command]
    try:
        result = subprocess.run(
            full_command, check=True, text=True, capture_output=True
        )
        if result.stdout:
            console.print(result.stdout)
        if result.stderr:
            console.print(f"[yellow]{result.stderr}[/yellow]")
    except subprocess.CalledProcessError as e:
        log.error(f"\n❌ Command failed: {' '.join(full_command)}")
        if e.stdout:
            console.print(e.stdout)
        if e.stderr:
            console.print(f"[bold red]{e.stderr}[/bold red]")
        raise typer.Exit(code=1)

--- END OF FILE ./src/shared/utils/subprocess_utils.py ---

--- START OF FILE ./src/shared/utils/yaml_processor.py ---
# src/shared/utils/yaml_processor.py
"""

Centralized YAML processor for constitutional compliance, providing consistent
parsing and validation of .intent/ files across all governance checks and tools.

This utility enforces dry_by_design by eliminating duplicate YAML loading logic
and provides constitutional features like:
- Safe loading with error context
- Duplicate key tolerance for diagnostic tools
- Schema validation hooks for future use
- Audit-friendly error reporting

All governance checks (manifest_lint, domain_placement, etc.) use this processor
to ensure consistent behavior and error handling across the constitutional audit
pipeline.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, Optional

from ruamel.yaml import YAML

from shared.logger import getLogger

log = getLogger("yaml_processor")


# ID: b824d032-49d4-486b-9182-99a76c78843f
class YAMLProcessor:
    """Centralized YAML processor for constitutional file operations."""

    def __init__(self, allow_duplicates: bool = False) -> None:
        """Initialize the YAML processor with constitutional configuration.

        Args:
            allow_duplicates: If True, allows duplicate keys for diagnostic tools
                             (default: False for strict constitutional compliance)
        """
        self.allow_duplicates = allow_duplicates
        self.yaml = YAML(typ="safe")
        if allow_duplicates:
            self.yaml.allow_duplicate_keys = True
            log.debug(
                "YAML processor configured for duplicate key tolerance (diagnostic mode)"
            )
        else:
            log.debug("YAML processor configured for strict constitutional compliance")

    # ID: 78d97bea-bfa3-49b2-ba62-8e4093d84fb0
    def load(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """Load and parse a constitutional YAML file with error context.

        This is the single entry point for all YAML loading in governance checks,
        ensuring consistent error handling and logging.

        Args:
            file_path: Path to the .intent/ YAML file (e.g., domain manifests, policies)

        Returns:
            Parsed YAML content as dict, or None if file doesn't exist

        Raises:
            ValueError: If file exists but has invalid YAML structure
            OSError: If file system errors occur during reading
        """
        if not file_path.exists():
            log.debug(f"YAML file not found (non-error): {file_path}")
            return None

        try:
            log.debug(f"Loading YAML from: {file_path}")
            with file_path.open("r", encoding="utf-8") as f:
                content = self.yaml.load(f)

            if content is None:
                log.warning(f"YAML file is empty: {file_path}")
                return {}

            if not isinstance(content, dict):
                raise ValueError(
                    f"YAML root must be a mapping (dict), got {type(content).__name__}: {file_path}"
                )

            log.debug(f"Successfully loaded YAML: {file_path} ({len(content)} keys)")
            return content

        except Exception as e:
            log.error(f"YAML parsing failed for {file_path}: {e}")
            raise ValueError(
                f"Failed to parse constitutional YAML {file_path}: {e}"
            ) from e

    # ID: 7e913478-a8e9-4e75-bd12-54f2264487c6
    def load_strict(self, file_path: Path) -> Dict[str, Any]:
        """Load YAML with strict constitutional validation (no duplicate keys).

        Use for policy files and schemas where duplicate keys indicate errors.

        Args:
            file_path: Path to the .intent/ YAML file

        Returns:
            Parsed YAML content as dict

        Raises:
            ValueError: If file doesn't exist, has invalid structure, or contains duplicate keys
        """
        if self.allow_duplicates:
            raise ValueError(
                "Cannot use strict mode with duplicate key tolerance enabled"
            )

        content = self.load(file_path)
        if content is None:
            raise ValueError(f"Required constitutional file missing: {file_path}")

        return content

    # ID: 91acfb90-7639-41f3-b1cc-064e7d8a0d46
    def dump(self, data: Dict[str, Any], file_path: Path) -> None:
        """Write YAML content with constitutional formatting.

        Ensures consistent formatting for .intent/ files, preserving order and
        avoiding unnecessary whitespace.

        Args:
            data: Dict to write as YAML
            file_path: Path to write the YAML file

        Raises:
            OSError: If file system errors occur during writing
        """
        file_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            log.debug(f"Dumping YAML to: {file_path}")
            with file_path.open("w", encoding="utf-8") as f:
                self.yaml.dump(data, f)
            log.debug(f"Successfully wrote YAML: {file_path}")
        except Exception as e:
            log.error(f"YAML write failed for {file_path}: {e}")
            raise OSError(
                f"Failed to write constitutional YAML {file_path}: {e}"
            ) from e


# Global instance for convenience (used by all governance checks)
# This follows the constitutional pattern of shared singletons for utilities
yaml_processor = YAMLProcessor(
    allow_duplicates=True
)  # Diagnostic tools need duplicate tolerance

# Strict processor for policy/schema validation
strict_yaml_processor = YAMLProcessor(allow_duplicates=False)

--- END OF FILE ./src/shared/utils/yaml_processor.py ---

--- START OF FILE ./tests/__init__.py ---
# This file makes the 'tests' directory a Python package.

--- END OF FILE ./tests/__init__.py ---

--- START OF FILE ./tests/admin/test_guard_drift_cli.py ---
# tests/admin/test_guard_drift_cli.py
from __future__ import annotations

from pathlib import Path
from unittest.mock import AsyncMock

import pytest

from features.introspection.drift_service import run_drift_analysis_async
from shared.models import CapabilityMeta


def write(p: Path, text: str) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(text, encoding="utf-8")


@pytest.mark.anyio
async def test_drift_analysis_clean(tmp_path: Path, mocker):
    """
    Tests that drift analysis reports a clean state when manifest and
    (mocked) code capabilities are in sync.
    """
    # ARRANGE
    # Mock the two data sources the service uses.
    # --- THIS IS THE FIX ---
    # The mocked return value MUST be a dictionary mapping strings to CapabilityMeta instances.
    mocker.patch(
        "features.introspection.drift_service.load_manifest_capabilities",
        return_value={
            "alpha.cap": CapabilityMeta(key="alpha.cap"),
            "beta.cap": CapabilityMeta(key="beta.cap"),
        },
    )
    # --- END OF FIX ---

    mock_graph_data = {
        "symbols": {
            "file1::func_a": {"key": "alpha.cap"},
            "file2::func_b": {"key": "beta.cap"},
        }
    }
    mocker.patch(
        "core.knowledge_service.KnowledgeService.get_graph",
        new_callable=AsyncMock,
        return_value=mock_graph_data,
    )

    # ACT
    report = await run_drift_analysis_async(tmp_path)

    # ASSERT
    assert not report.missing_in_code
    assert not report.undeclared_in_manifest
    assert not report.mismatched_mappings


@pytest.mark.anyio
async def test_drift_analysis_detects_drift(tmp_path: Path, mocker):
    """
    Tests that drift analysis correctly identifies discrepancies between
    the manifest and the (mocked) code capabilities.
    """
    # ARRANGE
    # Mock the manifest to declare one capability
    # --- THIS IS THE FIX ---
    mocker.patch(
        "features.introspection.drift_service.load_manifest_capabilities",
        return_value={"manifest.only.cap": CapabilityMeta(key="manifest.only.cap")},
    )
    # --- END OF FIX ---

    # Mock the code scan to find a different capability
    mock_graph_data = {"symbols": {"file1::func_a": {"key": "code.only.cap"}}}
    mocker.patch(
        "core.knowledge_service.KnowledgeService.get_graph",
        new_callable=AsyncMock,
        return_value=mock_graph_data,
    )

    # ACT
    report = await run_drift_analysis_async(tmp_path)

    # ASSERT
    assert report.missing_in_code == ["manifest.only.cap"]
    assert report.undeclared_in_manifest == ["code.only.cap"]

--- END OF FILE ./tests/admin/test_guard_drift_cli.py ---

--- START OF FILE ./tests/api/__init__.py ---
# This file makes the 'api' subdirectory a Python package.

--- END OF FILE ./tests/api/__init__.py ---

--- START OF FILE ./tests/api/test_knowledge_api.py ---
# tests/api/test_knowledge_api.py
import pytest
from httpx import ASGITransport, AsyncClient
from services.database.models import Capability
from sqlalchemy import insert


@pytest.mark.anyio
async def test_list_capabilities_endpoint(mock_core_env, get_test_session, mocker):
    from core.main import create_app

    # Seed the database with a capability for the service to find
    async with get_test_session as session:
        await session.execute(
            insert(Capability).values(
                name="test.cap", title="Test Cap", owner="test", domain="test"
            )
        )
        await session.commit()

    app = create_app()

    # Use AsyncClient with ASGITransport for FastAPI app testing
    transport = ASGITransport(app=app)
    async with AsyncClient(transport=transport, base_url="http://test") as client:
        # The lifespan context has started, but we're in test mode
        # so we need to manually initialize services
        await app.state.core_context.cognitive_service.initialize()
        await app.state.core_context.auditor_context.load_knowledge_graph()

        # Now make the request
        response = await client.get("/v1/knowledge/capabilities")
        assert response.status_code == 200
        assert response.json()["capabilities"] == ["test.cap"]

--- END OF FILE ./tests/api/test_knowledge_api.py ---

--- START OF FILE ./tests/conftest.py ---
# tests/conftest.py
"""
Central pytest configuration and fixtures for the CORE project.
"""

from __future__ import annotations

import asyncio
from pathlib import Path

import pytest
import sqlparse  # <-- ADD THIS IMPORT
import yaml
from shared.config import settings
from sqlalchemy import text
from sqlalchemy.ext.asyncio import (
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
)


@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for each test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture
def mock_fs_with_constitution(tmp_path: Path) -> Path:
    # This fixture is correct, no changes needed here.
    intent_dir = tmp_path / ".intent"
    charter_dir = intent_dir / "charter"
    policies_dir = charter_dir / "policies"
    agent_policies_dir = policies_dir / "agent"
    governance_policies_dir = policies_dir / "governance"
    mind_dir = intent_dir / "mind"
    prompts_dir = mind_dir / "prompts"
    for p in [agent_policies_dir, governance_policies_dir, prompts_dir]:
        p.mkdir(parents=True, exist_ok=True)
    (tmp_path / ".git").mkdir()
    meta_content = {
        "charter": {
            "policies": {
                "agent": {
                    "micro_proposal_policy": "charter/policies/agent/micro_proposal_policy.yaml",
                    "agent_policy": "charter/policies/agent/agent_policy.yaml",
                },
                "governance": {
                    "available_actions_policy": "charter/policies/governance/available_actions_policy.yaml"
                },
            }
        },
        "mind": {"prompts": {"planner_agent": "mind/prompts/planner_agent.prompt"}},
    }
    (intent_dir / "meta.yaml").write_text(yaml.dump(meta_content))
    available_actions_content = {
        "policy_id": "mock-uuid-actions",
        "id": "available_actions_policy",
        "version": "1.0.0",
        "title": "Mock Actions",
        "purpose": "...",
        "status": "active",
        "owners": {"primary": "test"},
        "review": {"frequency": "annual"},
        "actions": [
            {
                "name": "create_file",
                "description": "Creates a file.",
                "parameters": [{"name": "file_path", "type": "string"}],
            },
            {
                "name": "autonomy.self_healing.format_code",
                "description": "Formats code.",
                "parameters": [{"name": "file_path", "type": "string"}],
            },
            {
                "name": "system.dangerous.execute_shell",
                "description": "A dangerous action.",
                "parameters": [],
            },
        ],
    }
    (governance_policies_dir / "available_actions_policy.yaml").write_text(
        yaml.dump(available_actions_content)
    )
    micro_proposal_content = {
        "policy_id": "mock-uuid-micro",
        "id": "micro_proposal_policy",
        "version": "1.0.0",
        "title": "Mock Micro Proposal",
        "purpose": "...",
        "status": "active",
        "owners": {"primary": "test"},
        "review": {"frequency": "annual"},
        "rules": [
            {
                "id": "safe_actions",
                "description": "...",
                "enforcement": "error",
                "allowed_actions": ["autonomy.self_healing.format_code"],
            },
            {
                "id": "safe_paths",
                "description": "...",
                "enforcement": "error",
                "allowed_paths": ["src/safe_dir/*"],
                "forbidden_paths": [".intent/**"],
            },
        ],
    }
    (agent_policies_dir / "micro_proposal_policy.yaml").write_text(
        yaml.dump(micro_proposal_content)
    )
    agent_policy_content = {
        "policy_id": "mock-uuid-agent",
        "id": "agent_policy",
        "version": "1.0.0",
        "title": "Mock Agent Policy",
        "purpose": "...",
        "status": "active",
        "owners": {"primary": "test"},
        "review": {"frequency": "annual"},
        "rules": [],
        "execution_agent": {"max_correction_attempts": 2},
    }
    (agent_policies_dir / "agent_policy.yaml").write_text(
        yaml.dump(agent_policy_content)
    )
    (prompts_dir / "planner_agent.prompt").write_text("Plan for: {goal}")
    return tmp_path


@pytest.fixture
def mock_core_env(mock_fs_with_constitution: Path, monkeypatch) -> Path:
    monkeypatch.setattr(settings, "REPO_PATH", mock_fs_with_constitution)
    settings.initialize_for_test(mock_fs_with_constitution)
    return mock_fs_with_constitution


@pytest.fixture(scope="function")
async def get_test_session(monkeypatch) -> AsyncSession:
    db_url = settings.DATABASE_URL
    if not db_url or "testdb" not in db_url:
        pytest.fail(
            "DATABASE_URL for testing must be set and point to the 'testdb' database."
        )

    engine = create_async_engine(db_url, echo=False)
    TestSession = async_sessionmaker(
        engine, expire_on_commit=False, class_=AsyncSession
    )

    real_repo_root = Path(__file__).parent.parent
    schema_sql_path = real_repo_root / "sql" / "001_consolidated_schema.sql"
    if not schema_sql_path.exists():
        pytest.fail(f"Could not find schema file at {schema_sql_path}")
    schema_sql = schema_sql_path.read_text(encoding="utf-8")

    async with engine.begin() as conn:
        # --- THIS IS THE FIX ---
        # Split the SQL file into individual statements and execute them one by one.
        for statement in sqlparse.split(schema_sql):
            if statement.strip():
                await conn.execute(text(statement))
        # --- END OF FIX ---

    async with TestSession() as session:
        yield session

    async with engine.begin() as conn:
        await conn.execute(text("DROP SCHEMA IF EXISTS core CASCADE;"))

    await engine.dispose()

--- END OF FILE ./tests/conftest.py ---

--- START OF FILE ./tests/core/test_cognitive_service.py ---
# tests/core/test_cognitive_service.py
from contextlib import asynccontextmanager
from unittest.mock import AsyncMock

import pytest
from services.database.models import CognitiveRole, LlmResource
from sqlalchemy import insert


@pytest.mark.anyio
async def test_cognitive_service_selects_cheapest_model(
    mock_core_env, get_test_session, mocker
):
    from core.cognitive_service import CognitiveService

    @asynccontextmanager
    async def session_manager_mock():
        yield get_test_session

    mocker.patch("core.cognitive_service.get_session", session_manager_mock)

    # Mock the config_service - need to patch where it's imported
    async def mock_get(key, default=None):
        config_map = {
            "CHEAP_API_URL": "http://cheap.api",
            "CHEAP_API_KEY": "cheap_key",
            "CHEAP_MODEL_NAME": "cheap-model",
            "EXPENSIVE_API_URL": "http://expensive.api",
            "EXPENSIVE_API_KEY": "expensive_key",
            "EXPENSIVE_MODEL_NAME": "expensive-model",
        }
        return config_map.get(key, default)

    # Patch in the cognitive_service module where it's imported
    mock_config = mocker.patch("core.cognitive_service.config_service")
    mock_config.get = AsyncMock(side_effect=mock_get)

    resources_data = [
        {
            "name": "expensive_model",
            "env_prefix": "EXPENSIVE",
            "provided_capabilities": ["nlu"],
            "performance_metadata": {"cost_rating": 5},
        },
        {
            "name": "cheap_model",
            "env_prefix": "CHEAP",
            "provided_capabilities": ["nlu"],
            "performance_metadata": {"cost_rating": 1},
        },
    ]
    roles_data = [
        {
            "role": "TestRole",
            "required_capabilities": ["nlu"],
            "assigned_resource": None,
        }
    ]

    async with get_test_session as session:
        await session.execute(insert(LlmResource), resources_data)
        await session.execute(insert(CognitiveRole), roles_data)
        await session.commit()

    service = CognitiveService(mock_core_env)
    await service.initialize()

    client = await service.aget_client_for_role("TestRole")
    assert client.provider.model_name == "cheap-model"

--- END OF FILE ./tests/core/test_cognitive_service.py ---

--- START OF FILE ./tests/governance/test_local_mode_governance.py ---
# tests/governance/test_local_mode_governance.py
"""
Tests to ensure that CORE's governance principles are correctly
reflected in its configuration files.
"""

from shared.config_loader import load_yaml_file
from shared.path_utils import get_repo_root


def test_local_fallback_requires_git_checkpoint():
    """Ensure local_mode.yaml correctly enforces Git validation."""
    repo_root = get_repo_root()
    # --- THIS IS THE FIX ---
    # The config file now lives in the 'mind' directory.
    config_path = repo_root / ".intent" / "mind" / "config" / "local_mode.yaml"
    # --- END OF FIX ---

    # Check that the file actually exists before testing its content
    assert config_path.exists(), "local_mode.yaml configuration file is missing."

    config = load_yaml_file(config_path)

    # This is a critical safety check: local mode must not bypass Git commits.
    ignore_validation = config.get("apis", {}).get("git", {}).get("ignore_validation")
    assert (
        ignore_validation is False
    ), "CRITICAL: local_mode.yaml is configured to ignore Git validation."

--- END OF FILE ./tests/governance/test_local_mode_governance.py ---

--- START OF FILE ./tests/integration/test_full_run.py ---
# tests/integration/test_full_run.py
from contextlib import asynccontextmanager
from unittest.mock import AsyncMock

import pytest
from httpx import ASGITransport, AsyncClient
from services.database.models import CognitiveRole, LlmResource
from sqlalchemy import insert


@pytest.mark.anyio
async def test_execute_goal_end_to_end(mock_core_env, get_test_session, mocker):
    from core.main import create_app

    mock_develop = mocker.patch(
        "features.autonomy.autonomous_developer.develop_from_goal",
        new_callable=AsyncMock,
    )

    @asynccontextmanager
    async def session_manager_mock():
        yield get_test_session

    mocker.patch(
        "api.v1.development_routes.get_db_session", return_value=get_test_session
    )
    mocker.patch("core.cognitive_service.get_session", session_manager_mock)
    # Patch knowledge_service's get_session instead of audit_context
    mocker.patch("core.knowledge_service.get_session", session_manager_mock)

    # Mock config_service for cognitive service initialization
    async def mock_get(key, default=None):
        config_map = {
            "TEST_API_URL": "http://test.api",
            "TEST_API_KEY": "test_key",
            "TEST_MODEL_NAME": "test-model",
        }
        return config_map.get(key, default)

    mock_config = mocker.patch("core.cognitive_service.config_service")
    mock_config.get = AsyncMock(side_effect=mock_get)

    async with get_test_session as session:
        await session.execute(
            insert(LlmResource).values(
                name="test_resource",
                env_prefix="TEST",
                provided_capabilities=["planning"],
            )
        )
        await session.execute(
            insert(CognitiveRole).values(
                role="AutonomousDeveloper", assigned_resource="test_resource"
            )
        )
        await session.commit()

    app = create_app()

    # Manually trigger the lifespan
    async with app.router.lifespan_context(app):
        # Set the test mode flag and manually initialize services
        app.state.core_context._is_test_mode = True
        await app.state.core_context.cognitive_service.initialize()
        await app.state.core_context.auditor_context.load_knowledge_graph()

        # Create client for making requests
        transport = ASGITransport(app=app)
        async with AsyncClient(transport=transport, base_url="http://test") as client:
            response = await client.post("/v1/develop/goal", json={"goal": "test goal"})

    assert response.status_code == 202
    assert "task_id" in response.json()
    task_id = response.json()["task_id"]

    mock_develop.assert_awaited_once()
    call_args = mock_develop.call_args[0]
    assert call_args[1] == "test goal"
    assert call_args[2] == task_id

--- END OF FILE ./tests/integration/test_full_run.py ---

--- START OF FILE ./tests/unit/__init__.py ---
# This file makes the 'unit' subdirectory a Python package.

--- END OF FILE ./tests/unit/__init__.py ---

--- START OF FILE ./tests/unit/test_config.py ---
# tests/unit/test_config.py

import pytest

from shared.config import Settings


def test_settings_loads_defined_attributes(monkeypatch):
    """Test that explicitly defined attributes are loaded correctly."""
    monkeypatch.setenv("LOG_LEVEL", "DEBUG")
    # We must create a new instance to re-evaluate the env var
    settings = Settings()
    assert settings.LOG_LEVEL == "DEBUG"


def test_settings_loads_extra_vars_via_constructor():
    """
    Tests that extra variables passed to the constructor are handled correctly
    in Pydantic v2 with extra='allow'.
    """
    # Arrange: Create a settings instance with extra keyword arguments.
    # This is the correct way to test the "extra" fields behavior.
    settings = Settings(
        MY_DYNAMIC_VARIABLE="hello_world",
        CHEAP_API_KEY="cheap_key_123",
        _env_file=None,  # Prevent loading the real .env file for test isolation
    )

    # Assert: In Pydantic v2, extra fields ARE accessible as direct attributes.
    assert hasattr(settings, "MY_DYNAMIC_VARIABLE")
    assert settings.MY_DYNAMIC_VARIABLE == "hello_world"
    assert hasattr(settings, "CHEAP_API_KEY")
    assert settings.CHEAP_API_KEY == "cheap_key_123"

    # Assert: They are ALSO correctly stored in the model_extra dictionary.
    assert "MY_DYNAMIC_VARIABLE" in settings.model_extra
    assert settings.model_extra["MY_DYNAMIC_VARIABLE"] == "hello_world"
    assert "CHEAP_API_KEY" in settings.model_extra
    assert settings.model_extra["CHEAP_API_KEY"] == "cheap_key_123"

    # Assert: They are not confused with the model's formally defined fields.
    defined_fields = set(Settings.model_fields.keys())
    assert "MY_DYNAMIC_VARIABLE" not in defined_fields


def test_settings_accessing_nonexistent_attribute_raises_error():
    """Test that accessing a truly non-existent attribute raises an AttributeError."""
    settings = Settings(_env_file=None)
    with pytest.raises(AttributeError):
        _ = settings.THIS_DOES_NOT_EXIST

--- END OF FILE ./tests/unit/test_config.py ---

--- START OF FILE ./tests/unit/test_execution_agent.py ---
# tests/unit/test_execution_agent.py
from __future__ import annotations

from unittest.mock import AsyncMock, MagicMock

import pytest
from core.agents.execution_agent import ExecutionAgent
from shared.models import ExecutionTask, PlanExecutionError, TaskParams


@pytest.fixture
def mock_execution_agent(mock_core_env):
    """Uses the canonical mock environment to create a valid ExecutionAgent."""
    # --- THIS IS THE FIX ---
    # The constructor expects coder_agent, plan_executor, and auditor_context.
    mock_coder_agent = MagicMock()
    mock_plan_executor = MagicMock()
    mock_plan_executor.execute_plan = AsyncMock()
    mock_auditor_context = MagicMock()

    agent = ExecutionAgent(
        coder_agent=mock_coder_agent,
        plan_executor=mock_plan_executor,
        auditor_context=mock_auditor_context,
    )
    return agent, mock_plan_executor
    # --- END OF FIX ---


@pytest.mark.anyio
async def test_execute_plan_success(mock_execution_agent):
    """Tests that a valid plan is passed to the plan executor."""
    agent, mock_executor = mock_execution_agent
    valid_plan = [
        ExecutionTask(
            step="Read a file",
            action="read_file",
            params=TaskParams(file_path="src/main.py"),
        )
    ]

    success, message = await agent.execute_plan("A test goal", valid_plan)

    assert success
    assert message == "✅ Plan executed successfully."
    mock_executor.execute_plan.assert_awaited_once_with(valid_plan)


@pytest.mark.anyio
async def test_execute_plan_handles_executor_failure(mock_execution_agent):
    """Tests that the agent correctly reports a failure from the plan executor."""
    agent, mock_executor = mock_execution_agent
    mock_executor.execute_plan.side_effect = PlanExecutionError("Something went wrong")

    plan = [
        ExecutionTask(
            step="Read a file",
            action="read_file",
            params=TaskParams(file_path="src/main.py"),
        )
    ]

    success, message = await agent.execute_plan("A test goal", plan)

    assert not success
    assert "Plan execution failed during orchestration: Something went wrong" in message

--- END OF FILE ./tests/unit/test_execution_agent.py ---

--- START OF FILE ./tests/unit/test_git_service.py ---
# tests/unit/test_git_service.py
from unittest.mock import MagicMock, call

import pytest

from core.git_service import GitService


@pytest.fixture
def mock_git_service(mocker, tmp_path):
    """Creates a GitService instance with a mocked subprocess.run."""
    (tmp_path / ".git").mkdir()

    mock_run = mocker.patch("subprocess.run")

    # Configure mock for the common flow: status -> add -A -> commit
    mock_run.side_effect = [
        MagicMock(stdout="?? new_file.py", stderr="", returncode=0),  # status
        MagicMock(stdout="", stderr="", returncode=0),  # add -A
        MagicMock(stdout="commit success", stderr="", returncode=0),  # commit
    ]

    service = GitService(repo_path=str(tmp_path))
    return service, mock_run


def test_git_add(mock_git_service):
    """Tests that the add method calls subprocess.run with the correct arguments."""
    service, mock_run = mock_git_service
    # Reset side_effect for this simple, single-call test
    mock_run.side_effect = None
    mock_run.return_value = MagicMock(stdout="", stderr="", returncode=0)

    file_to_add = "src/core/main.py"
    service.add(file_to_add)

    mock_run.assert_called_once_with(
        ["git", "add", file_to_add],
        cwd=service.repo_path,
        capture_output=True,
        text=True,
        check=True,
    )


def test_git_commit(mock_git_service):
    """Tests that commit runs: status -> add -A -> commit."""
    service, mock_run = mock_git_service
    commit_message = "feat(agent): Test commit"

    service.commit(commit_message)

    # With robust GitService: status -> add -A -> commit
    assert mock_run.call_count == 3

    expected_calls = [
        call(
            ["git", "status", "--porcelain"],
            cwd=service.repo_path,
            capture_output=True,
            text=True,
            check=True,
        ),
        call(
            ["git", "add", "-A"],
            cwd=service.repo_path,
            capture_output=True,
            text=True,
            check=True,
        ),
        call(
            ["git", "commit", "-m", commit_message],
            cwd=service.repo_path,
            capture_output=True,
            text=True,
            check=True,
        ),
    ]
    mock_run.assert_has_calls(expected_calls)


def test_is_git_repo_true(tmp_path):
    """Returns True when a .git directory exists."""
    (tmp_path / ".git").mkdir()
    service = GitService(repo_path=str(tmp_path))
    assert service.is_git_repo() is True


def test_is_git_repo_false(tmp_path):
    """Raises ValueError if .git is missing on init."""
    with pytest.raises(ValueError):
        GitService(repo_path=str(tmp_path))

--- END OF FILE ./tests/unit/test_git_service.py ---

--- START OF FILE ./tests/unit/test_intent_translator.py ---
# tests/unit/test_intent_translator.py
import json
from unittest.mock import MagicMock

import pytest

from core.agents.intent_translator import IntentTranslator
from shared.config import settings


@pytest.fixture
def mock_cognitive_service(mocker):
    """Mocks the CognitiveService and its client to return a predictable, structured response."""
    mock_client = MagicMock()
    mock_ai_response = json.dumps(
        {
            "status": "vague",
            "suggestion": "The user's goal is a bit vague. Based on the roadmap, did you mean to ask: 'Refactor the codebase to remove the obsolete BaseLLMClient and use CognitiveService'?",
        }
    )
    mock_client.make_request.return_value = mock_ai_response

    mock_service = MagicMock()
    mock_service.get_client_for_role.return_value = mock_client
    return mock_service


@pytest.fixture
def mock_prompt_pipeline(mocker):
    """Mocks the PromptPipeline to prevent file system access during the unit test."""
    mock_pipeline = mocker.patch("core.agents.intent_translator.PromptPipeline")
    mock_instance = mock_pipeline.return_value
    mock_instance.process.side_effect = lambda prompt: prompt
    return mock_instance


def test_translator_handles_vague_goal(
    mock_cognitive_service, mock_prompt_pipeline, tmp_path
):
    """
    Tests if the IntentTranslator can take a vague, human goal
    and produce a structured, actionable goal.
    """
    (tmp_path / ".intent" / "prompts").mkdir(parents=True)
    prompt_file = tmp_path / ".intent" / "prompts" / "intent_translator.prompt"
    prompt_file.write_text("User Request: {user_input}")

    settings.MIND = tmp_path / ".intent"

    translator = IntentTranslator(mock_cognitive_service)
    vague_goal = "optimize AI client usage"

    ai_json_response = translator.translate(vague_goal)

    response_lower = ai_json_response.lower()
    assert "did you mean to ask" in response_lower
    assert "basellmclient" in response_lower
    assert "cognitiveservice" in response_lower

--- END OF FILE ./tests/unit/test_intent_translator.py ---

--- START OF FILE ./tests/unit/test_planner_agent.py ---
# tests/unit/test_planner_agent.py
import json
from unittest.mock import AsyncMock, MagicMock

import pytest

from core.agents.planner_agent import PlannerAgent
from core.cognitive_service import CognitiveService
from shared.models import ExecutionTask, PlanExecutionError


@pytest.fixture
def mock_cognitive_service():
    """Mocks the CognitiveService and its client for async methods."""
    mock_client = MagicMock()
    mock_client.make_request_async = AsyncMock()

    mock_service = MagicMock(spec=CognitiveService)
    mock_service.aget_client_for_role = AsyncMock(return_value=mock_client)

    return mock_service


@pytest.mark.anyio
async def test_create_execution_plan_success(mock_cognitive_service, mock_core_env):
    """Tests that the planner can successfully parse a valid high-level plan."""
    agent = PlannerAgent(cognitive_service=mock_cognitive_service)
    goal = "Test goal"

    plan_json = json.dumps(
        [
            {
                "step": "A valid step",
                "action": "create_file",
                "params": {"file_path": "src/test.py"},
            }
        ]
    )
    mock_client = await mock_cognitive_service.aget_client_for_role("Planner")
    mock_client.make_request_async.return_value = f"```json\n{plan_json}\n```"

    plan = await agent.create_execution_plan(goal)

    assert len(plan) == 1
    assert isinstance(plan[0], ExecutionTask)
    assert plan[0].action == "create_file"


@pytest.mark.anyio
async def test_create_execution_plan_raises_plan_error_on_bad_data(
    mock_cognitive_service, mock_core_env
):
    """
    Tests that the planner raises a PlanExecutionError after failing to
    parse a structurally invalid response from the LLM after all retries.
    """
    agent = PlannerAgent(cognitive_service=mock_cognitive_service)
    goal = "Test goal"

    invalid_plan_json = json.dumps(
        [{"step": "Invalid structure", "action": "create_file"}]
    )

    mock_client = await mock_cognitive_service.aget_client_for_role("Planner")
    mock_client.make_request_async.return_value = f"```json\n{invalid_plan_json}\n```"

    with pytest.raises(
        PlanExecutionError, match="Failed to create a valid plan after max retries"
    ):
        await agent.create_execution_plan(goal)

--- END OF FILE ./tests/unit/test_planner_agent.py ---

--- END OF PROJECT CONTEXT BUNDLE ---
