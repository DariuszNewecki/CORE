--- START OF FILE project_context.txt ---

--- START OF PROJECT CONTEXT BUNDLE ---

--- START OF FILE ./.intent/charter/constitution/ACTIVE ---
v2

--- END OF FILE ./.intent/charter/constitution/ACTIVE ---

--- START OF FILE ./.intent/charter/constitution/amendment_process.md ---
# .intent/charter/constitution/amendment_process.md
#
# This document is the single, canonical source of truth for the process of
# amending the CORE Constitution. Adherence is mandatory for all changes to
# files governed by the Charter.

## SECTION 1: CORE PRINCIPLES OF AMENDMENT

1.  **Safety First:** The process is designed to prevent accidental or unauthorized changes. All critical changes require explicit, verifiable human approval.
2.  **Clarity and Intent:** Every proposed change must be accompanied by a clear justification that links it to the system's core principles or mission.
3.  **Auditability:** Every step of the amendment process, from proposal to ratification, must be traceable and recorded.

## SECTION 2: THE STANDARD AMENDMENT PROCESS

This process applies to any modification of a file within the `.intent/charter/` directory.

1.  **Proposal Creation:**
    *   An authorized operator MUST create a formal proposal file (`cr-*.yaml`) according to the `proposal_schema.json`.
    *   The `target_path` MUST be the canonical path to the Charter file being changed.
    *   The `justification` MUST clearly state the reason for the change and which CORE principle it serves.

2.  **Signature and Quorum:**
    *   The proposal MUST be signed by one or more authorized approvers as defined in `approvers.yaml`.
    *   The number of valid signatures MUST meet the quorum requirements defined in `approvers.yaml` for the current operational mode (`development` or `production`).
    *   For changes targeting files listed in `critical_paths.yaml`, the **critical** quorum is required. For all other Charter files, the **standard** quorum applies.

3.  **Validation and Ratification:**
    *   The proposed change MUST pass a full constitutional audit (`core-admin check ci audit`).
    *   The change MAY be subject to a canary deployment as defined in the `canary_policy.yaml`.
    *   Once all checks pass and the quorum is met, the change is considered ratified and can be merged.

## SECTION 3: EMERGENCY PROCEDURES

Emergency procedures, such as the revocation of a compromised key, are detailed in `charter/constitution/operator_lifecycle.md`. Such actions are considered critical amendments and always require the **critical** quorum.
--- END OF FILE ./.intent/charter/constitution/amendment_process.md ---

--- START OF FILE ./.intent/charter/constitution/approvers.yaml ---
# .intent/charter/constitution/approvers.yaml
#
# This file defines the human operators authorized to approve constitutional
# changes and the rules governing that process.

approvers:
  - identity: "core-team@core-system.ai"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEA3dK7Jt4jJh6+QvZvY6XcGx3q8R0e7m5JwqYk8qFtU9U=
      -----END PUBLIC KEY-----
    created_at: "2025-08-05T15:50:53.995534+00:00"
    role: "maintainer"
    description: "Primary CORE development team"

  - identity: "security-audit@core-system.ai"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEApJ+8mNvL7wY2XfDcR9q3Q5t4yZx7v6hB8gKj0sF3T5U=
      -----END PUBLIC KEY-----
    created_at: "2025-08-05T15:50:53.995534+00:00"
    role: "security"
    description: "Security audit team for constitutional changes"

  - identity: "d.newecki@gmail.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VwAyEAmcbNEgYFEUUNf8XYGZEscamfzqrYHpgKoPHehtPuiDQ=
      -----END PUBLIC KEY-----
    created_at: "2025-08-12T10:36:49.000000Z"
    role: "maintainer"
    description: "Mentor"

quorum:
  # The operational mode should reflect the CORE_ENV variable from mind/config/runtime_requirements.yaml.
  # The system MUST enforce the 'production' quorum rules when CORE_ENV=production.
  current_mode: development
  development:
    standard: 1
    critical: 1
  production:
    standard: 2
    critical: 3

# The list of paths requiring the 'critical' quorum is now managed centrally.
critical_paths_source: "charter/constitution/critical_paths.yaml"

emergency_contact: "security-emergency@core-system.ai"
--- END OF FILE ./.intent/charter/constitution/approvers.yaml ---

--- START OF FILE ./.intent/charter/constitution/approvers.yaml.example ---
# .intent/constitution/approvers.yaml
#
# PURPOSE: This file enables cryptographic verification of constitutional approvals,
# preventing unauthorized changes. It contains the public keys of all authorized
# constitutional approvers for this instance of CORE.
#
# TO ADD A NEW APPROVER:
# 1. Run the command: `core-admin keygen "your.email@example.com"`
# 2. The command will output a JSON/YAML block.
# 3. Paste that block into the 'approvers' list below.

approvers:
  - identity: "your.name@example.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
      -----END PUBLIC KEY-----
    created_at: "YYYY-MM-DDTHH:MM:SSZ"
    role: "maintainer"
    description: "Primary maintainer of this CORE instance"

  - identity: "another.approver@example.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB=
      -----END PUBLIC KEY-----
    created_at: "YYYY-MM-DDTHH:MM:SSZ"
    role: "contributor"
    description: "Authorized contributor"

# Minimum number of valid signatures required to approve a constitutional amendment.
quorum:
  # Regular amendments (e.g., changing a policy) require 1 signature.
  standard: 1
  # Critical changes (e.g., altering this file) require 2 signatures.
  critical: 2

# A list of file paths that are considered "critical". Any proposal targeting
# these files will require the 'critical' quorum to be met.
critical_paths:
  - ".intent/policies/intent_guard.yaml"
  - ".intent/constitution/approvers.yaml"
  - ".intent/meta.yaml"

--- END OF FILE ./.intent/charter/constitution/approvers.yaml.example ---

--- START OF FILE ./.intent/charter/constitution/critical_paths.yaml ---
# .intent/charter/constitution/critical_paths.yaml
#
# This is the single source of truth for all file paths within the constitution
# that are considered critical to the system's safety, integrity, and governance.
#
# Any proposed change targeting one of these paths requires the "critical" quorum
# of approvers as defined in `approvers.yaml`.

paths:
  # The master index of the constitution.
  - ".intent/meta.yaml"

  # Core governance and identity files.
  - "charter/constitution/approvers.yaml"
  - "charter/constitution/operator_lifecycle.md"
  - "charter/constitution/amendment_process.md" # This file itself is critical.

  # The system's core mission statement.
  - "charter/mission/northstar.yaml"
  - "charter/mission/principles.yaml"

  # The most fundamental safety and governance policies.
  - "charter/policies/safety_policy.yaml"
  - "charter/policies/intent_guard_policy.yaml"

  # The definition of enforcement itself.
  - "charter/policies/enforcement_model_policy.yaml"
--- END OF FILE ./.intent/charter/constitution/critical_paths.yaml ---

--- START OF FILE ./.intent/charter/constitution/operator_lifecycle.md ---
# Human Operator Lifecycle Procedures

This document defines the formal, constitutionally-mandated procedures for managing human operators who have the authority to approve changes to the CORE constitution. Adherence to these procedures is mandatory and enforced by peer review during any change to the `approvers.yaml` file.

## Onboarding a New Approver

1.  **Key Generation:** The candidate operator MUST generate a new, secure Ed25519 key pair using the following command from the CORE repository root:
    ```bash
    poetry run core-admin keygen "candidate.email@example.com"
    ```
2.  **Proposal Creation:** A currently active, authorized approver MUST create a formal constitutional amendment proposal.
    - The `target_path` of the proposal MUST be `.intent/constitution/approvers.yaml`.
    - The `justification` MUST clearly state the reason for adding the new approver, including their role and identity.
    - The `content` of the proposal MUST be the complete `approvers.yaml` file with the new approver's YAML block appended.
3.  **Ratification:** The proposal must be signed and approved, meeting the required quorum as defined in `approvers.yaml`. Upon successful canary validation and approval, the new operator is considered active.

## Standard Revocation of an Approver

1.  **Proposal Creation:** An authorized approver MUST create a formal proposal to remove the target operator's block from the `approvers` list in `approvers.yaml`.
2.  **Justification:** The `justification` MUST clearly state the non-emergency reason for revocation (e.g., operator has left the project).
3.  **Ratification:** The proposal must be signed and approved, meeting the required quorum.

## Emergency Revocation of a Compromised Key

1.  **Proposal Creation:** In the event of a suspected or confirmed private key compromise, any active approver MUST immediately create an emergency revocation proposal targeting `approvers.yaml`.
2.  **Quorum:** This proposal requires the **critical** quorum to be met for approval.
3.  **Immediate Invalidation:** The moment a revocation proposal is created for an identity, that identity's signature is considered invalid for all quorum calculations on that proposal and any subsequent proposals until the matter is resolved.
--- END OF FILE ./.intent/charter/constitution/operator_lifecycle.md ---

--- START OF FILE ./.intent/charter/mission/manifesto.md ---
# CORE Manifesto

CORE is not a framework. It is not a plugin system.  
CORE is a system that evolves itself — governed by explicit declarations of purpose, constraints, and structure.

---

### Beliefs

- Software is never done, but it must always be explainable.
- Changes must serve a declared intent — or challenge it explicitly.
- LLMs are powerful, but must be contained by purpose, not left to guess.
- Governance is not bureaucracy; it’s alignment between vision and action.

---

### CORE exists to:

- Understand its own capabilities
- Apply changes safely and transparently
- Validate every change against declared rules
- Learn from its outcomes and improve
- Inspire new systems to govern themselves

---

CORE does not pretend to be perfect.  
But it refuses to be mindless.

--- END OF FILE ./.intent/charter/mission/manifesto.md ---

--- START OF FILE ./.intent/charter/mission/northstar.yaml ---
# .intent/mission/northstar.yaml

# PURPOSE: This fulfills evolvable_structure and defines CORE's NorthStar.
name: CORE
version: "0.1.0"
purpose: >
  CORE is a self-improving, intent-aware development system.
  Its purpose is to orchestrate safe, meaningful, and governed changes
  to its own codebase and related artifacts through intent bundles and introspective loops.

scope:
  - Planning and decomposition of tasks
  - Code generation via LLMs
  - Change validation and governance enforcement
  - Self-introspection and structural analysis
  - Knowledge management via manifests and graphs
  - Continuous self-evaluation and auditability

values:
  - Clarity over cleverness
  - Safety before speed
  - Traceability of every action
  - Alignment with declared purpose
  - Capability-driven reasoning

notes:
  - CORE evolves iteratively, but never silently.
  - All changes must fulfill a declared intent or generate a proposal to revise that intent.
--- END OF FILE ./.intent/charter/mission/northstar.yaml ---

--- START OF FILE ./.intent/charter/mission/principles.yaml ---
# .intent/charter/mission/principles.yaml
#
# CORE's Constitution: clear, enforceable, and readable by humans and LLMs.
# Any agent (including future LLMs) must understand and obey these rules.
# This file contains high-level, aspirational values. Specific, machine-enforceable
# rules are defined in the relevant policy files.

principles:

  - id: clarity_first
    description: >
      Prioritize clear, understandable code and documentation that effectively
      communicates its intent to both humans and machines. If something is
      ambiguous, it must be simplified.

  - id: safe_by_default
    description: >
      Every change must assume rollback or rejection unless explicitly validated.
      No file write, code execution, or intent update may proceed without confirmation.
      Rollback must be possible at every stage.

  - id: reason_with_purpose
    description: >
      Every autonomous planning step must be traceable to a core constitutional
      principle or a declared high-level goal, ensuring all actions are deliberate
      and auditable.

  - id: evolvable_structure
    description: >
      The system's structure and constitution must be designed to evolve safely.
      Self-modification must be governed by a formal, secure, and auditable
      amendment process.

  - id: no_orphaned_logic
    description: >
      No function, file, or rule may exist without being discoverable and traceable
      through the system's operational database. All logic must serve a declared purpose.

  - id: use_intent_bundle
    description: >
      All significant autonomous actions must be executed via a structured
      IntentBundle that reflects the Ten-Phase Loop of Reasoned Action. No phase
      may be skipped.

  - id: minimalism_over_completeness
    description: >
      Prefer small, focused changes. Do not generate stubs, placeholders, or
      unused functions. Unused or untestable logic is a liability and must be removed.

  - id: dry_by_design
    description: >
      "Don't Repeat Yourself." No logic or configuration may be duplicated. If a
      function, pattern, or rule exists in one place, it must be reused or
      referenced, not rewritten.

  - id: single_source_of_truth
    description: >
      The `.intent/` directory is the single source of constitutional truth (the laws).
      The operational database is the single source of operational truth (the current state).
      Derived artifacts (e.g., reports) must be generated from these sources.

  - id: separation_of_concerns
    description: >
      Each architectural domain must have a single, clearly defined responsibility.
      Inter-domain communication must be explicitly declared and governed by the
      constitution.

  - id: predictable_side_effects
    description: >
      Any action that modifies the system's state (e.g., a file write) must be
      explicit, logged, and reversible. Silent or unlogged changes are forbidden.

  - id: policy_change_requires_human_review
    description: >
      Any change to a policy file within the `.intent/policies/` directory must be
      ratified through the formal constitutional amendment process, requiring
      human review and approval.
--- END OF FILE ./.intent/charter/mission/principles.yaml ---

--- START OF FILE ./.intent/charter/policies/agent/agent_policy.yaml ---
policy_id: 18f048cb-b084-4faa-ac62-17fca55fed77
id: agent_policy
version: "1.2.0" # Version bump for SSOT alignment
title: "Agent Governance Policy"
status: active
purpose: >
  The single source of truth for all rules governing the behavior, reasoning,
  and operational safety of all AI agents within the CORE system.

scope:
  applies_to: ["agents"]

owners:
  accountable: "Core Maintainer"

review:
  frequency: "annual"

rules:
  # --- Safety & Compliance Rules ---
  - id: agent.compliance.no_write_intent
    statement: "Agents MUST NOT write directly to '.intent/charter/**'. Constitutional changes require a formal, human-approved proposal."
    enforcement: error

  - id: agent.compliance.respect_cli_registry
    statement: "All tool invocations and system actions MUST be routed through commands registered in the `core.cli_commands` database table."
    enforcement: error

  # --- Reasoning & Auditing Rules ---
  - id: agent.reasoning.trace_required
    statement: "Agents MUST produce a concise, inspectable trace for non-trivial tasks, including inputs, tools used, and outcomes for auditability."
    enforcement: warn

  - id: agent.reasoning.source_attribution
    statement: "All claims, especially those influencing code generation or policy changes, MUST provide source attribution (e.g., file paths, policy IDs)."
    enforcement: warn

  # --- Execution & Safety Rules ---
  - id: agent.execution.no_unverified_code
    statement: "Agents MUST NOT execute or commit any generated or refactored code without first passing it through the full validation pipeline (lint, test, constitutional audit)."
    enforcement: error

  - id: agent.execution.fail_closed
    statement: "If an agent encounters ambiguous instructions or a high-risk uncertainty, it MUST halt its current task and escalate for human clarification rather than proceeding."
    enforcement: warn

  - id: agent.execution.limit_scope
    statement: "Agent actions MUST be limited to the immediate scope of the declared goal. Broad, opportunistic refactoring requires a separate, explicit intent and plan."
    enforcement: warn

# --- Resource Selection Logic (Merged from deduction_policy) ---
resource_selection:
  scoring_weights:
    cost: 0.5
    speed: 0.3
    quality: 0.1
    reasoning: 0.1

  task_specific_overrides:
    - task_keywords: ["docstringwriter", "propose a domain name", "label cluster"]
      weights:
        cost: 0.9
        speed: 0.1
        quality: 0.0
        reasoning: 0.0
    - task_keywords: ["refactor", "architect", "planner", "generate"]
      weights:
        cost: 0.1
        speed: 0.1
        quality: 0.4
        reasoning: 0.4
--- END OF FILE ./.intent/charter/policies/agent/agent_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/agent/micro_proposal_policy.yaml ---
policy_id: a5cbb67a-bbe6-4221-b187-1d88692c1124
id: micro_proposal_policy
version: "1.0.0"
title: "Micro-Proposal Policy (Autonomous Fast Track)"
status: active
purpose: >
  Defines the rules and scope for low-risk, autonomous changes that can be
  auto-approved without requiring the full human-in-the-loop constitutional
  amendment process. This is the primary gate for A1 autonomy.

scope:
  applies_to: [agents, cli]

owners:
  primary: "Governance Lead"
  reviewers: ["Core Maintainer"]

review:
  frequency: "6 months"

rules:
  # Rule 1: Define the set of "safe" actions that can be auto-approved.
  # Initially, we only allow actions that are highly deterministic and low-risk.
  - id: safe_actions
    description: "A list of capability keys that are permitted in micro-proposals."
    enforcement: error
    allowed_actions:
      - "autonomy.self_healing.fix_docstrings"
      - "autonomy.self_healing.format_code"
      - "autonomy.self_healing.fix_headers"

  # Rule 2: Define which parts of the codebase are safe for autonomous modification.
  # We explicitly forbid any changes to the constitution itself (.intent/) or
  # the core governance machinery.
  - id: safe_paths
    description: "Glob patterns for file paths that are safe for autonomous modification."
    enforcement: error
    allowed_paths:
      - "docs/**/*.md"
      - "tests/**/*.py"
      - "src/**/*.py"
    forbidden_paths:
      - ".intent/**"
      - "src/system/governance/**"
      - "src/core/**"
      - "pyproject.toml"
      - "Makefile"

  # Rule 3: All micro-proposals must be validated before application.
  # This ensures that even a safe action on a safe file doesn't introduce errors.
  - id: require_validation
    description: "A micro-proposal must include evidence of a successful pre-flight validation (lint, test, audit)."
    enforcement: error
    required_evidence:
      - "validation_report_id"
--- END OF FILE ./.intent/charter/policies/agent/micro_proposal_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/capability_linter_policy.yaml ---
policy_id: cb081a61-5c2b-4623-8249-f26796e68d40
id: capability_linter_policy
version: "1.1.0" # Version bump to reflect #ID change
title: "Capability Linter Policy"
status: active
purpose: >
  To keep the capability catalog clean, owned, and useful by enforcing meaningful
  descriptions, owners, and the correct identity linking mechanism.
scope:
  applies_to: [code, governance, discovery]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "semiannual"

rules:
  - id: caps.meaningful_description
    statement: Capability descriptions in the database MUST be specific and non-placeholder.
    enforcement: error
  - id: caps.owner_required
    statement: Active capabilities in the database MUST have an assigned owner (agent/team).
    enforcement: error
  - id: caps.no_placeholder_text
    statement: Descriptions such as "TBD" or "N/A" are forbidden in the database.
    enforcement: error
  - id: caps.id_format
    statement: "Source code linkers MUST use the form '# ID: <uuid>'."
    enforcement: error
--- END OF FILE ./.intent/charter/policies/code/capability_linter_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/code_health_policy.yaml ---
policy_id: f7680a46-87ad-4e1a-8bdb-d6751d787309
id: code_health_policy
version: "1.0.0"
title: "Code Health Policy"
status: active
purpose: >
  To maintain small, focused modules and functions, limit complexity, and encourage
  refactoring before entropy accumulates.
scope:
  applies_to: [code, agents, auditor]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "semiannual"

rules:
  max_cognitive_complexity: 15
  max_nesting_depth: 4
  max_line_length:
    limit: 120
    enforcement: soft
  max_module_lloc: 300
  max_function_lloc: 80
  outlier_standard_deviations: 2.0
  enforce_dead_public_symbols: true
--- END OF FILE ./.intent/charter/policies/code/code_health_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/code_style_policy.yaml ---
policy_id: aaa0a228-125d-4013-bfed-c1b58cec0f66
id: code_style_policy
version: "1.0.0"
title: "Code Style Policy"
status: active
purpose: >
  To ensure consistent, readable code across the repository by standardizing
  tooling and expectations for contributors and agents.
scope:
  applies_to: [code, agents, cli]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "annual"

rules:
  - id: style.linter_required
    statement: All changes MUST pass ruff (lint) before merge.
    enforcement: error
  - id: style.formatter_required
    statement: All changes MUST be formatted by black; CI runs black --check.
    enforcement: error
  - id: style.docstrings_public_apis
    statement: Public APIs MUST have docstrings summarizing intent and parameters; private/dunder excluded.
    enforcement: warn
  - id: style.import_order
    statement: Imports MUST follow grouping/order and avoid unused imports (enforced by linter).
    enforcement: warn
  - id: style.fail_on_style_in_ci
    statement: CI MUST fail on style or lint violations (no auto-fixing in CI).
    enforcement: error
--- END OF FILE ./.intent/charter/policies/code/code_style_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/naming_conventions_policy.yaml ---
policy_id: fae5215d-a1ee-424f-b5e1-46b08cecc5b9
# .intent/charter/policies/naming_conventions_policy.yaml
id: naming_conventions_policy
version: "1.0.0"
title: "Constitutional Naming Conventions"
status: active
purpose: >
  To provide a single, machine-readable source of truth for all naming
  conventions across the entire repository. This policy governs the structure of
  the constitution itself (.intent/) and the codebase (src/), ensuring clarity
  and predictability as the system evolves.

scope:
  applies_to: ["auditor", "repository"]

owners:
  accountable: "Core Maintainer"

review:
  frequency: "annual"

rules:
  # ==============================================================================
  # PART 1: GOVERNANCE OF THE CONSTITUTION ITSELF (.intent/)
  # ==============================================================================

  - id: intent.policy_file_naming
    description: "All policy files must use snake_case and end with '_policy.yaml'."
    enforcement: error
    scope: ".intent/charter/policies/*.yaml"
    target: "filename"
    pattern: "^[a-z0-9_]+_policy\\.yaml$"

  - id: intent.policy_schema_naming
    description: "Schemas for policy files must end with '_policy_schema.json'."
    enforcement: error
    scope: ".intent/charter/schemas/*_policy_schema.json"
    target: "filename"
    pattern: "^[a-z0-9_]+_policy_schema\\.json$"

  - id: intent.artifact_schema_naming
    description: "Schemas for non-policy artifacts must end with '_schema.[json|yaml]'."
    enforcement: error
    scope: ".intent/charter/schemas/*"
    target: "filename"
    pattern: "^[a-z0-9_]+_schema\\.(json|yaml)$"
    exclusions:
      - "*_policy_schema.json" # Exclude policy schemas to avoid rule conflict.

  - id: intent.prompt_file_naming
    description: "All prompt files must use snake_case and end with '.prompt'."
    enforcement: error
    scope: ".intent/mind/prompts/*.prompt"
    target: "filename"
    pattern: "^[a-z0-9_]+\\.prompt$"

  - id: intent.proposal_file_naming
    description: "All proposal files must follow the 'cr-*.yaml' naming convention."
    enforcement: warn
    scope: ".intent/proposals/*.yaml"
    target: "filename"
    pattern: "^cr-[a-zA-Z0-9_-]+\\.yaml$"
    exclusions:
      - "README.md" # The README is not a proposal.

  # ==============================================================================
  # PART 2: GOVERNANCE OF THE CODEBASE (src/ and tests/)
  # ==============================================================================

  - id: code.python_module_naming
    description: "All Python source files must use snake_case naming."
    enforcement: error
    scope: "src/**/*.py"
    target: "filename"
    pattern: "^[a-z0-9_]+\\.py$"
    exclusions:
      - "__init__.py"

  - id: code.python_test_module_naming
    description: "All Python test files must be prefixed with 'test_'."
    enforcement: error
    scope: "tests/**/*.py"
    target: "filename"
    pattern: "^test_[a-z0-9_]+\\.py$"
    exclusions:
      - "__init__.py"
      - "conftest.py"
--- END OF FILE ./.intent/charter/policies/code/naming_conventions_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/code/refactoring_patterns_policy.yaml ---
policy_id: 17725f7a-25e0-4747-85c4-8d98ea03310e
id: refactoring_patterns_policy
version: "1.0.0"
title: "Refactoring Patterns Policy"
status: active
purpose: >
  Provide safe, repeatable refactoring patterns for agents and developers to
  reduce risk during structural changes.
scope:
  applies_to: [code, agents, cli]
owners:
  primary: "Core Maintainer"
  reviewers: ["Governance Lead"]
review:
  frequency: "12 months"

patterns:
  - id: extract_function
    description: Move a coherent block of logic into a new function with a clear name and docstring.
    guardrails:
      - must_keep_behavior: true
      - add_unit_tests: true
      - run_audit: true

  - id: extract_module
    description: Move related functions/classes into a new module; update imports and domain boundaries.
    guardrails:
      - must_keep_behavior: true
      - update_import_map: true
      - run_audit: true

  - id: introduce_facade
    description: Add a facade/API layer to hide complexity behind a small, stable surface.
    guardrails:
      - document_contract: true
      - avoid_breaking_changes: true
      - run_audit: true

rules:
  - id: refactor.requires_tests
    statement: Any refactor that changes public behavior MUST include tests or proof of equivalence.
    enforcement: error
  - id: refactor.update_capabilities
    statement: When moving symbols, update capability tags and manifests accordingly.
    enforcement: warn
  - id: refactor.audit_after
    statement: A constitutional audit MUST run after refactors before merge.
    enforcement: error

checklist:
  - Confirm chosen pattern’s guardrails are satisfied.
  - Validate imports/domains after moves (no boundary violations).
  - Run tests + audit and attach results to the change.
--- END OF FILE ./.intent/charter/policies/code/refactoring_patterns_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/data/database_policy.yaml ---
policy_id: 1fb8949c-02db-486a-8b9d-556191456de3
id: database_policy
version: "3.0.0" # Major version bump for SSOT mandate
title: "Consolidated Database Governance Policy"
purpose: >
  Provide a durable, auditable ledger of what actually happened in CORE (events, audits, CLI runs, capability history)
  with the database as the single source of operational truth for all runtime configuration and knowledge.
scope:
  applies_to: [postgresql]
owners:
  primary: "Security Lead"
  reviewers: ["Governance Lead"]
review:
  frequency: "12 months"
engine:
  type: postgresql
  schema: core
migrations:
  directory: sql
  order:
    - "001_init.sql"
    - "002_catalog.sql"
    - "003_implementations.sql"
    - "004_refactor_to_symbols.sql"
    - "005_add_symbol_key.sql"
    - "0_knowledge_graph_view.sql"
    - "007_capabilities_registry.sql"
    - "008_operational_tables.sql"
    - "009_domain_and_vector_tables.sql"
rules:
  - id: db.mind_remains_source
    statement: "`.intent/**` is the single source of constitutional truth; DB is the authoritative source for operational data."
    enforcement: error
  - id: db.schema_declared
    statement: "All tables and columns MUST be declared in the database schema/migrations and validated at CI time."
    enforcement: error
  - id: db.migrations_logged
    statement: "Schema migrations MUST have an id, description, created_at, and approver recorded in the repo."
    enforcement: warn
  - id: db.write_via_governed_cli
    statement: "All writes MUST originate from registered CLI commands (see CLI Governance Policy)."
    enforcement: error
  - id: db.domains_in_db
    statement: "Capability domains MUST be stored in and queried from the database, with `.intent/mind/knowledge/domains.yaml` as a read-only export."
    enforcement: error
  - id: db.vector_index_in_db
    statement: "Vector index data MUST be stored in the database with metadata for EMBED_MODEL_REVISION and LOCAL_EMBEDDING_DIM, with file-based indices as read-only exports."
    enforcement: error
  - id: db.privacy.no_pii_or_secrets
    statement: "Personal data and secrets MUST NOT be stored in operational tables unless explicitly exempted."
    enforcement: error
  - id: db.privacy.masking
    statement: "Logs and audit records MUST redact tokens, keys, and secrets before persistence."
    enforcement: error
  - id: db.privacy.access_least_privilege
    statement: "Access to operational data MUST follow least-privilege via roles/groups."
    enforcement: warn

  # --- START OF NEW SSOT MANDATES ---
  - id: db.cli_registry_in_db
    statement: "The canonical list of CLI commands MUST be stored in and queried from the database. `.intent/mind/knowledge/cli_registry.yaml` is deprecated and considered a read-only export."
    enforcement: error
  - id: db.llm_resources_in_db
    statement: "The manifest of available LLM resources MUST be stored in and queried from the database. `.intent/mind/knowledge/resource_manifest.yaml` is deprecated and considered a read-only export."
    enforcement: error
  - id: db.cognitive_roles_in_db
    statement: "The definition of cognitive roles MUST be stored in and queried from the database. `.intent/mind/knowledge/cognitive_roles.yaml` is deprecated and considered a read-only export."
    enforcement: error
  # --- END OF NEW SSOT MANDATES ---

retention:
  audit_runs_days: 180
  cli_runs_days: 90
  capability_history_days: 365
  proposals_days: 1095
drift:
  development: warn
  production: block
backup_restore:
  cadence: daily
  test_restore_quarterly: true
quorum:
  changes_require_critical_paths: true
--- END OF FILE ./.intent/charter/policies/data/database_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/data/secrets_management_policy.yaml ---
policy_id: ffb2dcec-3a90-45e6-aea7-691dd3b339b3
id: secrets_management_policy
version: "1.0.0"
title: "Secrets Management Policy"
status: active
purpose: >
  Define rules for handling secrets to prevent accidental exposure in source code,
  logs, and outputs.
scope:
  applies_to: [code, ci, logs]
owners:
  primary: "Security Lead"
  reviewers: ["Core Maintainer"]
review:
  frequency: "12 months"

rules:
  - id: no_hardcoded_secrets
    statement: Source code MUST NOT contain hardcoded secrets (API keys, passwords). Use environment variables.
    enforcement: error
    detection:
      patterns:
        - "(A|B|S|G)K[0-9A-Za-z]{30,}" # Common API key patterns
        - 'password\s*[:=]\s*[''""].+[''""]'
      exclude:
        - "tests/**"
        - ".env.example"

  - id: redact_secrets_in_logs
    statement: Logs and telemetry MUST redact sensitive data (tokens, keys, passwords) before persistence.
    enforcement: warn

checklist:
  - Auditor scans for hardcoded secret patterns and fails the build if found outside excluded paths.
  - Periodic manual review of logs to ensure redaction is working as expected.
--- END OF FILE ./.intent/charter/policies/data/secrets_management_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/audit_ignore_policy.yaml ---
policy_id: 232934da-44d6-4ddc-9404-162382caddb7
id: audit_ignore_policy
version: "1.0.1" # Version bump for new ignore rule
title: "Audit Ignore Policy"
status: active
purpose: >
  To allow narrow, explicit exceptions for files or symbols to reduce audit noise
  without weakening the Constitution. Every ignore must have a reason and an expiry date.
scope:
  applies_to: [governance, ci]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "semiannual"

rules:
  - id: ignore.must_expire
    statement: Every ignore MUST include an 'expires' date to force review.
    enforcement: warn
  - id: ignore.must_have_reason
    statement: Every ignore MUST include a reason.
    enforcement: error

ignores:
  - path: "src/**/generated/**"
    reason: "Generated code; excluded from style/health checks."
    expires: "2026-01-01"
  - path: "vendor/**"
    reason: "Third-party code; owned externally."
    expires: "2026-01-01"
  - check_id: "knowledge.source.direct_access"
    path: "src/features/governance/checks/knowledge_source_check.py"
    reason: "This check must contain the string 'knowledge_graph.json' to perform its function. It is an intentional and safe usage."
    expires: "2026-01-01"
  
symbol_ignores:
  - key: "src/core/intent_guard.py::_get_rules_by_severity"
    reason: "Internal helper method, not a public capability."
    expires: "2026-01-01"
  - key: "src/features/governance/checks/health_checks.py::HealthChecks"
    reason: "Temporarily ignoring self-duplication warnings after major refactor."
    expires: "2025-12-01"
  - key: "src/features/governance/checks/style_checks.py::StyleChecks"
    reason: "Temporarily ignoring self-duplication warnings after major refactor."
    expires: "2025-12-01"
  - key: "src/features/governance/checks/security_checks.py::SecurityChecks"
    reason: "Temporarily ignoring self-duplication warnings after major refactor."
    expires: "2025-12-01"
  - key: "src/features/governance/checks/file_checks.py::FileChecks"
    reason: "Temporarily ignoring self-duplication warnings after major refactor."
    expires: "2025-12-01"
  - key: "src/features/governance/checks/manifest_lint.py::ManifestLintCheck"
    reason: "Temporarily ignoring self-duplication warnings after major refactor."
    expires: "2025-12-01"
  - key: "src/features/governance/checks/capability_coverage.py::CapabilityCoverageCheck"
    reason: "Temporarily ignoring self-duplication warnings after major refactor."
    expires: "2025-12-01"

--- END OF FILE ./.intent/charter/policies/governance/audit_ignore_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/auditor_policy.yaml ---
policy_id: ee4d8b52-0b76-41c5-bdd5-41de41126974
id: auditor_policy
version: "1.1.0" # Version bump for SSOT alignment
title: "Constitutional Auditor Policy"
status: active
purpose: >
  Define the identity, responsibilities, and operational mandate of the Constitutional Auditor.
  The Auditor enforces all Charter policies and fails CI when error-level violations are present.

scope:
  applies_to: ["governance", "ci", "system"]

owners:
  accountable: "Core Maintainer"

review:
  frequency: "annual"

rules:
  - id: auditor.must_enforce_error_level
    statement: "If any guard rule with enforcement=error is violated, the audit MUST fail."
    enforcement: error

  - id: auditor.must_report_policy_and_path
    statement: "Findings MUST include policy id, rule id, severity, message, and a concrete file path or symbol key."
    enforcement: error

  - id: auditor.must_respect_ignores
    statement: "The Auditor MUST respect scoped ignores defined in '.intent/charter/policies/audit_ignore_policy.yaml'."
    enforcement: warn

  - id: auditor.output_location
    statement: "When a report file is requested, it MUST be written under 'reports/' and use a stable JSON structure."
    enforcement: warn

  - id: auditor.single_active_constitution
    statement: "Exactly one active constitution version MUST be referenced by '.intent/charter/constitution/ACTIVE'."
    enforcement: error

# The rule 'auditor.cli_registry_source_of_truth' has been removed as it is now
# redundant. The canonical source for CLI commands is governed by 'cli_governance_policy.yaml'
# and 'database_policy.yaml', which this auditor is mandated to enforce.
--- END OF FILE ./.intent/charter/policies/governance/auditor_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/available_actions_policy.yaml ---
policy_id: 47eab093-749a-444f-bc97-3d816c2d631c
id: available_actions_policy
version: "1.1.0" # Version bump for new parameter schema
title: "Available Actions Policy"
status: active
purpose: >
  Defines the canonical list of atomic actions that the PlannerAgent is
  constitutionally permitted to include in an execution plan.
scope:
  applies_to: [agents, planner_agent]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "annual"

actions:
  # --- START OF AMENDMENT ---
  - name: "read_file"
    description: "Reads the entire content of a specified file to provide context for subsequent steps. Fails if the path is a directory."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The repository-relative path to the file to be read."
        required: true

  - name: "list_files"
    description: "Lists all files and subdirectories within a specified directory to understand its contents. Fails if the path is not a directory."
    parameters:
      - name: "file_path" # Use a consistent parameter name for all path-based operations
        type: "string"
        description: "The repository-relative path to the directory to be listed."
        required: true

  - name: "edit_file"
    description: "Performs a surgical replacement of a block of code within an existing file."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The repository-relative path to the file to be edited."
        required: true
      - name: "new_content"
        type: "string"
        description: "The complete new content for the specified block of code."
        required: true
      - name: "start_line"
        type: "integer"
        description: "The starting line number of the block to be replaced (1-based)."
        required: true
      - name: "end_line"
        type: "integer"
        description: "The ending line number of the block to be replaced (1-based)."
        required: true

  - name: "create_file"
    description: "Creates a new source code or documentation file at a specified path with the given content."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The repository-relative path where the new file will be created."
        required: true
      - name: "code"
        type: "string"
        description: "The full source code or content for the new file."
        required: true

  - name: "edit_function"
    description: "Surgically replaces the code of an existing function or class within a file."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The path to the file containing the symbol to be edited."
        required: true
      - name: "symbol_name"
        type: "string"
        description: "The name of the function or class to be replaced."
        required: true
      - name: "code"
        type: "string"
        description: "The new, complete source code for the function or class."
        required: true

  - name: "delete_file"
    description: "Deletes an existing file from the repository."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The path of the file to be deleted."
        required: true

  - name: "create_proposal"
    description: "Creates a formal, human-in-the-loop constitutional amendment proposal (a cr-*.yaml file)."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The repository-relative path of the file the proposal will target."
        required: true
      - name: "justification"
        type: "string"
        description: "A clear, human-readable rationale for the proposed change."
        required: true
      - name: "code"
        type: "string"
        description: "The full new content for the target file."
        required: true

  - name: "add_capability_tag"
    description: "Adds a new # ID tag to a specific function or class. (Note: This is a legacy action, prefer 'fix assign-ids')."
    parameters:
      - name: "file_path"
        type: "string"
        description: "The path to the file containing the symbol."
        required: true
      - name: "symbol_name"
        type: "string"
        description: "The name of the function or class to tag."
        required: true
      - name: "tag"
        type: "string"
        description: "The UUID tag to add."
        required: true
  
--- END OF FILE ./.intent/charter/policies/governance/available_actions_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/cli_governance_policy.yaml ---
policy_id: cd37bddd-52c8-445a-b842-e229b29e6980
id: cli_governance_policy
version: "1.1.0" # Version bump for SSOT alignment
title: "CLI Governance Policy"
status: active
purpose: >
  To govern the structure, registration, and evolution of all commands
  exposed via the core-admin CLI, ensuring clarity and safety.
scope:
  applies_to: [cli, auditor]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "annual"

rules:
  - id: cli.must_be_registered
    statement: "All CLI commands MUST be declaratively registered in the `core.cli_commands` database table, which serves as the single source of truth."
    enforcement: error
  - id: cli.must_have_summary
    statement: "Every registered CLI command MUST have a concise summary for help text."
    enforcement: warn
  - id: cli.must_implement_capability
    statement: "Every CLI command SHOULD implement at least one semantic capability."
    enforcement: warn
--- END OF FILE ./.intent/charter/policies/governance/cli_governance_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/enforcement_model_policy.yaml ---
policy_id: 56dbc018-cb44-44a3-81aa-e9a2dd429069
# .intent/charter/policies/enforcement_model_policy.yaml
version: "2.0.0" # Version bump to signify harmonization
title: "Canonical Enforcement Model Policy"
purpose: >
  Provide the single, consistent meaning for all policy enforcement levels. This
  vocabulary MUST be used by all other policies and schemas to ensure the auditor
  can interpret and act on findings deterministically.

scope:
  applies_to:
    - governance
    - ci
    - agents

owners:
  primary: ["Governance Lead"]
  reviewers: ["Core Maintainer"]

review:
  frequency: "12 months"

levels:
  error:
    description: "A critical violation. This finding MUST block a merge/deploy. The auditor MUST return a non-zero exit code."
    ci_behavior: "fail"
    runtime_behavior: "block"
  warn:
    description: "A non-critical issue or deviation. The finding MUST be reported but SHOULD NOT block a merge/deploy. It must be tracked for resolution."
    ci_behavior: "pass_with_warnings"
    runtime_behavior: "log_and_continue"
  info:
    description: "An informational finding or observation. No action is required, but it provides context for a human reviewer."
    ci_behavior: "ignore"
    runtime_behavior: "ignore"

rules:
  - id: level_must_be_declared
    statement: "Every rule in every policy MUST specify an 'enforcement' level from the set {error, warn, info}."
    enforcement: error
  - id: auditor_maps_levels
    statement: "The constitutional auditor MUST map all findings to the levels defined above and respect their specified CI/runtime behavior."
    enforcement: error
--- END OF FILE ./.intent/charter/policies/governance/enforcement_model_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/intent_crate_policy.yaml ---
policy_id: 6bf96cc9-ffb4-493b-b6f8-05493b4b9d74
id: intent_crate_policy
version: "1.0.0"
title: "Intent Crate Processing Policy"
status: active
purpose: >
  To govern the autonomous, asynchronous processing of change requests (Intent Crates),
  ensuring all changes to the system's state are auditable, validated, and formally managed.

scope:
  applies_to: [governance, system]

owners:
  primary: "Governance Lead"
  reviewers: ["Core Maintainer"]

review:
  frequency: "6 months"

rules:
  - id: crate.location.inbox
    statement: "All new, unprocessed Intent Crates MUST reside in 'work/crates/inbox/'."
    enforcement: error
  
  - id: crate.state.must_move
    statement: "A crate MUST be moved from the inbox to 'processing', and then to 'accepted' or 'rejected'. It MUST NOT be modified in place."
    enforcement: error

  - id: crate.result.required
    statement: "Every processed crate in 'accepted' or 'rejected' MUST contain a 'result.yaml' file detailing the outcome and justification."
    enforcement: error

  - id: crate.acceptance.meta_sync
    statement: "Upon accepting a CONSTITUTIONAL_AMENDMENT crate that adds a new policy or schema, the system MUST autonomously update and commit '.intent/meta.yaml'."
    enforcement: error
  
  - id: crate.acceptance.scaffold_work
    statement: "If a new policy is accepted and requires new auditor checks, the system SHOULD scaffold the necessary check files or methods."
    enforcement: warn
--- END OF FILE ./.intent/charter/policies/governance/intent_crate_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/intent_guard_policy.yaml ---
policy_id: 1d311fd2-97db-462a-bacc-862e8f3c3777
version: "2.1.0" # Version bump for SSOT alignment
title: "Intent Guard Policy"
purpose: >
  Ensure that all actions remain aligned with the Constitution. This policy
  protects the immutable Charter from unauthorized modification and governs the
  process for safe, dynamic updates to the working Mind.

scope:
  applies_to:
    - agents
    - cli
    - governance
    - services

owners:
  primary: ["Governance Lead"]
  reviewers: ["Core Maintainer"]

review:
  frequency: "6 months"

rules:
  - id: charter.write_block
    statement: "The Charter (.intent/charter/**) is immutable. Runtime systems, agents, and tools MUST NOT write to it. Changes require a formal amendment."
    enforcement: error

  - id: charter.change_requires_proposal
    statement: "Any change to the Charter (.intent/charter/**) MUST be executed via a ratified proposal with the required approver quorum."
    enforcement: error

  - id: mind.writes_must_be_governed
    statement: "Writes to the Mind (.intent/mind/**) are permitted but MUST be governed. They must originate from a registered capability and pass all relevant safety and validation checks."
    enforcement: warn

  - id: single_active_constitution
    statement: "Exactly one active constitution version MUST be referenced by '.intent/charter/constitution/ACTIVE'."
    enforcement: error

  - id: auditor.block_on_violation
    statement: "If any guard rule with enforcement=error is violated, the constitutional auditor MUST fail."
    enforcement: error

# The rule 'cli.registry_source_of_truth' has been removed as it is now redundant.
# The canonical source for CLI commands is governed by 'cli_governance_policy.yaml'
# and 'database_policy.yaml', which the auditor is mandated to enforce.
--- END OF FILE ./.intent/charter/policies/governance/intent_guard_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/knowledge_source_policy.yaml ---
policy_id: 23a18582-a3a3-459b-b874-78b41f925097
id: knowledge_source_policy
version: "1.0.0"
title: "Knowledge Source of Truth Policy"
status: active
purpose: >
  Enforces the constitutional principle that the database is the single source
  of operational truth. This policy defines the narrow, explicit exceptions
  for tools that are permitted to interact with legacy or intermediate knowledge
  artifacts like knowledge_graph.json.
scope:
  applies_to: [governance, auditor]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "annual"

# This is the list of system tools that are granted a constitutional exception
# to read or write the knowledge_graph.json artifact. All other access is forbidden.
allowed_access_paths:
  - "src/features/introspection/knowledge_graph_service.py"
  - "src/features/governance/checks/knowledge_source_check.py"
--- END OF FILE ./.intent/charter/policies/governance/knowledge_source_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/logging_policy.yaml ---
policy_id: a1b2c3d4-e5f6-7a8b-9c0d-1e2f3a4b5c6e
id: logging_policy
version: "1.0.0"
title: "Unified Logging Policy"
status: active
purpose: >
  To ensure all system output is standardized, structured, and auditable by mandating
  the use of the shared logger and forbidding unsanctioned output channels like print().

scope:
  applies_to: [code, auditor]

owners:
  primary: "Core Maintainer"

review:
  frequency: "annual"

rules:
  - id: log.no_print_statements
    statement: "The use of print() is forbidden in application code (core, features, services). It is only permitted in the CLI layer for direct user output."
    enforcement: error
    allowed_paths:
      - "src/cli/**"
      - "tests/**" # Allow print in tests for debugging

  - id: log.no_direct_logging_import
    statement: "Direct import and configuration of the standard 'logging' module is forbidden. All loggers must be acquired via 'from shared.logger import getLogger'."
    enforcement: error
    allowed_paths:
      - "src/shared/logger.py" # The module itself is exempt.

--- END OF FILE ./.intent/charter/policies/governance/logging_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/reporting_policy.yaml ---
policy_id: 93e18e85-75cb-47bf-8665-0f793d89b65d
id: reporting_policy
version: 1
title: "Generated Artifacts & Reporting Policy"
status: active
purpose: >
  To constitutionally define the location, format, and governance of all
  transient, machine-generated artifacts (reports, logs, bundles, etc.).
  This policy enforces a strict separation between the permanent, source-of-truth
  "Mind" (.intent/) and the ephemeral outputs of system actions.

scope:
  applies_to: ["agents", "tooling", "cli", "auditor"]

owners:
  accountable: "Core Maintainer"

review:
  frequency: "12 months"

rules:
  - id: reports.canonical_output_directory
    statement: "All generated reports and non-constitutional artifacts MUST be written to the 'reports/' directory at the repository root."
    enforcement: error
  - id: reports.require_header
    statement: "All human-readable text or YAML reports MUST begin with a standardized header block that clearly identifies them as generated artifacts."
    enforcement: warn
  - id: reports.retention_policy
    statement: "Reports are considered ephemeral and MAY be deleted after 30 days. They SHOULD NOT be checked into source control."
    enforcement: info

definitions:
  output_directory: "reports/"
  header_template: |
    # ==============================================================================
    # WARNING: THIS IS A GENERATED REPORT. DO NOT EDIT MANUALLY.
    # It is a transient artifact and SHOULD NOT be used as a primary source of data.
    # Source of Constitutional Truth: .intent/
    # Source of Operational History: CORE Database
    # Generated By: {tool_name}
    # Generated At: {timestamp_utc}
    # ==============================================================================

--- END OF FILE ./.intent/charter/policies/governance/reporting_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/governance/tooling_policy.yaml ---
policy_id: b7c95ae3-05be-4138-8b27-12cd5670f4e9
id: tooling_policy
version: "1.1.0" # Version bump for SSOT alignment
title: "Tooling Policy"
status: active
purpose: >
  To define the sanctioned tools and how they are invoked by agents and operators,
  ensuring reproducibility, performance, and constitutional compliance.
scope:
  applies_to: [cli, agents, services]
owners:
  accountable: "Core Maintainer"
review:
  frequency: "annual"

rules:
  - id: tools.registered_only
    statement: "Only tools/commands registered in the `core.cli_commands` database table may be invoked by agents/services."
    enforcement: error
  - id: tools.version_pinned
    statement: "Critical tools (linters, formatters, test runners) MUST be version-pinned in pyproject or lockfiles."
    enforcement: warn
  - id: tools.no_write_intent
    statement: "Tooling MUST NOT write to '.intent/charter/**'; proposals are the only path for constitutional changes."
    enforcement: error
--- END OF FILE ./.intent/charter/policies/governance/tooling_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/operations/canary_policy.yaml ---
policy_id: 5bc9a0e5-a6fb-47be-ab2f-82bce1217c84
id: canary_policy
version: "1.0.0"
title: "Canary Policy"
status: active

owners:
  - "Governance Lead"

review:
  frequency: "annual"

canary:
  enabled: true
  scope:
    paths:
      - "src/**"
      - "cli/**"
      - "agents/**"
    modes:
      - "development"
      - "staging"
  abort_conditions:
    - "audit:level=error"
    - "tests:failed>0"
    - "latency:p95>threshold"
  metrics:
    - name: "audit.errors"
      threshold: 0
      direction: "less"
    - name: "tests.failed"
      threshold: 0
      direction: "less"
    - name: "latency.p95.ms"
      threshold: 500
      direction: "less"

--- END OF FILE ./.intent/charter/policies/operations/canary_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/operations/dev_fastpath_policy.yaml ---
policy_id: 4923517d-7e30-48dc-8134-b119714ed2b8
id: dev_fastpath_policy
version: "1.0.0"
title: Developer Fastpath Policy
purpose: >
  Allow fast, local feedback loops for developers while preserving safety via CI hard gates.
scope:
  applies_to:
    - developers
    - cli
    - ci
owners:
  primary: ["DX Lead"]
  reviewers: ["Governance Lead"]
review:
  frequency: "12 months"

rules:
  thresholds:
    max_changed_files: 20
    allow_intent_changes: false
  required_checks:
    - syntax
    - linter
    - formatter
  disallowed_paths:
    - ".intent/**"
    - "src/system/governance/**"

checklist:
  - Pre-commit runs syntax + linter + formatter on changed files (< thresholds).
  - Pre-push runs a mini-audit; CI enforces full auditor run.
  - Any change under .intent/** or governance/** bypasses fastpath and triggers full checks.
--- END OF FILE ./.intent/charter/policies/operations/dev_fastpath_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/operations/incident_response_policy.yaml ---
policy_id: 95df7291-6fc2-4a5b-9f6d-78e9af5ac200
id: incident_response_policy
version: "1.0.0"
title: "Incident Response Policy"
status: active
purpose: >
  Provide a lightweight, auditable process to respond to security and governance incidents.
scope:
  applies_to: [security, governance, ci, services]
owners:
  primary: "Security Lead"
  reviewers: ["Governance Lead"]
review:
  frequency: "12 months"

severity:
  - low
  - medium
  - high
  - critical

rules:
  - id: ir.triage_required
    statement: All incidents MUST be triaged within 24h with severity and owner assigned.
    enforcement: error
  - id: ir.timeline
    statement: A minimal timeline (what happened, when, who, evidence) MUST be recorded.
    enforcement: warn
  - id: ir.comms
    statement: Notifications MUST be sent to maintainers for high/critical incidents.
    enforcement: warn
  - id: ir.postmortem
    statement: High/critical incidents REQUIRE a short postmortem with actions and owners.
    enforcement: warn

checklist:
  - Auditor verifies incident records exist for flagged events (secrets exposure, DB policy violations).
  - Auditor checks postmortems for high/critical incidents within 7 days.
--- END OF FILE ./.intent/charter/policies/operations/incident_response_policy.yaml ---

--- START OF FILE ./.intent/charter/policies/safety_policy.yaml ---
policy_id: 05ffbf34-2e0e-4069-b77e-473923537077
id: safety_policy
version: "1.0.0"
title: "System Safety & Security Policy"
status: active
purpose: >
  The single source of truth for all security and safety policies governing
  code generation, execution, and self-modification.
scope:
  applies_to: [agents, code, ci, services]
owners:
  primary: "Security Lead"
  reviewers: ["Core Maintainer"]
review:
  frequency: "12 months"

rules:
  # ===================================================================
  # RULE: Govern Self-Modification (Immutable Constitution)
  # ===================================================================
  - id: immutable_constitution
    description: >
      The core mission files are immutable and can only be changed via the full,
      human-in-the-loop constitutional amendment process.
    enforcement: warn # This is a meta-rule; its enforcement is the amendment process itself.
    applies_to:
      paths:
        - "charter/mission/principles.yaml"
        - "charter/mission/manifesto.md"
        - "charter/mission/northstar.yaml"

  # ===================================================================
  # RULE: No self-modification of core loop
  # ===================================================================
  - id: deny_core_loop_edit
    description: >
      CORE cannot modify its own core orchestration and governance engine
      without explicit human review via the formal amendment process.
    enforcement: error # An automated change here is a critical violation.
    applies_to:
      paths:
        - "src/core/main.py"
        - "src/core/intent_guard.py"
        - "charter/policies/intent_guard_policy.yaml"
        - "charter/policies/safety_policy.yaml"
    action: require_human_approval
    feedback: |
      🔒 Core logic modification detected. Human review required before application.

  # ===================================================================
  # RULE: Block dangerous execution primitives
  # ===================================================================
  - id: no_dangerous_execution
    description: >
      Generated or modified code must not contain calls to dangerous functions
      that enable arbitrary code execution, shell access, or unsafe deserialization.
    enforcement: error
    scope:
      domains: [core, agents, features]
      exclude:
        - path: "tests/**"
          rationale: "Test files require direct execution for validation"
        - path: "src/core/git_service.py"
          rationale: >
            This file is exempt as it safely uses subprocess.run() without shell=True.
    detection:
      type: regex
      patterns:
        - "eval\\("
        - "exec\\("
        - "compile\\("
        - "os\\.system\\("
        - "os\\.popen\\("
        - "subprocess\\.(run|Popen|call)\\([^)]*shell\\s*=\\s*True"
        - "shutil\\.rmtree\\("
        - "os\\.remove\\("
        - "os\\.rmdir\\("
    action: reject
    feedback: |
      ❌ Dangerous execution detected: '{{pattern}}'. Use safe wrappers or avoid shell=True.

  # ===================================================================
  # RULE: Restrict network access
  # ===================================================================
  - id: restrict_network_access
    description: >
      Only explicitly allowed domains may be contacted. All outbound network
      calls must be through approved integration points.
    enforcement: error
    evidence: ["network_access_log"]
    allowed_domains:
      - "api.openai.com"
      - "github.com"
      - "api.deepseek.com"
      - "api.anthropic.com"
    action: reject
    feedback: |
      ❌ Attempt to contact unauthorized domain: {{domain}}. Update safety_policy.yaml to allow if needed.

  # ===================================================================
  # RULE: All changes must be logged
  # ===================================================================
  - id: change_must_be_logged
    description: >
      Every file change must be preceded by a log entry recorded at CORE_ACTION_LOG_PATH
      with IntentBundle ID and description.
    enforcement: error
    triggers:
      - before_write
    validator: change_log_checker
    action: reject_if_unlogged
    feedback: |
      ❌ No prior log entry found for this change. Write to CORE_ACTION_LOG_PATH before modifying files.
--- END OF FILE ./.intent/charter/policies/safety_policy.yaml ---

--- START OF FILE ./.intent/charter/schemas/agent/agent_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/agent_policy_schema.json",
  "title": "Agent Policy",
  "description": "Constitutional schema for the single, authoritative agent governance policy.",
  "type": "object",
  "required": ["id", "version", "title", "status", "purpose", "rules"],
  "properties": {
    "id": { "const": "agent_policy" },
    "version": { "type": "string", "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$" },
    "title": { "type": "string" },
    "status": { "enum": ["active", "draft", "deprecated"] },
    "purpose": { "type": "string" },
    "scope": { "type": "object" },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "rules": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "statement", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "enforcement": { "enum": ["info", "warn", "error"] }
        },
        "additionalProperties": false
      }
    },
    "resource_selection": {
      "type": "object"
    }
  },
  "additionalProperties": true
}
--- END OF FILE ./.intent/charter/schemas/agent/agent_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/agent/cognitive_roles_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Cognitive Roles Policy",
  "description": "Schema for defining abstract cognitive roles and assigning them to named resources.",
  "type": "object",
  "required": ["cognitive_roles"],
  "properties": {
    "cognitive_roles": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["role", "description", "assigned_resource", "required_capabilities"],
        "properties": {
          "role": {
            "type": "string",
            "description": "The unique name of the cognitive role (e.g., 'Planner')."
          },
          "description": {
            "type": "string"
          },
          "assigned_resource": {
            "type": "string",
            "description": "The named resource (e.g., 'deepseek_chat') to assign to this role."
          },
          "required_capabilities": {
            "type": "array",
            "description": "A list of skills required by this role for validation.",
            "items": { "type": "string" }
          }
        }
      }
    }
  },
  "additionalProperties": false
}
--- END OF FILE ./.intent/charter/schemas/agent/cognitive_roles_schema.json ---

--- START OF FILE ./.intent/charter/schemas/agent/micro_proposal_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/micro_proposal_policy_schema.json",
  "title": "Micro-Proposal Policy",
  "description": "Constitutional schema for the policy governing low-risk, autonomous changes.",
  "type": "object",
  "required": ["id", "version", "title", "status", "purpose", "rules"],
  "properties": {
    "id": { "const": "micro_proposal_policy" },
    "version": { "type": "string", "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$" },
    "title": { "type": "string" },
    "status": { "enum": ["active", "draft", "deprecated"] },
    "purpose": { "type": "string" },
    "scope": { "type": "object" },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "rules": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "description", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "description": { "type": "string" },
          "enforcement": { "enum": ["info", "warn", "error"] },
          "allowed_actions": {
            "type": "array",
            "items": { "type": "string" }
          },
          "allowed_paths": {
            "type": "array",
            "items": { "type": "string" }
          },
          "forbidden_paths": {
            "type": "array",
            "items": { "type": "string" }
          },
          "required_evidence": {
            "type": "array",
            "items": { "type": "string" }
          }
        },
        "additionalProperties": true
      }
    }
  },
  "additionalProperties": true
}
--- END OF FILE ./.intent/charter/schemas/agent/micro_proposal_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/agent/resource_manifest_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "LLM Resource Manifest Policy",
  "description": "Constitutional schema for the policy defining available LLM resources.",
  "type": "object",
  "allOf": [{ "$ref": "policy_schema.json" }],
  "properties": {
    "id": { "const": "resource_manifest_policy" },
    "llm_resources": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["name", "provided_capabilities", "env_prefix"],
        "properties": {
          "name": { "type": "string" },
          "provided_capabilities": { "type": "array", "items": { "type": "string" } },
          "env_prefix": { "type": "string" },
          "performance_metadata": { "type": "object" }
        }
      }
    }
  },
  "required": ["llm_resources"]
}
--- END OF FILE ./.intent/charter/schemas/agent/resource_manifest_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/code/capability_tag_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "CORE Capability Tag Definition",
  "description": "The formal schema for a single, well-defined capability in the CORE system.",
  "type": "object",
  "required": [
    "key",
    "title",
    "description",
    "owner",
    "status",
    "risk_level"
  ],
  "properties": {
    "key": {
      "type": "string",
      "description": "The unique, canonical identifier for the capability, following the 'domain.action' pattern.",
      "pattern": "^[a-z0-9_]+(\\.[a-z0-9_]+)+$"
    },
    "title": {
      "type": "string",
      "description": "A short, human-readable title for the capability.",
      "minLength": 5
    },
    "description": {
      "type": "string",
      "description": "A clear, one-sentence explanation of what this capability does.",
      "minLength": 10
    },
    "owner": {
      "type": "string",
      "description": "The architectural domain that owns and is responsible for this capability."
    },
    "status": {
      "type": "string",
      "description": "The current lifecycle status of the capability.",
      "enum": ["active", "deprecated", "experimental"]
    },
    "risk_level": {
      "type": "string",
      "description": "The assessed risk of invoking this capability (low, medium, or high).",
      "enum": ["low", "medium", "high"]
    },
    "aliases": {
      "type": "array",
      "description": "A list of old or alternative names for this capability to ensure backward compatibility.",
      "items": {
        "type": "string"
      },
      "uniqueItems": true
    },
    "policy_refs": {
      "type": "array",
      "description": "A list of policy files that govern or relate to this capability.",
      "items": {
        "type": "string"
      }
    },
    "vector": {
      "type": "array",
      "description": "A pre-computed semantic vector representing the meaning of the capability's source code.",
      "items": {
        "type": "number"
      }
    }
  },
  "additionalProperties": false
}
--- END OF FILE ./.intent/charter/schemas/code/capability_tag_schema.json ---

--- START OF FILE ./.intent/charter/schemas/code/knowledge_graph_entry_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://core.system/schema/knowledge_graph_entry.json",
  "title": "Knowledge Graph Symbol Entry",
  "description": "Schema for a single symbol (function or class) in the knowledge_graph.json file.",
  "type": "object",
  "required": [
    "key",
    "name",
    "type",
    "file",
    "capability",
    "intent",
    "last_updated",
    "calls",
    "line_number",
    "is_async",
    "parameters",
    "is_class",
    "structural_hash"
  ],
  "properties": {
    "key": { "type": "string", "description": "The unique identifier for the symbol (e.g., 'path/to/file.py::MyClass')." },
    "name": { "type": "string", "description": "The name of the function or class." },
    "type": { "type": "string", "enum": ["FunctionDef", "ClassDef", "AsyncFunctionDef"] },
    "file": { "type": "string", "description": "The relative path to the source file." },
    "tags": {
      "type": "array",
      "items": { "type": "string" },
      "description": "A list of domain tags classifying the symbol's purpose."
    },
    "owner": {
      "type": "string",
      "description": "The agent or team responsible for this capability."
    },
    "capability": { "type": "string", "description": "The unique UUID of the capability this symbol implements, or 'unassigned'." },
    "intent": { "type": "string", "description": "A clear, concise statement of the symbol's purpose." },
    "docstring": { "type": ["string", "null"], "description": "The raw docstring from source code." },
    "calls": { "type": "array", "items": { "type": "string" }, "description": "List of other functions called by this one." },
    "line_number": { "type": "integer", "minimum": 0 },
    "is_async": { "type": "boolean" },
    "parameters": { "type": "array", "items": { "type": "string" } },
    "entry_point_type": { "type": ["string", "null"], "description": "Type of entry point if applicable (e.g., 'fastapi_route_post')." },
    "last_updated": { "type": "string", "format": "date-time" },
    "is_class": { "type": "boolean", "description": "True if the symbol is a class definition." },
    "base_classes": {
      "type": "array",
      "items": { "type": "string" },
      "description": "A list of base classes this symbol inherits from (if it is a class)."
    },
    "entry_point_justification": {
      "type": ["string", "null"],
      "description": "The name of the pattern that identified this symbol as an entry point."
    },
    "parent_class_key": {
      "type": ["string", "null"],
      "description": "The key of the parent class, if this symbol is a method."
    },
    "structural_hash": {
      "type": "string",
      "description": "A SHA256 hash of the symbol's structure, ignoring comments and docstrings."
    },
    "vector": {
      "type": ["array", "null"],
      "description": "A pre-computed semantic vector representing the meaning of the capability's source code.",
      "items": {
        "type": "number"
      }
    },
    "end_line_number": {
      "type": ["integer", "null"],
      "description": "The line number where the symbol's definition ends."
    },
    "source_code": {
      "type": ["string", "null"],
      "description": "The exact, unparsed source code of the symbol."
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/charter/schemas/code/knowledge_graph_entry_schema.json ---

--- START OF FILE ./.intent/charter/schemas/constitutional/intent_bundle_schema.json ---
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Intent Bundle",
    "description": "A schema for the structured data package representing a single, reasoned action by the CORE system.",
    "type": "object",
    "required": [
        "bundle_id",
        "initiator",
        "created_at",
        "goal",
        "justification",
        "risk_tier",
        "status",
        "evidence"
    ],
    "properties": {
        "bundle_id": {
            "type": "string",
            "description": "A unique identifier for this bundle of work."
        },
        "initiator": {
            "type": "string",
            "description": "The human operator or system agent that initiated the action."
        },
        "created_at": {
            "type": "string",
            "format": "date-time"
        },
        "goal": {
            "type": "string",
            "description": "The high-level goal this bundle is intended to achieve."
        },
        "justification": {
            "type": "string",
            "description": "The constitutional principle(s) this action serves."
        },
        "risk_tier": {
            "type": "string",
            "enum": ["low", "medium", "high"],
            "description": "The assessed risk level of the proposed change."
        },
        "status": {
            "type": "string",
            "enum": ["draft", "planned", "validated", "approved", "executed", "archived", "failed"],
            "description": "The current state in the lifecycle of the bundle."
        },
        "evidence": {
            "type": "object",
            "description": "A collection of links to artifacts that support this action.",
            "properties": {
                "plan_id": { "type": "string" },
                "validation_report_id": { "type": "string" },
                "canary_report_id": { "type": "string" },
                "test_report_id": { "type": "string" },
                "approval_signature_ids": {
                    "type": "array",
                    "items": { "type": "string" }
                }
            }
        }
    }
}
--- END OF FILE ./.intent/charter/schemas/constitutional/intent_bundle_schema.json ---

--- START OF FILE ./.intent/charter/schemas/constitutional/intent_crate_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/intent_crate_schema.json",
  "title": "Intent Crate Manifest",
  "description": "The constitutional schema for a manifest.yaml file within an Intent Crate. This defines a formal, auditable request for change.",
  "type": "object",
  "required": ["crate_id", "author", "intent", "type"],
  "properties": {
    "crate_id": {
      "type": "string",
      "description": "A unique identifier for this crate, typically matching the directory name.",
      "pattern": "^[a-zA-Z0-9_-]+$"
    },
    "author": {
      "type": "string",
      "description": "The identity of the human or system that created the crate (e.g., an email address)."
    },
    "intent": {
      "type": "string",
      "description": "A clear, one-sentence justification for the proposed change.",
      "minLength": 20
    },
    "type": {
      "type": "string",
      "description": "The type of change being proposed.",
      "enum": ["CONSTITUTIONAL_AMENDMENT", "CODE_MODIFICATION"]
    },
    "payload_files": {
        "type": "array",
        "description": "A list of the files included in this crate that are part of the change.",
        "items": {
            "type": "string"
        }
    }
  },
  "additionalProperties": false
}
--- END OF FILE ./.intent/charter/schemas/constitutional/intent_crate_schema.json ---

--- START OF FILE ./.intent/charter/schemas/constitutional/proposal_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://core.local/schemas/proposal.schema.json",
  "title": "CORE Proposal (v1)",
  "type": "object",
  "additionalProperties": false,
  "required": ["target_path", "action", "justification", "content"],
  "properties": {
    "target_path": {
      "type": "string",
      "description": "Repo-relative path to the file to be replaced. Must not be inside .intent/proposals/.",
      "pattern": "^(?!\\.intent\\/proposals\\/)[\\w\\-\\.\\/]+$",
      "$comment": "Allows any path as long as it's not writing into the proposals directory itself."
    },
    "action": {
      "type": "string",
      "enum": ["replace_file"],
      "description": "Currently only full file replacement is supported."
    },
    "justification": {
      "type": "string",
      "minLength": 10,
      "description": "Human-readable rationale for the change.",
      "pattern": "\\S"
    },
    "content": {
      "type": "string",
      "minLength": 1
    },
    "signatures": {
      "type": "array",
      "description": "Optional array of signature objects.",
      "items": { "$ref": "#/$defs/signature" }
    }
  },
  "$defs": {
    "signature": {
      "type": "object",
      "additionalProperties": false,
      "required": ["identity", "signature_b64", "token", "timestamp"],
      "properties": {
        "identity": { "type": "string" },
        "signature_b64": { "type": "string", "contentEncoding": "base64" },
        "token": {
          "type": "string",
          "pattern": "^core-proposal-v[0-9]+:[a-f0-9]{64}$",
          "$comment": "Allows any version number for the token, e.g., v1, v6."
        },
        "timestamp": { "type": "string", "format": "date-time" }
      }
    }
  }
}
--- END OF FILE ./.intent/charter/schemas/constitutional/proposal_schema.json ---

--- START OF FILE ./.intent/charter/schemas/data/database_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Database Policy Schema",
  "type": "object",
  "required": ["id", "version", "title", "engine", "migrations", "rules", "drift"],
  "properties": {
    "id": { "const": "database_policy" },
    "version": { "type": "string" },
    "title": { "type": "string" },
    "engine": {
      "type": "object",
      "required": ["type", "schema"],
      "properties": {
        "type": { "type": "string", "enum": ["postgresql"] },
        "schema": { "type": "string", "minLength": 1 }
      }
    },
    "migrations": {
      "type": "object",
      "required": ["directory", "order"],
      "properties": {
        "directory": { "type": "string" },
        "order": {
          "type": "array",
          "items": { "type": "string", "pattern": "^\\d{3}_.+\\.sql$" },
          "minItems": 1
        }
      }
    },
    "rules": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "statement", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "enforcement": { "enum": ["error", "warn", "info"] }
        },
        "additionalProperties": false
      }
    },
    "retention": {
      "type": "object",
      "properties": {
        "audit_runs_days": { "type": "integer", "minimum": 1 },
        "proposals_days": { "type": "integer", "minimum": 1 }
      },
      "additionalProperties": true
    },
    "drift": {
      "type": "object",
      "required": ["development", "production"],
      "properties": {
        "development": { "type": "string", "enum": ["warn", "block"] },
        "production": { "type": "string", "enum": ["warn", "block"] }
      }
    },
    "backup_restore": {
      "type": "object",
      "properties": {
        "cadence": { "type": "string" },
        "test_restore_quarterly": { "type": "boolean" }
      }
    },
    "quorum": {
      "type": "object",
      "properties": {
        "changes_require_critical_paths": { "type": "boolean" }
      }
    }
  },
  "additionalProperties": true
}
--- END OF FILE ./.intent/charter/schemas/data/database_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/data/database_schema.yaml ---
version: 1
description: >
  Initial schema for CORE's operational database.
  Stores auditable history of events, not constitutional truth.

tables:
  capabilities:
    description: >
      Current catalog of capabilities with their owners and tags.
      Mirrors .intent/knowledge/domains but may include runtime metadata.
    columns:
      - name: key
        type: text
        constraints: [primary_key]
      - name: title
        type: text
      - name: description
        type: text
      - name: owner
        type: text
      - name: tags
        type: jsonb
      - name: updated_at
        type: timestamptz

  capability_history:
    description: Versioned history of capability changes.
    columns:
      - name: id
        type: bigserial
        constraints: [primary_key]
      - name: capability_key
        type: text
      - name: change_type
        type: text   # created, updated, deleted
      - name: diff
        type: jsonb
      - name: changed_at
        type: timestamptz

  cli_runs:
    description: >
      Each execution of a core-admin command with timestamp and result.
    columns:
      - name: id
        type: bigserial
        constraints: [primary_key]
      - name: command
        type: text
      - name: args
        type: jsonb
      - name: result
        type: text   # success, fail
      - name: run_at
        type: timestamptz

  audits:
    description: >
      Records every constitutional audit or validation run.
    columns:
      - name: id
        type: bigserial
        constraints: [primary_key]
      - name: scope
        type: text
      - name: result
        type: jsonb
      - name: run_at
        type: timestamptz

migrations:
  - id: 0001-initial
    description: Initial schema creation for capabilities, capability_history, cli_runs, audits.
    created_at: "2025-09-18"
    approved_by: TBD


--- END OF FILE ./.intent/charter/schemas/data/database_schema.yaml ---

--- START OF FILE ./.intent/charter/schemas/governance/available_actions_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Available Actions Policy",
  "description": "Defines the complete set of actions available to the PlannerAgent.",
  "type": "object",
  "allOf": [{ "$ref": "../policy_schema.json" }],
  "properties": {
    "id": { "const": "available_actions_policy" },
    "actions": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["name", "description"],
        "properties": {
          "name": { "type": "string" },
          "description": { "type": "string" },
          "required_parameters": { "type": "array", "items": { "type": "string" } }
        }
      }
    }
  },
  "required": ["actions"]
}
--- END OF FILE ./.intent/charter/schemas/governance/available_actions_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/governance/cli_registry_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "CLI Registry Policy",
  "description": "The constitutional policy that serves as the single source of truth for all registered core-admin CLI commands.",
  "type": "object",
  "allOf": [{ "$ref": "../policy_schema.json" }],
  "properties": {
    "id": { "const": "cli_registry_policy" },
    "commands": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["name", "summary", "entrypoint", "category"],
        "properties": {
          "name": { "type": "string" },
          "module": { "type": "string" },
          "entrypoint": { "type": "string" },
          "summary": { "type": "string" },
          "category": { "type": "string" }
        }
      }
    }
  },
  "required": ["commands"]
}
--- END OF FILE ./.intent/charter/schemas/governance/cli_registry_schema.json ---

--- START OF FILE ./.intent/charter/schemas/governance/enforcement_model_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Enforcement Model Policy Schema",
  "description": "Schema for the canonical enforcement model policy.",
  "type": "object",
  "required": [
    "version",
    "title",
    "purpose",
    "levels",
    "rules"
  ],
  "properties": {
    "version": { "type": "string", "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$" },
    "title": { "type": "string" },
    "purpose": { "type": "string" },
    "scope": { "type": "object" },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "levels": {
      "type": "object",
      "required": ["error", "warn", "info"],
      "properties": {
        "error": { "$ref": "#/definitions/level" },
        "warn": { "$ref": "#/definitions/level" },
        "info": { "$ref": "#/definitions/level" }
      },
      "additionalProperties": false
    },
    "rules": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["id", "statement", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "enforcement": { "enum": ["error", "warn", "info"] }
        }
      }
    }
  },
  "additionalProperties": false,
  "definitions": {
    "level": {
      "type": "object",
      "required": ["description", "ci_behavior", "runtime_behavior"],
      "properties": {
        "description": { "type": "string" },
        "ci_behavior": { "type": "string" },
        "runtime_behavior": { "type": "string" }
      },
      "additionalProperties": false
    }
  }
}
--- END OF FILE ./.intent/charter/schemas/governance/enforcement_model_schema.json ---

--- START OF FILE ./.intent/charter/schemas/governance/intent_guard_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Intent Guard Policy",
  "description": "Schema for the core IntentGuard rules that prevent unauthorized system modifications.",
  "type": "object",
  "required": ["rules"],
  "properties": {
    "rules": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "description", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "description": { "type": "string" },
          "enforcement": {
            "type": "string",
            "enum": ["error", "warn", "info"]
          },
          "applies_to": {
            "type": "object",
            "properties": {
              "paths": { "type": "array", "items": { "type": "string" } }
            }
          },
          "exclude": {
            "type": "object",
            "properties": {
              "paths": { "type": "array", "items": { "type": "string" } }
            }
          }
        }
      }
    }
  },
  "additionalProperties": false
}
--- END OF FILE ./.intent/charter/schemas/governance/intent_guard_schema.json ---

--- START OF FILE ./.intent/charter/schemas/governance/reporting_policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/reporting_policy.schema.json",
  "title": "Reporting Policy",
  "description": "Constitutional schema for governing generated artifacts and reports.",
  "type": "object",
  "required": ["id", "version", "purpose", "rules", "definitions"],
  "additionalProperties": false,
  "properties": {
    "id": { "const": "reporting_policy" },
    "version": { "type": "integer" },
    "title": { "type": "string" },
    "status": { "enum": ["active", "draft"] },
    "purpose": { "type": "string" },
    "scope": { "type": "object" },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "rules": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["id", "statement", "enforcement"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "enforcement": { "enum": ["info", "warn", "error"] }
        }
      }
    },
    "definitions": {
      "type": "object",
      "required": ["output_directory", "header_template"],
      "properties": {
        "output_directory": { "type": "string" },
        "header_template": { "type": "string" }
      }
    }
  }
}
--- END OF FILE ./.intent/charter/schemas/governance/reporting_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/operations/canary_policy_schema.json ---
{
"$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "canary_policy.schema.json",
  "title": "Canary Policy",
  "type": "object",
  "required": ["id", "version", "title", "owners", "review", "canary"],
  "properties": {
    "id": { "type": "string", "pattern": "^[a-z0-9_.-]+$" },
    "version": { "type": "string", "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$" },
    "title": { "type": "string" },
    "status": { "type": "string", "enum": ["draft", "active", "deprecated"] },
    "owners": { "type": "array", "items": { "type": "string" }, "minItems": 1 },
    "review": {
      "type": "object",
      "required": ["frequency"],
      "properties": {
        "frequency": { "type": "string", "enum": ["monthly", "quarterly", "semiannual", "annual"] },
        "last_reviewed": { "type": "string", "format": "date" }
      },
      "additionalProperties": false
    },
    "canary": {
      "type": "object",
      "required": ["enabled", "scope", "abort_conditions"],
      "properties": {
        "enabled": { "type": "boolean" },
        "scope": {
          "type": "object",
          "properties": {
            "paths": { "type": "array", "items": { "type": "string" } },
            "modes": { "type": "array", "items": { "type": "string", "enum": ["development", "staging", "production"] } }
          },
          "additionalProperties": false
        },
        "abort_conditions": { "type": "array", "items": { "type": "string" }, "minItems": 1 },
        "metrics": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["name", "threshold", "direction"],
            "properties": {
              "name": { "type": "string" },
              "threshold": { "type": "number" },
              "direction": { "type": "string", "enum": ["greater", "less"] }
            },
            "additionalProperties": false
          }
        }
      },
      "additionalProperties": false
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/charter/schemas/operations/canary_policy_schema.json ---

--- START OF FILE ./.intent/charter/schemas/operations/config_schema.yaml ---
# .intent/schemas/config_schema.yaml
git:
  ignore_validation:
    type: boolean
    default: false
    description: >
      If true, skips Git pre-write checks. MUST be false in production or fallback modes
      to maintain rollback safety. Only for emergency recovery.
--- END OF FILE ./.intent/charter/schemas/operations/config_schema.yaml ---

--- START OF FILE ./.intent/charter/schemas/operations/runtime_requirements_schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Runtime Requirements",
  "type": "object",
  "required": ["id", "version", "title", "status", "variables", "owners", "review"],
  "properties": {
    "id": { "const": "runtime_requirements" },
    "version": { "type": "integer", "minimum": 1 },
    "title": { "type": "string" },
    "status": { "enum": ["active", "draft", "archived"] },
    "owners": { "type": "object" },
    "review": { "type": "object" },
    "variables": {
      "type": "object",
      "minProperties": 1,
      "patternProperties": {
        "^[A-Z0-9_]+$": {
          "type": "object",
          "required": ["description", "source", "required", "type", "used_by"],
          "properties": {
            "description": { "type": "string" },
            "source": { "enum": ["env", "secret", "cli"] },
            "required": { "type": "boolean" },
            "type": { "enum": ["string", "integer", "bool", "enum", "uri", "path"] },
            "allowed": { "type": "array", "items": { "type": "string" } },
            "default": {},
            "used_by": { "type": "array", "items": { "type": "string" } },
            "required_when": { "type": "string" }
          }
        }
      },
      "additionalProperties": false
    }
  },
  "additionalProperties": false
}
--- END OF FILE ./.intent/charter/schemas/operations/runtime_requirements_schema.json ---

--- START OF FILE ./.intent/charter/schemas/policy_schema.json ---
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://core.local/schemas/policy_schema.json",
  "title": "CORE Policy",
  "description": "The canonical schema for all constitutional policy files in .intent/charter/policies/.",
  "type": "object",
  "required": ["policy_id", "id", "version", "title", "purpose", "status", "owners", "review"],
  "properties": {
    "policy_id": {
      "type": "string",
      "description": "A unique and stable UUID for this policy document.",
      "pattern": "^[0-9a-fA-F]{8}-([0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12}$"
    },
    "id": {
      "type": "string",
      "description": "The unique, snake_case identifier for the policy, matching the file name (e.g., 'agent_policy').",
      "pattern": "^[a-z0-9_]+_policy$"
    },
    "version": {
      "type": "string",
      "description": "The semantic version of the policy document.",
      "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$"
    },
    "title": {
      "type": "string",
      "description": "A human-readable, Title Case name for the policy."
    },
    "purpose": {
      "type": "string",
      "description": "A concise, one or two-sentence explanation of why this policy exists."
    },
    "status": {
      "type": "string",
      "description": "The current lifecycle status of the policy.",
      "enum": ["active", "draft", "deprecated"]
    },
    "owners": {
      "type": "object",
      "description": "Defines the roles responsible for maintaining this policy.",
      "properties": {
        "primary": { "type": "string" },
        "reviewers": { "type": "array", "items": { "type": "string" } }
      },
      "required": ["primary"]
    },
    "review": {
      "type": "object",
      "description": "Specifies the review cadence for this policy.",
      "properties": {
        "frequency": { "type": "string", "description": "e.g., '12 months', 'quarterly'" },
        "last_reviewed": { "type": "string", "format": "date" }
      },
      "required": ["frequency"]
    }
  },
  "additionalProperties": true
}
--- END OF FILE ./.intent/charter/schemas/policy_schema.json ---

--- START OF FILE ./.intent/meta.yaml ---
version: "0.7.0" # Ratification of the Final Constitutional Alignment
# PURPOSE: This is the master index for the entire CORE constitution. It maps
# abstract concepts to their concrete file paths, fully embracing the new
# hierarchical policy structure for maximum clarity and governance.

charter:
  constitution:
    active_version: "charter/constitution/ACTIVE"
    amendment_process: "charter/constitution/amendment_process.md"
    approvers: "charter/constitution/approvers.yaml"
    critical_paths: "charter/constitution/critical_paths.yaml"
    operator_lifecycle: "charter/constitution/operator_lifecycle.md"

  mission:
    manifesto: "charter/mission/manifesto.md"
    northstar: "charter/mission/northstar.yaml"
    principles: "charter/mission/principles.yaml"

  policies:
    safety_policy: "charter/policies/safety_policy.yaml"

    agent:
      agent_policy: "charter/policies/agent/agent_policy.yaml"
      micro_proposal_policy: "charter/policies/agent/micro_proposal_policy.yaml"

    code:
      capability_linter_policy: "charter/policies/code/capability_linter_policy.yaml"
      code_health_policy: "charter/policies/code/code_health_policy.yaml"
      code_style_policy: "charter/policies/code/code_style_policy.yaml"
      naming_conventions_policy: "charter/policies/code/naming_conventions_policy.yaml"
      refactoring_patterns_policy: "charter/policies/code/refactoring_patterns_policy.yaml"

    data:
      database_policy: "charter/policies/data/database_policy.yaml"
      secrets_management_policy: "charter/policies/data/secrets_management_policy.yaml"

    governance:
      audit_ignore_policy: "charter/policies/governance/audit_ignore_policy.yaml"
      auditor_policy: "charter/policies/governance/auditor_policy.yaml"
      available_actions_policy: "charter/policies/governance/available_actions_policy.yaml"
      cli_governance_policy: "charter/policies/governance/cli_governance_policy.yaml"
      enforcement_model_policy: "charter/policies/governance/enforcement_model_policy.yaml"
      intent_crate_policy: "charter/policies/governance/intent_crate_policy.yaml"
      intent_guard_policy: "charter/policies/governance/intent_guard_policy.yaml"
      knowledge_source_policy: "charter/policies/governance/knowledge_source_policy.yaml"
      logging_policy: "charter/policies/governance/logging_policy.yaml"
      reporting_policy: "charter/policies/governance/reporting_policy.yaml"
      tooling_policy: "charter/policies/governance/tooling_policy.yaml"

    operations:
      canary_policy: "charter/policies/operations/canary_policy.yaml"
      dev_fastpath_policy: "charter/policies/operations/dev_fastpath_policy.yaml"
      incident_response_policy: "charter/policies/operations/incident_response_policy.yaml"

  schemas:
    policy_schema: "charter/schemas/policy_schema.json"
    
    agent:
      agent_policy_schema: "charter/schemas/agent/agent_policy_schema.json"
      cognitive_roles_schema: "charter/schemas/agent/cognitive_roles_schema.json"
      micro_proposal_policy_schema: "charter/schemas/agent/micro_proposal_policy_schema.json"
      resource_manifest_policy_schema: "charter/schemas/agent/resource_manifest_policy_schema.json"

    code:
      capability_tag_schema: "charter/schemas/code/capability_tag_schema.json"
      knowledge_graph_entry_schema: "charter/schemas/code/knowledge_graph_entry_schema.json"
      
    constitutional:
      intent_bundle_schema: "charter/schemas/constitutional/intent_bundle_schema.json"
      intent_crate_schema: "charter/schemas/constitutional/intent_crate_schema.json"
      proposal_schema: "charter/schemas/constitutional/proposal_schema.json"

    data:
      database_policy_schema: "charter/schemas/data/database_policy_schema.json"
      database_schema: "charter/schemas/data/database_schema.yaml"
      
    governance:
      available_actions_policy_schema: "charter/schemas/governance/available_actions_policy_schema.json"
      cli_registry_schema: "charter/schemas/governance/cli_registry_schema.json"
      enforcement_model_schema: "charter/schemas/governance/enforcement_model_schema.json"
      intent_guard_schema: "charter/schemas/governance/intent_guard_schema.json"
      reporting_policy_schema: "charter/schemas/governance/reporting_policy_schema.json"

    operations:
      canary_policy_schema: "charter/schemas/operations/canary_policy_schema.json"
      config_schema: "charter/schemas/operations/config_schema.yaml"
      runtime_requirements_schema: "charter/schemas/operations/runtime_requirements_schema.json"

mind:
  project_manifest: "mind/project_manifest.yaml"

  config:
    local_mode: "mind/config/local_mode.yaml"
    runtime_requirements: "mind/config/runtime_requirements.yaml"

  evaluation:
    score_policy: "mind/evaluation/score_policy.yaml"
    audit_checklist: "mind/evaluation/audit_checklist.yaml"

  knowledge:
    entry_point_patterns: "mind/knowledge/entry_point_patterns.yaml"
    file_handlers: "mind/knowledge/file_handlers.yaml"
    source_structure: "mind/knowledge/source_structure.yaml"

  prompts:
    capability_definer: "mind/prompts/capability_definer.prompt"
    code_peer_review: "mind/prompts/code_peer_review.prompt"
    constitutional_review: "mind/prompts/constitutional_review.prompt"
    fix_capability_manifest: "mind/prompts/fix_capability_manifest.prompt"
    fix_function_docstring: "mind/prompts/fix_function_docstring.prompt"
    fix_header: "mind/prompts/fix_header.prompt"
    fix_line_length: "mind/prompts/fix_line_length.prompt"
    goal_assessor: "mind/prompts/goal_assessor.prompt"
    intent_translator: "mind/prompts/intent_translator.prompt"
    module_docstring_writer: "mind/prompts/module_docstring_writer.prompt"
    new_capability_generator: "mind/prompts/new_capability_generator.prompt"
    planner_agent: "mind/prompts/planner_agent.prompt"
    refactor_for_clarity: "mind/prompts/refactor_for_clarity.prompt"
    refactor_outlier: "mind/prompts/refactor_outlier.prompt"
    standard_task_generator: "mind/prompts/standard_task_generator.prompt"
    vectorizer: "mind/prompts/vectorizer.prompt"

--- END OF FILE ./.intent/meta.yaml ---

--- START OF FILE ./.intent/mind/config/local_mode.yaml ---
# .intent/mind/config/local_mode.yaml

mode: local_fallback
apis:
  llm:
    enabled: false
    fallback: local_validator
  git:
    ignore_validation: false

# Development-specific overrides
dev_fastpath: true        # allow auto-sign in dev env only
--- END OF FILE ./.intent/mind/config/local_mode.yaml ---

--- START OF FILE ./.intent/mind/config/runtime_requirements.yaml ---
# .intent/mind/config/runtime_requirements.yaml
id: runtime_requirements
version: 1
title: "Runtime Requirements"
status: active
owners:
  accountable: "Platform SRE"
  responsible: ["Core Maintainer"]
review:
  frequency: "6 months"

variables:
  MIND:
    description: "The relative path to the system's declarative 'mind' (.intent directory)."
    source: env
    required: true
    type: path
    used_by: ["system", "auditor"]
  BODY:
    description: "The relative path to the system's executable 'body' (src directory)."
    source: env
    required: true
    type: path
    used_by: ["system"]
  REPO_PATH:
    description: "The absolute path to the root of the repository."
    source: env
    required: true
    type: path
    used_by: ["system","auditor"]
  LLM_ENABLED:
    description: "Master flag to enable or disable all LLM-related capabilities."
    source: env
    required: true
    type: bool
    allowed: ["true","false"]
    used_by: ["agents"]
  KEY_STORAGE_DIR:
    description: "The secure directory for storing operator private keys."
    source: env
    required: true
    type: path
    default: ".intent/keys"
    used_by: ["system"]
  CORE_ACTION_LOG_PATH:
    description: "Path to the action/change log file, required for the safety policy 'change_must_be_logged'."
    source: env
    required: true
    type: path
    used_by: ["auditor", "system"]
  CORE_ENV:
    description: "Runtime mode: 'development' or 'production'."
    source: env
    required: true
    type: enum
    allowed: ["development","production"]
    used_by: ["system"]
  LOG_LEVEL:
    description: "Logging level."
    source: env
    required: true
    type: enum
    allowed: ["DEBUG","INFO","WARNING","ERROR"]
    used_by: ["system"]
  CORE_DEV_FASTPATH:
    description: "Enable development fastpath."
    source: env
    required: false
    type: bool
    used_by: ["system"]
    required_when: "CORE_ENV == 'development'"
  CORE_MAX_CONCURRENT_REQUESTS:
    description: "The maximum number of simultaneous outbound LLM requests to prevent rate-limiting."
    source: env
    required: false
    type: integer
    default: 5
    used_by: ["system"]
  DEEPSEEK_CHAT_API_URL:
    description: "API URL for the 'deepseek_chat' resource."
    source: env
    required: false
    type: uri
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CHAT_API_KEY:
    description: "API key for the 'deepseek_chat' resource."
    source: secret
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CHAT_MODEL_NAME:
    description: "Model name for the 'deepseek_chat' resource."
    source: env
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CODER_API_URL:
    description: "API URL for the 'deepseek_coder' resource."
    source: env
    required: false
    type: uri
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CODER_API_KEY:
    description: "API key for the 'deepseek_coder' resource."
    source: secret
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  DEEPSEEK_CODER_MODEL_NAME:
    description: "Model name for the 'deepseek_coder' resource."
    source: env
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  ANTHROPIC_CLAUDE_SONNET_API_URL:
    description: "API URL for the 'anthropic_claude_sonnet' resource."
    source: env
    required: false
    type: uri
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  ANTHROPIC_CLAUDE_SONNET_API_KEY:
    description: "API key for the 'anthropic_claude_sonnet' resource."
    source: secret
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"
  ANTHROPIC_CLAUDE_SONNET_MODEL_NAME:
    description: "Model name for the 'anthropic_claude_sonnet' resource."
    source: env
    required: false
    type: string
    used_by: ["agents"]
    required_when: "LLM_ENABLED == true"

  # --- START: Governed Embedding & Vector Store Configuration ---
  LOCAL_EMBEDDING_API_URL:
    description: "API URL for the local embedding resource."
    source: env
    required: true
    type: uri
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  LOCAL_EMBEDDING_API_KEY:
    description: "API key for the local embedding resource (if required)."
    source: secret
    required: false
    type: string
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  LOCAL_EMBEDDING_MODEL_NAME:
    description: "Model name for the local embedding resource."
    source: env
    required: true
    type: string
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  LOCAL_EMBEDDING_DIM:
    description: "The output dimension of the embedding model."
    source: env
    required: true
    type: integer
    default: 768
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  EMBED_MODEL_REVISION:
    description: "A revision tag/date for the embedding model to track provenance."
    source: env
    required: true
    type: string
    default: "2025-09-15"
    used_by: ["system"]
    required_when: "LLM_ENABLED == true"
  EMBEDDING_MAX_CONCURRENT_REQUESTS:
    description: "Maximum concurrent requests to the embedding model to prevent overload."
    source: env
    required: false
    type: integer
    default: 4 # A safer default for local models
    used_by: ["system"]

  QDRANT_URL:
    description: "URL for the Qdrant vector database instance."
    source: env
    required: true
    type: uri
    used_by: ["system"]
  QDRANT_COLLECTION_NAME:
    description: "The name of the collection within Qdrant to use for capabilities."
    source: env
    required: true
    type: string
    default: "core_capabilities"
    used_by: ["system"]
--- END OF FILE ./.intent/mind/config/runtime_requirements.yaml ---

--- START OF FILE ./.intent/mind/evaluation/audit_checklist.yaml ---
audit_checklist:
  - id: declared_intent
    item: "Was the intent declared before the change?"
    required: true
  - id: explanation
    item: "Was the change explained or justified?"
    required: true
  - id: manifest_sync
    item: "Did the change include a manifest update?"
    required: true
  - id: checkpoint
    item: "Was a rollback plan or checkpoint created?"
    required: false
  - id: quality_verified
    item: "Was code quality verified post-write?"
    required: true
  - id: audit.database_schema_declared
    description: Database schema must be present and valid.
    policy: "charter/policies/database_policy.yaml"
  - id: quorum-evidence-for-risky-changes
    title: "Quorum evidence recorded for medium/high risk"
    applies_when:
      risk_tier_in: ["medium", "high"]
    require:
      - "evidence.quorum.approvers"       # list of approvers
      - "evidence.quorum.mode"            # development/staging/production
      - "evidence.quorum.timestamp"       # ISO 8601
    severity: "block"
    guidance: "Attach the approver list and timestamp. Fails if missing."
--- END OF FILE ./.intent/mind/evaluation/audit_checklist.yaml ---

--- START OF FILE ./.intent/mind/evaluation/score_policy.yaml ---
score_policy:
  strategy: weighted_criteria

  criteria:
    - id: intent_alignment
      description: "Does this change serve a declared intent?"
      weight: 0.4

    - id: structural_compliance
      description: "Does it follow folder conventions and manifest structure?"
      weight: 0.2

    - id: safety
      description: "Was the change gated by a test or checkpoint?"
      weight: 0.2

    - id: code_quality
      description: "Does it pass formatting, linting, and basic semantic checks?"
      weight: 0.2

  # --- THRESHOLD LOGIC ---
  # Pass: score >= 0.7
  # Warn: score >= 0.5 and score < 0.7
  # Fail: score < 0.5
  thresholds:
    pass: 0.7
    warn: 0.5

# ----- RISK-TIER GATES (append) ---------------------------------------------
risk_tier_gates:
  # For medium-risk changes we require a governance checkpoint and a canary run.
  medium:
    min_score: 0.80        # tighten pass threshold
    require:
      - checkpoint         # e.g., human-in-the-loop signoff recorded
      - canary             # canary run ID must be present

  # For high-risk changes we raise the bar and require approver quorum too.
  high:
    min_score: 0.90
    require:
      - checkpoint
      - canary
      - approver_quorum    # follow your constitution/approvers

# The source of truth for what constitutes a "critical path".
# The auditor MUST use this file to evaluate the conditions below.
critical_paths_source: "charter/constitution/critical_paths.yaml"

# Declarative conditions so the auditor can enforce gates consistently.
gate_conditions:
  checkpoint_required_when: "risk_tier in ['medium','high']"
  canary_required_when: "risk_tier in ['medium','high'] or any change touches a file listed in critical_paths_source"
  approver_quorum_required_when: "risk_tier == 'high' or any change touches a file listed in critical_paths_source"
--- END OF FILE ./.intent/mind/evaluation/score_policy.yaml ---

--- START OF FILE ./.intent/mind/knowledge/entry_point_patterns.yaml ---
# .intent/knowledge/entry_point_patterns.yaml
#
# A declarative set of rules for the KnowledgeGraphBuilder to identify valid
# system entry points that are not discoverable through simple call-graph analysis.
# This prevents the auditor from incorrectly flagging valid code as "dead."

patterns:
  - name: "python_magic_method"
    description: "Standard Python __dunder__ methods are entry points called by the interpreter."
    match:
      type: "function"
      name_regex: "^__.+__$"
    entry_point_type: "magic_method"

  - name: "ast_visitor_method"
    description: "Methods in ast.NodeVisitor subclasses starting with 'visit_' are entry points for the visitor pattern."
    match:
      type: "function"
      name_regex: "^visit_"
      base_class_includes: "NodeVisitor"
    entry_point_type: "visitor_method"

  - name: "capability_implementation"
    description: "Any symbol tagged with a # CAPABILITY is a primary entry point for the CORE system's reasoning loop."
    match:
      has_capability_tag: true
    entry_point_type: "capability"
    
  - name: "typer_cli_command"
    description: "Functions registered as Typer CLI commands are valid entry points called by the user."
    match:
      # This is a heuristic. A more robust check might look for specific decorators,
      # but for our project, checking the module path is very effective.
      module_path_contains: "src/system/admin"
      is_public_function: true # i.e., does not start with an underscore
    entry_point_type: "cli_command"

  # --- THIS IS THE FIX ---
  - name: "core_tooling_component"
    description: "Classes within the internal tooling directory are considered essential system components and are always live."
    match:
      type: "class"
      module_path_contains: "src/system/tools"
    entry_point_type: "core_tool"
  # --- END OF FIX ---

  - name: "framework_base_class"
    description: "Classes that other components inherit from are valid entry points."
    match:
      type: "class"
      is_base_class: true
    entry_point_type: "base_class"

  - name: "pydantic_model"
    description: "Pydantic models are data structures, not callable code, and are valid entry points."
    match:
      type: "class"
      base_class_includes: "BaseModel"
    entry_point_type: "data_model"

  - name: "pydantic_property"
    description: "Functions decorated with @property in Pydantic models are data accessors, not callable logic."
    match:
      has_decorator: "property"
      base_class_includes: "BaseSettings"
    entry_point_type: "data_property"

  - name: "enum_definition"
    description: "Enum classes are data structures, not callable code, and are valid entry points."
    match:
      type: "class"
      base_class_includes: "Enum"
    entry_point_type: "enum"

  - name: "dataclass_definition"
    description: "Dataclasses are data structures, not callable code, and are valid entry points."
    match:
      type: "class"
      has_decorator: "dataclass"
    entry_point_type: "data_model"

--- END OF FILE ./.intent/mind/knowledge/entry_point_patterns.yaml ---

--- START OF FILE ./.intent/mind/knowledge/file_handlers.yaml ---
handlers:
  - type: python
    extensions: [".py"]
    parse_as: ast
    editable: true
    description: Python source code with manifest-enforced governance

  - type: markdown
    extensions: [".md"]
    parse_as: text
    editable: true
    description: Human-readable docs. Require manual review in sensitive areas.

  - type: yaml
    extensions: [".yaml", ".yml"]
    parse_as: structured
    editable: true
    description: Configuration, policies, intent declarations

  - type: json
    extensions: [".json"]
    parse_as: structured
    editable: true
    description: Machine-readable manifests and graphs

  - type: binary
    extensions: [".png", ".jpg", ".pdf"]
    parse_as: none
    editable: false
    description: Visual artifacts — viewable only

--- END OF FILE ./.intent/mind/knowledge/file_handlers.yaml ---

--- START OF FILE ./.intent/mind/knowledge/source_structure.yaml ---
# .intent/mind/knowledge/source_structure.yaml
# CONSTITUTIONAL BLUEPRINT for the Layered, DB-Driven Architecture (V4 FINAL)

structure:
  - domain: api
    path: src/api
    description: "FastAPI routers ONLY. The HTTP Entrypoint."
    allowed_imports: [api, core, features, services, shared]

  - domain: cli
    path: src/cli
    description: "Typer commands ONLY. The CLI Entrypoint."
    allowed_imports: [cli, core, features, services, shared]

  - domain: core
    path: src/core
    description: "Orchestration Layer. Connects entrypoints to features and contains agents."
    allowed_imports: [core, features, services, shared]

  - domain: features
    path: src/features
    description: "Self-contained business capabilities, mapped to the DB capabilities table."
    allowed_imports: [features, services, shared, core] # <-- FIX: Added 'core'

  - domain: services
    path: src/services
    description: "Cross-cutting infrastructure services (DB access, external clients)."
    allowed_imports: [services, shared]

  - domain: shared
    path: src/shared
    description: "Project-agnostic utilities and core data models."
    allowed_imports: [shared]

  # The system/governance directory is intentionally omitted from runtime contracts,
  # making it a constitutionally protected, governance-only domain.
--- END OF FILE ./.intent/mind/knowledge/source_structure.yaml ---

--- START OF FILE ./.intent/mind/project_manifest.yaml ---
name: CORE
version: 0.5.0
intent: A self-governing system that can safely and autonomously evolve its own codebase.
capabilities: []
--- END OF FILE ./.intent/mind/project_manifest.yaml ---

--- START OF FILE ./.intent/mind/prompts/capability_definer.prompt ---
# Capability Key Generation Prompt

You are an **expert software architect** specializing in the **CORE** system. Your task is to **analyze a Python source code snippet** and propose a **single, canonical, dot-notation capability key** that accurately describes its primary purpose.

## Constitutional Rules for Naming

1. **Use the Domain Pyramid**
   The key **MUST** follow a hierarchical `domain.subdomain.action` pattern.

2. **Be Specific**
   Avoid vague terms. ✅ `auth.user.create` is good; ❌ `utils.do_stuff` is bad.

3. **Use Verbs for Actions**
   The final part of the key **must** be an **action verb** (e.g., `create`, `validate`, `sync`, `get`, `delete`).

4. **Stay Consistent**
   Use the existing domains and patterns as a guide. Here is a list of existing, valid capability keys for reference:

   ```yaml
   [[context:.intent/mind/project_manifest.yaml]]
   ```

## Source Code to Analyze

```python
{code}
```

## Your Task

Respond with **ONLY** the single, most appropriate capability key and **nothing else**.

### Example of a PERFECT Response

```
auth.user.create
```

---

## Developer Workflow

With this prompt file in place, the entire **`system integrate`** workflow is complete and ready for its first run.

### Steps to Use

1. **Write your code**
   Make your desired changes to one or more files in the `src/` directory. For example, create a new file:

   ```bash
   src/features/greetings/service.py
   ```

   with a `say_hello` function.

2. **Stage your changes**

   ```bash
   git add src/features/greetings/service.py
   ```

3. **Run the integration command**

   ```bash
   poetry run core-admin system integrate -m "feat: Add new greeting service"
   ```

--- END OF FILE ./.intent/mind/prompts/capability_definer.prompt ---

--- START OF FILE ./.intent/mind/prompts/code_peer_review.prompt ---
You are an expert Senior Staff Software Engineer, renowned for your insightful, pragmatic, and constructive code reviews. You prioritize clarity, simplicity, and robustness over cleverness or over-engineering.

You will be provided with a Python source code file from the CORE project. Your task is to analyze it and provide a better, improved version along with a clear justification for your changes.

Your entire output MUST be in Markdown format and follow this structure precisely:

### 1. Overall Assessment
A brief, high-level summary of the code's quality, strengths, and primary areas for improvement.

### 2. Justification for Changes
A bulleted list explaining *why* you are making each change. Reference specific principles like clarity, efficiency, or robustness. Be concise but clear.

### 3. Improved Code
Provide the complete, final, and improved version of the source code inside a single Python markdown block.

**CRITICAL RULES:**
- **Do not over-engineer.** The goal is improvement, not a total rewrite into a different paradigm.
- **Preserve functionality.** The improved code must do exactly what the original code did, just better.
- **Respect the existing style.** Maintain the overall coding style of the file.
- **Your output must be the full file content.** Do not provide only a diff or a snippet.

Begin your review. The source code is provided below.
--- END OF FILE ./.intent/mind/prompts/code_peer_review.prompt ---

--- START OF FILE ./.intent/mind/prompts/constitutional_review.prompt ---
You are an expert AI system architect and a specialist in writing clear, machine-readable governance documents.

You will be provided with a "constitutional bundle" from a self-governing software system named CORE. This bundle contains the entire ".intent/" directory, which is the system's "Mind". It defines all of the system's principles, policies, capabilities, and self-knowledge.

Your task is to perform a critical peer review of this constitution. Your goal is to provide actionable suggestions to improve its clarity, completeness, and internal consistency.

Analyze the entire bundle and provide your feedback in the following format:

**1. Overall Assessment:**
A brief, high-level summary of the constitution's strengths and weaknesses.

**2. Specific Suggestions for Improvement:**
Provide a numbered list of specific, actionable suggestions. For each suggestion, you MUST include:
- **File:** The full path to the file that should be changed (e.g., `.intent/mission/principles.yaml`).
- **Justification:** A clear, concise reason explaining WHY this change is an improvement and which core principle it serves (e.g., "This serves the `clarity_first` principle by making the rule less ambiguous.").
- **Proposed Change:** A concrete example of the new content. Use a git-style diff format if possible (lines starting with '-' for removal, '+' for addition).

**3. Gaps and Missing Concepts:**
Identify any potential gaps in the constitution. Are there missing policies, undefined principles, or areas that seem incomplete? For example, is there a policy for data privacy? Is the process for adding new human operators clearly defined?

**Review Criteria:**
- **Clarity:** Is every rule and principle easy to understand for both a human and an LLM? Is there any ambiguity?
- **Completeness:** Does the constitution cover all critical aspects of the system's governance?
- **Consistency:** Are there any conflicting rules or principles?
- **Actionability:** Are the rules specific enough to be automatically enforced?

Begin your review now. The constitutional bundle is provided below.

--- END OF FILE ./.intent/mind/prompts/constitutional_review.prompt ---

--- START OF FILE ./.intent/mind/prompts/docs_clarity_review.prompt ---
You are an expert technical writer and developer advocate. Your primary skill is explaining complex software concepts to intelligent, but busy, programmers.

You will be provided with a bundle of all the human-facing documentation (.md files) for a software project called CORE.

Your task is to perform a "human clarity audit." Read all the documents and then answer the following questions from the perspective of a first-time reader who is a skilled developer but knows nothing about this project.

Your entire output MUST be in Markdown format.

**1. The "Stijn Test": What Does It Do?**
In one or two simple sentences, what is CORE and what problem does it solve? If you cannot answer this clearly, state that the documentation has failed this primary test.

**2. Overall Clarity Score (1-10):**
Give a score from 1 (completely incomprehensible) to 10 (perfectly clear). Justify your score with specific examples from the text.

**3. Suggestions for Improvement:**
Provide a numbered list of the top 3-5 concrete suggestions to improve the documentation's clarity. For each suggestion, quote the confusing text and explain WHY it is confusing.

**4. Conceptual Gaps:**
Are there any obvious questions a new user would have that the documentation doesn't answer? (e.g., "Who is this for?", "What's the difference between this and X?").

Begin your audit now. The documentation bundle is provided below.
--- END OF FILE ./.intent/mind/prompts/docs_clarity_review.prompt ---

--- START OF FILE ./.intent/mind/prompts/fix_capability_manifest.prompt ---
You are an expert software architect for the CORE system. Your task is to fix a capability manifest entry that has placeholder content.

Analyze the provided source code and its context, then generate a concise, one-sentence description and infer the most appropriate owner agent from the list below.

**Available Owners:**

* `core_agent`: For core application logic, services, and core capabilities.
* `planner_agent`: For goal decomposition and planning.
* `generic_agent`: For general agentic behaviors and utilities.
* `validator_agent`: For validation, auditing, and governance checks.
* `tooling_agent`: For internal developer tools, builders, and introspection.

**Source Code of the Capability:**

```python
{source_code}
```

Your Task:
Respond with ONLY a single, valid JSON object with three keys: "title", "description", and "owner".
"title": A clean, Title-Cased version of the capability name.
"description": A concise, one-sentence explanation of what the capability does.
"owner": The single most appropriate agent from the list above.
Example of a PERFECT response for governance.review\.ai\_peer\_review:

```json
{
  "title": "Ai Peer Review",
  "description": "Submits a source file to an AI expert for a peer review and improvement suggestions.",
  "owner": "generic_agent"
}
```

Now, analyze the provided source code and generate the JSON for the capability {capability\_key}.

--- END OF FILE ./.intent/mind/prompts/fix_capability_manifest.prompt ---

--- START OF FILE ./.intent/mind/prompts/fix_function_docstring.prompt ---
# Prompt: Python Function Docstring Writer

You are an expert Python technical writer. Your only task is to write a single, concise, and accurate PEP 257 compliant docstring for the provided Python function/method.

**CRITICAL RULES:**
1.  **Analyze the code's purpose.** Look at the function name, parameters, and body to understand what it does.
2.  **Write a one-line summary.** The docstring must start with a short, imperative summary (e.g., "Generate a new key pair," not "This function generates...").
3.  **Keep it concise.** The entire docstring should ideally be one line. Only add more detail if absolutely necessary for clarity.
4.  **Return ONLY the docstring content.** Do not include the triple quotes (`"""`). Do not include any other text, explanations, or markdown.

**Function Source Code to Document:**
```python
{source_code}

Example of a PERFECT output for def __init__(self, context)::
Initializes the check with a shared auditor context.
--- END OF FILE ./.intent/mind/prompts/fix_function_docstring.prompt ---

--- START OF FILE ./.intent/mind/prompts/fix_header.prompt ---
# Prompt: Constitutional Header Fixer

You are an expert technical writer and linter for a Python project named CORE. Your only task is to fix the header of a given Python file to be 100% compliant with the project's constitutional style guide.

INPUTS:
- file_path: {file_path}
- source_code: {source_code}

CONSTITUTIONAL HEADER RULES:
1) The first non-empty line MUST be a file path comment exactly matching the provided file_path (e.g., `# src/core/main.py`).
2) Immediately after that, there MUST be a single module-level docstring.
3) Immediately after the docstring, there MUST be a line `from __future__ import annotations`.
4) There MUST be exactly one blank line between (1), (2), and (3).
5) All other code must follow after these header elements.

Special cases:
- If `from __future__ import annotations` exists elsewhere, MOVE it to the required position and remove any duplicates.
- If other `from __future__ import ...` statements exist, keep them directly below the annotations line (do not combine them with the annotations import).
- If multiple module-level docstrings exist, merge them into one concise docstring.
- If a filepath comment already exists but does not match the provided file_path, replace it with the exact file_path.
- Do not change any code outside the header other than the moves/removals described above.

OUTPUT CONTRACT (critical):
- Return the complete, corrected source code for the entire file.
- Do NOT wrap the output in markdown fences or add any commentary.
- If the file is already compliant, return the original content unchanged.

Example (illustrative only — do NOT include fences in your output):
# {file_path}
"""
One-sentence module-level docstring explaining the file's purpose.
"""

from __future__ import annotations

# ...rest of the original code (unchanged)
--- END OF FILE ./.intent/mind/prompts/fix_header.prompt ---

--- START OF FILE ./.intent/mind/prompts/fix_line_length.prompt ---
You are an expert Python programmer specializing in code clarity and readability. Your sole task is to refactor the provided Python code to ensure no single line exceeds 100 characters while maintaining identical functionality.

**CRITICAL RULES:**

1. **ABSOLUTE PROHIBITION ON LOGIC CHANGES:** Do not modify variable names, add/remove imports, change string contents, alter numeric values, comments content, or modify any functionality whatsoever. Only change whitespace, line breaks, and indentation.

2. **LINE LENGTH ENFORCEMENT:** Break any line longer than 100 characters using intelligent, Pythonic methods.

3. **PYTHON VERSION:** Assume Python 3.8+ unless otherwise specified. Use modern syntax features appropriately.

**LINE BREAKING GUIDELINES:**

- Use parentheses for implicit line continuation (preferred over backslashes)
- Break after operators, not before (except for 'and'/'or' in conditionals)  
- For function calls: break after commas, keep related parameters together
- For long strings: use implicit string concatenation or triple quotes with proper indentation
- For dictionaries/lists: break after commas, align values appropriately
- For method chaining: break before the dot, align methods
- Align continuation lines appropriately with opening delimiters

**EDGE CASE HANDLING:**

- URLs, file paths, or strings that cannot be broken: leave as-is even if >100 chars, add comment `# LINE TOO LONG - CANNOT BREAK`
- Comments >100 chars: break at word boundaries, maintain meaning
- Preserve existing docstring formatting unless line length violations occur
- Extract Python content whether provided with or without markdown code blocks

**ERROR HANDLING:**

- If code contains syntax errors: respond with "ERROR: [specific issue description]"
- If code cannot be parsed: respond with "ERROR: Unable to parse Python code"

**VALIDATION REQUIREMENTS:**

Before returning code, verify:
- All lines are ≤100 characters (except unavoidable cases marked with comment)
- No syntax errors introduced
- All imports remain intact and functional
- String literals maintain original content
- Indentation follows PEP 8 standards

**OUTPUT FORMAT:**

Return ONLY the complete, raw Python source code. No markdown code blocks, no explanations, no commentary. The output must be immediately copy-paste ready and functionally identical to the input.

**Input File to Refactor:**

```python
{source_code}
```

--- END OF FILE ./.intent/mind/prompts/fix_line_length.prompt ---

--- START OF FILE ./.intent/mind/prompts/goal_assessor.prompt ---
You are an expert project manager for the CORE system. Your job is to assess a user's goal for clarity.

First, review the project's roadmap to understand the current priorities:
[[context:docs/04_ROADMAP.md]]

Now, analyze the user's goal.
- If the goal is clear, specific, and actionable, respond with a JSON object: `{"status": "clear", "goal": "The clear goal here."}`.
- If the goal is vague, identify the MOST LIKELY specific task the user wants based on the roadmap. Respond with a JSON object containing a helpful suggestion: `{"status": "vague", "suggestion": "The user's goal is a bit vague. Based on the roadmap, did you mean to ask: '[Your suggested, more specific goal]'?"}`.

User Goal: "{user_input}"

Your output MUST be a single, valid JSON object and nothing else.
--- END OF FILE ./.intent/mind/prompts/goal_assessor.prompt ---

--- START OF FILE ./.intent/mind/prompts/intent_translator.prompt ---
You are an expert user of the CORE Admin CLI. Your job is to translate a user's natural language goal into a single, precise, and executable `core-admin` command.

You must only use commands that are available in the CLI. Here is the full help text for `core-admin --help` to use as your reference:
\[\[include\:reports/cli\_help.txt]]

Analyze the user's request and determine the single best command to achieve their goal.

**CRITICAL RULES:**

1. Your output MUST be a single, valid JSON object and nothing else.
2. The JSON object must have one key: "command".
3. The value of "command" must be a string containing the complete and correct `core-admin` command, ready to be executed in a shell.
4. If the user's request is too ambiguous to map to a single command, respond with an "error" key and a helpful message.

**User Request:** "{user\_input}"

**Example of a PERFECT response for "check my project's health":**

```json
{
  "command": "core-admin system check"
}
```

**Example of a PERFECT response for an AMBIGUOUS request:**

```json
{
  "error": "Your request is a bit ambiguous. Could you clarify if you want to 'review the documentation' or 'run a constitutional audit'?"
}
```


--- END OF FILE ./.intent/mind/prompts/intent_translator.prompt ---

--- START OF FILE ./.intent/mind/prompts/micro_planner.prompt ---
You are the Micro-Planner Agent for the CORE system. Your sole purpose is to decompose a high-level goal into a series of small, safe, and independently verifiable actions that comply with the `micro_proposal_policy.yaml`.

You will be given a user's goal and the contents of the `micro_proposal_policy.yaml`.

Your task is to generate a valid JSON array of action steps. Each step must be an object with the keys "action" and "params".

**CONSTITUTIONAL CONSTRAINTS (`micro_proposal_policy.yaml`):**
{policy_content}

**CRITICAL RULES:**
1.  You MUST ONLY use actions listed in `safe_actions`.
2.  All file paths in `params` MUST conform to the `safe_paths` rules. You MUST NOT target a forbidden path.
3.  Your final step MUST ALWAYS be `core.validation.validate_code` to ensure the change is safe.
4.  If the user's goal cannot be achieved within these strict constraints, you MUST respond with an empty JSON array `[]`.

**User Goal:**
"{user_goal}"

Respond with ONLY the JSON array of tasks. Do not include any other text, explanations, or markdown formatting.
--- END OF FILE ./.intent/mind/prompts/micro_planner.prompt ---

--- START OF FILE ./.intent/mind/prompts/module_docstring_writer.prompt ---
# Prompt — Module-Level Docstring Writer

You are an expert technical writer for a Python project called CORE. Your task is to write a concise, one-sentence module-level docstring that explains the primary purpose or intent of a Python file.

---

## Critical Rules

1. **Output Format:** Your output MUST be a single line of text with no quotes, markdown, or code blocks.

2. **Content Requirements:**
   - Describe the module's primary responsibility or purpose
   - Use present tense, active voice when possible
   - Start with a verb or descriptive phrase (avoid "This module...")
   - Be specific about what the module does, not just what domain it covers
   - Keep it under 80 characters when possible for readability

3. **Analysis Guidelines:**
   - Focus on the main classes, functions, or primary workflow
   - If the module has multiple responsibilities, identify the unifying theme
   - Consider the module's role within the broader CORE project architecture
   - Ignore utility functions, imports, or minor helper code when determining primary purpose

---

## Error Handling

- If the file is empty or contains only imports/comments: respond with "ERROR: Insufficient code content to determine module purpose"
- If the file contains syntax errors: respond with "ERROR: Cannot parse Python code due to syntax errors"
- If the module purpose is unclear: focus on the most prominent functionality

---

## Style Guidelines

**Good Examples:**
- "Handles the discovery and loading of constitutional proposal files from disk."
- "Provides authentication and session management for user accounts."
- "Implements core encryption algorithms for secure data transmission."
- "Manages database connections and transaction handling."

**Avoid:**
- "This module contains functions for..." (too verbose)
- "Utilities for..." (too vague)
- "Various helpers..." (not descriptive)
- Generic descriptions that could apply to any module

---

## File Content

```python
{source_code}
```

---

## Expected Output Format

[Single sentence describing the module's primary purpose, ending with a period]

--- END OF FILE ./.intent/mind/prompts/module_docstring_writer.prompt ---

--- START OF FILE ./.intent/mind/prompts/new_capability_generator.prompt ---
You are an expert software architect and technical writer for the CORE system.
Your task is to analyze a Python function's source code and generate a complete, structured capability definition for it.

**CONTEXT:**
- A **capability** is a single, discrete function the system can perform.
- **Tags** are classifiers chosen from a predefined list that describe the capability's purpose.

**PREDEFINED TAGS (Choose one or more relevant tags):**
{valid_tags}

**SOURCE CODE TO ANALYZE:**
```python
{source_code}

INSTRUCTIONS:
Thoroughly analyze the source code to understand its primary purpose.
Generate a title that is a human-readable, Title Case version of the function name.
Write a concise, one-sentence description that clearly explains what the function does.
Select the most relevant tags from the predefined list above.
Determine the most appropriate owner agent for this capability.
Your entire output MUST be a single, valid JSON object containing the title, description, tags (as a list of strings), and owner.
EXAMPLE OF A PERFECT RESPONSE:

JSON
{{
  "title": "Run All Checks",
  "description": "Run all checks: lint, test, and a full constitutional audit.",
  "tags": ["system", "governance", "cli"],
  "owner": "validator_agent"
}}
--- END OF FILE ./.intent/mind/prompts/new_capability_generator.prompt ---

--- START OF FILE ./.intent/mind/prompts/planner_agent.prompt ---
You are a meticulous software architect and senior engineer. Your task is to decompose a high-level user goal into a series of precise, step-by-step actions.

You must respond with a JSON array of tasks. Each task must be an object with three fields: "step", "action", and "params".

- `step`: A string describing the purpose of this action in plain English.
- `action`: The name of the action to be performed. Must be one of the available actions.
- `params`: An object containing the parameters for the action. The keys must match the required parameters for that action.

**CRITICAL CONTEXT: This information has been provided by a reconnaissance agent.**
You MUST use this context to inform your plan.
{reconnaissance_report}

**CRITICAL RULES:**
1.  You MUST use the file paths provided in the "Relevant Files Identified by Semantic Search" section of the report. Do not invent file paths.
2.  If the plan involves editing code, your first step should almost always be to `read_file` to understand the current state before proposing an `edit_file` or `edit_function` action.
3.  **ABSOLUTE RULE: For any action that has a `code` parameter (like `edit_file`, `create_file`, `edit_function`), you MUST set its value to `null`. The code will be generated later by a different agent. Do NOT generate any code yourself.**

Available Actions:
{action_descriptions}

Now, create a plan for the following goal.

Goal: "{goal}"

Respond with ONLY the JSON array of tasks. Do not include any other text, explanations, or markdown formatting.
--- END OF FILE ./.intent/mind/prompts/planner_agent.prompt ---

--- START OF FILE ./.intent/mind/prompts/refactor_for_clarity.prompt ---
You are an expert Python programmer specializing in code clarity and readability, operating under the CORE constitution. Your sole task is to refactor the provided Python code to improve its clarity and simplicity while maintaining identical functionality.

**CONSTITUTIONAL PRINCIPLES TO UPHOLD:**
- `clarity_first`: The code must be easier to understand.
- `separation_of_concerns`: If a function is doing too much, break it into smaller, well-named helper methods.
- `safe_by_default`: Do not change any logic, only the structure. Preserve all existing decorators and functionality.

**REFACTORING ACTIONS:**
- Break down long, complex functions into smaller, logical units.
- Improve variable names for better readability.
- Simplify complex conditional logic.
- Adhere to a maximum line length of 100 characters.

**OUTPUT FORMAT:**
Return ONLY the complete, raw, and refactored Python source code. Do not include markdown code blocks, explanations, or commentary. The output must be ready to be written directly to a file.

**Input File to Refactor:**
```python
{source_code}

--- END OF FILE ./.intent/mind/prompts/refactor_for_clarity.prompt ---

--- START OF FILE ./.intent/mind/prompts/refactor_outlier.prompt ---
You are an expert Python refactoring engine. Your only task is to break down the provided large Python file into multiple, smaller, logically cohesive files.

**CRITICAL INSTRUCTIONS:**

1. **Analyze Responsibilities:** Identify the distinct responsibilities in the input file (e.g., CLI commands, data processing, helper functions).
2. **Create New Files:** Group the logic for each responsibility into a new file. Use clear, descriptive filenames (e.g., `knowledge_cli.py`, `knowledge_orchestrator.py`).
3. **Preserve All Logic:** All original functionality must be preserved. Do not add or remove any logic, only move it.
4. **Fix Imports:** Add all necessary `from . import ...` statements to reconnect the separated files.
5. **Output Format:** Your entire response MUST consist of one or more `[[write:file/path/here.py]]...[[/write]]` blocks. Do not add any other commentary or explanations.

**INPUT FILE TO REFACTOR:**

```python
{source_code}
```

**EXAMPLE OF A PERFECT OUTPUT:**
\[\[write\:src/system/admin/knowledge\_cli.py]]
src/system/admin/knowledge\_cli.py
"""
CLI commands for the knowledge system.
"""
from .knowledge\_orchestrator import orchestrate\_vectorization
... CLI command functions here ...
\[\[/write]]
\[\[write\:src/system/admin/knowledge\_orchestrator.py]]
src/system/admin/knowledge\_orchestrator.py
"""
Orchestrates the vectorization process.
"""
from .knowledge\_helpers import extract\_source\_code
... orchestrate\_vectorization function here ...
\[\[/write]]

Now, refactor the input file and provide only the \[\[write:]] blocks.


--- END OF FILE ./.intent/mind/prompts/refactor_outlier.prompt ---

--- START OF FILE ./.intent/mind/prompts/standard_task_generator.prompt ---
# .intent/mind/prompts/standard_task_generator.prompt
You are an expert Python programmer operating under the CORE constitution, tasked with generating a single, complete block of Python code to fulfill a specific step in a larger plan.

**CONTEXT FOR YOUR TASK:**
- **Overall Goal:** {goal}
- **Current Step:** {step}
- **Target File Path:** {file_path}
- **Target Symbol (if editing):** {symbol_name}

---
**OUTPUT CONTRACT (ABSOLUTE RULES):**
1.  You MUST generate the complete, final Python code for the entire file or function. Do not use placeholders, snippets, or diffs.
2.  Your output MUST BE PURE CODE. Do NOT include any markdown fences (```python...```), explanations, or any text other than the code itself.
3.  The generated code must be clean, readable, and adhere to standard Python conventions.
---

Now, generate the required code.
--- END OF FILE ./.intent/mind/prompts/standard_task_generator.prompt ---

--- START OF FILE ./.intent/mind/prompts/vectorizer.prompt ---
Analyze the following Python code snippet. Your task is to generate a 1024-dimensional semantic embedding vector that represents its meaning.

CRITICAL INSTRUCTIONS:
- Your output MUST be a single, valid JSON array of floating-point numbers.
- Do NOT include any other text, explanations, or markdown formatting like ```json.
- The array must contain exactly 1024 numbers.

Source Code:
```python
{source_code}
--- END OF FILE ./.intent/mind/prompts/vectorizer.prompt ---

--- START OF FILE ./.intent/proposals/README.md ---
# Proposals

Create proposals here with filename pattern `cr-*.yaml`.

## Format
- `target_path`: repo-relative path (e.g., `.intent/policies/safety_policies.yaml`)
- `action`: currently only `replace_file`
- `justification`: why this is needed
- `content`: full new file contents (string)
- `rollback_plan` (optional): notes to revert
- `signatures`: added by `core-admin proposals-sign`

See `cr-example.yaml` for a starter.

--- END OF FILE ./.intent/proposals/README.md ---

--- START OF FILE ./CONTRIBUTING.md ---
# Contributing to CORE

Thank you for joining CORE’s mission to pioneer self-governing software! Your contribution helps shape AI-driven development.

---

## Our Philosophy: Principled Contributions

CORE is governed by a **“constitution”** (rules in `.intent/`). All contributions must align with principles like `clarity_first`. Start with these docs:

*   **README.md**: Project vision and quick demo.
*   **Architecture (`docs/02_ARCHITECTURE.md`)**: The Mind-Body architecture and the role of the database.
*   **Governance (`docs/03_GOVERNANCE.md`)**: How changes are made safely.

**Key Concepts**:
*   A **`# ID: <uuid>`** tag in the source code is a permanent linker that connects a piece of code (the Body) to its definition in the database (the Mind).
*   A **"constitutional change"** updates files in `.intent/charter/`, requiring a signed proposal and a full audit.

---

## Contribution Workflow

1. **Find/Open an Issue**
   Discuss your proposed change in a GitHub Issue.

   ↓

2. **Write Your Code**
   Implement the feature or fix in `src/`.

   ↓

3. **Integrate Your Changes**
   Run `poetry run core-admin system integrate "Your commit message"` to tag, sync, and validate your work.

   ↓

4. **Submit a Pull Request**
   Link your PR to the issue.

---

## How to Contribute Code

Code contributions must follow CORE’s governance.

#### 1. Add Your Code
Write your functions, classes, and tests in the `src/` directory, following the established architectural domains.

#### 2. Assign IDs and Synchronize
After writing your code, you must integrate it with the system's Mind.

   *   **Assign IDs to new functions:**
     ```bash
     poetry run core-admin fix assign-ids --write
     ```
   *   **Synchronize with the database:**
     ```bash
     poetry run core-admin knowledge sync --write
     ```
   *   **(Optional) For major changes, run the full integration command:**
     ```bash
     poetry run core-admin system integrate "feat: Your descriptive commit message"
     ```

#### 3. Run Checks
Before submitting, ensure all checks pass. The `integrate` command does this for you, but you can also run them manually.

   *   `poetry run core-admin check ci audit`: Run the full constitutional audit (**required**).
   *   `make check`: A convenient shortcut for the full audit and other checks.
   *   `make format`: Auto-format your code.

#### 4. Submit Your PR
Submit your Pull Request, linking it to the relevant GitHub Issue.

---

## Questions?

Ask in **GitHub Issues**. We’re excited to collaborate!
--- END OF FILE ./CONTRIBUTING.md ---

--- START OF FILE ./LICENSE ---
MIT License

Copyright (c) 2024 Dariusz Newecki

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
--- END OF FILE ./LICENSE ---

--- START OF FILE ./Makefile ---
# FILE: Makefile
# Makefile for CORE – Cognitive Orchestration Runtime Engine
# This file provides convenient shortcuts to the canonical 'core-admin' CLI commands.

SHELL := /bin/bash
.SHELLFLAGS := -eu -o pipefail -c
.DEFAULT_GOAL := help

# ---- Configurable knobs -----------------------------------------------------
POETRY  ?= python3 -m poetry
APP     ?= src.core.main:app
HOST    ?= 0.0.0.0
PORT    ?= 8000
RELOAD  ?= --reload
ENV_FILE ?= .env

OUTPUT_PATH ?= docs/10_CAPABILITY_REFERENCE.md

.PHONY: help install lock run stop audit lint format test check fast-check clean distclean nuke docs check-docs cli-tree integrate

help:
	@echo "CORE Development Makefile"
	@echo "-------------------------"
	@echo "This Makefile provides shortcuts to the main CLI."
	@echo "For all commands, see: 'poetry run core-admin --help'"
	@echo ""
	@echo "Common Shortcuts:"
	@echo "make install       - Install dependencies"
	@echo "make integrate     - Run the full, canonical integration sequence"
	@echo "make check         - Run all checks including vectorization (for CI)"
	@echo "make fast-check    - Run linting and tests (RECOMMENDED FOR LOCAL DEV)"
	@echo "make lint          - Check code format and quality (read-only)"
	@echo "make format        - Fix code format and quality issues"
	@echo "make test          - Run tests via 'core-admin check ci test'"
	@echo "make run           - Start the API server"
	@echo "make cli-tree      - Display the full CLI command tree"
	@echo "make clean         - Remove temporary files"
	@echo "make docs          - Generate capability documentation"

install:
	@echo "📦 Installing dependencies..."
	$(POETRY) install

lock:
	@echo "🔒 Resolving and locking dependencies..."
	$(POETRY) lock

run:
	@echo "🚀 Starting FastAPI server at http://$(HOST):$(PORT)"
	$(POETRY) run uvicorn $(APP) --host $(HOST) --port $(PORT) $(RELOAD) --env-file $(ENV_FILE)

stop:
	@echo "🛑 Stopping any process on port $(PORT)..."
	@lsof -t -i:$(PORT) | xargs kill -9 2>/dev/null || true


audit:
	$(POETRY) run core-admin check ci audit

lint:
	$(POETRY) run core-admin check ci lint

format:
	$(POETRY) run core-admin fix format

test:
	$(POETRY) run core-admin check ci test

cli-tree:
	@echo "🌳 Generating CLI command tree..."
	$(POETRY) run core-admin check diagnostics cli-tree

fast-check:
	$(POETRY) run core-admin check ci lint
	$(POETRY) run core-admin check ci test
	
fix-lines:
	@echo "📏 Fixing long lines with AI assistant..."
	$(POETRY) run core-admin fix line-lengths --write
	
fix-docs:
	@echo "✍️  Adding missing docstrings with AI assistant..."
	$(POETRY) run core-admin fix docstrings --write

build-graph:
	@echo "🏗️  Building knowledge graph..."
	$(POETRY) run core-admin build graph

vectorize: build-graph
	@echo "🧠 Vectorizing knowledge graph..."
	$(POETRY) run core-admin run vectorize

check:
	@echo "🤝 Running full constitutional audit and documentation check..."
	$(MAKE) lint
	$(MAKE) test
	$(MAKE) audit
	@$(MAKE) check-docs

integrate:
	@echo "🤝 Running Canonical Integration Sequence..."
	@$(MAKE) format
	@poetry run core-admin fix assign-ids --write
	@poetry run core-admin knowledge sync --write
	@poetry run core-admin run vectorize --write
	@poetry run core-admin fix orphaned-vectors --write
	@poetry run core-admin check ci audit
	@echo "✅ Integration sequence complete. Please commit your changes."

docs:
	@echo "📚 Generating capability documentation..."
	$(POETRY) run core-admin build docs

check-docs: docs
	@echo "🔎 Checking for documentation drift..."
	@git diff --exit-code --quiet $(OUTPUT_PATH) || (echo "❌ ERROR: Documentation is out of sync. Please run 'make docs' and commit the changes." && exit 1)
	@echo "✅ Documentation is up to date."

# ---- Clean targets ---------------------------------------------------------

clean:
	@echo "🧹 Cleaning up temporary files and caches..."
	find . -type f -name '*.pyc' -delete
	find . -type d -name '__pycache__' -prune -exec rm -rf {} +
	rm -rf .pytest_cache .ruff_cache .mypy_cache .cache
	rm -f .coverage
	rm -rf htmlcov
	rm -rf build dist *.egg-info
	rm -rf pending_writes sandbox
	@echo "✅ Clean complete."

distclean: clean
	@echo "🧨 Distclean: removing virtual environments and build leftovers..."
	rm -rf .venv
	@echo "✅ Distclean complete."

nuke:
	@echo "☢️  Running 'git clean -fdx' in 3s (CTRL+C to cancel)..."
	@sleep 3
	git clean -fdx
	@echo "✅ Repo nuked (untracked files/dirs removed)."
--- END OF FILE ./Makefile ---

--- START OF FILE ./README.md ---
# CORE — The Self-Improving System Architect

> **Where Intelligence Lives.**

[![Status: Architectural Prototype](https://img.shields.io/badge/status-architectural%20prototype-blue.svg)](#-project-status)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![codecov](https://codecov.io/gh/DariuszNewecki/CORE/graph/badge.svg)](https://codecov.io/gh/DariuszNewecki/CORE)

CORE is a self-governing, constitutionally aligned AI development framework that can plan, write, validate, and evolve software systems — autonomously and safely. It is designed for environments where **trust, traceability, and governance matter**.

---

## 🏛️ Project Status: Architectural Prototype

The core self-governance and constitutional amendment loop is complete and stable. The system can audit and modify its own constitution via a human-in-the-loop, cryptographically signed approval process.

The next phase is to expand agent capabilities so CORE can generate and manage entirely new applications based on user intent. We’re making the project public now to invite collaboration on this foundational architecture.

---

## 🧠 What is CORE?

Traditional codebases often suffer from **architectural drift** — the code no longer matches the original design. Linters catch syntax errors, but architectural mistakes slip through.

CORE solves this by using a **“constitution”** (a set of machine-readable rules in `.intent/`) and an AI-powered **`ConstitutionalAuditor`** to ensure your code stays true to its design.

It’s built on a simple **Mind–Body–Will** philosophy:

* **Mind (`.intent/`)**: The Constitution. You declare your project's rules and goals here.
* **Body (`src/`)**: The Machinery. Simple, reliable tools that act on the code.
* **Will (AI Agents)**: The Reasoning Layer. AI agents that read the Mind and use the Body's tools to achieve your goals, while the Auditor ensures they never break the rules.

---

## 🚀 Getting Started (5-Minute Demo)

See CORE in action by running the worked example: create a simple API, intentionally break an architectural rule, and watch CORE's auditor catch it.

👉 **[Run the Worked Example (`docs/09_WORKED_EXAMPLE.md`)](docs/09_WORKED_EXAMPLE.md)**

---

## 📖 Documentation Portal

* **[What is CORE? (`docs/00_WHAT_IS_CORE.md`)](docs/00_WHAT_IS_CORE.md)** — The vision and philosophy.
* **[Architecture (`docs/02_ARCHITECTURE.md`)](docs/02_ARCHITECTURE.md)** — Technical details of the Mind and Body.
* **[Governance (`docs/03_GOVERNANCE.md`)](docs/03_GOVERNANCE.md)** — How changes are made safely.
* **[Roadmap (`docs/04_ROADMAP.md`)](docs/04_ROADMAP.md)** — See where we're going.
* **[Technical Debt Log (`docs/05_TECHNICAL_DEBT.md`)](docs/05_TECHNICAL_DEBT.md)** — Our formal plan for architectural improvements.
* **[Contributing (`CONTRIBUTING.md`)](CONTRIBUTING.md)** — Join our mission!

---

## ⚙️ Installation & Quick Start

**Requirements**: Python 3.12+, Poetry

```bash
# Clone and install
git clone https://github.com/DariuszNewecki/CORE.git
cd CORE
poetry install

# Set up environment
cp .env.example .env
# Edit .env with your LLM API keys

# Verify setup is clean by running the full system check
poetry run core-admin system check

# Try the conversational command!
poetry run core-admin chat "make me a simple command-line tool that prints a random number"

# 🌱 Contributing
We welcome all contributors! The best place to start is our Contributing Guide.

Check the Project Roadmap for "Next Up" tasks and see our open issues on GitHub.
# 📄 License
Licensed under the MIT License. See LICENSE.

--- END OF FILE ./README.md ---

--- START OF FILE ./assesment.prompt ---
# assesment.prompt
# This is the canonical prompt for a full architectural and constitutional review of the CORE project.

**You are an expert AI Systems Architect specializing in self-governing software and constitutional design. You have been retained to conduct a comprehensive architectural and constitutional review of a project named CORE.**

### Project Philosophy & Context

Before you begin, you must understand CORE's fundamental principles. CORE is not a typical software project; it is a self-governing system designed to evolve safely under a machine-readable "constitution."

Your entire assessment must be grounded in this philosophy.

1.  **The Architectural Trinity:** The system is strictly divided into three parts:
    *   🏛️ **The Mind (`.intent/`):** The Constitution. This is the single source of truth, divided into two parts:
        *   **The Charter (`.intent/charter/`):** The immutable, human-governed laws, principles, and schemas.
        *   **The Working Mind (`.intent/mind/`):**  The dynamic, system-maintained knowledge, with the database as the primary source of truth and .intent/mind/ files acting as human-readable sources for that truth.
    *   🦾 **The Body (`src/`):** The Machinery. The complete, implemented source code and tools that perform actions but do not make decisions.
    *   🧠 **The Will (AI Agents):** The Reasoning Layer. AI agents that read the Mind and use the Body's tools to achieve goals. Their actions are policed by the `ConstitutionalAuditor`.

2.  **Key Constitutional Principles:** Your review must be based on these values:
    *   **`clarity_first`**: Is the system's purpose and structure easy to understand?
    *   **`safe_by_default`**: Are changes validated and reversible? Is the system resilient?
    *   **`separation_of_concerns`**: Does the code respect the Mind-Body architecture? Is logic in the correct domain?
    *   **`dry_by_design`**: Is there duplicated logic or configuration that should be centralized?
    *   **`evolvable_structure`**: Can the system's constitution and code be changed safely and formally?

3.  **Project Goal: The Autonomy Ladder:** CORE's goal is to climb a ladder of self-governance, from observing code (A0) to autonomously generating new applications (A4). The project has recently completed a major constitutional refactoring ("The Great Simplification") and is stabilizing its foundation for the A1/A2 autonomy levels.

### Your Task

You will be provided with a complete snapshot of the CORE project. Your task is to perform a deep analysis and provide a strategic report to **identify the highest-leverage next steps to accelerate its progress toward greater autonomy.**

Your report must follow this exact structure:

---

### 1. Executive Summary

Provide a brief, high-level assessment of the project's current state post-refactoring. What is its greatest strength and its most significant remaining architectural weakness?

### 2. Architectural Scorecard (1-5)

Score each of the following dimensions on a scale of 1 (poor) to 5 (excellent). For each score, provide a concise one-sentence justification.

*   **Constitutional Integrity:** [Score] - Justification:
*   **Clarity & Simplicity:** [Score] - Justification:
*   **Architectural Purity (SoC & DRY):** [Score] - Justification:
*   **Safety & Governance:** [Score] - Justification:
*   **Readiness for Autonomy (A1/A2):** [Score] - Justification:

### 3. Strategic Gaps & Misalignments

Identify the top 3 high-level gaps or architectural misalignments that are holding the project back. **Focus on issues that prevent the system from becoming more autonomous.**

*   **Gap/Misalignment 1:** (e.g., Stale Operational Tooling)
    *   **Problem:** ...
    *   **Risk:** ...
*   **Gap/Misalignment 2:** (e.g., Implicit Service Dependencies)
    *   **Problem:** ...
    *   **Risk:** ...

### 4. Actionable Roadmap to A1 Autonomy

This is the most critical section. Provide a prioritized, actionable roadmap of tasks to address the issues you've identified. The goal is to make CORE capable of proposing and executing simple, safe changes to its own codebase (A1).

| Priority | Task Description | Constitutional Principle Served | Suggested First Step |
| :--- | :--- | :--- | :--- |
| **High** | *e.g., Consolidate all tooling into the `src/system/` domain.* | `separation_of_concerns` | *e.g., Create a migration plan in `docs/migrations/` for the `tools/` directory.* |
| **Medium** | *e.g., Implement a formal Dependency Injection container for core services.* | `clarity_first`, `evolvable_structure` | *e.g., Refactor `src/system/admin/__init__.py` to create a single context object.* |
| **Low** | *e.g., Create a self-healing agent to fix documentation drift.* | `reason_with_purpose` | *e.g., Draft a new prompt in `.intent/mind/prompts/` for a `docs_linter_agent`.* |

---

**Final Instruction:** Ground all your feedback in CORE's established principles. Your goal is not to suggest generic best practices, but to help this unique system become a better version of itself.

**The codebase bundle to review is provided below:**
--- END OF FILE ./assesment.prompt ---

--- START OF FILE ./docker-compose.yml ---
# docker-compose.yml
version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant:v1.15.1
    container_name: core_qdrant_db
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - /mnt/vector_db/qdrant-core:/qdrant/storage
    restart: always

--- END OF FILE ./docker-compose.yml ---

--- START OF FILE ./pyproject.toml ---
[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "core"
version = "0.1.0"
description = "CORE: A self-governing, intent-driven software development system."
authors = ["Dariusz Newecki <d.newecki@gmail.com>"]
license = "MIT"
readme = "README.md"
packages = [
    { include = "api", from = "src" },
    { include = "cli", from = "src" },
    { include = "core", from = "src" },
    { include = "features", from = "src" },
    { include = "services", from = "src" },
    { include = "shared", from = "src" },
]

[tool.poetry.dependencies]
python = ">=3.12"
fastapi = ">=0.95.0"
uvicorn = {extras = ["standard"], version = ">=0.21.0"}
pyyaml = ">=6.0"
httpx = ">=0.25.0"
python-dotenv = ">=1.0.0"
pydantic = ">=2.11"
pydantic-settings = "^2.10.1"
cryptography = ">=42.0.0"
rich = "^13"
black = "^24"
jsonschema = "^4"
typer = {extras = ["rich"], version = "^0.16.1"}
radon = ">=5.1.0"
filelock = "^3.13.0"
ruamel-yaml = "^0.18.6"
qdrant-client = ">=1.10.0"
numpy = "^2.3.2"
scikit-learn = "^1.5.1"
scipy = "^1.14.0"
sqlalchemy = ">=2.0"
asyncpg = "^0.30.0"
sqlparse = "^0.5.3"
networkx = "^3.3"

[tool.poetry.group.dev.dependencies]
pytest = ">=7.0,<8.0"
pytest-asyncio = "==0.21.0"
pytest-mock = "^3.12.0"
pytest-cov = "^6.2"
pytest-dotenv = "^0.5.2"
aiosqlite = "^0.21.0"

[tool.poetry.scripts]
core-admin = "cli.admin_cli:app"

[tool.black]
line-length = 88

[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = ["E", "W", "F", "I"]
ignore = ["E402", "E501"]

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
# This line tells pytest to add the 'src' directory to the Python path
# so that imports like 'from core.knowledge_service' work correctly.
pythonpath = ["src"]
addopts = ["-c", "pyproject.toml"]
env_files = [
    ".env"
]
--- END OF FILE ./pyproject.toml ---

--- START OF FILE ./scripts/build_llm_context.py ---
#!/usr/-bin/env python3
# tools/build_llm_context.py
import argparse, hashlib, json, os, sys, time, fnmatch, subprocess
from pathlib import Path

TEXT_EXTS = {
    ".py",".pyi",".md",".txt",".yaml",".yml",".toml",".ini",".cfg",".json",".sql",
    ".sh",".bash",".zsh",".ps1",".bat",".gitignore",".dockerignore",".env.example",
    ".rst",".csv"
}
BINARY_EXTS = {
    ".png",".jpg",".jpeg",".gif",".webp",".ico",".bmp",".tiff",".svg",
    ".mp3",".wav",".flac",".ogg",".mp4",".webm",".mov",".avi",
    ".pdf",".zip",".tar",".gz",".xz",".7z",".rar",".whl",".so",".dll",".dylib",
    ".pyc",".pyo"
}
DEFAULT_EXCLUDE_DIRS = {
    ".git",".venv","venv","__pycache__",".pytest_cache",".ruff_cache",".mypy_cache",
    "logs","sandbox","pending_writes","dist","build",".idea",".vscode","demo","work"
}
ROOT_DEFAULTS = ["pyproject.toml","poetry.lock","README.md","LICENSE","Makefile",".gitignore"]

# --- START OF MODIFICATION ---
# We are adding the 'sql' directory to the developer and full profiles
# to ensure the database schema is included in the AI context.
PROFILES = {
    "minimal": {
        "include_dirs": ["src", ".intent", "docs"],
        "root_files": ROOT_DEFAULTS,
    },
    "dev": {
        "include_dirs": ["src", ".intent", "docs", "tests", "sql"], # <-- ADDED 'sql'
        "root_files": ROOT_DEFAULTS,
    },
    "full": {
        "include_dirs": ["src", ".intent", "docs", "tests", "scripts", "tools", "sql"], # <-- ADDED 'sql'
        "root_files": ROOT_DEFAULTS,
    },
    "intent-only": {
        "include_dirs": [".intent"],
        "root_files": [],
    },
}
# --- END OF MODIFICATION ---

def is_probably_binary(path: Path) -> bool:
    if path.suffix.lower() in BINARY_EXTS:
        return True
    try:
        with path.open("rb") as f:
            chunk = f.read(4096)
        if b"\x00" in chunk:
            return True
    except Exception:
        return True
    return False

def sha256_of_bytes(b: bytes) -> str:
    h = hashlib.sha256()
    h.update(b)
    return h.hexdigest()

def read_text_head(path: Path, max_bytes: int) -> bytes:
    with path.open("rb") as f:
        data = f.read(max_bytes)
    try:
        size = path.stat().st_size
    except Exception:
        size = len(data)
    trailer = b""
    if size > len(data):
        trailer = f"\n[... TRUNCATED: kept first {len(data)} bytes of {size} ...]\n".encode("utf-8")
    return data + trailer

def collect_files(root: Path, include_dirs, extra_paths, exclude_dirs, allow_exts,
                  include_root_files, name_excludes: list[str]):
    files = []
    # add root files if present
    for rf in include_root_files:
        p = root / rf
        if p.exists() and p.is_file():
            files.append(p)

    todo_dirs = []
    for d in include_dirs:
        p = root / d
        if p.exists() and p.is_dir():
            todo_dirs.append(p)

    for extra in extra_paths:
        p = root / extra
        if p.exists():
            if p.is_file():
                files.append(p)
            elif p.is_dir():
                todo_dirs.append(p)

    # Walk allowlisted dirs
    for base in todo_dirs:
        for dirpath, dirnames, filenames in os.walk(base, followlinks=False):
            # prune excluded dirs
            dirnames[:] = [dn for dn in dirnames if dn not in exclude_dirs]
            for fn in filenames:
                # skip by name globs if requested
                if any(fnmatch.fnmatch(fn, pat) for pat in name_excludes):
                    continue
                p = Path(dirpath) / fn
                if p.suffix.lower() in BINARY_EXTS:
                    continue
                if p.suffix.lower() in allow_exts or p.suffix.lower() == "":
                    files.append(p)
                elif p.name in (".env",):
                    # avoid secrets by default
                    continue
    # de-dup + sort deterministically
    uniq = sorted({str(p) for p in files})
    return [Path(u) for u in uniq]

def git_changed_files(since: str) -> set:
    try:
        r = subprocess.run(
            ["git", "diff", "--name-only", since, "HEAD"],
            check=True, capture_output=True, text=True
        )
        return {line.strip() for line in r.stdout.splitlines() if line.strip()}
    except Exception:
        return set()

def write_chunks(outdir: Path, entries, max_chunk_bytes: int):
    outdir.mkdir(parents=True, exist_ok=True)
    chunk_idx = 1
    current = bytearray()
    paths = []

    def flush():
        nonlocal current, chunk_idx, paths
        if not current:
            return None
        name = f"context_{chunk_idx:04d}.txt"
        (outdir / name).write_bytes(current)
        paths.append(name)
        chunk_idx += 1
        current = bytearray()
        return name

    for e in entries:
        block = (
            f"--- START OF FILE {e['path']} ---\n".encode("utf-8")
            + e["bytes"]
            + f"\n--- END OF FILE {e['path']} ---\n\n".encode("utf-8")
        )
        if len(current) + len(block) > max_chunk_bytes and current:
            flush()
        if len(block) > max_chunk_bytes:
            if current:
                flush()
            current.extend(block[:max_chunk_bytes])
            current.extend(b"\n[... CHUNK TRUNCATED ...]\n")
            flush()
        else:
            current.extend(block)
    flush()

    return paths

def main():
    ap = argparse.ArgumentParser(description="Build compact, chunked LLM context from a repo.")
    ap.add_argument("--profile", choices=PROFILES.keys(), default="minimal")
    ap.add_argument("--paths", help="Comma-separated extra paths to include (files or dirs).", default="")
    ap.add_argument("--exclude-dirs", help="Comma-separated dirs to exclude in addition to defaults.", default="")
    ap.add_argument("--names-exclude", help="Comma-separated filename globs to exclude (e.g. '*.md,*.csv')", default="")
    ap.add_argument("--max-file-bytes", type=int, default=300_000, help="Max bytes per file to capture.")
    ap.add_argument("--max-chunk-bytes", type=int, default=12_000_000, help="Max bytes per output chunk.")
    ap.add_argument("--max-files", type=int, default=0, help="Stop after N files (0 = no limit).")
    ap.add_argument("--outdir", default="llm_context", help="Output directory.")
    ap.add_argument("--since", help="Only include files changed since this git ref (e.g. v0.2.0)", default=None)
    ap.add_argument("--print-summary", action="store_true")
    args = ap.parse_args()

    root = Path.cwd()
    prof = PROFILES[args.profile]
    include_dirs = prof["include_dirs"]
    include_root_files = prof["root_files"]

    extra_paths = [p.strip() for p in args.paths.split(",") if p.strip()]
    exclude_dirs = set(DEFAULT_EXCLUDE_DIRS)
    exclude_dirs |= {d.strip() for d in args.exclude_dirs.split(",") if d.strip()}
    name_excludes = [p.strip() for p in args.names_exclude.split(",") if p.strip()]

    candidates = collect_files(
        root, include_dirs, extra_paths, exclude_dirs, TEXT_EXTS, include_root_files, name_excludes
    )

    if args.since:
        changed = git_changed_files(args.since)
        if changed:
            candidates = [p for p in candidates if str(p.relative_to(root)) in changed]
        else:
            candidates = []

    # Deterministic order, then cap if needed
    candidates = sorted(candidates, key=lambda p: str(p))
    if args.max_files and args.max_files > 0:
        candidates = candidates[: args.max_files]

    entries = []
    total_bytes = 0
    total_files = 0
    skipped_binaries = []
    unreadable = 0
    for p in candidates:
        try:
            if is_probably_binary(p):
                skipped_binaries.append(str(p))
                continue
            data = read_text_head(p, args.max_file_bytes)
            total_bytes += len(data)
            total_files += 1
            entries.append({
                "path": str(p.relative_to(root)),
                "sha256": sha256_of_bytes(data),
                "size_bytes_captured": len(data),
                "bytes": data,
            })
        except Exception:
            unreadable += 1
            continue

    entries.sort(key=lambda e: e["path"])
    outdir = Path(args.outdir)
    chunk_paths = write_chunks(outdir, entries, args.max_chunk_bytes)

    manifest = {
        "created_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "root": str(root),
        "profile": args.profile,
        "include_dirs": include_dirs,
        "extra_paths": extra_paths,
        "exclude_dirs": sorted(list(exclude_dirs)),
        "names_exclude": name_excludes,
        "max_file_bytes": args.max_file_bytes,
        "max_chunk_bytes": args.max_chunk_bytes,
        "max_files": args.max_files,
        "total_files": total_files,
        "total_bytes_captured": total_bytes,
        "chunks": chunk_paths,
        "files": [{"path": e["path"], "sha256": e["sha256"], "size_bytes_captured": e["size_bytes_captured"]} for e in entries],
        "skipped_binary_like": skipped_binaries[:200],
        "unreadable_count": unreadable,
    }
    (outdir / "index.json").write_text(json.dumps(manifest, indent=2))

    # Write a brief human summary
    (outdir / "summary.txt").write_text(
        "\n".join([
            f"Created: {manifest['created_at']}",
            f"Profile: {manifest['profile']}",
            f"Files captured: {total_files}",
            f"Bytes captured: {total_bytes}",
            f"Chunks: {len(chunk_paths)}",
            f"Skipped (binary-like): {len(skipped_binaries)}",
            f"Unreadable: {unreadable}",
            f"Outdir: {outdir}",
        ]) + "\n"
    )

    if args.print_summary:
        mb = total_bytes / (1024*1024)
        print(f"[OK] Captured {total_files} files, {mb:.2f} MiB into {len(chunk_paths)} chunk(s):")
        for c in chunk_paths:
            print(f"  - {c}")
        print(f"Manifest: {outdir/'index.json'}")
        print(f"Summary : {outdir/'summary.txt'}")

if __name__ == "__main__":
    sys.exit(main())
--- END OF FILE ./scripts/build_llm_context.py ---

--- START OF FILE ./scripts/concat_intent.sh ---
#!/usr/bin/env bash
#
# concat_bundle.sh
# A constitutionally-aware script to bundle all relevant .intent/ files
# into a single text file for external AI review and analysis.
#
# This script respects the Charter/Mind separation and excludes sensitive or
# irrelevant files to create a clean, focused context bundle.
#

set -euo pipefail

# --- Configuration ---
# The final output file for the bundle.
OUTPUT_FILE="constitutional_bundle.txt"
# The root of the constitution.
INTENT_DIR=".intent"
# --- End Configuration ---

# Ensure we are in the project root where .intent directory exists
if [ ! -d "$INTENT_DIR" ]; then
    echo "❌ Error: This script must be run from the CORE project root directory."
    exit 1
fi

echo "🚀 Generating constitutional bundle for AI review..."
echo "   -> Output will be saved to: $OUTPUT_FILE"

# Start with a clean slate
> "$OUTPUT_FILE"

# Helper function to append a directory's contents to the bundle
# It takes a title and the directory path as arguments.
append_directory() {
    local title="$1"
    local dir_path="$2"
    local file_count=0

    # Check if the directory exists and has files
    if [ -d "$dir_path" ] && [ -n "$(find "$dir_path" -maxdepth 1 -type f)" ]; then
        echo "" | tee -a "$OUTPUT_FILE" > /dev/null
        echo "--- START OF SECTION: $title ---" >> "$OUTPUT_FILE"
        echo "" >> "$OUTPUT_FILE"

        # Use find to handle files gracefully, sorted for deterministic output
        for file in $(find "$dir_path" -maxdepth 1 -type f -name "*.yaml" -o -name "*.yml" -o -name "*.md" -o -name "*.json" | sort); do
            if [ -f "$file" ]; then
                echo "--- START OF FILE $file ---" >> "$OUTPUT_FILE"
                cat "$file" >> "$OUTPUT_FILE"
                echo -e "\n--- END OF FILE $file ---\n" >> "$OUTPUT_FILE"
                file_count=$((file_count + 1))
            fi
        done
        echo "--- END OF SECTION: $title ($file_count files) ---" >> "$OUTPUT_FILE"
    fi
}

# 1. Start with the Master Index
echo "--- START OF FILE $INTENT_DIR/meta.yaml ---" >> "$OUTPUT_FILE"
cat "$INTENT_DIR/meta.yaml" >> "$OUTPUT_FILE"
echo -e "\n--- END OF FILE $INTENT_DIR/meta.yaml ---\n" >> "$OUTPUT_FILE"

# 2. Append the entire Charter
echo "==============================================================================" >> "$OUTPUT_FILE"
echo "                            PART 1: THE CHARTER" >> "$OUTPUT_FILE"
echo " (The Immutable Laws, Mission, and Foundational Principles of the System)" >> "$OUTPUT_FILE"
echo "==============================================================================" >> "$OUTPUT_FILE"
append_directory "Constitution" "$INTENT_DIR/charter/constitution"
append_directory "Mission" "$INTENT_DIR/charter/mission"
append_directory "Policies" "$INTENT_DIR/charter/policies"
append_directory "Schemas" "$INTENT_DIR/charter/schemas"

# 3. Append the entire Working Mind
echo "" >> "$OUTPUT_FILE"
echo "==============================================================================" >> "$OUTPUT_FILE"
echo "                            PART 2: THE WORKING MIND" >> "$OUTPUT_FILE"
echo " (The Dynamic Knowledge, Configuration, and Evaluation Logic of the System)" >> "$OUTPUT_FILE"
echo "==============================================================================" >> "$OUTPUT_FILE"
append_directory "Configuration" "$INTENT_DIR/mind/config"
append_directory "Evaluation" "$INTENT_DIR/mind/evaluation"
append_directory "Knowledge" "$INTENT_DIR/mind/knowledge"

# Note: We intentionally exclude prompts/ as they are often very large and context-specific.
# We also exclude generated artifacts like knowledge_graph.json and sensitive files like keys/.

TOTAL_SIZE=$(wc -c < "$OUTPUT_FILE")
echo ""
echo "✅ Constitutional bundle successfully generated!"
echo "   -> Total size: $TOTAL_SIZE bytes."
echo "   -> You can now copy the content of '$OUTPUT_FILE' and provide it to an external AI for review."
--- END OF FILE ./scripts/concat_intent.sh ---

--- START OF FILE ./scripts/concat_project.sh ---
#!/usr/bin/env python3
# scripts/concat_project.sh
"""
A constitutionally-aware script to bundle the CORE project's essence for AI review.
It includes the Mind, Body, and operational tooling while excluding transient state,
sensitive data, and generated artifacts. It produces a structured output for clarity.
"""
from __future__ import annotations

import argparse
import sys
from pathlib import Path

# Use `tomllib` for modern TOML parsing, available in Python 3.11+
# For older versions, this would require `pip install tomli`
if sys.version_info >= (3, 11):
    import tomllib
else:
    import tomli as tomllib

# --- Configuration ---
OUTPUT_FILE = "project_context.txt"
ROOT_MARKER = "pyproject.toml"

# Define what to exclude, based on our architectural principles.
EXCLUDE_PATTERNS = [
    ".git", ".venv", "__pycache__", ".pytest_cache", ".ruff_cache",
    "logs", "sandbox", "pending_writes", "demo", "work", "dist", "build",
    ".env", ".intent/keys", "poetry.lock",
    "*.png", "*.jpg", "*.jpeg", "*.gif", "*.webp", "*.ico", "*.pyc", "*.so",
    "*.DS_Store", "Thumbs.db",
]
# --- End Configuration ---


def is_excluded(path: Path, root: Path, exclude_patterns: list[str]) -> bool:
    """Check if a path should be excluded based on the patterns."""
    relative_path_str = str(path.relative_to(root))
    for pattern in exclude_patterns:
        if path.match(pattern) or relative_path_str.startswith(pattern):
            return True
    return False

def is_likely_binary(path: Path) -> bool:
    """Heuristic to check if a file is binary by looking for null bytes."""
    try:
        with path.open("rb") as f:
            return b"\x00" in f.read(1024)
    except Exception:
        return True

def get_include_dirs_from_pyproject(root: Path) -> list[str]:
    """Reads pyproject.toml to get the list of source directories."""
    pyproject_path = root / ROOT_MARKER
    config = tomllib.loads(pyproject_path.read_text("utf-8"))
    packages = config.get("tool", {}).get("poetry", {}).get("packages", [])
    
    # Extract the 'include' value from each package dictionary
    source_dirs = {pkg["include"] for pkg in packages if "include" in pkg}
    
    # Add other key directories that are not formal packages
    return sorted(list(source_dirs | {".intent", "tests", "scripts", "sql"}))

def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(description="Generate Project Context Bundle for AI review.")
    parser.add_argument(
        "--output", default=OUTPUT_FILE, help="Path for the output bundle file."
    )
    args = parser.parse_args()
    output_path = Path(args.output).resolve()

    root_path = Path.cwd()
    if not (root_path / ROOT_MARKER).exists():
        print(f"❌ Error: This script must be run from the CORE project root directory.")
        return 1

    print(f"🚀 Generating Project Context Bundle for AI review...")

    # --- START OF AMENDMENT: Declarative directory discovery ---
    include_dirs = get_include_dirs_from_pyproject(root_path)
    print(f"   -> Including source directories from pyproject.toml: {include_dirs}")
    
    include_root_files = [
        "pyproject.toml", "README.md", "CONTRIBUTING.md", "LICENSE", "Makefile",
        ".gitignore", "assesment.prompt", "docker-compose.yml"
    ]
    # --- END OF AMENDMENT ---
    
    final_exclude_patterns = EXCLUDE_PATTERNS + [str(output_path.relative_to(root_path))]

    files_to_bundle = []
    for dir_name in include_dirs:
        dir_path = root_path / dir_name
        if dir_path.is_dir():
            files_to_bundle.extend(dir_path.rglob("*"))

    for file_name in include_root_files:
        file_path = root_path / file_name
        if file_path.is_file():
            files_to_bundle.append(file_path)

    output_path.parent.mkdir(parents=True, exist_ok=True)
    file_count = 0
    with output_path.open("w", encoding="utf-8") as outfile:
        outfile.write("--- START OF FILE project_context.txt ---\n\n")
        outfile.write("--- START OF PROJECT CONTEXT BUNDLE ---\n\n")

        unique_files = sorted(list(set(files_to_bundle)))

        for file in unique_files:
            if (
                not file.is_file()
                or is_excluded(file, root_path, final_exclude_patterns)
                or is_likely_binary(file)
            ):
                continue

            file_count += 1
            relative_path = file.relative_to(root_path)
            outfile.write(f"--- START OF FILE ./{relative_path} ---\n")
            try:
                content = file.read_text("utf-8")
                if content:
                    outfile.write(content)
                else:
                    outfile.write("[EMPTY FILE]")
            except Exception as e:
                outfile.write(f"[ERROR READING FILE: {e}]")
            outfile.write(f"\n--- END OF FILE ./{relative_path} ---\n\n")

        outfile.write("--- END OF PROJECT CONTEXT BUNDLE ---\n")

    print(f"\n✅ Done. Concatenated {file_count} files into {output_path}.")
    return 0

if __name__ == "__main__":
    sys.exit(main())
--- END OF FILE ./scripts/concat_project.sh ---

--- START OF FILE ./scripts/create_qdrant_collection.py ---
import asyncio
from qdrant_client import AsyncQdrantClient, models

# --- Your Configuration ---
# These should match your .env file
QDRANT_URL = "http://192.168.20.22:6333"
COLLECTION_NAME = "core_capabilities"
VECTOR_DIMENSION = 768  # This must match your embedding model's output size
# --- End Configuration ---

async def create_collection():
    """
    Connects to Qdrant and idempotently creates the specified collection.
    """
    print(f"Connecting to Qdrant at {QDRANT_URL}...")
    client = AsyncQdrantClient(url=QDRANT_URL)

    try:
        # Check if the collection already exists
        collections_response = await client.get_collections()
        existing_collections = [c.name for c in collections_response.collections]
        
        if COLLECTION_NAME in existing_collections:
            print(f"✅ Collection '{COLLECTION_NAME}' already exists. Nothing to do.")
            return

        # If it doesn't exist, create it
        print(f"Collection '{COLLECTION_NAME}' not found. Creating it now...")
        await client.recreate_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(
                size=VECTOR_DIMENSION,
                distance=models.Distance.COSINE,
            ),
        )
        print(f"✅ Successfully created collection '{COLLECTION_NAME}'.")

    except Exception as e:
        print(f"❌ An error occurred: {e}")
        print("\nPlease ensure your Qdrant Docker container is running and accessible.")
    finally:
        await client.close()


if __name__ == "__main__":
    asyncio.run(create_collection())
--- END OF FILE ./scripts/create_qdrant_collection.py ---

--- START OF FILE ./scripts/gh_status_report.sh ---
#!/usr/bin/env bash
set -euo pipefail
OWNER="${OWNER:-DariuszNewecki}"
REPO="${REPO:-CORE}"

has_jq() { command -v jq >/dev/null 2>&1; }

out="GH_STATUS.md"
echo "# GitHub Status Report — $OWNER/$REPO" > "$out"
echo "" >> "$out"
echo "Generated: $(date -u +"%Y-%m-%d %H:%M:%SZ")" >> "$out"
echo "" >> "$out"

echo "## Repository" >> "$out"
gh api repos/$OWNER/$REPO > /tmp/repo.json
if has_jq; then
  jq '{name,visibility,default_branch,open_issues_count,description}' /tmp/repo.json >> "$out"
else
  cat /tmp/repo.json >> "$out"
fi
echo "" >> "$out"

echo "## Milestones" >> "$out"
gh api repos/$OWNER/$REPO/milestones --paginate > /tmp/miles.json || echo "[]">/tmp/miles.json
if has_jq; then
  jq '.[] | {number,title,state,due_on,open_issues,closed_issues,description}' /tmp/miles.json >> "$out"
else
  cat /tmp/miles.json >> "$out"
fi
echo "" >> "$out"

# --- THIS IS THE MODIFIED SECTION ---

echo "## Open Issues" >> "$out"
gh issue list --repo $OWNER/$REPO --state open --limit 200 \
  --json number,title,labels,milestone,url,createdAt > /tmp/issues_open.json
if has_jq; then
  jq '.[] | {number,title,milestone: .milestone.title,labels: [.labels[].name],url,createdAt}' /tmp/issues_open.json >> "$out"
else
  cat /tmp/issues_open.json >> "$out"
fi
echo "" >> "$out"

echo "## Recently Closed Issues" >> "$out"
gh issue list --repo $OWNER/$REPO --state closed --limit 30 \
  --json number,title,labels,milestone,url,closedAt > /tmp/issues_closed.json
if has_jq; then
  jq '.[] | {number,title,milestone: .milestone.title,labels: [.labels[].name],url,closedAt}' /tmp/issues_closed.json >> "$out"
else
  cat /tmp/issues_closed.json >> "$out"
fi
echo "" >> "$out"

# --- END OF MODIFIED SECTION ---

echo "## Labels" >> "$out"
gh label list --repo $OWNER/$REPO --json name,color,description > /tmp/labels.json
if has_jq; then
  jq '.[] | {name,color,description}' /tmp/labels.json >> "$out"
else
  cat /tmp/labels.json >> "$out"
fi
echo "" >> "$out"

echo "## Projects (Projects v2)" >> "$out"
gh project list --owner $OWNER > /tmp/projects.txt || true
cat /tmp/projects.txt >> "$out"
echo "" >> "$out"
if grep -Eo '#[0-9]+' /tmp/projects.txt >/dev/null 2>&1; then
  while read -r num; do
    pnum="${num//#/}"
    echo "### Project $pnum" >> "$out"
    gh project view "$pnum" --owner $OWNER --format json >> "$out" || true
    echo "" >> "$out"
  done < <(grep -Eo '#[0-9]+' /tmp/projects.txt | sort -u)
fi

echo "## Releases" >> "$out"
gh release list --repo $OWNER/$REPO >> "$out" || true
echo "" >> "$out"

echo "Report written to $out"
--- END OF FILE ./scripts/gh_status_report.sh ---

--- START OF FILE ./scripts/inspect_db_state.py ---
#!/usr/bin/env python3
# scripts/inspect_db_state.py
"""
Quick script to inspect the current state of CORE database tables
and identify which ones have data vs which are empty.
"""
from __future__ import annotations

import asyncio
import sys
from pathlib import Path

# Add src to path so we can import CORE modules
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from services.database.session_manager import get_async_session
from sqlalchemy import text


async def inspect_database():
    """Check the state of all CORE database tables."""
    
    # List of tables we expect to exist based on the migrations
    tables_to_check = [
        'proposals',
        'proposal_signatures', 
        'audit_runs',
        'symbols',
        'capabilities',
        'symbol_capabilities',
        'domains',
        'llm_resources',
        'cognitive_roles',
        'cli_commands',
        'runtime_services'
    ]
    
    async with get_async_session() as session:
        print("CORE Database State Inspection")
        print("=" * 50)
        
        for table in tables_to_check:
            try:
                # Get row count
                result = await session.execute(text(f"SELECT COUNT(*) FROM core.{table}"))
                count = result.scalar()
                
                # Get a sample row if data exists
                if count > 0:
                    sample_result = await session.execute(text(f"SELECT * FROM core.{table} LIMIT 1"))
                    sample_row = sample_result.fetchone()
                    columns = list(sample_row._mapping.keys()) if sample_row else []
                    status = f"{count} rows - Columns: {', '.join(columns[:5])}{'...' if len(columns) > 5 else ''}"
                else:
                    status = "EMPTY"
                    
            except Exception as e:
                status = f"ERROR: {e}"
            
            print(f"  {table:20} {status}")
        
        print("\n" + "=" * 50)
        
        # Also check if there are any views
        try:
            views_result = await session.execute(text("""
                SELECT table_name FROM information_schema.views 
                WHERE table_schema = 'core'
            """))
            views = [row[0] for row in views_result.fetchall()]
            if views:
                print(f"Database Views: {', '.join(views)}")
            else:
                print("No views found")
        except Exception as e:
            print(f"Could not check views: {e}")


if __name__ == "__main__":
    asyncio.run(inspect_database())
--- END OF FILE ./scripts/inspect_db_state.py ---

--- START OF FILE ./scripts/register_all_capabilities.py ---
#!/usr/bin/env python3
# scripts/register_all_capabilities.py
"""
A helper script to automatically register all unassigned capabilities
found in the knowledge graph.
"""

import json
import subprocess
import sys
from pathlib import Path

from rich.console import Console
from rich.progress import track

# --- Configuration ---
REPO_ROOT = Path(__file__).resolve().parents[1]
KNOWLEDGE_GRAPH_PATH = REPO_ROOT / ".intent" / "knowledge" / "knowledge_graph.json"
# --- End Configuration ---

console = Console()


def main():
    """Main execution function."""
    console.print(
        "[bold cyan]🚀 Batch Registering All Unassigned Capabilities...[/bold cyan]"
    )

    if not KNOWLEDGE_GRAPH_PATH.exists():
        console.print(
            f"[bold red]❌ Error: Knowledge graph not found at {KNOWLEDGE_GRAPH_PATH}[/bold red]"
        )
        sys.exit(1)

    with KNOWLEDGE_GRAPH_PATH.open("r", encoding="utf-8") as f:
        graph = json.load(f)

    symbols = graph.get("symbols", {}).values()

    # --- THIS IS THE DEFINITIVE FIX ---
    # The script now uses the same, correct logic as the auditor to identify
    # only the PUBLIC symbols that are unassigned.
    unassigned_symbols = [
        s
        for s in symbols
        if s.get("capability") == "unassigned" and not s.get("name", "").startswith("_")
    ]
    # --- END OF FIX ---

    if not unassigned_symbols:
        console.print(
            "[bold green]✅ Success! No unassigned public capabilities found.[/bold green]"
        )
        sys.exit(0)

    console.print(
        f"   -> Found {len(unassigned_symbols)} unassigned public capabilities to register."
    )
    console.print(
        "[yellow]This will make multiple calls to the LLM and will take some time.[/yellow]"
    )
    if input("Proceed? (y/N): ").lower() != "y":
        console.print("[bold red]Aborted.[/bold red]")
        sys.exit(0)

    success_count = 0
    fail_count = 0

    for symbol in track(
        unassigned_symbols, description="Registering capabilities..."
    ):
        symbol_key = symbol.get("key")
        if not symbol_key:
            continue

        command = [
            "poetry",
            "run",
            "core-admin",
            "capability",
            "new",
            symbol_key,
        ]

        try:
            subprocess.run(
                command,
                check=True,
                capture_output=True,
                text=True,
                cwd=REPO_ROOT,
            )
            success_count += 1
        except subprocess.CalledProcessError as e:
            console.print(
                f"\n[bold red]❌ Failed to register '{symbol_key}':[/bold red]"
            )
            console.print(e.stderr)
            fail_count += 1

    console.print("\n--- Batch Registration Summary ---")
    console.print(
        f"[bold green]✅ Successfully registered: {success_count}[/bold green]"
    )
    if fail_count > 0:
        console.print(f"[bold red]❌ Failed to register: {fail_count}[/bold red]")
    
    console.print("\n[bold cyan]🧠 Rebuilding knowledge graph to reflect all changes...[/bold cyan]")
    try:
        subprocess.run(
            ["poetry", "run", "core-admin", "knowledge", "build-graph"],
            check=True,
            capture_output=True,
            text=True,
            cwd=REPO_ROOT,
        )
        console.print("[bold green]✅ Knowledge graph successfully updated.[/bold green]")
    except subprocess.CalledProcessError as e:
        console.print("[bold red]❌ Failed to rebuild knowledge graph:[/bold red]")
        console.print(e.stderr)
        sys.exit(1)
    
    if fail_count > 0:
        sys.exit(1)


if __name__ == "__main__":
    main()
    
--- END OF FILE ./scripts/register_all_capabilities.py ---

--- START OF FILE ./sql/001_init.sql ---
CREATE SCHEMA IF NOT EXISTS core;

-- proposals (evidence of constitutional change attempts)
CREATE TABLE IF NOT EXISTS core.proposals (
  id               BIGSERIAL PRIMARY KEY,
  target_path      TEXT NOT NULL,
  content_sha256   CHAR(64) NOT NULL,
  justification    TEXT NOT NULL,
  risk_tier        TEXT CHECK (risk_tier IN ('low','medium','high')) DEFAULT 'low',
  is_critical      BOOLEAN NOT NULL DEFAULT FALSE,
  status           TEXT CHECK (status IN ('open','approved','rejected','superseded')) NOT NULL DEFAULT 'open',
  created_at       TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  created_by       TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS core.proposal_signatures (
  proposal_id       BIGINT NOT NULL REFERENCES core.proposals(id) ON DELETE CASCADE,
  approver_identity TEXT NOT NULL,
  signature_base64  TEXT NOT NULL,
  signed_at         TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  is_valid          BOOLEAN NOT NULL DEFAULT TRUE,
  PRIMARY KEY (proposal_id, approver_identity)
);

-- audit runs (scores + pass/fail over time)
CREATE TABLE IF NOT EXISTS core.audit_runs (
  id         BIGSERIAL PRIMARY KEY,
  source     TEXT NOT NULL,           -- 'nightly' | 'pr' | 'manual'
  commit_sha CHAR(40),
  score      NUMERIC(4,3),
  passed     BOOLEAN NOT NULL,
  started_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  finished_at TIMESTAMPTZ
);

--- END OF FILE ./sql/001_init.sql ---

--- START OF FILE ./sql/002_capability_domains.sql ---
-- FILE: sql/002_capability_domains.sql

-- 1) Capabilities can be multi-domain via a junction; keep the column
--    but make it nullable for backward-compat/exports.
ALTER TABLE core.capabilities
  ALTER COLUMN domain DROP NOT NULL;

-- 2) Junction table for capability <-> domain
CREATE TABLE IF NOT EXISTS core.capability_domains (
  capability_key TEXT NOT NULL
    REFERENCES core.capabilities(key) ON DELETE CASCADE,
  domain_key TEXT NOT NULL
    REFERENCES core.domains(key) ON DELETE RESTRICT,
  is_primary BOOLEAN NOT NULL DEFAULT FALSE,
  PRIMARY KEY (capability_key, domain_key)
);

-- 3) At most one primary domain per capability (optional but helpful).
CREATE UNIQUE INDEX IF NOT EXISTS uq_capability_primary_domain
  ON core.capability_domains (capability_key)
  WHERE is_primary = TRUE;

-- 4) Seed the junction from the legacy single-column if present.
INSERT INTO core.capability_domains (capability_key, domain_key, is_primary)
SELECT c.key, c.domain, TRUE
FROM core.capabilities c
JOIN core.domains d ON d.key = c.domain
WHERE c.domain IS NOT NULL
ON CONFLICT DO NOTHING;

--- END OF FILE ./sql/002_capability_domains.sql ---

--- START OF FILE ./sql/002_catalog.sql ---
-- Minimal catalog tables for domains + capabilities (DB = SSOT)
create table if not exists core.domains (
  key          text primary key,
  title        text not null,
  description  text,
  parent_key   text references core.domains(key),
  status       text check (status in ('active','deprecated')) not null default 'active',
  last_seen_at timestamptz not null default now()
);

create table if not exists core.capabilities (
  key          text primary key,
  title        text not null,
  domain       text not null references core.domains(key) on update cascade,
  owner        text not null default 'unassigned',
  status       text check (status in ('active','deprecated')) not null default 'active',
  last_seen_at timestamptz not null default now()
);

create index if not exists idx_capabilities_domain on core.capabilities(domain);

--- END OF FILE ./sql/002_catalog.sql ---

--- START OF FILE ./sql/003_capability_domains_idx.sql ---
-- FILE: sql/003_capability_domains_idx.sql

-- Helpful indexes for common lookups
CREATE INDEX IF NOT EXISTS idx_capability_domains_domain
  ON core.capability_domains(domain_key);

CREATE INDEX IF NOT EXISTS idx_capability_domains_cap_primary
  ON core.capability_domains(capability_key)
  WHERE is_primary = TRUE;

--- END OF FILE ./sql/003_capability_domains_idx.sql ---

--- START OF FILE ./sql/003_implementations.sql ---
--- START OF FILE sql/003_implementations.sql ---
-- FILE: sql/003_implementations.sql

CREATE TABLE IF NOT EXISTS core.implementations (
  capability_key TEXT NOT NULL REFERENCES core.capabilities(key) ON DELETE CASCADE,
  file_path TEXT NOT NULL,
  symbol_path TEXT NOT NULL,
  structural_hash CHAR(64) NOT NULL,
  PRIMARY KEY (capability_key, file_path, symbol_path)
);
--- END OF FILE sql/003_implementations.sql ---
--- END OF FILE ./sql/003_implementations.sql ---

--- START OF FILE ./sql/004_refactor_to_symbols.sql ---
-- FILE: sql/004_refactor_to_symbols.sql

-- Step 1: Create a temporary mapping table to hold the old implementation data.
-- This block is wrapped in a DO statement to execute conditionally, making the script safe to re-run.
DO $$
BEGIN
   IF EXISTS (SELECT FROM pg_tables WHERE schemaname = 'core' AND tablename  = 'implementations') THEN
      -- Create the temp table only if the source table exists
      CREATE TABLE core.implementations_temp AS TABLE core.implementations;
   END IF;
END $$;

-- Step 2: Explicitly drop the dependent objects first.
DROP TABLE IF EXISTS core.capability_domains;
DROP VIEW IF EXISTS core.knowledge_graph;
DROP VIEW IF EXISTS core.capabilities_view;

-- Step 3: Now it is safe to drop the old tables.
DROP TABLE IF EXISTS core.implementations;
DROP TABLE IF EXISTS core.capabilities;
DROP TABLE IF EXISTS core.domains;

-- Step 4: Create the new, universal symbols table.
CREATE TABLE core.symbols (
    uuid            TEXT PRIMARY KEY,
    symbol_path     TEXT NOT NULL UNIQUE,
    file_path       TEXT NOT NULL,
    is_public       BOOLEAN NOT NULL DEFAULT FALSE,
    title           TEXT,
    description     TEXT,
    owner           TEXT NOT NULL DEFAULT 'unassigned_agent',
    status          TEXT NOT NULL DEFAULT 'active',
    structural_hash CHAR(64),
    vector_id       TEXT,
    created_at      TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at      TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Add indexes for performance
CREATE INDEX idx_symbols_filepath ON core.symbols (file_path);
CREATE INDEX idx_symbols_is_public ON core.symbols (is_public);

-- NOTE: Data migration from the temp table will be handled by the new 'knowledge sync' command.
--- END OF FILE ./sql/004_refactor_to_symbols.sql ---

--- START OF FILE ./sql/005_add_symbol_key.sql ---
-- FILE: sql/005_add_symbol_key.sql
--
-- CONSTITUTIONAL AMENDMENT: Add a canonical, human-readable key to the symbols table.
--
-- Justification: This amendment forges the missing link between the human-readable
-- capabilities declared in .intent/mind/project_manifest.yaml and their concrete
-- implementations tracked in the database. It is a critical step to making the
-- ConstitutionalAuditor's 'check_capability_coverage' function correctly and
-- serves the 'clarity_first' principle.

-- Step 1: Add the new 'key' column. It is nullable for now to allow a phased data migration.
ALTER TABLE core.symbols
ADD COLUMN key TEXT;

-- Step 2: Create a unique index to ensure that each human-readable key can only
-- be assigned to one symbol, enforcing the single_source_of_truth principle.
-- The index is created on non-null values only.
CREATE UNIQUE INDEX IF NOT EXISTS uq_symbols_key
ON core.symbols (key)
WHERE key IS NOT NULL;
--- END OF FILE ./sql/005_add_symbol_key.sql ---

--- START OF FILE ./sql/006_knowledge_graph_view.sql ---
-- FILE: sql/006_knowledge_graph_view.sql
--
-- CONSTITUTIONAL AMENDMENT: Create a database view to act as a stable,
-- queryable interface to the system's knowledge graph.
--
-- Justification: This view provides a drop-in replacement for the legacy
-- knowledge_graph.json file, serving the 'evolvable_structure' and 'clarity_first'
-- principles by abstracting the underlying table structure from consumers
-- like the ConstitutionalAuditor.

CREATE OR REPLACE VIEW core.knowledge_graph AS
SELECT
    s.uuid,
    s.key AS capability, -- Alias 'key' to 'capability' for backward compatibility
    s.symbol_path,
    s.file_path AS file,
    s.title,
    s.description AS intent,
    s.owner,
    s.status,
    s.is_public,
    s.structural_hash,
    s.vector_id,
    s.updated_at AS last_updated,
    -- NOTE: These fields are placeholders to match the old schema for now.
    '[]'::jsonb AS tags,
    '[]'::jsonb AS calls,
    '[]'::jsonb AS parameters,
    '[]'::jsonb AS base_classes,
    (s.symbol_path LIKE '%__init__') AS is_class,
    (s.symbol_path LIKE '%Test%') AS is_test,
    -- --- THIS IS THE FIX ---
    -- Add the missing columns that downstream tools rely on.
    -- For now, we use placeholders or simple derivations.
    0 AS line_number, -- Placeholder
    0 AS end_line_number, -- Placeholder
    NULL AS source_code, -- Placeholder
    NULL AS docstring, -- Placeholder
    NULL AS entry_point_type, -- Placeholder
    NULL AS entry_point_justification, -- Placeholder
    NULL AS parent_class_key, -- Placeholder
    FALSE AS is_async -- Placeholder
    -- --- END OF FIX ---
FROM
    core.symbols s;
--- END OF FILE ./sql/006_knowledge_graph_view.sql ---

--- START OF FILE ./sql/007_capabilities_registry.sql ---
-- FILE: sql/007_capabilities_registry.sql
CREATE TABLE IF NOT EXISTS core.capabilities (
  id TEXT PRIMARY KEY,          -- The canonical key, e.g., 'introspection.vectorize'
  title TEXT NOT NULL,
  owner TEXT NOT NULL,
  implementing_files JSONB,     -- A JSON array of file paths like ["src/features/introspection/service.py"]
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
--- END OF FILE ./sql/007_capabilities_registry.sql ---

--- START OF FILE ./sql/008_operational_tables.sql ---
-- FILE: sql/008_operational_tables.sql
--
-- CONSTITUTIONAL AMENDMENT: Make the database the Single Source of Truth for operational knowledge.
--
-- Justification: This migration creates tables to house runtime operational knowledge
-- that was previously stored in difficult-to-manage YAML files. This serves the
-- 'single_source_of_truth' principle by centralizing configuration, making it
-- transactionally safe, and queryable by all system components.

-- Table for LLM Resources, replacing resource_manifest_policy.yaml
CREATE TABLE IF NOT EXISTS core.llm_resources (
    name TEXT PRIMARY KEY,
    provided_capabilities JSONB NOT NULL DEFAULT '[]'::jsonb,
    env_prefix TEXT NOT NULL UNIQUE,
    performance_metadata JSONB
);

-- Table for Cognitive Roles, replacing cognitive_roles_policy.yaml
CREATE TABLE IF NOT EXISTS core.cognitive_roles (
    "role" TEXT PRIMARY KEY,
    description TEXT,
    assigned_resource TEXT REFERENCES core.llm_resources(name),
    required_capabilities JSONB NOT NULL DEFAULT '[]'::jsonb
);

-- Table for Runtime Services, replacing runtime_services.yaml
CREATE TABLE IF NOT EXISTS core.runtime_services (
    name TEXT PRIMARY KEY,
    implementation TEXT NOT NULL UNIQUE
);

-- Table for CLI Commands, replacing cli_registry_policy.yaml
CREATE TABLE IF NOT EXISTS core.cli_commands (
    name TEXT PRIMARY KEY,
    module TEXT NOT NULL,
    entrypoint TEXT NOT NULL,
    summary TEXT,
    category TEXT
);
--- END OF FILE ./sql/008_operational_tables.sql ---

--- START OF FILE ./sql/009_domain_and_vector_tables.sql ---
-- Migration to make the database the single source of truth for domains.
-- This aligns with the 'single_source_of_truth' and 'evolvable_structure' principles.

-- Step 1: Create the new 'domains' table.
-- This table will store the canonical list of all architectural domains.
CREATE TABLE IF NOT EXISTS core.domains (
    key          TEXT PRIMARY KEY,
    title        TEXT NOT NULL,
    description  TEXT,
    created_at   TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
CREATE INDEX IF NOT EXISTS idx_domains_key ON core.domains (key);


-- Step 2: Add a 'domain' column to the 'symbols' table.
-- This creates the formal link between a piece of code (a symbol) and its
-- architectural domain. It is nullable to support symbols that are not
-- yet classified.
ALTER TABLE core.symbols
ADD COLUMN domain_key TEXT REFERENCES core.domains(key) ON DELETE SET NULL;
CREATE INDEX IF NOT EXISTS idx_symbols_domain_key ON core.symbols (domain_key);


-- Step 3: Add a 'vector' column to the 'symbols' table.
-- This makes the database the single source of truth for embeddings, directly
-- associating a vector with the symbol it represents. We use JSONB for
-- portability instead of a vendor-specific VECTOR type.
ALTER TABLE core.symbols
ADD COLUMN vector JSONB;
CREATE INDEX IF NOT EXISTS idx_symbols_vector_gin ON core.symbols USING GIN (vector);


-- Step 4: Create a junction table for many-to-many relationships between symbols and capabilities.
-- This is a forward-looking change to support future scenarios where a single
-- function might implement multiple, distinct capabilities.
CREATE TABLE IF NOT EXISTS core.symbol_capabilities (
    symbol_uuid TEXT NOT NULL REFERENCES core.symbols(uuid) ON DELETE CASCADE,
    capability_key TEXT NOT NULL, -- This will be validated by the application layer
    PRIMARY KEY (symbol_uuid, capability_key)
);
CREATE INDEX IF NOT EXISTS idx_symbol_capabilities_capability_key ON core.symbol_capabilities (capability_key);

--- END OF FILE ./sql/009_domain_and_vector_tables.sql ---

--- START OF FILE ./tests/admin/test_guard_drift_cli.py ---
# tests/admin/test_guard_drift_cli.py
from __future__ import annotations

from pathlib import Path
from unittest.mock import AsyncMock

import pytest

from features.introspection.drift_service import run_drift_analysis_async
from shared.models import CapabilityMeta


def write(p: Path, text: str) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(text, encoding="utf-8")


@pytest.mark.anyio
async def test_drift_analysis_clean(tmp_path: Path, mocker):
    """
    Tests that drift analysis reports a clean state when manifest and
    (mocked) code capabilities are in sync.
    """
    # ARRANGE
    # Mock the two data sources the service uses.
    # --- THIS IS THE FIX ---
    # The mocked return value MUST be a dictionary mapping strings to CapabilityMeta instances.
    mocker.patch(
        "features.introspection.drift_service.load_manifest_capabilities",
        return_value={
            "alpha.cap": CapabilityMeta(key="alpha.cap"),
            "beta.cap": CapabilityMeta(key="beta.cap"),
        },
    )
    # --- END OF FIX ---

    mock_graph_data = {
        "symbols": {
            "file1::func_a": {"key": "alpha.cap"},
            "file2::func_b": {"key": "beta.cap"},
        }
    }
    mocker.patch(
        "core.knowledge_service.KnowledgeService.get_graph",
        new_callable=AsyncMock,
        return_value=mock_graph_data,
    )

    # ACT
    report = await run_drift_analysis_async(tmp_path)

    # ASSERT
    assert not report.missing_in_code
    assert not report.undeclared_in_manifest
    assert not report.mismatched_mappings


@pytest.mark.anyio
async def test_drift_analysis_detects_drift(tmp_path: Path, mocker):
    """
    Tests that drift analysis correctly identifies discrepancies between
    the manifest and the (mocked) code capabilities.
    """
    # ARRANGE
    # Mock the manifest to declare one capability
    # --- THIS IS THE FIX ---
    mocker.patch(
        "features.introspection.drift_service.load_manifest_capabilities",
        return_value={"manifest.only.cap": CapabilityMeta(key="manifest.only.cap")},
    )
    # --- END OF FIX ---

    # Mock the code scan to find a different capability
    mock_graph_data = {"symbols": {"file1::func_a": {"key": "code.only.cap"}}}
    mocker.patch(
        "core.knowledge_service.KnowledgeService.get_graph",
        new_callable=AsyncMock,
        return_value=mock_graph_data,
    )

    # ACT
    report = await run_drift_analysis_async(tmp_path)

    # ASSERT
    assert report.missing_in_code == ["manifest.only.cap"]
    assert report.undeclared_in_manifest == ["code.only.cap"]

--- END OF FILE ./tests/admin/test_guard_drift_cli.py ---

--- START OF FILE ./tests/api/test_knowledge_api.py ---
# tests/api/test_knowledge_api.py
from unittest.mock import AsyncMock

import pytest
from fastapi.testclient import TestClient


@pytest.mark.anyio
async def test_list_capabilities_endpoint(mock_core_env, mocker):
    from core.main import create_app

    mocker.patch(
        "core.knowledge_service.KnowledgeService.list_capabilities",
        new_callable=AsyncMock,
        return_value=["test.cap"],
    )
    app = create_app()
    with TestClient(app) as client:
        response = client.get("/knowledge/capabilities")
        assert response.status_code == 200
        assert response.json()["capabilities"] == ["test.cap"]

--- END OF FILE ./tests/api/test_knowledge_api.py ---

--- START OF FILE ./tests/core/test_cognitive_service.py ---
# tests/core/test_cognitive_service.py
import pytest
from sqlalchemy import insert

from services.database.models import CognitiveRole, LlmResource


@pytest.mark.anyio
async def test_cognitive_service_selects_cheapest_model(
    mock_core_env, get_test_session, monkeypatch
):
    from core.cognitive_service import CognitiveService

    monkeypatch.setenv("CHEAP_API_URL", "http://cheap.api")
    monkeypatch.setenv("CHEAP_API_KEY", "cheap_key")
    monkeypatch.setenv("CHEAP_MODEL_NAME", "cheap-model")
    monkeypatch.setenv("EXPENSIVE_API_URL", "http://expensive.api")
    monkeypatch.setenv("EXPENSIVE_API_KEY", "expensive_key")
    monkeypatch.setenv("EXPENSIVE_MODEL_NAME", "expensive-model")

    resources = [
        {
            "name": "expensive_model",
            "env_prefix": "EXPENSIVE",
            "provided_capabilities": ["nlu"],
            "performance_metadata": {"cost_rating": 5},
        },
        {
            "name": "cheap_model",
            "env_prefix": "CHEAP",
            "provided_capabilities": ["nlu"],
            "performance_metadata": {"cost_rating": 1},
        },
    ]
    roles = [{"role": "TestRole", "required_capabilities": ["nlu"]}]
    await get_test_session.execute(insert(LlmResource), resources)
    await get_test_session.execute(insert(CognitiveRole), roles)
    await get_test_session.commit()

    service = CognitiveService(mock_core_env)
    client = service.get_client_for_role("TestRole")
    assert client.model_name == "cheap-model"

--- END OF FILE ./tests/core/test_cognitive_service.py ---

--- START OF FILE ./tests/governance/test_local_mode_governance.py ---
# tests/governance/test_local_mode_governance.py
"""
Tests to ensure that CORE's governance principles are correctly
reflected in its configuration files.
"""
from shared.config_loader import load_yaml_file
from shared.path_utils import get_repo_root


def test_local_fallback_requires_git_checkpoint():
    """Ensure local_mode.yaml correctly enforces Git validation."""
    repo_root = get_repo_root()
    # --- THIS IS THE FIX ---
    # The config file now lives in the 'mind' directory.
    config_path = repo_root / ".intent" / "mind" / "config" / "local_mode.yaml"
    # --- END OF FIX ---

    # Check that the file actually exists before testing its content
    assert config_path.exists(), "local_mode.yaml configuration file is missing."

    config = load_yaml_file(config_path)

    # This is a critical safety check: local mode must not bypass Git commits.
    ignore_validation = config.get("apis", {}).get("git", {}).get("ignore_validation")
    assert (
        ignore_validation is False
    ), "CRITICAL: local_mode.yaml is configured to ignore Git validation."

--- END OF FILE ./tests/governance/test_local_mode_governance.py ---

--- START OF FILE ./tests/integration/test_full_run.py ---
# tests/integration/test_full_run.py
from unittest.mock import AsyncMock, patch

import pytest
from fastapi.testclient import TestClient


@pytest.mark.anyio
async def test_execute_goal_end_to_end(mock_core_env, mocker):
    from core.main import create_app
    from shared.config import settings

    mocker.patch(
        "core.agents.execution_agent.ExecutionAgent.execute_plan",
        new_callable=AsyncMock,
        return_value=(True, "Success"),
    )

    with patch.object(settings, "LLM_ENABLED", True):
        app = create_app()
        with TestClient(app) as client:
            response = client.post("/execute_goal", json={"goal": "test goal"})
            assert response.status_code == 200, response.text
            assert response.json()["status"] == "success"

--- END OF FILE ./tests/integration/test_full_run.py ---

--- START OF FILE ./tests/unit/test_agent_utils.py ---
# tests/unit/test_agent_utils.py
import re
import textwrap

import pytest

from core.agents.code_editor import CodeEditor


@pytest.fixture
def code_editor():
    """Provides an instance of the CodeEditor."""
    return CodeEditor()


@pytest.fixture
def sample_code():
    """Provides a sample Python code snippet for testing."""
    return textwrap.dedent(
        """
        # A sample file
        import os

        class MyClass:
            def method_one(self):
                \"\"\"This is the first method.\"\"\"
                return 1

        def top_level_function(a, b):
            \"\"\"A function at the top level.\"\"\"
            return a + b
    """
    )


def test_replace_simple_function(code_editor, sample_code):
    """Tests replacing a top-level function with a new version."""
    new_function_code = textwrap.dedent(
        """
        def top_level_function(a, b):
            \"\"\"A modified function.\"\"\"
            # Added a comment
            return a * b  # Changed the operation
    """
    )
    modified_code = code_editor.replace_symbol_in_code(
        sample_code, "top_level_function", new_function_code
    )

    assert "return a * b" in modified_code
    assert "return a + b" not in modified_code
    assert "class MyClass:" in modified_code
    assert "method_one" in modified_code
    assert "# A sample file" in modified_code


def test_replace_method_in_class(code_editor, sample_code):
    """Tests replacing a method within a class."""
    new_method_code = textwrap.dedent(
        """
        def method_one(self):
            \"\"\"A new docstring for the method.\"\"\"
            return 100
    """
    )
    modified_code = code_editor.replace_symbol_in_code(
        sample_code, "method_one", new_method_code
    )

    assert "return 100" in modified_code
    assert not re.search(r"(?m)^\s*return\s+1\s*$", modified_code)
    assert "top_level_function" in modified_code
    assert "class MyClass:" in modified_code


def test_replace_symbol_not_found_raises_error(code_editor, sample_code):
    """Tests that a ValueError is raised if the target symbol doesn't exist."""
    new_code = "def new_func(): return None"
    with pytest.raises(ValueError, match="Symbol 'non_existent_function' not found"):
        code_editor.replace_symbol_in_code(
            sample_code, "non_existent_function", new_code
        )


def test_replace_with_invalid_original_syntax_raises_error(code_editor):
    """Tests that a ValueError is raised if the original code has a syntax error."""
    invalid_original_code = "def top_level_function(a, b) return a + b"
    new_code = "def top_level_function(a,b): return a*b"
    with pytest.raises(
        ValueError, match="Could not parse original code due to syntax error"
    ):
        code_editor.replace_symbol_in_code(
            invalid_original_code, "top_level_function", new_code
        )

--- END OF FILE ./tests/unit/test_agent_utils.py ---

--- START OF FILE ./tests/unit/test_config.py ---
# tests/unit/test_config.py

import pytest

from shared.config import Settings


def test_settings_loads_defined_attributes(monkeypatch):
    """Test that explicitly defined attributes are loaded correctly."""
    monkeypatch.setenv("LOG_LEVEL", "DEBUG")
    # We must create a new instance to re-evaluate the env var
    settings = Settings()
    assert settings.LOG_LEVEL == "DEBUG"


def test_settings_loads_extra_vars_via_constructor():
    """
    Tests that extra variables passed to the constructor are handled correctly
    in Pydantic v2 with extra='allow'.
    """
    # Arrange: Create a settings instance with extra keyword arguments.
    # This is the correct way to test the "extra" fields behavior.
    settings = Settings(
        MY_DYNAMIC_VARIABLE="hello_world",
        CHEAP_API_KEY="cheap_key_123",
        _env_file=None,  # Prevent loading the real .env file for test isolation
    )

    # Assert: In Pydantic v2, extra fields ARE accessible as direct attributes.
    assert hasattr(settings, "MY_DYNAMIC_VARIABLE")
    assert settings.MY_DYNAMIC_VARIABLE == "hello_world"
    assert hasattr(settings, "CHEAP_API_KEY")
    assert settings.CHEAP_API_KEY == "cheap_key_123"

    # Assert: They are ALSO correctly stored in the model_extra dictionary.
    assert "MY_DYNAMIC_VARIABLE" in settings.model_extra
    assert settings.model_extra["MY_DYNAMIC_VARIABLE"] == "hello_world"
    assert "CHEAP_API_KEY" in settings.model_extra
    assert settings.model_extra["CHEAP_API_KEY"] == "cheap_key_123"

    # Assert: They are not confused with the model's formally defined fields.
    defined_fields = set(Settings.model_fields.keys())
    assert "MY_DYNAMIC_VARIABLE" not in defined_fields


def test_settings_accessing_nonexistent_attribute_raises_error():
    """Test that accessing a truly non-existent attribute raises an AttributeError."""
    settings = Settings(_env_file=None)
    with pytest.raises(AttributeError):
        _ = settings.THIS_DOES_NOT_EXIST

--- END OF FILE ./tests/unit/test_config.py ---

--- START OF FILE ./tests/unit/test_execution_agent.py ---
# tests/unit/test_execution_agent.py
from __future__ import annotations

from unittest.mock import AsyncMock, MagicMock

import pytest

from core.agents.execution_agent import ExecutionAgent
from shared.models import ExecutionTask, PlanExecutionError, TaskParams

VALID_PLAN = [
    ExecutionTask(
        step="...",
        action="autonomy.self_healing.format_code",
        params=TaskParams(file_path="src/safe_dir/test.py"),
    )
]
INVALID_ACTION_PLAN = [
    ExecutionTask(
        step="...",
        action="system.dangerous.execute_shell",
        params=TaskParams(file_path="src/safe_dir/test.py"),
    )
]
INVALID_PATH_PLAN = [
    ExecutionTask(
        step="...",
        action="autonomy.self_healing.format_code",
        params=TaskParams(file_path=".intent/charter/policies/safety_policy.yaml"),
    )
]


@pytest.fixture
def mock_execution_agent(mock_core_env):
    """Uses the canonical mock environment to create a valid ExecutionAgent."""
    mock_cognitive_service = MagicMock()
    mock_prompt_pipeline = MagicMock()
    mock_auditor_context = MagicMock()
    mock_git_service = MagicMock(repo_path=mock_core_env)
    mock_plan_executor = MagicMock(git_service=mock_git_service)
    mock_plan_executor.execute_plan = AsyncMock()

    # The agent will now correctly load its policies from the mock environment
    agent = ExecutionAgent(
        cognitive_service=mock_cognitive_service,
        prompt_pipeline=mock_prompt_pipeline,
        plan_executor=mock_plan_executor,
        auditor_context=mock_auditor_context,
    )
    return agent


def test_verify_plan_accepts_valid_plan(mock_execution_agent):
    mock_execution_agent._verify_plan(VALID_PLAN)


def test_verify_plan_rejects_invalid_action(mock_execution_agent):
    with pytest.raises(
        PlanExecutionError, match="not in the list of allowed safe actions"
    ):
        mock_execution_agent._verify_plan(INVALID_ACTION_PLAN)


def test_verify_plan_rejects_forbidden_path(mock_execution_agent):
    with pytest.raises(PlanExecutionError, match="is explicitly forbidden"):
        mock_execution_agent._verify_plan(INVALID_PATH_PLAN)


@pytest.mark.asyncio
async def test_execute_plan_aborts_on_invalid_plan(mock_execution_agent):
    mock_execution_agent._generate_and_validate_all_tasks = AsyncMock()
    success, message = await mock_execution_agent.execute_plan(
        "A test goal", INVALID_ACTION_PLAN
    )
    assert not success
    assert "Action 'system.dangerous.execute_shell' is not in the list" in message
    mock_execution_agent._generate_and_validate_all_tasks.assert_not_called()

--- END OF FILE ./tests/unit/test_execution_agent.py ---

--- START OF FILE ./tests/unit/test_git_service.py ---
# tests/unit/test_git_service.py
from unittest.mock import MagicMock, call

import pytest

from core.git_service import GitService


@pytest.fixture
def mock_git_service(mocker, tmp_path):
    """Creates a GitService instance with a mocked subprocess.run."""
    (tmp_path / ".git").mkdir()

    mock_run = mocker.patch("subprocess.run")

    # Configure mock for the common flow: status -> add -A -> commit
    mock_run.side_effect = [
        MagicMock(stdout="?? new_file.py", stderr="", returncode=0),  # status
        MagicMock(stdout="", stderr="", returncode=0),  # add -A
        MagicMock(stdout="commit success", stderr="", returncode=0),  # commit
    ]

    service = GitService(repo_path=str(tmp_path))
    return service, mock_run


def test_git_add(mock_git_service):
    """Tests that the add method calls subprocess.run with the correct arguments."""
    service, mock_run = mock_git_service
    # Reset side_effect for this simple, single-call test
    mock_run.side_effect = None
    mock_run.return_value = MagicMock(stdout="", stderr="", returncode=0)

    file_to_add = "src/core/main.py"
    service.add(file_to_add)

    mock_run.assert_called_once_with(
        ["git", "add", file_to_add],
        cwd=service.repo_path,
        capture_output=True,
        text=True,
        check=True,
    )


def test_git_commit(mock_git_service):
    """Tests that commit runs: status -> add -A -> commit."""
    service, mock_run = mock_git_service
    commit_message = "feat(agent): Test commit"

    service.commit(commit_message)

    # With robust GitService: status -> add -A -> commit
    assert mock_run.call_count == 3

    expected_calls = [
        call(
            ["git", "status", "--porcelain"],
            cwd=service.repo_path,
            capture_output=True,
            text=True,
            check=True,
        ),
        call(
            ["git", "add", "-A"],
            cwd=service.repo_path,
            capture_output=True,
            text=True,
            check=True,
        ),
        call(
            ["git", "commit", "-m", commit_message],
            cwd=service.repo_path,
            capture_output=True,
            text=True,
            check=True,
        ),
    ]
    mock_run.assert_has_calls(expected_calls)


def test_is_git_repo_true(tmp_path):
    """Returns True when a .git directory exists."""
    (tmp_path / ".git").mkdir()
    service = GitService(repo_path=str(tmp_path))
    assert service.is_git_repo() is True


def test_is_git_repo_false(tmp_path):
    """Raises ValueError if .git is missing on init."""
    with pytest.raises(ValueError):
        GitService(repo_path=str(tmp_path))

--- END OF FILE ./tests/unit/test_git_service.py ---

--- START OF FILE ./tests/unit/test_intent_translator.py ---
# tests/unit/test_intent_translator.py
import json
from unittest.mock import MagicMock

import pytest

from core.agents.intent_translator import IntentTranslator
from shared.config import settings


@pytest.fixture
def mock_cognitive_service(mocker):
    """Mocks the CognitiveService and its client to return a predictable, structured response."""
    mock_client = MagicMock()
    mock_ai_response = json.dumps(
        {
            "status": "vague",
            "suggestion": "The user's goal is a bit vague. Based on the roadmap, did you mean to ask: 'Refactor the codebase to remove the obsolete BaseLLMClient and use CognitiveService'?",
        }
    )
    mock_client.make_request.return_value = mock_ai_response

    mock_service = MagicMock()
    mock_service.get_client_for_role.return_value = mock_client
    return mock_service


@pytest.fixture
def mock_prompt_pipeline(mocker):
    """Mocks the PromptPipeline to prevent file system access during the unit test."""
    mock_pipeline = mocker.patch("core.agents.intent_translator.PromptPipeline")
    mock_instance = mock_pipeline.return_value
    mock_instance.process.side_effect = lambda prompt: prompt
    return mock_instance


def test_translator_handles_vague_goal(
    mock_cognitive_service, mock_prompt_pipeline, tmp_path
):
    """
    Tests if the IntentTranslator can take a vague, human goal
    and produce a structured, actionable goal.
    """
    (tmp_path / ".intent" / "prompts").mkdir(parents=True)
    prompt_file = tmp_path / ".intent" / "prompts" / "intent_translator.prompt"
    prompt_file.write_text("User Request: {user_input}")

    settings.MIND = tmp_path / ".intent"

    translator = IntentTranslator(mock_cognitive_service)
    vague_goal = "optimize AI client usage"

    ai_json_response = translator.translate(vague_goal)

    response_lower = ai_json_response.lower()
    assert "did you mean to ask" in response_lower
    assert "basellmclient" in response_lower
    assert "cognitiveservice" in response_lower

--- END OF FILE ./tests/unit/test_intent_translator.py ---

--- START OF FILE ./tests/unit/test_planner_agent.py ---
# tests/unit/test_planner_agent.py
import json
from unittest.mock import MagicMock

import pytest

from core.agents.planner_agent import PlannerAgent
from core.cognitive_service import CognitiveService
from shared.config import settings
from shared.models import ExecutionTask, PlanExecutionError


@pytest.fixture
def mock_cognitive_service():
    mock_client = MagicMock()
    mock_service = MagicMock(spec=CognitiveService)
    mock_service.get_client_for_role.return_value = mock_client
    return mock_service


# REMOVED the old setup_test_environment function


def test_create_execution_plan_success(
    mock_cognitive_service, mock_fs_with_constitution, mocker
):
    """Tests that the planner can successfully parse a valid high-level plan."""
    mocker.patch.object(settings, "MIND", mock_fs_with_constitution / ".intent")

    agent = PlannerAgent(cognitive_service=mock_cognitive_service)
    goal = "Test goal"

    plan_json = json.dumps(
        [
            {
                "step": "A valid step",
                "action": "create_file",
                "params": {"file_path": "src/test.py"},
            }
        ]
    )
    mock_cognitive_service.get_client_for_role.return_value.make_request.return_value = (
        f"```json\n{plan_json}\n```"
    )

    plan = agent.create_execution_plan(goal)

    assert len(plan) == 1
    assert isinstance(plan[0], ExecutionTask)
    assert plan[0].action == "create_file"


def test_create_execution_plan_fails_on_invalid_action(
    mock_cognitive_service, mock_fs_with_constitution, mocker
):
    """Tests that the planner fails if the plan contains an invalid action."""
    mocker.patch.object(settings, "MIND", mock_fs_with_constitution / ".intent")

    agent = PlannerAgent(cognitive_service=mock_cognitive_service)
    goal = "Test goal"

    invalid_plan_json = json.dumps(
        [{"step": "Invalid action", "action": "make_coffee", "params": {}}]
    )
    mock_cognitive_service.get_client_for_role.return_value.make_request.return_value = (
        f"```json\n{invalid_plan_json}\n```"
    )

    with pytest.raises(PlanExecutionError):
        agent.create_execution_plan(goal)

--- END OF FILE ./tests/unit/test_planner_agent.py ---

--- END OF PROJECT CONTEXT BUNDLE ---
