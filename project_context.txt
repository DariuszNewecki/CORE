--- START OF FILE ./CONTRIBUTING.md ---
# Contributing to CORE

First off, thank you for considering a contribution. CORE is an ambitious project exploring a new frontier of self-governing software, and every contribution â€” from a simple bug report to a deep philosophical discussion â€” is incredibly valuable.

This document provides a guide for how you can get involved.

## The Philosophy: Principled Contributions

CORE is a system governed by a constitution. We ask that all contributions align with the principles laid out in our foundational documents. Before diving into code, we highly recommend reading:

1.  **[The CORE Philosophy (`docs/01_PHILOSOPHY.md`)](docs/01_PHILOSOPHY.md)** â€” to understand the *why* behind the project.
2.  **[The System Architecture (`docs/02_ARCHITECTURE.md`)](docs/02_ARCHITECTURE.md)** â€” to understand the *how* of the Mind/Body separation.

The most important principle for contributors is `clarity_first`. Every change should make the system easier to understand, not harder.

## How You Can Contribute

There are many ways to contribute, and many of them don't involve writing a single line of code.

### ðŸ›ï¸ Discussing Architecture and Governance

The most valuable contributions at this early stage are discussions about the core architecture and governance model.

-   **Review our Roadmap:** Read our **[Strategic Plan (`docs/04_ROADMAP.md`)](docs/04_ROADMAP.md)**.
-   **Open an Issue:** Find a challenge on the roadmap that interests you (e.g., â€œScalability of the Manifestâ€) and open an issue to discuss our proposed approach or suggest a new one.

### ðŸž Reporting Bugs

If you find a bug or a constitutional inconsistency, please open an issue. A great bug report includes:
-   The command you ran.
-   The full output, including the error and traceback.
-   Your analysis of why you think it's happening.

Our goal is for the systemâ€™s `ConstitutionalAuditor` to catch all inconsistencies, but if you find one it missed, youâ€™ve found a valuable way to make our immune system stronger!

### âœï¸ Improving Documentation

If you find a part of our documentation confusing, unclear, or incomplete, a pull request to improve it is a massive contribution. Clear documentation is vital for the project's health.

### ðŸ’» Contributing Code

If you'd like to contribute code, please follow these steps:

1.  Find an open issue that you'd like to work on (or open a new one for discussion).
2.  Fork the repository and create a new branch.
3.  **Write the code.** Ensure your code includes docstrings and type hints.
4.  **Run the checks.** Before submitting, please run the full introspection cycle to ensure your changes are constitutionally valid:
    ```bash
    # Install dependencies
    poetry install
    # Run the full self-audit
    python -m src.core.capabilities
    ```
5.  Submit a pull request.

---

We are excited to build this new future for software development with you.

--- END OF FILE ./CONTRIBUTING.md ---

--- START OF FILE ./docs/01_PHILOSOPHY.md ---
# 1. The CORE Philosophy

## Prime Directive

**CORE exists to transform human intent into complete, evolving software systems â€” without drift, duplication, or degradation.**

It does not merely generate code; it **governs**, **learns**, and **rewrites** itself under the authority of an explicit, machine-readable constitution. It is a system designed to build other systems, safely and transparently.

## The CORE Belief System

Our architecture is founded on a set of core beliefs about the future of software development:

1.  **Intent, Not Instructions:** Software development should be about declaring a desired outcome (`intent`), not writing a list of procedural steps (`instructions`).
2.  **Governance is a Feature:** In a world of autonomous AI agents, safety, alignment, and auditability are not afterthoughtsâ€”they are the primary features of a trustworthy system.
3.  **Code is a Liability:** All code must justify its existence. It must be traceable to a declared purpose, validated against constitutional principles, and be as simple as possible. Unnecessary or un-auditable code is a source of risk.
4.  **A System Must Know Itself:** To evolve safely, a system must have a deep and accurate understanding of its own structure, capabilities, and rules. Self-awareness (`introspection`) is the prerequisite for self-improvement.

## The Ten-Phase Loop of Reasoned Action

All autonomous actions in CORE are governed by a ten-phase loop. This structure ensures that every action is deliberate, justified, and validated. It prevents the system from taking impulsive or un-auditable shortcuts.

The phases are:

1.  **GOAL:** A high-level objective is received from a human operator.
    *(e.g., "Add cryptographic signing to the approval process.")*

2.  **WHY:** The system links the goal to a core constitutional principle.
    *(e.g., "This serves the `safe_by_default` principle.")*

3.  **INTENT:** The goal and its justification are formalized into a clear, machine-readable intent.
    *(e.g., Formalize the request into a plan to modify the `core-admin` tool.)*

4.  **AGENT:** The system selects the appropriate agent(s) for the task.
    *(e.g., The `PlannerAgent` is assigned.)*

5.  **MEANS:** The agent consults its capabilities and knowledge to determine *how* it can achieve the intent.
    *(e.g., The agent knows it has `code_generation` and `introspection` capabilities.)*

6.  **PLAN:** The agent produces a detailed, step-by-step, auditable plan.
    *(e.g., 1. Add `cryptography` library. 2. Add `keygen` function. 3. Modify `approve` function...)*

7.  **ACTION:** The system executes the plan, one validated step at a time.
    *(e.g., The `GeneratorAgent` writes new code to files.)*

8.  **FEEDBACK:** The system's "immune system" (`ConstitutionalAuditor`, `pytest`, linters) provides feedback on the action.
    *(e.g., "The new code fails a linting check." or "All tests pass.")*

9.  **ADAPTATION:** The system uses the feedback to self-correct or confirm the change.
    *(e.g., The `SelfCorrectionEngine` fixes the linting error, or `GitService` commits the successful change.)*

10. **EVOLUTION:** The system updates its self-image (`KnowledgeGraph`) to reflect its new state, completing the loop.

This loop ensures that CORE does not simply act, but *reasons*. Every change is a deliberate, auditable, and constitutionally-aligned evolution.
--- END OF FILE ./docs/01_PHILOSOPHY.md ---

--- START OF FILE ./docs/02_ARCHITECTURE.md ---
# 2. The CORE Architecture

## The Mind-Body Problem, Solved

The central architectural pattern in CORE is a strict separation between the system's "Mind" and its "Body."

-   **The Mind (`.intent/`):** A declarative, version-controlled, and human-readable collection of files that represents the system's complete self-knowledge, purpose, and rules. It is the single source of truth for what the system *is* and *should be*.
-   **The Body (`src/`):** An imperative, executable collection of Python modules that acts upon the world. Its sole purpose is to carry out the will of the Mind, and its every action is governed by the rules declared within the Mind.

This separation is not just a convention; it is a constitutional law enforced by the system itself. The `ConstitutionalAuditor` is the bridge between the two, constantly ensuring the Body is in perfect alignment with the Mind.

## The Anatomy of the Mind (`.intent/`)

The `.intent/` directory is structured to provide a complete and transparent view of the system's governance.

| Directory | Purpose | Key Files |
|---|---|---|
| **`/mission`** | **The Constitution's Soul:** High-level, philosophical principles. | `principles.yaml`, `northstar.yaml` |
| **`/policies`** | **The Constitution's Laws:** Specific, machine-readable rules that govern agent behavior. | `safety_policies.yaml`, `intent_guard.yaml` |
| **`/knowledge`** | **The System's Self-Image:** Declarative knowledge about the system's own structure. | `knowledge_graph.json`, `source_structure.yaml` |
| **`/constitution`** | **The Machinery of Governance:** Defines the human operators and processes for changing the constitution. | `approvers.yaml` |
| **`/proposals`** | **The Legislative Floor:** A safe, temporary "sandbox" for drafting and signing proposed constitutional changes. | `cr-*.yaml` |
| **`/config`** | **Environmental Awareness:** Declares the system's dependencies on its runtime environment. | `runtime_requirements.yaml` |
| **`/schemas`** | **The Blueprint:** JSON schemas that define the structure of the knowledge files. | `knowledge_graph_entry.schema.json` |

## The Anatomy of the Body (`src/`)

The `src/` directory is organized into strict architectural **domains**. These domains are defined in `.intent/knowledge/source_structure.yaml`, and cross-domain communication is tightly controlled by rules enforced by the `ConstitutionalAuditor`.

| Directory | Domain | Responsibility |
|---|---|---|
| **`/core`** | `core` | The central nervous system. Handles the main application loop, API, and core services like file handling and Git integration. |
| **`/agents`** | `agents` | The specialized AI actors. Contains the `PlannerAgent` and its related models and utilities. |
| **`/system`** | `system` | The machinery of self-governance. Contains the `ConstitutionalAuditor`, `core-admin` CLI, and introspection tools. |
| **`/shared`** | `shared` | The common library. Provides shared utilities like logging and configuration loading that are accessible by all other domains. |

## The Flow of Knowledge: From Code to Graph

The syst
--- END OF FILE ./docs/02_ARCHITECTURE.md ---

--- START OF FILE ./docs/03_GOVERNANCE.md ---
# 3. The CORE Governance Model

CORE's ability to evolve its own constitution is its most powerful and most dangerous capability. To ensure this process is safe, auditable, and aligned with human intent, it is governed by a strict, multi-stage **Constitutional Amendment Process**.

This process is designed to solve the central paradox of self-modification: **how can a system safely approve a change that might break its own ability to approve changes?**

## The Guiding Principle: The Canary Check

The entire process is built around a single, foolproof safety mechanism: the **"Canary" Check**.

Before any change is applied to the live constitution, the system performs a "what-if" simulation. It creates a temporary, isolated copy of itself in memory, applies the proposed change to this "canary," and then commands the canary to run a full self-audit.

* If the canary, operating under the new proposed rules, reports a perfect, error-free audit, the change is deemed safe and is automatically applied to the live system.
* If the canary's audit fails, it proves the change would create a broken or inconsistent state. The proposal is automatically rejected, and the live system is never touched.

This mechanism ensures that CORE can never approve an amendment that would render it unable to govern itself.

## The Life of a Constitutional Amendment

A change to any file within the `.intent/` directory follows a formal, five-step lifecycle.

### Step 1: Proposal (`.intent/proposals/`)

An AI agent or a human developer determines that a constitutional change is needed. They do not edit the target file directly. Instead, they create a formal **proposal file** in the `.intent/proposals/` directory.

This proposal is a YAML file containing:

* `target_path`: The file to be changed.
* `justification`: A human-readable reason for the change.
* `content`: The full proposed new content of the file.

### Step 2: Signing (`core-admin proposals-sign`)

Constitutional changes require formal, cryptographic proof of human intent. A human operator uses the `core-admin` tool to sign the proposal with their private key.

```bash
# Generate a personal key pair (one-time setup)
core-admin keygen "your.name@example.com"

# Sign a pending proposal
core-admin proposals-sign cr-new-capability.yaml
```

This action adds a verifiable signature to the proposal file.

### Step 3: Quorum Verification

The system checks `.intent/constitution/approvers.yaml` to determine how many signatures are required (the "quorum").

* Standard changes (like adding a capability) might require only one signature.
* Critical changes (like modifying the approver list itself) require a higher quorum, such as two or more signatures.

### Step 4: Approval & The Canary Check (`core-admin proposals-approve`)

Once a proposal has a sufficient number of valid signatures, any authorized operator can initiate the final approval.

```bash
core-admin proposals-approve cr-new-capability.yaml
```

This command triggers the automated canary check. The operator watches the log as the system simulates the change and runs its self-audit.

### Step 5: Ratification

If the canary check passes, the change is automatically applied to the live `.intent/` directory. The original proposal file is deleted, and the system now operates under its new, evolved constitution. The entire transaction is recorded in an auditable history log.

This rigorous process ensures that every change to CORE's "mind" is deliberate, secure, and verifiably safe.

--- END OF FILE ./docs/03_GOVERNANCE.md ---

--- START OF FILE ./docs/04_ROADMAP.md ---
# 4. The CORE Project Roadmap

## Preamble: From Foundation to Future

The initial development of CORE focused on building a stable, self-aware, and constitutionally governed foundation. The major phases of this foundational work are now complete, as documented in our **[Strategic Plan](StrategicPlan.md)**. That document tells the story of how we achieved our current stable state.

**This document outlines our future direction.** With a stable and secure foundation in place, the project is now moving into its next major phase: **enabling true autonomous application development.**

-   âœ… **A Unified "Mind":** The system's self-knowledge has been consolidated into a single, verifiable Knowledge Graph.
-   âœ… **A Unified Governance Engine:** The `ConstitutionalAuditor` is now the single, dynamic engine for enforcing all constitutional principles.
-   âœ… **Constitutional Compliance:** The system now passes its own strict self-audit with zero errors, proving its internal consistency.
-   âœ… **A Secure Amendment Process:** A robust, human-in-the-loop, cryptographically signed process for evolving the system's own constitution has been implemented and verified.

The following sections outline the key architectural challenges and features on our roadmap. We welcome discussion and contributions on these topics.

---

## Phase 1: Scaling the Constitution

As identified in our external architectural reviews, the current constitutional structure, while sound, faces several scalability challenges. Our next priority is to evolve the `.intent/` directory to support a system that can manage hundreds or thousands of files.

### 1.1: Implement Modular Manifests

-   **Challenge:** The current `project_manifest.yaml` is monolithic. At scale, this becomes a bottleneck and a single point of failure.
-   **Goal:** Refactor the system to support **domain-specific manifests** (e.g., `src/agents/manifest.yaml`). A master process will aggregate these into a global view, but day-to-day management will become modular.
-   **Status:** â³ **Not Started**

### 1.2: Implement Hierarchical Capabilities

-   **Challenge:** The current `capability_tags.yaml` is a flat list. This will become unmanageable as the system's skills grow.
-   **Goal:** Evolve the capability system to support **namespacing and hierarchy**. This will allow for a more organized and expressive taxonomy of actions (e.g., `data.storage.write`, `ui.render.table`).
-   **Status:** â³ **Not Started**

### 1.3: Implement Hierarchical Domains

-   **Challenge:** The architectural domains in `source_structure.yaml` are flat. Real-world applications require nested and layered architectures.
-   **Goal:** Evolve the domain model to support **parent-child relationships**, allowing domains to inherit permissions and creating a true architectural tree.
-   **Status:** â³ **Not Started**

---

## Phase 2: Enhancing Agent Reasoning

The next step is to make the system's AI agents smarter and safer in how they interpret and act upon the constitution.

### 2.1: Implement a Precedence of Principles

-   **Challenge:** AI agents lack the intuition to resolve conflicts between high-level principles (e.g., `clarity_first` vs. `safe_by_default`).
-   **Goal:** Create a new constitutional file that defines a **clear hierarchy or weighting system** for principles. This will provide agents with a deterministic framework for making decisions when rules conflict.
-   **Status:** â³ **Not Started**

### 2.2: Enforce Auditable Justification Logs

-   **Challenge:** An agent's "reasoning" for a particular plan can be opaque.
-   **Goal:** Modify the `PlannerAgent` to require that every generated plan includes a **`justification` block**. This block will explicitly state which constitutional principle the plan serves and provide a brief, human-readable explanation of the agent's reasoning. This log will become a critical part of the audit trail.
-   **Status:** â³ **Not Started**

---

## Phase 3: Autonomous Application Generation

This is the ultimate goal of the CORE project. With a scalable constitution and smarter agents, we will build the capabilities for CORE to generate and manage new software projects from a high-level intent.

-   **Goal:** Develop the end-to-end flow where a user can provide a prompt like, "Build a simple web app to track my book collection," and have CORE:
    1.  Propose a new constitutional structure for the book app.
    2.  Generate the initial code, including models, API endpoints, and basic UI.
    3.  Continuously run its self-audit against the new application.
    4.  Accept further intents to evolve the new application.
-   **Status:** â³ **Not Started**

We believe that by solving the challenges in Phase 1 and 2, we will have built a foundation of trust and scalability that makes Phase 3 possible.

--- END OF FILE ./docs/04_ROADMAP.md ---

--- START OF FILE ./docs/05_BYOR.md ---
# 5. Bring Your Own Repo (BYOR) Quickstart

## The Guiding Principle: Ingestion Isomorphism

CORE is designed to be impartial. It applies the same rigorous constitutional analysis to any repository that it applies to itself. This principle, known as **Ingestion Isomorphism**, means that CORE can analyze, understand, and help govern any project without special treatment.

This guide will walk you through the process of pointing CORE at an existing repository and generating a starter constitution for it.

## The Goal: See Your Project Through CORE's Eyes

The `core-admin byor-init` command is a powerful introspection tool. It does not modify your code. Its purpose is to:

1. **Analyze** your repository's structure and capabilities.
2. **Infer** a set of domains based on your directory layout.
3. **Propose** a minimal, non-intrusive `.intent/` constitution based on its findings.

This gives you an instant health check and a starting point for bringing your project under CORE's governance.

---

## Step 1: The Safe Dry Run (Read-Only Analysis)

By default, the command runs in a safe, read-only **dry run** mode. It will show you what it would do **without changing a single file**.

**Commands**

```bash
# Analyze the current CORE repository
poetry run core-admin byor-init .

# Analyze a different project on your machine
poetry run core-admin byor-init /path/to/your/other/project
```

**Understanding the output**

The command first builds a Knowledge Graph of the target repository. Then, it shows the content of five constitutional files it proposes to create:

* `source_structure.yaml` â€” A map of your project, with each subdirectory in `src/` treated as a domain.
* `project_manifest.yaml` â€” An inventory of all the `# CAPABILITY` tags it discovered in your code.
* `capability_tags.yaml` â€” A dictionary for you to define and describe each of those capabilities.
* `principles.yaml` â€” A starter set of CORE's philosophical principles.
* `safety_policies.yaml` â€” A starter set of basic safety rules.

---

## Step 2: Applying the Constitution (Write Mode)

Once youâ€™ve reviewed the dry run output and youâ€™re happy with the proposed constitution, run the command again with the `--write` flag. This will create the `.intent/` directory and all proposed files inside your target repository.

**Command**

```bash
# Apply the starter constitution to the current repository
poetry run core-admin byor-init . --write
```

---

## Step 3: The First Audit

Your target repository is now **CORE-aware**â€”it has a nascent "Mind." The next step is to ask CORE to perform its first constitutional audit on the project.

From within the CORE project, configure the auditor to point at the new project. (In a future version, CORE will be able to attach to it directly.) The result is a continuous, automated health check on your project's architectural integrity and alignment with its newly declared principles.

This process is the first step in transforming any repository from a simple collection of code into a governed, self-aware system.

--- END OF FILE ./docs/05_BYOR.md ---

--- START OF FILE ./docs/06_STARTER_KITS.md ---
# ./docs/06\_STARTER\_KITS.md

# 6. Starter Kits & The Philosophy of Intent

## The CORE Partnership

CORE is not a vending machine for code. It is an intelligent partner designed to translate a human's intent into a governed, working software system. This partnership requires two things:

1. **The Human's Responsibility:** Provide a clear, high-level intentâ€”the "why" behind the project.
2. **CORE's Responsibility:** Translate that intent into a complete system, asking for clarification and guidance along the way.

If the human provides no intent ("I do not care"), CORE will do nothing. The partnership requires a starting point.

## Starter Kits: Your First Declaration of Intent

To facilitate this partnership, the `core-admin new` command uses **Starter Kits**. A starter kit is not just a collection of template files; it is a **pre-packaged declaration of intent**. It is a way for you to tell CORE about the *kind* of system you want to build from day one.

By choosing a starter kit, you are providing the "minimal viable intent" that CORE needs to begin its work.

### How to Use Starter Kits

When you create a new project, you can specify a `--profile` option. This tells the scaffolder which starter kit to use.

```bash
# Scaffold a new project using the 'default' balanced starter kit
poetry run core-admin new my-new-app --profile default

# Scaffold a project with high-security policies from the start
poetry run core-admin new my-secure-api --profile security
```

If you do not provide a profile, CORE will default to the safest, most balanced option.

## The Life of a Starter Kit

* **Scaffolding:** CORE creates your new project structure and populates the `.intent/` directory with the constitutional files from your chosen starter kit.
* **Ownership:** From that moment on, that constitution is **yours**. It is no longer a template. It is the living "Mind" of your new project.
* **Evolution:** You can (and should) immediately begin to amend and evolve your new constitution to perfectly match your project's unique goals, using the standard proposals workflow.

Starter kits are just the beginning of the conversation, not the end. They are the most effective way to kickstart the CORE partnership and begin the journey of building a truly intent-driven system.

---

# ./README.md

# CORE â€” The Self-Improving System Architect

> **Where Intelligence Lives.**

[![Status: Architectural Prototype](https://img.shields.io/badge/status-architectural%20prototype-blue.svg)](#-project-status)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

CORE is a self-governing, constitutionally aligned AI development framework that can plan, write, validate, and evolve software systems â€” autonomously and safely. It is designed for environments where **trust, traceability, and governance matter**.

---

## ðŸ›ï¸ Project Status: Architectural Prototype

The core self-governance and constitutional amendment loop is complete and stable. The system can audit and modify its own constitution via a human-in-the-loop, cryptographically signed approval process.

The next phase, as outlined in our **[Strategic Plan](docs/StrategicPlan.md)**, is to expand agent capabilities so CORE can generate and manage entirely new applications based on user intent.

Weâ€™re making the project public now to invite collaboration on this foundational architecture.

---

## ðŸ§  What CORE *is*

* ðŸ§¾ Evolves itself through **declared intent**, not hidden assumptions.
* ðŸ›¡ï¸ Enforces **constitutional rules**, **domain boundaries**, and **safety policies**.
* ðŸŒ± Creates new, governed applications from **[Starter Kits](docs/06_STARTER_KITS.md)** that capture initial intent.
* ðŸ§© Uses a modular agent architecture with a clear separation of concerns.
* ðŸ“š Ensures every decision is **documented, reversible, and introspectable**.

---

## ðŸ¦® Key Concepts

| Concept                     | Description                                                                              |
| --------------------------- | ---------------------------------------------------------------------------------------- |
| **`.intent/`**              | The â€œmindâ€ of CORE: constitution, policies, capability maps, and self-knowledge.         |
| **`ConstitutionalAuditor`** | The â€œimmune system,â€ continuously verifying code aligns with the constitution.           |
| **`PlannerAgent`**          | Decomposes high-level goals into executable plans.                                       |
| **`core-admin` CLI**        | Human-in-the-loop tool for managing the system's lifecycle.                              |
| **Starter Kits**            | Pre-packaged constitutions that serve as the user's first declaration of intent.         |
| **Canary Check**            | Applies proposed changes to an isolated copy and runs a full self-audit before approval. |
| **Knowledge Graph**         | Machine-readable map of symbols, roles, capabilities, and relationships.                 |

---

## ðŸš€ Getting Started

1. **Install dependencies**

   ```bash
   poetry install
   ```

2. **Set up environment**

   ```bash
   cp .env.example .env
   # Edit .env with your keys/URLs. See .intent/config/runtime_requirements.yaml for required variables.
   ```

3. **Run a full self-audit**

   ```bash
   make audit
   ```

---

## ðŸ§‘â€âš–ï¸ Human-in-the-Loop (CLI)

The `core-admin` CLI is your primary tool for guiding the system.

### Creating New Projects

```bash
# Create a new, governed application using a starter kit
core-admin new my-new-app --profile default
```

### Onboarding Existing Projects

```bash
# Analyze an existing repo and propose a starter constitution
core-admin byor-init /path/to/existing-repo
```

### Managing the Constitution

```bash
# List pending constitutional changes
core-admin proposals-list

# Sign a proposal with your key
core-admin proposals-sign cr-example.yaml

# Approve a proposal (runs a canary self-audit)
core-admin proposals-approve cr-example.yaml
```

> If `core-admin` isnâ€™t found, try: `poetry run core-admin ...`

---

## ðŸŒ± Contributing

We welcome contributions from AI engineers, DevOps pros, and governance experts.

* See **CONTRIBUTING.md** to get started.
* Check the **Strategic Plan** for where we're headed.

---

## ðŸ“„ License

Licensed under the MIT License. See **LICENSE**.

--- END OF FILE ./docs/06_STARTER_KITS.md ---

--- START OF FILE ./docs/NORTH_STAR.md ---
# CORE North Star

**Goal:** Turn high-level goals into governed, safe, running software â€” autonomously.

**Invariant:** Safety never degrades. Every change is proven aligned to intent before it lands (policies, tests, canary audit).

## The Autonomy Ladder

- **A0 Â· Observe** â€” Attach to any repo, build knowledge graph, run auditor (read-only).
- **A1 Â· Propose** â€” Open PRs with small, safe fixes (docstrings, capability tags, logging). Human merges.
- **A2 Â· Governed Writes** â€” Canary-validated changes self-apply with policy (human optional by risk tier).
- **A3 Â· BYOR** â€” CORE-fy arbitrary repos (including CORE), propose structure, capabilities, tests.
- **A4 Â· Birth New Apps** â€” From a goal, scaffold feature-first app (Mind/Body), tests, CI/CD.
- **A5 Â· Continuous Intent Dev** â€” Roadmap evolves in natural language; CORE plans, implements, releases within risk budgets.

## Guardrails
- **Risk tiers:** auto-merge only for low-risk scopes at first.
- **Evidence gates:** static policies + tests + canary must pass.
- **Idempotence:** ingestion produces stable results; re-runs donâ€™t thrash.
- **Kill switch:** rollback on anomaly/error-budget breach.

## KPIs
- Auditor errors/warnings â†’ sustained zero
- % code with capability tags
- Proposal acceptance rate / MTTR
- % PRs generated by CORE
- % no-human releases by risk tier
- Post-merge incident rate

## Current Status
- Project status: **Architectural Prototype**.
- Targeting **M1 (A0â†’A1)**: CORE overlay, CI audit, auto-PRs for docstrings/tags/logging.

--- END OF FILE ./docs/NORTH_STAR.md ---

--- START OF FILE ./docs/StrategicPlan.md ---
# Project CORE: A Strategic Plan for Refactoring and Evolution

## 0. Context: The Story of Our Foundation

**This document is a historical record.** It outlines the foundational work that was completed to evolve CORE from an early prototype into the stable, self-governing system it is today. It tells the story of "how we got here."

For our future plans and the challenges we are tackling next, please see the living **[Project Roadmap (`04_ROADMAP.md`)](04_ROADMAP.md)**.

---

## 1. Preamble: From Diagnosis to Vision

This document outlines the strategic plan to evolve the CORE system from its current state to a truly self-governing, resilient, and evolvable architecture.

The initial feeling of "running in circles" was a correct diagnosis of a system struggling with internal inconsistencies. The recent comprehensive audit, while displaying numerous errors and warnings, was not a sign of failure. It was the **first successful act of self-diagnosis** by the system's nascent "brain." The audit provided a clear, actionable roadmap, revealing a fundamental disconnect between the declared `intent` and the `source code` reality.

This plan details the two major phases of our work:
*   **Part A: Foundational Refactoring.** To achieve a stable, constitutionally compliant baseline by fixing the issues diagnosed by the audit.
*   **Part B: Enabling True Self-Governance.** To build the necessary mechanisms for the system to evolve its own code and constitution safely and autonomously.

---

## Part A: Foundational Refactoring (Achieving Stability)

This phase focuses on acting on the audit's results to create a clean, consistent, and understandable codebase. It is the work required to teach the system what a "good" state looks like.

### Step A1: Unify the "Brain"
*   **Goal:** Eliminate data redundancy and create a single source of truth for the system's knowledge of its own code.
*   **Status:** âœ… **COMPLETE**
*   **Outcome:** The dual `codegraph.json` and `function_manifest.json` files have been replaced by a single, comprehensive `.intent/knowledge/knowledge_graph.json`. The `KnowledgeGraphBuilder` tool is now the sole producer of this artifact.

### Step A2: Consolidate Governance
*   **Goal:** Eliminate redundant tools and establish a single, authoritative script for verifying the system's integrity.
*   **Status:** âœ… **COMPLETE**
*   **Outcome:** The `ConstitutionalAuditor` is now the master tool for all static analysis. Older, fragmented tools (`architectural_auditor`, `principle_validator`, etc.) have been merged into it or deleted.

### Step A3: Stabilize the System
*   **Goal:** Ensure the system has a reliable safety net for development.
*   **Status:** âœ… **COMPLETE**
*   **Outcome:** The test suite has been repaired. Obsolete tests were deleted, and configuration issues (`pythonpath`, dependency conflicts) were resolved, resulting in a stable and passing test run.

### Step A4: Achieve Constitutional Compliance
*   **Goal:** Resolve all critical errors reported by the `ConstitutionalAuditor`.
*   **Status:** âœ… **COMPLETE**
*   **Outcome:** All structural errors (domain mismatches, illegal imports) have been fixed. The "mind-body problem" has been solved by manually annotating the existing codebase with `# CAPABILITY:` tags, fully aligning the `project_manifest.yaml` with the implementation. The audit now reports **ZERO critical errors.**

---

## Part B: The Path Forward (Enabling Evolution)

With a stable foundation, we can now build the mechanisms that allow CORE to fulfill its prime directive: to evolve itself safely.

### Step B1: Trust the Brain (Simplification & Cleanup)

*   **Goal:** Eliminate all audit warnings by making the system's "brain" more intelligent.
*   **Guiding Principle:** We did not blindly patch the auditor. Instead, we enhanced the `KnowledgeGraphBuilder` so it could understand more complex, valid code patterns, thus resolving the root cause of the false warnings.
*   **Status:** âœ… **COMPLETE**
*   **Outcome:** The `KnowledgeGraphBuilder` has been upgraded with a context-aware AST visitor and a declarative pattern-matching engine (`.intent/knowledge/entry_point_patterns.yaml`). It now correctly identifies inheritance, framework callbacks, and CLI entry points. All schema violations were corrected, and all code was fully documented. The `ConstitutionalAuditor` now reports **ZERO errors and ZERO warnings.**

### Step B2: Build the Immune System (Governed Creation)

*   **Goal:** Evolve the `PlannerAgent` to ensure that all *newly generated* code is automatically compliant with the constitution, preventing future errors.
*   **Status:** âœ… **COMPLETE**
*   **Outcome:** The `PlannerAgent._execute_task` method now follows a complete `Generate -> Govern -> Validate -> Self-Correct -> Write` loop. It automatically adds capability tags, enforces docstrings, validates the code, and triggers a self-correction cycle on any validation failure, ensuring only constitutionally compliant code is ever written to disk.

### Step B3: The Constitutional Amendment Process

*   **Goal:** Transform `.intent/` from a "notepad" into a true constitution with a formal, safe amendment process. This allows CORE to evolve its own brain.
*   **Status:** âœ… **COMPLETE**
*   **The Mechanism:**
    1.  **The Waiting Room (`.intent/proposals/`):** A dedicated directory for drafting changes to the constitution. Files here are not active.
    2.  **The Proposal Bundle:** A standardized YAML format for change requests (e.g., `cr-....yaml`) containing the `target_path`, `action`, `justification`, and proposed `content`.
    3.  **The Governance Layer:**
        *   **`IntentGuard`:** Enforces a new, critical rule: **No direct writes are allowed into `.intent/` except to the `proposals/` directory.**
        *   **`ConstitutionalAuditor`:** Scans `proposals/`, validates the format of any pending proposals, and reports them in a new "Pending Amendments" section of its report.
    4.  **The Ratification Mechanism (`core-admin` tool):** A human operator uses a simple CLI tool to manage the amendment lifecycle. The key command is `approve`.
    5.  **The "Canary" Pre-flight Check:** The `core-admin approve` command is designed to be fundamentally safe and solves the "how does it know it's broken if it's broken?" paradox.
        *   It spawns a **temporary, isolated "canary" instance** of CORE with the proposed change applied *in memory*.
        *   It commands this canary instance to run a full self-audit.
        *   If the canary audit succeeds, the change is permanently applied to the real `.intent/` directory.
        *   If the canary audit fails, the change is rejected, and the real `.intent/` directory is never touched, preventing system failure.

## 2. Conclusion

Upon completion of this plan, CORE will have evolved from a promising but inconsistent prototype into a robust, self-aware system. It will possess a stable foundation, a clear understanding of its own structure, andâ€”most importantlyâ€”a safe, governed process for both creating code and evolving its own foundational intent.

This plan transforms CORE from a system that is merely *audited* to one that is truly *governed*.

--- END OF FILE ./docs/StrategicPlan.md ---

--- START OF FILE ./docs/TheDocument.md ---
# The CORE Constitution

## Section 1: Prime Directive

**CORE exists to transform human intent into complete, evolving software systems â€” without drift, duplication, or degradation.**

It does not merely generate code; it **governs**, **learns**, and **rewrites** itself under the authority of an explicit, machine-readable constitution. It is a system designed to build other systems, safely and transparently.

---

## Section 2: Purpose of This Document

This document defines the **philosophy** and **operating principles** of the CORE system. It serves as the primary, human-facing contract that justifies and explains all of the system's automated governance mechanisms.

All rules enforced by the system's "Mind" (`.intent/`) are derived from the principles explained here. For a deeper dive into the specific mechanics, please refer to the foundational documents:

1.  **[The CORE Philosophy (`01_PHILOSOPHY.md`)](01_PHILOSOPHY.md)** â€” The *why* behind the project and the Ten-Phase Loop of Reasoned Action.
2.  **[The System Architecture (`02_ARCHITECTURE.md`)](02_ARCHITECTURE.md)** â€” The *how* of the Mind/Body separation.
3.  **[The Governance Model (`03_GOVERNANCE.md`)](03_GOVERNANCE.md)** â€” The formal process for safe, constitutional change.

---

## Section 3: The Ten-Phase Loop of Reasoned Action

All autonomous actions in CORE are governed by a ten-phase loop. This structure ensures that every action is deliberate, justified, validated, and traceable to a core principle. It prevents the system from taking impulsive or un-auditable shortcuts.

**GOAL** â†’ **WHY** â†’ **INTENT** â†’ **AGENT** â†’ **MEANS** â†’ **PLAN** â†’ **ACTION** â†’ **FEEDBACK** â†’ **ADAPTATION** â†’ **EVOLUTION**

This loop ensures that CORE does not simply act, but *reasons*. Every change is a deliberate, auditable, and constitutionally-aligned evolution of the system.

--- END OF FILE ./docs/TheDocument.md ---

--- START OF FILE ./.env.example ---
# CORE Environment Variables
# ---------------------------
# Copy this file to .env and fill in the values below.
# The .env file is ignored by git, so your secrets will not be committed.

# --- Path Configuration ---
# These are typically not needed to be changed if running from the repo root.
MIND=".intent"
BODY="src"
REPO_PATH="."

# --- Orchestrator LLM Configuration (For high-level planning) ---
# Example for a local model server: http://localhost:11434/v1
ORCHESTRATOR_API_URL=""
ORCHESTRATOR_API_KEY="your_api_key_here"
ORCHESTRATOR_MODEL_NAME="deepseek-chat"

# --- Generator LLM Configuration (For code generation) ---
# Example for a local model server: http://localhost:11434/v1
GENERATOR_API_URL=""
GENERATOR_API_KEY="your_api_key_here"
GENERATOR_MODEL_NAME="deepseek-coder"
--- END OF FILE ./.env.example ---

--- START OF FILE ./.github/ISSUE_TEMPLATE/governance_proposal.yml ---
name: Constitutional Proposal
description: Propose a change to the Mind (.intent) via a signed proposal file.
labels: ["audit","roadmap"]
body:
  - type: textarea
    id: justification
    attributes:
      label: Justification
      description: Why is this change needed?
    validations:
      required: true
  - type: input
    id: target
    attributes:
      label: Target path
      placeholder: .intent/policies/safety_policies.yaml
    validations:
      required: true
  - type: textarea
    id: content
    attributes:
      label: Proposed content
      description: Paste or attach content that will replace the file.
  - type: markdown
    attributes:
      value: |
        **How to submit:**
        1. Create `.intent/proposals/cr-*.yaml` using the schema.
        2. (Optional) Sign with `core-admin proposals-sign`.
        3. Ask a maintainer to run `proposals-approve`.

--- END OF FILE ./.github/ISSUE_TEMPLATE/governance_proposal.yml ---

--- START OF FILE ./.github/PULL_REQUEST_TEMPLATE.md: ---
## Summary
<!-- What does this change do? -->

## Checks
- [ ] I ran `poetry install && poetry run python -m src.core.capabilities` and it passed.
- [ ] I ran `poetry run ruff check .` and `poetry run black --check .`.
- [ ] I ran `poetry run pytest -q`.

## Governance
- [ ] If I changed `.intent/*`, I used a proposal (`.intent/proposals/cr-*.yaml`) that validates against `proposal.schema.json`.

--- END OF FILE ./.github/PULL_REQUEST_TEMPLATE.md: ---

--- START OF FILE ./.github/workflows/core-ci.yml ---
name: CORE CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

permissions:
  contents: read

jobs:
  build_and_test:
    name: Build & Test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Poetry
        run: pipx install poetry

      - name: Cache Poetry
        uses: actions/cache@v4
        with:
          path: ~/.cache/pypoetry
          key: poetry-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            poetry-${{ runner.os }}-

      - name: Install deps
        run: poetry install --no-interaction --no-root

      - name: Lint (Ruff)
        run: poetry run ruff check .

      - name: Format check (Black)
        run: poetry run black --check .

      - name: Self-audit (KnowledgeGraph + Auditor)
        run: poetry run python -m src.core.capabilities

      - name: Tests
        run: poetry run pytest -q
        
  governance:
    name: Governance & Drift Check
    runs-on: ubuntu-latest
    needs: build_and_test
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python & Poetry
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: pipx install poetry

      - name: Install deps
        run: poetry install --no-interaction --no-root

      - name: Generate Knowledge Graph Artifact
        run: poetry run core-admin guard kg-export

      - name: Detect Capability Drift (strict)
        run: |
          set -e
          poetry run core-admin guard drift --strict-intent --fail-on any --format json

--- END OF FILE ./.github/workflows/core-ci.yml ---

--- START OF FILE ./.github/workflows/nightly-audit.yml ---
name: Nightly Constitutional Audit

on:
  schedule:
    - cron: "15 2 * * *"   # 02:15 UTC nightly
  workflow_dispatch:

permissions:
  contents: read
  issues: write

jobs:
  audit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install Poetry
        run: pipx install poetry
      - name: Install deps
        run: poetry install --no-interaction --no-root
      - name: Run self-audit
        id: audit
        run: |
          set +e
          poetry run python -m src.core.capabilities
          echo "exit_code=$?" >> $GITHUB_OUTPUT
      - name: Open/Update issue on failure
        if: steps.audit.outputs.exit_code != '0'
        uses: actions/github-script@v7
        with:
          script: |
            const title = "âŒ Nightly Constitutional Audit failed";
            const body = "The nightly audit failed. Please check CI logs for details.\n\n- Workflow: nightly-audit\n- Run: " + process.env.GITHUB_RUN_ID;
            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner, repo: context.repo.repo, state: "open", labels: "ci,audit"
            });
            const existing = issues.find(i => i.title === title);
            if (existing) {
              await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: existing.number, body });
            } else {
              await github.rest.issues.create({
                owner: context.repo.owner, repo: context.repo.repo,
                title, body, labels: ["ci","audit"]
              });
            }
      - name: Close issue on success
        if: steps.audit.outputs.exit_code == '0'
        uses: actions/github-script@v7
        with:
          script: |
            const title = "âŒ Nightly Constitutional Audit failed";
            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner, repo: context.repo.repo, state: "open", labels: "ci,audit"
            });
            const existing = issues.find(i => i.title === title);
            if (existing) {
              await github.rest.issues.update({
                owner: context.repo.owner, repo: context.repo.repo,
                issue_number: existing.number, state: "closed"
              });
            }

--- END OF FILE ./.github/workflows/nightly-audit.yml ---

--- START OF FILE ./.gitignore ---
# Python
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.so
*.egg-info/
__pypackages__/

# Virtual environments
.venv/
.env

# Typing / linting
.mypy_cache/
.ruff_cache/

# Testing
.pytest_cache/
.coverage
htmlcov/
*.log

# Editors
.vscode/
.idea/
*.swp

# System/OS
.DS_Store
Thumbs.db

# CORE-specific
logs/
pending_writes/
sandbox/
*.jsonl
*.lock
reports/knowledge_graph.json
reports/drift_report.json


# Cache or checkpoints
*.bak
*.tmp

work/*
!work/.gitkeep

--- END OF FILE ./.gitignore ---

--- START OF FILE ./.intent/config/local_mode.yaml ---
# .intent/config/local_mode.yaml

mode: local_fallback
apis:
  llm:
    enabled: false
    fallback: local_validator
  git:
    ignore_validation: false
--- END OF FILE ./.intent/config/local_mode.yaml ---

--- START OF FILE ./.intent/config/runtime_requirements.yaml ---
# .intent/config/runtime_requirements.yaml
#
# PURPOSE: This file makes the system aware of its dependencies on the external environment.
# It allows the ConstitutionalAuditor to verify that the system is correctly configured
# at startup, without ever reading secret values.

required_environment_variables:
  - name: "MIND"
    description: "The relative path to the system's declarative 'mind' (.intent directory)."
    type: "config"
    required: true

  - name: "BODY"
    description: "The relative path to the system's executable 'body' (src directory)."
    type: "config"
    required: true

  - name: "REPO_PATH"
    description: "The absolute path to the root of the repository."
    type: "config"
    required: true

  - name: "ORCHESTRATOR_API_URL"
    description: "The API endpoint for the high-level planning LLM."
    type: "config"
    required: true

  - name: "ORCHESTRATOR_API_KEY"
    description: "The API key for the high-level planning LLM."
    type: "secret"
    required: true

  - name: "ORCHESTRATOR_MODEL_NAME"
    description: "The name of the model to use for orchestration."
    type: "config"
    required: true

  - name: "GENERATOR_API_URL"
    description: "The API endpoint for the code generation LLM."
    type: "config"
    required: true

  - name: "GENERATOR_API_KEY"
    description: "The API key for the code generation LLM."
    type: "secret"
    required: true
--- END OF FILE ./.intent/config/runtime_requirements.yaml ---

--- START OF FILE ./.intent/constitution/approvers.yaml ---
# .intent/constitution/approvers.yaml
#
# PURPOSE: This fulfills security_intents principle by enabling cryptographic verification
# of constitutional approvals, preventing compromised CLI tools from approving malicious changes.
#
# This file contains the public keys of all authorized constitutional approvers.
# Each key must be in PEM format and accompanied by the approver's identity.
#
# TO ADD A NEW APPROVER:
# 1. Run the command: `core-admin keygen "your.email@example.com"`
# 2. The command will output a YAML block.
# 3. Paste that block into the 'approvers' list below.

approvers:
  - identity: "core-team@core-system.ai"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEA3dK7Jt4jJh6+QvZvY6XcGx3q8R0e7m5JwqYk8qFtU9U=
      -----END PUBLIC KEY-----
    created_at: "2025-08-05T15:50:53.995534+00:00"
    role: "maintainer"
    description: "Primary CORE development team"

  - identity: "security-audit@core-system.ai"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEApJ+8mNvL7wY2XfDcR9q3Q5t4yZx7v6hB8gKj0sF3T5U=
      -----END PUBLIC KEY-----
    created_at: "2025-08-05T15:50:53.995534+00:00"
    role: "security"
    description: "Security audit team for constitutional changes"

  - identity: "d.newecki@gmail.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VwAyEA+V4iUN4DElKdqXmU4ivNthnG8VgPb7QqZgzdJuh4igs=
      -----END PUBLIC KEY-----
    role: "maintainer"
    description: "Mentor"


# Minimum number of signatures required for constitutional amendments
quorum:
  # Regular amendments require 1 signature
  standard: 1
  # Critical changes (e.g., altering approval process) require 2 signatures
  critical: 2

# Critical policy paths that require higher quorum
critical_paths:
  - ".intent/policies/intent_guard.yaml"
  - ".intent/constitution/approvers.yaml"
  - ".intent/meta.yaml"

--- END OF FILE ./.intent/constitution/approvers.yaml ---

--- START OF FILE ./.intent/constitution/approvers.yaml.example ---
# .intent/constitution/approvers.yaml
#
# PURPOSE: This file enables cryptographic verification of constitutional approvals,
# preventing unauthorized changes. It contains the public keys of all authorized
# constitutional approvers for this instance of CORE.
#
# TO ADD A NEW APPROVER:
# 1. Run the command: `core-admin keygen "your.email@example.com"`
# 2. The command will output a JSON/YAML block.
# 3. Paste that block into the 'approvers' list below.

approvers:
  - identity: "your.name@example.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
      -----END PUBLIC KEY-----
    created_at: "YYYY-MM-DDTHH:MM:SSZ"
    role: "maintainer"
    description: "Primary maintainer of this CORE instance"

  - identity: "another.approver@example.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB=
      -----END PUBLIC KEY-----
    created_at: "YYYY-MM-DDTHH:MM:SSZ"
    role: "contributor"
    description: "Authorized contributor"

# Minimum number of valid signatures required to approve a constitutional amendment.
quorum:
  # Regular amendments (e.g., changing a policy) require 1 signature.
  standard: 1
  # Critical changes (e.g., altering this file) require 2 signatures.
  critical: 2

# A list of file paths that are considered "critical". Any proposal targeting
# these files will require the 'critical' quorum to be met.
critical_paths:
  - ".intent/policies/intent_guard.yaml"
  - ".intent/constitution/approvers.yaml"
  - ".intent/meta.yaml"

--- END OF FILE ./.intent/constitution/approvers.yaml.example ---

--- START OF FILE ./.intent/evaluation/audit_checklist.yaml ---
audit_checklist:
  - id: declared_intent
    item: "Was the intent declared before the change?"
    required: true
  - id: explanation
    item: "Was the change explained or justified?"
    required: true
  - id: manifest_sync
    item: "Did the change include a manifest update?"
    required: true
  - id: checkpoint
    item: "Was a rollback plan or checkpoint created?"
    required: false
  - id: quality_verified
    item: "Was code quality verified post-write?"
    required: true

--- END OF FILE ./.intent/evaluation/audit_checklist.yaml ---

--- START OF FILE ./.intent/evaluation/score_policy.yaml ---
score_policy:
  strategy: weighted_criteria

  criteria:
    - id: intent_alignment
      description: "Does this change serve a declared intent?"
      weight: 0.4

    - id: structural_compliance
      description: "Does it follow folder conventions and manifest structure?"
      weight: 0.2

    - id: safety
      description: "Was the change gated by a test or checkpoint?"
      weight: 0.2

    - id: code_quality
      description: "Does it pass formatting, linting, and basic semantic checks?"
      weight: 0.2

  thresholds:
    pass: 0.7
    warn: 0.5
    fail: 0.4

--- END OF FILE ./.intent/evaluation/score_policy.yaml ---

--- START OF FILE ./.intent/knowledge/agent_roles.yaml ---
# .intent/knowledge/agent_roles.yaml

roles:
  planner:
    description: "Responsible for breaking down intents, sequencing tasks, and preparing bundles."
    allowed_tags:
      - planning
      - introspection
      - orchestration

  builder:
    description: "Executes generation and modification tasks according to a validated plan."
    allowed_tags:
      - generation
      - validation
      - testing

  reviewer:
    description: "Evaluates changes for safety, structure, and declared alignment."
    allowed_tags:
      - validation
      - governance
      - testing

  orchestrator:
    description: "Coordinates flows, executes bundles, and manages lifecycle rules."
    allowed_tags:
      - orchestration
      - governance
      - llm

  guardian:
    description: "Handles enforcement of rules and monitors intent integrity."
    allowed_tags:
      - governance
      - validation

--- END OF FILE ./.intent/knowledge/agent_roles.yaml ---

--- START OF FILE ./.intent/knowledge/capability_tags.yaml ---
# .intent/knowledge/capability_tags.yaml
#
# This is the canonical dictionary of all valid capability tags in the CORE system.
# The ConstitutionalAuditor verifies that any # CAPABILITY tag used in the source code
# is defined in this file.

tags:
  # --- System & Governance ---
  - name: introspection
    description: "Enables self-analysis of the system's own structure, code, or intent."
  - name: alignment_checking
    description: "Verifies that system components or actions align with constitutional principles."
  - name: manifest_updating
    description: "Modifies or generates knowledge artifacts like the knowledge_graph.json."
  - name: self_review
    description: "Enables the system to analyze its own code for quality, correctness, or improvements."
  - name: intent_guarding
    description: "Enforces constitutional rules at runtime, preventing forbidden actions."
  - name: change_safety_enforcement
    description: "Implements safety checks or operations related to modifying files or state (e.g., Git)."
  - name: system_logging
    description: "Provides system-wide logging capabilities."

  # --- Code Validation & Quality ---
  - name: semantic_validation
    description: "Performs semantic analysis on code, beyond simple syntax checks."
  - name: syntax_validation
    description: "Performs syntax validation on code or configuration files."
  - name: code_quality_analysis
    description: "Runs a pipeline of quality checks (e.g., formatting, linting)."
  - name: test_execution
    description: "Executes automated tests (e.g., pytest) and reports results."

  # --- LLM & Agent Orchestration ---
  - name: llm_orchestration
    description: "Manages the flow of requests and plans to one or more LLMs."
  - name: prompt_interpretation
    description: "Processes and enriches prompts with context before sending them to an LLM."
  - name: code_generation
    description: "Specifically handles the generation of new source code."
  - name: self_correction
    description: "Attempts to automatically fix errors based on validation or test feedback."

  # --- Constitutional Auditor Checks (discoverable micro-capabilities) ---
  - name: audit.check.required_files
    description: "Auditor check: Verifies the existence of critical .intent files."
  - name: audit.check.syntax
    description: "Auditor check: Validates the syntax of all .intent YAML/JSON files."
  - name: audit.check.project_manifest
    description: "Auditor check: Validates the integrity of project_manifest.yaml."
  - name: audit.check.capability_coverage
    description: "Auditor check: Ensures all required capabilities are implemented."
  - name: audit.check.capability_definitions
    description: "Auditor check: Ensures all implemented capabilities are defined in this file."
  - name: audit.check.knowledge_graph_schema
    description: "Auditor check: Validates all knowledge graph symbols against the schema."
  - name: audit.check.domain_integrity
    description: "Auditor check: Checks for domain mismatches and illegal imports."
  - name: audit.check.docstrings
    description: "Auditor check: Finds symbols missing docstrings or having generic intents."
  - name: audit.check.dead_code
    description: "Auditor check: Detects unreferenced public symbols."
  - name: audit.check.orphaned_intent_files
    description: "Auditor check: Finds .intent files that are not referenced in meta.yaml."
  - name: audit.check.environment
    description: "Auditor check: Verifies that required environment variables are set."
  - name: audit.check.proposals_schema
    description: "Auditor check: Validates each proposal against its JSON schema."
  - name: audit.check.proposals_drift
    description: "Auditor check: Detects if a proposal's content has changed after being signed."
  - name: audit.check.proposals_list
    description: "Auditor check: Lists all pending proposals for visibility during an audit."
  # --- THIS IS THE NEW CAPABILITY ---
  - name: audit.check.duplication
    description: "Auditor check: Finds structurally identical code, violating the 'dry_by_design' principle."

  # --- Planned or Placeholder Capabilities ---
  - name: add_missing_docstrings
    description: "A planned capability to automatically add docstrings to undocumented code."

--- END OF FILE ./.intent/knowledge/capability_tags.yaml ---

--- START OF FILE ./.intent/knowledge/entry_point_patterns.yaml ---
# .intent/knowledge/entry_point_patterns.yaml
#
# A declarative set of rules for the KnowledgeGraphBuilder to identify valid
# system entry points that are not discoverable through simple call-graph analysis.
# This prevents the auditor from incorrectly flagging valid code as "dead."

patterns:
  - name: "python_magic_method"
    description: "Standard Python __dunder__ methods are entry points called by the interpreter."
    match:
      type: "function"
      name_regex: "^__.+__$"
    entry_point_type: "magic_method"

  - name: "ast_visitor_method"
    description: "Methods in ast.NodeVisitor subclasses starting with 'visit_' are entry points for the visitor pattern."
    match:
      type: "function"
      name_regex: "^visit_"
      # This requires the builder to know the base classes of a symbol.
      base_class_includes: "NodeVisitor"
    entry_point_type: "visitor_method"

  - name: "capability_implementation"
    description: "Any symbol tagged with a # CAPABILITY is a primary entry point for the CORE system's reasoning loop."
    match:
      # This will be matched based on the 'capability' field in the symbol data.
      has_capability_tag: true
    entry_point_type: "capability"

  - name: "framework_base_class"
    description: "Classes that other components inherit from are valid entry points."
    match:
      type: "class"
      is_base_class: true # This will be true if any other class inherits from it.
    entry_point_type: "base_class"

  - name: "pydantic_model"
    description: "Pydantic models are data structures, not callable code, and are valid entry points."
    match:
      type: "class"
      base_class_includes: "BaseModel"
    entry_point_type: "data_model"

  - name: "enum_definition"
    description: "Enum classes are data structures, not callable code, and are valid entry points."
    match:
      type: "class"
      base_class_includes: "Enum"
    entry_point_type: "enum"

  - name: "dataclass_definition"
    description: "Dataclasses are data structures, not callable code, and are valid entry points."
    match:
      type: "class"
      # The builder checks for the @dataclass decorator. This is a conceptual rule.
      # A simple way to implement is to check for __post_init__ or other dataclass markers if available.
      # For now, we will rely on Pydantic and Enum, which covers our current warnings.
      # We can add a more specific `is_dataclass` check to the builder if needed.
    entry_point_type: "data_model"
--- END OF FILE ./.intent/knowledge/entry_point_patterns.yaml ---

--- START OF FILE ./.intent/knowledge/file_handlers.yaml ---
handlers:
  - type: python
    extensions: [".py"]
    parse_as: ast
    editable: true
    description: Python source code with manifest-enforced governance

  - type: markdown
    extensions: [".md"]
    parse_as: text
    editable: true
    description: Human-readable docs. Require manual review in sensitive areas.

  - type: yaml
    extensions: [".yaml", ".yml"]
    parse_as: structured
    editable: true
    description: Configuration, policies, intent declarations

  - type: json
    extensions: [".json"]
    parse_as: structured
    editable: true
    description: Machine-readable manifests and graphs

  - type: binary
    extensions: [".png", ".jpg", ".pdf"]
    parse_as: none
    editable: false
    description: Visual artifacts â€” viewable only

--- END OF FILE ./.intent/knowledge/file_handlers.yaml ---

--- START OF FILE ./.intent/knowledge/knowledge_graph.json ---
{
  "schema_version": "2.0.0",
  "metadata": {
    "files_scanned": 49,
    "total_symbols": 234,
    "timestamp_utc": "2025-08-11T16:43:39.083403+00:00"
  },
  "symbols": {
    "src/core/clients.py::BaseLLMClient": {
      "key": "src/core/clients.py::BaseLLMClient",
      "name": "BaseLLMClient",
      "type": "ClassDef",
      "file": "src/core/clients.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Base class for LLM clients, handling common request logic for Chat APIs.",
      "docstring": "Base class for LLM clients, handling common request logic for Chat APIs.\nProvides shared initialization and error handling for all LLM clients.",
      "calls": [
        "AsyncClient",
        "ValueError",
        "debug",
        "endswith",
        "error",
        "json",
        "post",
        "raise_for_status",
        "rstrip"
      ],
      "line_number": 18,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "base_class",
      "last_updated": "2025-08-11T16:43:38.564430+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "framework_base_class",
      "structural_hash": "6ebc9f3fe58beda2f728125c9c709c18659ffe22b472f12b4bbda935855521cc"
    },
    "src/core/clients.py::__init__": {
      "key": "src/core/clients.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/core/clients.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Initialize the LLM client with API URL, key, and model name, setting up headers and async client.",
      "docstring": "Initialize the LLM client with API URL, key, and model name, setting up headers and async client.",
      "calls": [
        "__init__",
        "info",
        "super"
      ],
      "line_number": 111,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.573942+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/core/clients.py::GeneratorClient",
      "structural_hash": "f263dd3b032d716198f799d5f7c7418c4ca46c174978f0faa35da0d1ac898ce1"
    },
    "src/core/clients.py::make_request": {
      "key": "src/core/clients.py::make_request",
      "name": "make_request",
      "type": "FunctionDef",
      "file": "src/core/clients.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Sends a prompt to the configured Chat Completions API. (Synchronous)",
      "docstring": "Sends a prompt to the configured Chat Completions API. (Synchronous)",
      "calls": [
        "debug",
        "error",
        "json",
        "post",
        "raise_for_status"
      ],
      "line_number": 47,
      "is_async": false,
      "parameters": [
        "self",
        "prompt",
        "user_id"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.567874+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/clients.py::BaseLLMClient",
      "structural_hash": "db6a3c19bcc087cf1580cbd8a5a5e5d14fceb57641eeb5f27f3c0a8963240dd9"
    },
    "src/core/clients.py::make_request_async": {
      "key": "src/core/clients.py::make_request_async",
      "name": "make_request_async",
      "type": "AsyncFunctionDef",
      "file": "src/core/clients.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Sends a prompt asynchronously to the configured Chat Completions API.",
      "docstring": "Sends a prompt asynchronously to the configured Chat Completions API.",
      "calls": [
        "debug",
        "error",
        "json",
        "post",
        "raise_for_status"
      ],
      "line_number": 70,
      "is_async": true,
      "parameters": [
        "self",
        "prompt",
        "user_id"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.570472+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/clients.py::BaseLLMClient",
      "structural_hash": "98c49e780fa5ae0371d459f230b9b7abeba4316a45fcad7652f3edca27fe83e6"
    },
    "src/core/clients.py::OrchestratorClient": {
      "key": "src/core/clients.py::OrchestratorClient",
      "name": "OrchestratorClient",
      "type": "ClassDef",
      "file": "src/core/clients.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Client for the Orchestrator LLM (e.g., GPT-4, Claude 3).",
      "docstring": "Client for the Orchestrator LLM (e.g., GPT-4, Claude 3).\nResponsible for high-level planning and intent interpretation.",
      "calls": [
        "__init__",
        "info",
        "super"
      ],
      "line_number": 93,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.571587+00:00",
      "is_class": true,
      "base_classes": [
        "BaseLLMClient"
      ],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "29a94da6ed639ce032676e4ee04047d9675813359a676e98e1eb463ebcd25e3c"
    },
    "src/core/clients.py::GeneratorClient": {
      "key": "src/core/clients.py::GeneratorClient",
      "name": "GeneratorClient",
      "type": "ClassDef",
      "file": "src/core/clients.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Client for the Generator LLM (e.g., a specialized coding model).",
      "docstring": "Client for the Generator LLM (e.g., a specialized coding model).\nResponsible for code generation and detailed implementation.",
      "calls": [
        "__init__",
        "info",
        "super"
      ],
      "line_number": 106,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.573202+00:00",
      "is_class": true,
      "base_classes": [
        "BaseLLMClient"
      ],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "2a43d65ec498bbafe6314a942478e57e49694578e3fb86d29ad6cc447705b60d"
    },
    "src/core/validation_pipeline.py::_load_safety_policies": {
      "key": "src/core/validation_pipeline.py::_load_safety_policies",
      "name": "_load_safety_policies",
      "type": "FunctionDef",
      "file": "src/core/validation_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Loads and caches the safety policies from the .intent directory.",
      "docstring": "Loads and caches the safety policies from the .intent directory.",
      "calls": [
        "get",
        "get_repo_root",
        "load_config"
      ],
      "line_number": 28,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.578095+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "0736888d998ff773d207c79a3b34f0926bc9bb9439474d5a3d7b146ab0fec7ad"
    },
    "src/core/validation_pipeline.py::_get_full_attribute_name": {
      "key": "src/core/validation_pipeline.py::_get_full_attribute_name",
      "name": "_get_full_attribute_name",
      "type": "FunctionDef",
      "file": "src/core/validation_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Recursively builds the full name of an attribute call (e.g., 'os.path.join').",
      "docstring": "Recursively builds the full name of an attribute call (e.g., 'os.path.join').",
      "calls": [
        "insert",
        "isinstance",
        "join"
      ],
      "line_number": 38,
      "is_async": false,
      "parameters": [
        "node"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.579104+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "5f5a1af65af3fe0d7367a16ecf13b95c818fbbe4ba21999798c8de70809235d8"
    },
    "src/core/validation_pipeline.py::_find_dangerous_patterns": {
      "key": "src/core/validation_pipeline.py::_find_dangerous_patterns",
      "name": "_find_dangerous_patterns",
      "type": "FunctionDef",
      "file": "src/core/validation_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Scans the AST for calls and imports forbidden by safety policies.",
      "docstring": "Scans the AST for calls and imports forbidden by safety policies.",
      "calls": [
        "Path",
        "_get_full_attribute_name",
        "_load_safety_policies",
        "any",
        "append",
        "get",
        "isinstance",
        "match",
        "replace",
        "set",
        "split",
        "update",
        "walk"
      ],
      "line_number": 49,
      "is_async": false,
      "parameters": [
        "tree",
        "file_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.583334+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "7b41b31db2cbf4832239a322f7b6601cf8d3e38ab3dd27a56c127241904b8ec2"
    },
    "src/core/validation_pipeline.py::_check_for_todo_comments": {
      "key": "src/core/validation_pipeline.py::_check_for_todo_comments",
      "name": "_check_for_todo_comments",
      "type": "FunctionDef",
      "file": "src/core/validation_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Scans source code for TODO/FIXME comments and returns them as violations.",
      "docstring": "Scans source code for TODO/FIXME comments and returns them as violations.",
      "calls": [
        "append",
        "enumerate",
        "split",
        "splitlines",
        "strip"
      ],
      "line_number": 103,
      "is_async": false,
      "parameters": [
        "code"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.585066+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "34c7c62a42f9186c3149022eb6eaf6f9d5b4ac93b4ec1bfd497d53b8683712ea"
    },
    "src/core/validation_pipeline.py::_check_semantics": {
      "key": "src/core/validation_pipeline.py::_check_semantics",
      "name": "_check_semantics",
      "type": "FunctionDef",
      "file": "src/core/validation_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "semantic_validation",
      "intent": "Runs all policy-aware semantic checks on a string of Python code.",
      "docstring": "Runs all policy-aware semantic checks on a string of Python code.",
      "calls": [
        "_find_dangerous_patterns",
        "parse"
      ],
      "line_number": 119,
      "is_async": false,
      "parameters": [
        "code",
        "file_path"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.585870+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "ece39488d6b3609566d4f20ff02c786df62f4ff875186071eec67a74bf757fde"
    },
    "src/core/validation_pipeline.py::_validate_python_code": {
      "key": "src/core/validation_pipeline.py::_validate_python_code",
      "name": "_validate_python_code",
      "type": "FunctionDef",
      "file": "src/core/validation_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Internal pipeline for Python code validation.",
      "docstring": "Internal pipeline for Python code validation.\nReturns the final code and a list of all found violations.",
      "calls": [
        "_check_for_todo_comments",
        "_check_semantics",
        "any",
        "append",
        "check_syntax",
        "extend",
        "fix_and_lint_code_with_ruff",
        "format_code_with_black",
        "str"
      ],
      "line_number": 128,
      "is_async": false,
      "parameters": [
        "path_hint",
        "code"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.587480+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "d073d1a6569d001b85857fece67ef49fa49a8338aeec6fa70f65e07827671ffd"
    },
    "src/core/validation_pipeline.py::_validate_yaml": {
      "key": "src/core/validation_pipeline.py::_validate_yaml",
      "name": "_validate_yaml",
      "type": "FunctionDef",
      "file": "src/core/validation_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Internal pipeline for YAML validation.",
      "docstring": "Internal pipeline for YAML validation.",
      "calls": [
        "append",
        "safe_load"
      ],
      "line_number": 161,
      "is_async": false,
      "parameters": [
        "code"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.588703+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "d898124cd4f9896ce3ac4882fef8bf678ddc03a78cba380d7d49d8d4c9436174"
    },
    "src/core/validation_pipeline.py::_get_file_classification": {
      "key": "src/core/validation_pipeline.py::_get_file_classification",
      "name": "_get_file_classification",
      "type": "FunctionDef",
      "file": "src/core/validation_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Determines the file type based on its extension.",
      "docstring": "Determines the file type based on its extension.",
      "calls": [
        "Path",
        "lower"
      ],
      "line_number": 175,
      "is_async": false,
      "parameters": [
        "file_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.589674+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "e19906999900626b56fdd8f4f99e738f7eba4902463206d542d1b14e594f3eee"
    },
    "src/core/validation_pipeline.py::validate_code": {
      "key": "src/core/validation_pipeline.py::validate_code",
      "name": "validate_code",
      "type": "FunctionDef",
      "file": "src/core/validation_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "code_quality_analysis",
      "intent": "Validate a file's code by routing it to the appropriate validation pipeline based on its file type, returning a standardized dictionary with status, violations, and processed code.",
      "docstring": "Validate a file's code by routing it to the appropriate validation pipeline based on its file type, returning a standardized dictionary with status, violations, and processed code.",
      "calls": [
        "_get_file_classification",
        "_validate_python_code",
        "_validate_yaml",
        "any",
        "debug",
        "get"
      ],
      "line_number": 184,
      "is_async": false,
      "parameters": [
        "file_path",
        "code",
        "quiet"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.591356+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "c08839228c63a9b5971aae49298220bfa2a5722b52ec4bc0ed6520cf99121584"
    },
    "src/core/git_service.py::GitService": {
      "key": "src/core/git_service.py::GitService",
      "name": "GitService",
      "type": "ClassDef",
      "file": "src/core/git_service.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Encapsulates Git operations for the CORE system.",
      "docstring": "Encapsulates Git operations for the CORE system.\nEnsures all file changes are committed with traceable messages.",
      "calls": [
        "Path",
        "RuntimeError",
        "ValueError",
        "_run_command",
        "debug",
        "error",
        "info",
        "is_dir",
        "is_git_repo",
        "join",
        "lower",
        "resolve",
        "run",
        "str",
        "strip",
        "warning"
      ],
      "line_number": 22,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.596842+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "354d269bf8dce8b7fd8702e7c1829f9d6a2b70abb51de4edc9437b1f8950ba79"
    },
    "src/core/git_service.py::__init__": {
      "key": "src/core/git_service.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/core/git_service.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Initialize GitService with the resolved absolute path to the Git repository; raises ValueError if path is not a valid Git repo.",
      "docstring": "Initialize GitService with the resolved absolute path to the Git repository; raises ValueError if path is not a valid Git repo.",
      "calls": [
        "Path",
        "ValueError",
        "info",
        "is_git_repo",
        "resolve"
      ],
      "line_number": 28,
      "is_async": false,
      "parameters": [
        "self",
        "repo_path"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.597772+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/core/git_service.py::GitService",
      "structural_hash": "ef32ae74dfb49ea27846ae880fe6519f7cb0c1d10c79cbbbcc36f23a1e88171d"
    },
    "src/core/git_service.py::_run_command": {
      "key": "src/core/git_service.py::_run_command",
      "name": "_run_command",
      "type": "FunctionDef",
      "file": "src/core/git_service.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "change_safety_enforcement",
      "intent": "Run a Git command and return stdout.",
      "docstring": "Run a Git command and return stdout.\n\nArgs:\n    command (list): Git command as a list (e.g., ['git', 'status']).\n\nReturns:\n    str: Command output, or raises RuntimeError on failure.",
      "calls": [
        "RuntimeError",
        "debug",
        "error",
        "join",
        "run",
        "strip"
      ],
      "line_number": 36,
      "is_async": false,
      "parameters": [
        "self",
        "command"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.599011+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/core/git_service.py::GitService",
      "structural_hash": "e2cb9747f42458c7cb0017ff424868af56a19a25b6ca2a5e514aab86387e73d2"
    },
    "src/core/git_service.py::add": {
      "key": "src/core/git_service.py::add",
      "name": "add",
      "type": "FunctionDef",
      "file": "src/core/git_service.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Stage a file or directory for commit.",
      "docstring": "Stage a file or directory for commit.\n\nArgs:\n    file_path (str): Path to stage. Defaults to '.' (all changes).",
      "calls": [
        "ValueError",
        "_run_command",
        "resolve"
      ],
      "line_number": 56,
      "is_async": false,
      "parameters": [
        "self",
        "file_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.600129+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/git_service.py::GitService",
      "structural_hash": "4324b2b36783dbddd72723e1459732a9a493140953a791673ab12cd70e75d480"
    },
    "src/core/git_service.py::commit": {
      "key": "src/core/git_service.py::commit",
      "name": "commit",
      "type": "FunctionDef",
      "file": "src/core/git_service.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Commit staged changes with a message.",
      "docstring": "Commit staged changes with a message.\nIf there are no changes to commit, this operation is a no-op and will not raise an error.\n\nArgs:\n    message (str): Commit message explaining the change.",
      "calls": [
        "_run_command",
        "info",
        "lower",
        "str"
      ],
      "line_number": 68,
      "is_async": false,
      "parameters": [
        "self",
        "message"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.601414+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/git_service.py::GitService",
      "structural_hash": "5ea6f473bf196feb192fc8cfb2f8cdc3d502247ab8abb250de852f643bf5923d"
    },
    "src/core/git_service.py::is_git_repo": {
      "key": "src/core/git_service.py::is_git_repo",
      "name": "is_git_repo",
      "type": "FunctionDef",
      "file": "src/core/git_service.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Check if the configured path is a valid Git repository.",
      "docstring": "Check if the configured path is a valid Git repository.\n\nReturns:\n    bool: True if it's a Git repo, False otherwise.",
      "calls": [
        "is_dir"
      ],
      "line_number": 96,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.602115+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/git_service.py::GitService",
      "structural_hash": "bb78f9e2f12bb6726d8f52e1699af9136d078d50eb79b14dff9abfdaffdf8880"
    },
    "src/core/git_service.py::get_current_commit": {
      "key": "src/core/git_service.py::get_current_commit",
      "name": "get_current_commit",
      "type": "FunctionDef",
      "file": "src/core/git_service.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Gets the full SHA hash of the current commit (HEAD).",
      "docstring": "Gets the full SHA hash of the current commit (HEAD).",
      "calls": [
        "_run_command"
      ],
      "line_number": 106,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.602722+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/git_service.py::GitService",
      "structural_hash": "652a9292b257d0b11a698f85a9de723470f646f308259fbfcf0fbf91d571ac4a"
    },
    "src/core/git_service.py::reset_to_commit": {
      "key": "src/core/git_service.py::reset_to_commit",
      "name": "reset_to_commit",
      "type": "FunctionDef",
      "file": "src/core/git_service.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Performs a hard reset to a specific commit hash.",
      "docstring": "Performs a hard reset to a specific commit hash.\nThis will discard all current changes.",
      "calls": [
        "_run_command",
        "info",
        "warning"
      ],
      "line_number": 112,
      "is_async": false,
      "parameters": [
        "self",
        "commit_hash"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.603654+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/git_service.py::GitService",
      "structural_hash": "8bec1d271f714ef832c3138d5031875d3c156d2c8cc02261ecaa4e76506639d3"
    },
    "src/core/syntax_checker.py::check_syntax": {
      "key": "src/core/syntax_checker.py::check_syntax",
      "name": "check_syntax",
      "type": "FunctionDef",
      "file": "src/core/syntax_checker.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "syntax_validation",
      "intent": "Checks the given Python code for syntax errors and returns a list of violations, if any.",
      "docstring": "Checks the given Python code for syntax errors and returns a list of violations, if any.",
      "calls": [
        "endswith",
        "parse",
        "strip"
      ],
      "line_number": 13,
      "is_async": false,
      "parameters": [
        "file_path",
        "code"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.605389+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "7333fe5bb50162c650fd9ca4c22379352d1fb0441833d9c02317b2094363b44b"
    },
    "src/core/black_formatter.py::format_code_with_black": {
      "key": "src/core/black_formatter.py::format_code_with_black",
      "name": "format_code_with_black",
      "type": "FunctionDef",
      "file": "src/core/black_formatter.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Formats the given Python code using Black, raising `black.InvalidInput` for syntax errors or `Exception` for other formatting issues.",
      "docstring": "Formats the given Python code using Black, raising `black.InvalidInput` for syntax errors or `Exception` for other formatting issues.",
      "calls": [
        "Exception",
        "FileMode",
        "InvalidInput",
        "format_str"
      ],
      "line_number": 9,
      "is_async": false,
      "parameters": [
        "code"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.606845+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "15e647987d94914b55a1897be6e1c47df7f6840ebeaca3408e2719752a9e9fa1"
    },
    "src/core/test_runner.py::run_tests": {
      "key": "src/core/test_runner.py::run_tests",
      "name": "run_tests",
      "type": "FunctionDef",
      "file": "src/core/test_runner.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "test_execution",
      "intent": "Executes pytest on the tests/ directory, capturing stdout, stderr, exit code, and a summary, returning results as a structured dict.",
      "docstring": "Executes pytest on the tests/ directory, capturing stdout, stderr, exit code, and a summary, returning results as a structured dict.",
      "calls": [
        "Path",
        "_log_test_result",
        "_store_failure_if_any",
        "_summarize",
        "error",
        "getenv",
        "info",
        "int",
        "isoformat",
        "resolve",
        "run",
        "str",
        "strip",
        "utcnow",
        "warning"
      ],
      "line_number": 23,
      "is_async": false,
      "parameters": [
        "silent"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.612066+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "d3a32185efaa1749bc3b105509e7af1baadb1f4340c3efb532da6ebf67a86c79"
    },
    "src/core/test_runner.py::_summarize": {
      "key": "src/core/test_runner.py::_summarize",
      "name": "_summarize",
      "type": "FunctionDef",
      "file": "src/core/test_runner.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Error: Could not connect to LLM endpoint. Details: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out. (read timeout=180)",
      "docstring": "Error: Could not connect to LLM endpoint. Details: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out. (read timeout=180)",
      "calls": [
        "reversed",
        "splitlines",
        "strip"
      ],
      "line_number": 86,
      "is_async": false,
      "parameters": [
        "output"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.613439+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "8c0d11c470b2f2e028e8c955ab9369e0e41946c7fbd2c243fdf0140337c2ae84"
    },
    "src/core/test_runner.py::_log_test_result": {
      "key": "src/core/test_runner.py::_log_test_result",
      "name": "_log_test_result",
      "type": "FunctionDef",
      "file": "src/core/test_runner.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Appends a JSON record of a test run to the persistent log file.",
      "docstring": "Appends a JSON record of a test run to the persistent log file.",
      "calls": [
        "dumps",
        "open",
        "warning",
        "write"
      ],
      "line_number": 95,
      "is_async": false,
      "parameters": [
        "data"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.614461+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "83804f96879e56dce3b4fbe29258c654f55dd95d39ad71b6fe27178263fcebf9"
    },
    "src/core/test_runner.py::_store_failure_if_any": {
      "key": "src/core/test_runner.py::_store_failure_if_any",
      "name": "_store_failure_if_any",
      "type": "FunctionDef",
      "file": "src/core/test_runner.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Saves the details of a failed test run to a dedicated file for easy access.",
      "docstring": "Saves the details of a failed test run to a dedicated file for easy access.",
      "calls": [
        "dump",
        "exists",
        "get",
        "open",
        "remove",
        "warning"
      ],
      "line_number": 104,
      "is_async": false,
      "parameters": [
        "data"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.615779+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "95d784ba1e5795ba6547f6e35c2c45e51d373ff17ad289cd0bd53e773c7ed76b"
    },
    "src/core/main.py::lifespan": {
      "key": "src/core/main.py::lifespan",
      "name": "lifespan",
      "type": "AsyncFunctionDef",
      "file": "src/core/main.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "FastAPI lifespan handler \u2014 runs startup and shutdown logic.",
      "docstring": "FastAPI lifespan handler \u2014 runs startup and shutdown logic.",
      "calls": [
        "GeneratorClient",
        "GitService",
        "IntentGuard",
        "OrchestratorClient",
        "Path",
        "info",
        "introspection",
        "warning"
      ],
      "line_number": 33,
      "is_async": true,
      "parameters": [
        "app"
      ],
      "entry_point_type": "context_manager",
      "last_updated": "2025-08-11T16:43:38.618479+00:00",
      "is_class": false,
      "base_classes": [],
      "structural_hash": "e9b68e117b6519ebf97416f533f3cb98765c0a714753f184f70b9a282a78b366"
    },
    "src/core/main.py::GoalRequest": {
      "key": "src/core/main.py::GoalRequest",
      "name": "GoalRequest",
      "type": "ClassDef",
      "file": "src/core/main.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Defines the request body for the /execute_goal endpoint.",
      "docstring": "Defines the request body for the /execute_goal endpoint.",
      "calls": [],
      "line_number": 55,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.619073+00:00",
      "is_class": true,
      "base_classes": [
        "BaseModel"
      ],
      "entry_point_justification": "pydantic_model",
      "structural_hash": "08ef688f6285fa8d432bd6d7a0a1a10fb5383d96f9810676542b6e060d519a13"
    },
    "src/core/main.py::execute_goal": {
      "key": "src/core/main.py::execute_goal",
      "name": "execute_goal",
      "type": "AsyncFunctionDef",
      "file": "src/core/main.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Execute a high-level goal by planning and generating code.",
      "docstring": "Execute a high-level goal by planning and generating code.",
      "calls": [
        "FileHandler",
        "HTTPException",
        "JSONResponse",
        "PlannerAgent",
        "error",
        "execute_plan",
        "info",
        "post",
        "str"
      ],
      "line_number": 60,
      "is_async": true,
      "parameters": [
        "request_data",
        "request"
      ],
      "entry_point_type": "fastapi_route_post",
      "last_updated": "2025-08-11T16:43:38.621100+00:00",
      "is_class": false,
      "base_classes": [],
      "structural_hash": "d40b3c03d802ecadf894643e5b8975f2648c0e2738264a0ab56def172cd205a5"
    },
    "src/core/main.py::root": {
      "key": "src/core/main.py::root",
      "name": "root",
      "type": "AsyncFunctionDef",
      "file": "src/core/main.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Root endpoint \u2014 returns system status.",
      "docstring": "Root endpoint \u2014 returns system status.",
      "calls": [
        "get"
      ],
      "line_number": 95,
      "is_async": true,
      "parameters": [],
      "entry_point_type": "fastapi_route_get",
      "last_updated": "2025-08-11T16:43:38.621873+00:00",
      "is_class": false,
      "base_classes": [],
      "structural_hash": "2d3272d333883f1d08138369625bda6d9ac3c0f2a3d7f83921bfaf0cc31faa25"
    },
    "src/core/prompt_pipeline.py::PromptPipeline": {
      "key": "src/core/prompt_pipeline.py::PromptPipeline",
      "name": "PromptPipeline",
      "type": "ClassDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Processes and enriches user prompts by resolving directives like [[include:...]] and [[analysis:...]].",
      "docstring": "Processes and enriches user prompts by resolving directives like [[include:...]] and [[analysis:...]].\nEnsures the LLM receives full context before generating code.",
      "calls": [
        "Path",
        "_inject_analysis",
        "_inject_context",
        "_inject_includes",
        "_inject_manifest",
        "compile",
        "dump",
        "exists",
        "get",
        "group",
        "is_file",
        "isinstance",
        "read_text",
        "resolve",
        "safe_load",
        "split",
        "stat",
        "str",
        "strip",
        "sub"
      ],
      "line_number": 23,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.629854+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "08c6c20fe4e1adac3fbff62081bc117b96e3101be4a1f6518898f7f54f21eb90"
    },
    "src/core/prompt_pipeline.py::__init__": {
      "key": "src/core/prompt_pipeline.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Initialize PromptPipeline with repository root.",
      "docstring": "Initialize PromptPipeline with repository root.\n\nArgs:\n    repo_path (Path): Root path of the repository.",
      "calls": [
        "Path",
        "compile",
        "resolve"
      ],
      "line_number": 29,
      "is_async": false,
      "parameters": [
        "self",
        "repo_path"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.630728+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "1b22cb99e2b51b97bb2a3791bf287bc677d7be076f9fee4c1185b5726b37add2"
    },
    "src/core/prompt_pipeline.py::_replace_context_match": {
      "key": "src/core/prompt_pipeline.py::_replace_context_match",
      "name": "_replace_context_match",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Dynamically replaces a [[context:...]] regex match with file content or an error message if the file is missing, unreadable, or exceeds size limits.",
      "docstring": "Dynamically replaces a [[context:...]] regex match with file content or an error message if the file is missing, unreadable, or exceeds size limits.",
      "calls": [
        "exists",
        "group",
        "is_file",
        "read_text",
        "stat",
        "str",
        "strip"
      ],
      "line_number": 44,
      "is_async": false,
      "parameters": [
        "self",
        "match"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.632318+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "122b32086c2192f7026b3ab0c51b65bf63920a9bdbe09b92bf96fa15c148f211"
    },
    "src/core/prompt_pipeline.py::_inject_context": {
      "key": "src/core/prompt_pipeline.py::_inject_context",
      "name": "_inject_context",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Replaces [[context:file.py]] directives with actual file content.",
      "docstring": "Replaces [[context:file.py]] directives with actual file content.",
      "calls": [
        "sub"
      ],
      "line_number": 59,
      "is_async": false,
      "parameters": [
        "self",
        "prompt"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.632988+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "9dec30584551481830534026f4fdf8bc575f62facc5cd0ced5edf03b7f3c13e0"
    },
    "src/core/prompt_pipeline.py::_replace_include_match": {
      "key": "src/core/prompt_pipeline.py::_replace_include_match",
      "name": "_replace_include_match",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Dynamically replaces an [[include:...]] regex match with file content or an error message.",
      "docstring": "Dynamically replaces an [[include:...]] regex match with file content or an error message.",
      "calls": [
        "exists",
        "group",
        "is_file",
        "read_text",
        "stat",
        "str",
        "strip"
      ],
      "line_number": 64,
      "is_async": false,
      "parameters": [
        "self",
        "match"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.634497+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "84acb1cf0d90f434564302921b718ac1c44a7c77a0115343f19f93ca6ce136d7"
    },
    "src/core/prompt_pipeline.py::_inject_includes": {
      "key": "src/core/prompt_pipeline.py::_inject_includes",
      "name": "_inject_includes",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Replaces [[include:file.py]] directives with file content.",
      "docstring": "Replaces [[include:file.py]] directives with file content.",
      "calls": [
        "sub"
      ],
      "line_number": 78,
      "is_async": false,
      "parameters": [
        "self",
        "prompt"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.635162+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "1d837d4a2171fc0769a798a0805570b2a8bfe53f803d4ff04ccdbf159e85de71"
    },
    "src/core/prompt_pipeline.py::_replace_analysis_match": {
      "key": "src/core/prompt_pipeline.py::_replace_analysis_match",
      "name": "_replace_analysis_match",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Dynamically replaces an [[analysis:...]] regex match with a placeholder analysis message for the given file path.",
      "docstring": "Dynamically replaces an [[analysis:...]] regex match with a placeholder analysis message for the given file path.",
      "calls": [
        "group",
        "strip"
      ],
      "line_number": 82,
      "is_async": false,
      "parameters": [
        "self",
        "match"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.635904+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "52f7dcc034d6c1182006fc20f0428fd0881ef029aea36cb6f681b940b53716d9"
    },
    "src/core/prompt_pipeline.py::_inject_analysis": {
      "key": "src/core/prompt_pipeline.py::_inject_analysis",
      "name": "_inject_analysis",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Replaces [[analysis:file.py]] directives with code analysis.",
      "docstring": "Replaces [[analysis:file.py]] directives with code analysis.",
      "calls": [
        "sub"
      ],
      "line_number": 89,
      "is_async": false,
      "parameters": [
        "self",
        "prompt"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.636479+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "5f1731cbe73a16bb16a2d776db8805687765bf160e638d9446d52c06178e59eb"
    },
    "src/core/prompt_pipeline.py::_replace_manifest_match": {
      "key": "src/core/prompt_pipeline.py::_replace_manifest_match",
      "name": "_replace_manifest_match",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Dynamically replaces a [[manifest:...]] regex match with manifest data or an error.",
      "docstring": "Dynamically replaces a [[manifest:...]] regex match with manifest data or an error.",
      "calls": [
        "dump",
        "exists",
        "get",
        "group",
        "isinstance",
        "read_text",
        "safe_load",
        "split",
        "str",
        "strip"
      ],
      "line_number": 93,
      "is_async": false,
      "parameters": [
        "self",
        "match"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.638514+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "47ea4a80ad4c399fcb390b78921c240be52e076b8d3c958e4db2cbea6a76f5f0"
    },
    "src/core/prompt_pipeline.py::_inject_manifest": {
      "key": "src/core/prompt_pipeline.py::_inject_manifest",
      "name": "_inject_manifest",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Replaces [[manifest:field]] directives with data from project_manifest.yaml.",
      "docstring": "Replaces [[manifest:field]] directives with data from project_manifest.yaml.",
      "calls": [
        "sub"
      ],
      "line_number": 119,
      "is_async": false,
      "parameters": [
        "self",
        "prompt"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.639286+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "cfb3ae83be6177716797594e17a63015e3f7dae20c8876aa77c3cba7a155a979"
    },
    "src/core/prompt_pipeline.py::process": {
      "key": "src/core/prompt_pipeline.py::process",
      "name": "process",
      "type": "FunctionDef",
      "file": "src/core/prompt_pipeline.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "prompt_interpretation",
      "intent": "Processes the full prompt by sequentially resolving all directives.",
      "docstring": "Processes the full prompt by sequentially resolving all directives.\nThis is the main entry point for prompt enrichment.",
      "calls": [
        "_inject_analysis",
        "_inject_context",
        "_inject_includes",
        "_inject_manifest"
      ],
      "line_number": 124,
      "is_async": false,
      "parameters": [
        "self",
        "prompt"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.640016+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/core/prompt_pipeline.py::PromptPipeline",
      "structural_hash": "321c87aee4a7c73f0b8bf59f79b282006108f7ad11a66127fca9756c1f6a815a"
    },
    "src/core/capabilities.py::introspection": {
      "key": "src/core/capabilities.py::introspection",
      "name": "introspection",
      "type": "FunctionDef",
      "file": "src/core/capabilities.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "introspection",
      "intent": "Runs a full self-analysis cycle to inspect system structure and health.",
      "docstring": "Runs a full self-analysis cycle to inspect system structure and health.\nThis orchestrates the execution of the system's own introspection tools\nas separate, governed processes.",
      "calls": [
        "Path",
        "error",
        "info",
        "print",
        "resolve",
        "run",
        "warning"
      ],
      "line_number": 18,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "cli_entry_point",
      "last_updated": "2025-08-11T16:43:38.643296+00:00",
      "is_class": false,
      "base_classes": [],
      "structural_hash": "22cc87e423515fe4db4d16c291f72fd6fff1decda806507620f29d50bb13bacd"
    },
    "src/core/ruff_linter.py::fix_and_lint_code_with_ruff": {
      "key": "src/core/ruff_linter.py::fix_and_lint_code_with_ruff",
      "name": "fix_and_lint_code_with_ruff",
      "type": "FunctionDef",
      "file": "src/core/ruff_linter.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Fix and lint the provided Python code using Ruff's JSON output format.",
      "docstring": "Fix and lint the provided Python code using Ruff's JSON output format.\n\nArgs:\n    code (str): Source code to fix and lint.\n    display_filename (str): Optional display name for readable error messages.\n\nReturns:\n    A tuple containing:\n    - The potentially fixed code as a string.\n    - A list of structured violation dictionaries for any remaining issues.",
      "calls": [
        "NamedTemporaryFile",
        "append",
        "error",
        "exists",
        "get",
        "loads",
        "open",
        "read",
        "remove",
        "run",
        "write"
      ],
      "line_number": 19,
      "is_async": false,
      "parameters": [
        "code",
        "display_filename"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.647072+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "85cd34ffff340724006d2405af636c30782821272c4fbc068894113c6d855bee"
    },
    "src/core/file_handler.py::FileHandler": {
      "key": "src/core/file_handler.py::FileHandler",
      "name": "FileHandler",
      "type": "ClassDef",
      "file": "src/core/file_handler.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Central class for safe, auditable file operations in CORE.",
      "docstring": "Central class for safe, auditable file operations in CORE.\nAll writes are staged first and require confirmation. Validation is handled\nby the calling agent via the validation_pipeline.",
      "calls": [
        "Lock",
        "Path",
        "ValueError",
        "as_posix",
        "dumps",
        "exists",
        "info",
        "is_dir",
        "is_relative_to",
        "isoformat",
        "mkdir",
        "now",
        "pop",
        "resolve",
        "str",
        "unlink",
        "uuid4",
        "write_text"
      ],
      "line_number": 19,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.652686+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "f8b9c192883d652a5686f048f4a02ed2915a0798f3960a7684b1ddf80ff97383"
    },
    "src/core/file_handler.py::__init__": {
      "key": "src/core/file_handler.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/core/file_handler.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Initialize FileHandler with repository root.",
      "docstring": "Initialize FileHandler with repository root.",
      "calls": [
        "Lock",
        "Path",
        "ValueError",
        "is_dir",
        "mkdir",
        "resolve"
      ],
      "line_number": 26,
      "is_async": false,
      "parameters": [
        "self",
        "repo_path"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.653959+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/core/file_handler.py::FileHandler",
      "structural_hash": "3d94401a42e534ef715742cc343a85d66bed603d62479d4b6a63df2dd77d6e8c"
    },
    "src/core/file_handler.py::add_pending_write": {
      "key": "src/core/file_handler.py::add_pending_write",
      "name": "add_pending_write",
      "type": "FunctionDef",
      "file": "src/core/file_handler.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Stages a pending write operation for later confirmation.",
      "docstring": "Stages a pending write operation for later confirmation.",
      "calls": [
        "Path",
        "as_posix",
        "dumps",
        "isoformat",
        "now",
        "str",
        "uuid4",
        "write_text"
      ],
      "line_number": 49,
      "is_async": false,
      "parameters": [
        "self",
        "prompt",
        "suggested_path",
        "code"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.655290+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/file_handler.py::FileHandler",
      "structural_hash": "97eb206c07126bfc68d638e0ba15f2cc3b335a88c042e3516da7104fb5b47327"
    },
    "src/core/file_handler.py::confirm_write": {
      "key": "src/core/file_handler.py::confirm_write",
      "name": "confirm_write",
      "type": "FunctionDef",
      "file": "src/core/file_handler.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Confirms and applies a pending write to disk. Assumes content has been validated.",
      "docstring": "Confirms and applies a pending write to disk. Assumes content has been validated.",
      "calls": [
        "ValueError",
        "dumps",
        "exists",
        "info",
        "is_relative_to",
        "mkdir",
        "pop",
        "resolve",
        "str",
        "unlink",
        "write_text"
      ],
      "line_number": 70,
      "is_async": false,
      "parameters": [
        "self",
        "pending_id"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.657642+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/file_handler.py::FileHandler",
      "structural_hash": "f2ed9b1571e9ec2b36d9a7e163b7f48f23ea4d312f29c1ef34e922cdecbc8d4f"
    },
    "src/core/intent_model.py::IntentModel": {
      "key": "src/core/intent_model.py::IntentModel",
      "name": "IntentModel",
      "type": "ClassDef",
      "file": "src/core/intent_model.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Loads and provides an queryable interface to the source code structure",
      "docstring": "Loads and provides an queryable interface to the source code structure\ndefined in .intent/knowledge/source_structure.yaml.",
      "calls": [
        "FileNotFoundError",
        "Path",
        "ValueError",
        "_load_structure",
        "exists",
        "get",
        "isinstance",
        "items",
        "len",
        "read_text",
        "resolve",
        "safe_load",
        "sorted"
      ],
      "line_number": 19,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.661908+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "0e6ac01dbeac24d5b49daec6bb2a270c6fa1dd15f2a48c589c32192ba1bbb7fa"
    },
    "src/core/intent_model.py::__init__": {
      "key": "src/core/intent_model.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/core/intent_model.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Initializes the model by loading the source structure definition from the repository, inferring the root if not provided.",
      "docstring": "Initializes the model by loading the source structure definition from the repository, inferring the root if not provided.",
      "calls": [
        "Path",
        "_load_structure",
        "resolve"
      ],
      "line_number": 24,
      "is_async": false,
      "parameters": [
        "self",
        "repo_root"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.662861+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/core/intent_model.py::IntentModel",
      "structural_hash": "0b89ffe6785921e01c1f67095e382aaf8252f0ede85e30e4986e25527f54b0d2"
    },
    "src/core/intent_model.py::_load_structure": {
      "key": "src/core/intent_model.py::_load_structure",
      "name": "_load_structure",
      "type": "FunctionDef",
      "file": "src/core/intent_model.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Load the domain structure from .intent/knowledge/source_structure.yaml.",
      "docstring": "Load the domain structure from .intent/knowledge/source_structure.yaml.\n\nReturns:\n    Dict[str, dict]: Mapping of domain names to metadata (path, permissions, etc.).",
      "calls": [
        "FileNotFoundError",
        "ValueError",
        "exists",
        "isinstance",
        "read_text",
        "safe_load"
      ],
      "line_number": 37,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.664106+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/intent_model.py::IntentModel",
      "structural_hash": "e118f377f2c68ad9f65d680acb9b56d5d65d6ad80dde399e6b65a846c4baf2ff"
    },
    "src/core/intent_model.py::resolve_domain_for_path": {
      "key": "src/core/intent_model.py::resolve_domain_for_path",
      "name": "resolve_domain_for_path",
      "type": "FunctionDef",
      "file": "src/core/intent_model.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Given an absolute or relative path, determine which domain it belongs to.",
      "docstring": "Given an absolute or relative path, determine which domain it belongs to.\nPrefers deeper (more specific) paths over shorter ones.",
      "calls": [
        "items",
        "len",
        "resolve",
        "sorted"
      ],
      "line_number": 56,
      "is_async": false,
      "parameters": [
        "self",
        "file_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.665412+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/intent_model.py::IntentModel",
      "structural_hash": "1d8344850ddc5468f935a54669d5b428c5164d0cfec57ecbe7bb7ba72596151a"
    },
    "src/core/intent_model.py::get_domain_permissions": {
      "key": "src/core/intent_model.py::get_domain_permissions",
      "name": "get_domain_permissions",
      "type": "FunctionDef",
      "file": "src/core/intent_model.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "unassigned",
      "intent": "Return a list of allowed domains that the given domain can import from.",
      "docstring": "Return a list of allowed domains that the given domain can import from.\n\nArgs:\n    domain (str): The domain to query.\n\nReturns:\n    List[str]: List of allowed domain names, or empty list if not defined.",
      "calls": [
        "get",
        "isinstance"
      ],
      "line_number": 77,
      "is_async": false,
      "parameters": [
        "self",
        "domain"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.666300+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/intent_model.py::IntentModel",
      "structural_hash": "769b64c61696c6d075db8e62123853fe9b5c18e3b1fa7f8d0c7c0680ccea056d"
    },
    "src/core/intent_guard.py::IntentGuard": {
      "key": "src/core/intent_guard.py::IntentGuard",
      "name": "IntentGuard",
      "type": "ClassDef",
      "file": "src/core/intent_guard.py",
      "domain": "core",
      "agent": "validator_agent",
      "capability": "intent_guarding",
      "intent": "Central enforcement engine for CORE's safety and governance policies.",
      "docstring": "Central enforcement engine for CORE's safety and governance policies.\nEnsures all proposed file changes comply with declared rules and classifications.",
      "calls": [
        "Path",
        "_load_policies",
        "_load_source_manifest",
        "append",
        "exists",
        "extend",
        "get",
        "glob",
        "info",
        "is_dir",
        "isinstance",
        "len",
        "list",
        "load_config",
        "loads",
        "read_text",
        "resolve",
        "sorted",
        "values"
      ],
      "line_number": 19,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.670927+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "c103b2ac0f63e2775e30f626aee54ea5558ad0f8226ba9c40cb329277244b70f"
    },
    "src/core/intent_guard.py::__init__": {
      "key": "src/core/intent_guard.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/core/intent_guard.py",
      "domain": "core",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Initialize IntentGuard with repository path and load all policies.",
      "docstring": "Initialize IntentGuard with repository path and load all policies.",
      "calls": [
        "Path",
        "_load_policies",
        "_load_source_manifest",
        "info",
        "len",
        "resolve"
      ],
      "line_number": 25,
      "is_async": false,
      "parameters": [
        "self",
        "repo_path"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.672136+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/core/intent_guard.py::IntentGuard",
      "structural_hash": "e9b6e238e15765dc6a7ecd471c50b2e1aa50859c5f14cf337170baf2b3f866d8"
    },
    "src/core/intent_guard.py::_load_policies": {
      "key": "src/core/intent_guard.py::_load_policies",
      "name": "_load_policies",
      "type": "FunctionDef",
      "file": "src/core/intent_guard.py",
      "domain": "core",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Load rules from all YAML files in the `.intent/policies/` directory.",
      "docstring": "Load rules from all YAML files in the `.intent/policies/` directory.",
      "calls": [
        "extend",
        "glob",
        "is_dir",
        "isinstance",
        "load_config"
      ],
      "line_number": 40,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.673157+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/intent_guard.py::IntentGuard",
      "structural_hash": "5328b103e7c818e7ff9526f02e7c83a8ab6c6dc8fce2265b097e41326967b231"
    },
    "src/core/intent_guard.py::_load_source_manifest": {
      "key": "src/core/intent_guard.py::_load_source_manifest",
      "name": "_load_source_manifest",
      "type": "FunctionDef",
      "file": "src/core/intent_guard.py",
      "domain": "core",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Load the list of all known source files from the knowledge graph.",
      "docstring": "Load the list of all known source files from the knowledge graph.",
      "calls": [
        "exists",
        "get",
        "list",
        "loads",
        "read_text",
        "sorted",
        "values"
      ],
      "line_number": 49,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.674405+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/intent_guard.py::IntentGuard",
      "structural_hash": "208f1dd7f529278788efa3067d8b296561c44c008592416d370a5ef323fc9bed"
    },
    "src/core/intent_guard.py::check_transaction": {
      "key": "src/core/intent_guard.py::check_transaction",
      "name": "check_transaction",
      "type": "FunctionDef",
      "file": "src/core/intent_guard.py",
      "domain": "core",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Check if a proposed set of file changes complies with all active rules.",
      "docstring": "Check if a proposed set of file changes complies with all active rules.\nThis is the primary enforcement point for constitutional integrity.",
      "calls": [
        "append",
        "resolve"
      ],
      "line_number": 67,
      "is_async": false,
      "parameters": [
        "self",
        "proposed_paths"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.675752+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/core/intent_guard.py::IntentGuard",
      "structural_hash": "ebd1f6c05def4da80120bbbc4fa2e246dc83fb335490ee72380a6f51b5a7fe8c"
    },
    "src/core/self_correction_engine.py::attempt_correction": {
      "key": "src/core/self_correction_engine.py::attempt_correction",
      "name": "attempt_correction",
      "type": "FunctionDef",
      "file": "src/core/self_correction_engine.py",
      "domain": "core",
      "agent": "core_agent",
      "capability": "self_correction",
      "intent": "Attempts to fix a failed validation or test result by generating corrected code via an LLM prompt based on the provided failure context.",
      "docstring": "Attempts to fix a failed validation or test result by generating corrected code via an LLM prompt based on the provided failure context.",
      "calls": [
        "GeneratorClient",
        "add_pending_write",
        "dumps",
        "get",
        "items",
        "list",
        "make_request",
        "parse_write_blocks",
        "process",
        "strip",
        "validate_code"
      ],
      "line_number": 21,
      "is_async": false,
      "parameters": [
        "failure_context"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.679011+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "3e7e301782c375890781f24e40f1db7ba14720c118c53032b99a70d2193956f0"
    },
    "src/agents/utils.py::CodeEditor": {
      "key": "src/agents/utils.py::CodeEditor",
      "name": "CodeEditor",
      "type": "ClassDef",
      "file": "src/agents/utils.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Provides capabilities to surgically edit code files.",
      "docstring": "Provides capabilities to surgically edit code files.",
      "calls": [
        "ValueError",
        "_get_symbol_start_end_lines",
        "dedent",
        "hasattr",
        "isinstance",
        "join",
        "len",
        "lstrip",
        "parse",
        "splitlines",
        "strip",
        "walk"
      ],
      "line_number": 17,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.683372+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "5a5c187e86b7cce86f433bf3a916df579328513edc92e5c1ea412bc873ffd7dd"
    },
    "src/agents/utils.py::_get_symbol_start_end_lines": {
      "key": "src/agents/utils.py::_get_symbol_start_end_lines",
      "name": "_get_symbol_start_end_lines",
      "type": "FunctionDef",
      "file": "src/agents/utils.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Finds the 1-based start and end line numbers of a symbol.",
      "docstring": "Finds the 1-based start and end line numbers of a symbol.",
      "calls": [
        "hasattr",
        "isinstance",
        "walk"
      ],
      "line_number": 20,
      "is_async": false,
      "parameters": [
        "self",
        "tree",
        "symbol_name"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.684452+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/utils.py::CodeEditor",
      "structural_hash": "50681b95b789a7eb7618eedfe18a6612b9779a31945f2189ea558ae585e20fcd"
    },
    "src/agents/utils.py::replace_symbol_in_code": {
      "key": "src/agents/utils.py::replace_symbol_in_code",
      "name": "replace_symbol_in_code",
      "type": "FunctionDef",
      "file": "src/agents/utils.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Error: Could not connect to LLM endpoint. Details: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out. (read timeout=180)",
      "docstring": "Error: Could not connect to LLM endpoint. Details: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out. (read timeout=180)",
      "calls": [
        "ValueError",
        "_get_symbol_start_end_lines",
        "dedent",
        "join",
        "len",
        "lstrip",
        "parse",
        "splitlines",
        "strip"
      ],
      "line_number": 30,
      "is_async": false,
      "parameters": [
        "self",
        "original_code",
        "symbol_name",
        "new_code_str"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.686364+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/utils.py::CodeEditor",
      "structural_hash": "44c865f663f73935a3f4c27e5792add75aa3abedde57b8e0bc3b45eacd89da8e"
    },
    "src/agents/utils.py::SymbolLocator": {
      "key": "src/agents/utils.py::SymbolLocator",
      "name": "SymbolLocator",
      "type": "ClassDef",
      "file": "src/agents/utils.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Dedicated class for finding symbols in code files.",
      "docstring": "Dedicated class for finding symbols in code files.",
      "calls": [
        "FileNotFoundError",
        "RuntimeError",
        "exists",
        "isinstance",
        "parse",
        "read_text",
        "walk"
      ],
      "line_number": 69,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.687988+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "165d63d962637f6f1abae7a11cf3240e7c15cc9fa84409a397b54196f0e3d9db"
    },
    "src/agents/utils.py::find_symbol_line": {
      "key": "src/agents/utils.py::find_symbol_line",
      "name": "find_symbol_line",
      "type": "FunctionDef",
      "file": "src/agents/utils.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Finds the line number of a function, async function, or class definition matching `symbol_name` in the file at `file_path`, or None if not found.",
      "docstring": "Finds the line number of a function, async function, or class definition matching `symbol_name` in the file at `file_path`, or None if not found.",
      "calls": [
        "FileNotFoundError",
        "RuntimeError",
        "exists",
        "isinstance",
        "parse",
        "read_text",
        "walk"
      ],
      "line_number": 73,
      "is_async": false,
      "parameters": [
        "file_path",
        "symbol_name"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.689362+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/utils.py::SymbolLocator",
      "structural_hash": "6f664818dd24e2bf7d088437ea5760a5b20e7852b512f421631dfb41a5d29e90"
    },
    "src/agents/utils.py::PlanExecutionContext": {
      "key": "src/agents/utils.py::PlanExecutionContext",
      "name": "PlanExecutionContext",
      "type": "ClassDef",
      "file": "src/agents/utils.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Context manager for safe plan execution with rollback.",
      "docstring": "Context manager for safe plan execution with rollback.",
      "calls": [
        "error",
        "get_current_commit",
        "is_git_repo",
        "reset_to_commit",
        "warning"
      ],
      "line_number": 90,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.690931+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "544b8bc58ae62f102d1f0b30081f324a131c262e7ea2646f23c9fa0e354b9b16"
    },
    "src/agents/utils.py::__init__": {
      "key": "src/agents/utils.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/agents/utils.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Initializes the context with a reference to the calling agent.",
      "docstring": "Initializes the context with a reference to the calling agent.",
      "calls": [],
      "line_number": 93,
      "is_async": false,
      "parameters": [
        "self",
        "planner_agent"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.691507+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/agents/utils.py::PlanExecutionContext",
      "structural_hash": "4a05911e00cb09254d391a40f2ad6ab724877a959ae2403245276b7aaa2f3b3d"
    },
    "src/agents/utils.py::__enter__": {
      "key": "src/agents/utils.py::__enter__",
      "name": "__enter__",
      "type": "FunctionDef",
      "file": "src/agents/utils.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Sets up the execution context, capturing the initial git commit hash.",
      "docstring": "Sets up the execution context, capturing the initial git commit hash.",
      "calls": [
        "get_current_commit",
        "is_git_repo",
        "warning"
      ],
      "line_number": 98,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.692281+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/agents/utils.py::PlanExecutionContext",
      "structural_hash": "f0615c8b892d65b0287dbf79cf653bec6c073918f9c99a47683f7434c8194c38"
    },
    "src/agents/utils.py::__exit__": {
      "key": "src/agents/utils.py::__exit__",
      "name": "__exit__",
      "type": "FunctionDef",
      "file": "src/agents/utils.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Cleans up and handles rollback on failure.",
      "docstring": "Cleans up and handles rollback on failure.",
      "calls": [
        "error",
        "reset_to_commit",
        "warning"
      ],
      "line_number": 107,
      "is_async": false,
      "parameters": [
        "self",
        "exc_type",
        "exc_val",
        "exc_tb"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.693174+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/agents/utils.py::PlanExecutionContext",
      "structural_hash": "be2578060f851135cdfc30be53a4edd2baf4bd4a6cf5548a5983e4b28df2e81c"
    },
    "src/agents/planner_agent.py::PlanExecutionError": {
      "key": "src/agents/planner_agent.py::PlanExecutionError",
      "name": "PlanExecutionError",
      "type": "ClassDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Custom exception for failures during plan creation or execution.",
      "docstring": "Custom exception for failures during plan creation or execution.",
      "calls": [
        "__init__",
        "super"
      ],
      "line_number": 35,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.699478+00:00",
      "is_class": true,
      "base_classes": [
        "Exception"
      ],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "c490d9d2c28f1d0cf147abfbc874fecc5368a0f232ef44fe0ae86e53c7714d20"
    },
    "src/agents/planner_agent.py::__init__": {
      "key": "src/agents/planner_agent.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Initializes the PlannerAgent with all necessary service dependencies.",
      "docstring": "Initializes the PlannerAgent with all necessary service dependencies.",
      "calls": [
        "CodeEditor",
        "PlannerConfig",
        "PromptPipeline",
        "SymbolLocator",
        "ThreadPoolExecutor",
        "register"
      ],
      "line_number": 48,
      "is_async": false,
      "parameters": [
        "self",
        "orchestrator_client",
        "generator_client",
        "file_handler",
        "git_service",
        "intent_guard",
        "config"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.718157+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "a7aa18625b8aae9f1d398df6ea8d0641512426b95b892eb16bde1678f38bbd22"
    },
    "src/agents/planner_agent.py::PlannerAgent": {
      "key": "src/agents/planner_agent.py::PlannerAgent",
      "name": "PlannerAgent",
      "type": "ClassDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "code_generation",
      "intent": "The primary agent responsible for decomposing high-level goals into executable plans.",
      "docstring": "The primary agent responsible for decomposing high-level goals into executable plans.\nIt orchestrates the generation, validation, and commitment of code changes.",
      "calls": [
        "CodeEditor",
        "ExecutionProgress",
        "ExecutionTask",
        "FileExistsError",
        "FileNotFoundError",
        "PlanExecutionContext",
        "PlanExecutionError",
        "PlannerConfig",
        "PromptPipeline",
        "SymbolLocator",
        "ThreadPoolExecutor",
        "ValueError",
        "_cleanup_resources",
        "_execute_add_tag",
        "_execute_create_file",
        "_execute_edit_function",
        "_execute_task",
        "_execute_task_with_timeout",
        "_extract_json_from_response",
        "_find_symbol_line_async",
        "_generate_code_for_task",
        "_log_plan_summary",
        "_setup_logging_context",
        "_validate_task_params",
        "add",
        "add_pending_write",
        "all",
        "commit",
        "confirm_write",
        "create_execution_plan",
        "dedent",
        "enumerate",
        "error",
        "exists",
        "format",
        "get",
        "get_event_loop",
        "group",
        "hasattr",
        "info",
        "insert",
        "is_git_repo",
        "isinstance",
        "isoformat",
        "join",
        "len",
        "loads",
        "lstrip",
        "make_request",
        "now",
        "process",
        "range",
        "read_text",
        "register",
        "replace_symbol_in_code",
        "run_in_executor",
        "search",
        "set",
        "shutdown",
        "splitlines",
        "str",
        "strftime",
        "strip",
        "validate_code",
        "wait_for",
        "warning"
      ],
      "line_number": 42,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.716782+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "7bce6ee7b96e4a8bb9c7ab997554263c169a39da471f8249cdf8d39fc532fab8"
    },
    "src/agents/planner_agent.py::_cleanup_resources": {
      "key": "src/agents/planner_agent.py::_cleanup_resources",
      "name": "_cleanup_resources",
      "type": "FunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Clean up resources on shutdown.",
      "docstring": "Clean up resources on shutdown.",
      "calls": [
        "hasattr",
        "shutdown"
      ],
      "line_number": 70,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.718852+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "18a897c023e750b3102e9112cac0481342d69151e8f46df6f31add5133addc8a"
    },
    "src/agents/planner_agent.py::__del__": {
      "key": "src/agents/planner_agent.py::__del__",
      "name": "__del__",
      "type": "FunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Ensure resources are cleaned up when the agent is garbage collected.",
      "docstring": "Ensure resources are cleaned up when the agent is garbage collected.",
      "calls": [
        "_cleanup_resources"
      ],
      "line_number": 75,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.719388+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "23a9473b605a7ace503e984722b1ea788660e8db0762b63ab752f6683f72deb1"
    },
    "src/agents/planner_agent.py::_setup_logging_context": {
      "key": "src/agents/planner_agent.py::_setup_logging_context",
      "name": "_setup_logging_context",
      "type": "FunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Setup structured logging context for better observability.",
      "docstring": "Setup structured logging context for better observability.",
      "calls": [
        "isoformat",
        "now",
        "set"
      ],
      "line_number": 79,
      "is_async": false,
      "parameters": [
        "self",
        "goal",
        "plan_id"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.720028+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "80a4b86bed45426c0aec597d5655d0cbbdde478dacfa2315417d27a6af55520c"
    },
    "src/agents/planner_agent.py::_extract_json_from_response": {
      "key": "src/agents/planner_agent.py::_extract_json_from_response",
      "name": "_extract_json_from_response",
      "type": "FunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Extract JSON with multiple strategies and better error handling.",
      "docstring": "Extract JSON with multiple strategies and better error handling.",
      "calls": [
        "error",
        "group",
        "loads",
        "search",
        "warning"
      ],
      "line_number": 88,
      "is_async": false,
      "parameters": [
        "self",
        "text"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.721267+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "e1970a04bda00c33ad40841c6593b6fe49dd01f2714470c6acb6cb5291c7e606"
    },
    "src/agents/planner_agent.py::_log_plan_summary": {
      "key": "src/agents/planner_agent.py::_log_plan_summary",
      "name": "_log_plan_summary",
      "type": "FunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Log a readable summary of the execution plan.",
      "docstring": "Log a readable summary of the execution plan.",
      "calls": [
        "enumerate",
        "info",
        "len"
      ],
      "line_number": 112,
      "is_async": false,
      "parameters": [
        "self",
        "plan"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.722518+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "93ee204781a94919f60c4e6ec21d16cdee82d40346b6e9c3655a835a6a4dc8ed"
    },
    "src/agents/planner_agent.py::_validate_task_params": {
      "key": "src/agents/planner_agent.py::_validate_task_params",
      "name": "_validate_task_params",
      "type": "FunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Validates that a task has all the logically required parameters for its action.",
      "docstring": "Validates that a task has all the logically required parameters for its action.",
      "calls": [
        "PlanExecutionError",
        "all"
      ],
      "line_number": 118,
      "is_async": false,
      "parameters": [
        "self",
        "task"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.724082+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "693c6fdba76084568fb72322bb96cd87466bbb70f5cc9ea34818c6caa3249fdc"
    },
    "src/agents/planner_agent.py::create_execution_plan": {
      "key": "src/agents/planner_agent.py::create_execution_plan",
      "name": "create_execution_plan",
      "type": "FunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "llm_orchestration",
      "intent": "Creates a high-level, code-agnostic execution plan.",
      "docstring": "Creates a high-level, code-agnostic execution plan.",
      "calls": [
        "ExecutionTask",
        "PlanExecutionError",
        "ValueError",
        "_extract_json_from_response",
        "_log_plan_summary",
        "_setup_logging_context",
        "dedent",
        "error",
        "format",
        "info",
        "isinstance",
        "make_request",
        "now",
        "process",
        "range",
        "strftime",
        "strip",
        "warning"
      ],
      "line_number": 132,
      "is_async": false,
      "parameters": [
        "self",
        "high_level_goal"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.726375+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "e4844cd8faa79f645cf6905fb41e3a76a222c167a3954dc79f3367bab45d361c"
    },
    "src/agents/planner_agent.py::_generate_code_for_task": {
      "key": "src/agents/planner_agent.py::_generate_code_for_task",
      "name": "_generate_code_for_task",
      "type": "AsyncFunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Generates the code content for a single task.",
      "docstring": "Generates the code content for a single task.",
      "calls": [
        "dedent",
        "format",
        "info",
        "make_request",
        "process",
        "strip"
      ],
      "line_number": 187,
      "is_async": true,
      "parameters": [
        "self",
        "task",
        "goal"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.727764+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "fc01cfbd82cdc9f7e0612b93e4300c4c8bc2ebfc25913ff64023b4aa6ffafa9a"
    },
    "src/agents/planner_agent.py::_find_symbol_line_async": {
      "key": "src/agents/planner_agent.py::_find_symbol_line_async",
      "name": "_find_symbol_line_async",
      "type": "AsyncFunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Async version using shared thread pool for file I/O.",
      "docstring": "Async version using shared thread pool for file I/O.",
      "calls": [
        "get_event_loop",
        "run_in_executor"
      ],
      "line_number": 220,
      "is_async": true,
      "parameters": [
        "self",
        "file_path",
        "symbol_name"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.728609+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "c5ea7aa92b49213704a2a4e6c1ae6dcff7fb345bb65a9d07d50be4398f67ab8c"
    },
    "src/agents/planner_agent.py::_execute_task_with_timeout": {
      "key": "src/agents/planner_agent.py::_execute_task_with_timeout",
      "name": "_execute_task_with_timeout",
      "type": "AsyncFunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Execute task with timeout protection.",
      "docstring": "Execute task with timeout protection.",
      "calls": [
        "PlanExecutionError",
        "_execute_task",
        "wait_for"
      ],
      "line_number": 228,
      "is_async": true,
      "parameters": [
        "self",
        "task",
        "timeout"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.729587+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "231034b50c47ff8c90de80776ab3359723d7ec24fd1048ec9e496d996d3efc87"
    },
    "src/agents/planner_agent.py::execute_plan": {
      "key": "src/agents/planner_agent.py::execute_plan",
      "name": "execute_plan",
      "type": "AsyncFunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Creates and executes a plan in a two-step (Plan -> Generate -> Execute) process.",
      "docstring": "Creates and executes a plan in a two-step (Plan -> Generate -> Execute) process.",
      "calls": [
        "ExecutionProgress",
        "PlanExecutionContext",
        "PlanExecutionError",
        "_execute_task_with_timeout",
        "_generate_code_for_task",
        "create_execution_plan",
        "enumerate",
        "error",
        "get",
        "hasattr",
        "info",
        "len",
        "str"
      ],
      "line_number": 236,
      "is_async": true,
      "parameters": [
        "self",
        "high_level_goal"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.732315+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "98e9686fc24cdf8b79060ae287992d7c25c3f558c64c75a61955a94af03b352b"
    },
    "src/agents/planner_agent.py::_execute_add_tag": {
      "key": "src/agents/planner_agent.py::_execute_add_tag",
      "name": "_execute_add_tag",
      "type": "AsyncFunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Executes the surgical 'add_capability_tag' action.",
      "docstring": "Executes the surgical 'add_capability_tag' action.",
      "calls": [
        "PlanExecutionError",
        "_find_symbol_line_async",
        "add",
        "add_pending_write",
        "commit",
        "confirm_write",
        "exists",
        "info",
        "insert",
        "is_git_repo",
        "join",
        "len",
        "lstrip",
        "read_text",
        "splitlines",
        "validate_code"
      ],
      "line_number": 270,
      "is_async": true,
      "parameters": [
        "self",
        "params"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.735526+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "aaff0ea00c53c82698278eadeea1af4788cb8ca8c6ce4b291d5457c97666e83e"
    },
    "src/agents/planner_agent.py::_execute_create_file": {
      "key": "src/agents/planner_agent.py::_execute_create_file",
      "name": "_execute_create_file",
      "type": "AsyncFunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Executes the 'create_file' action.",
      "docstring": "Executes the 'create_file' action.",
      "calls": [
        "FileExistsError",
        "PlanExecutionError",
        "add",
        "add_pending_write",
        "commit",
        "confirm_write",
        "exists",
        "is_git_repo",
        "validate_code"
      ],
      "line_number": 304,
      "is_async": true,
      "parameters": [
        "self",
        "params"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.737559+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "3539a683b04b8e0f8b34e185950bf5f9db11786690d3c51c3f930708771c5551"
    },
    "src/agents/planner_agent.py::_execute_edit_function": {
      "key": "src/agents/planner_agent.py::_execute_edit_function",
      "name": "_execute_edit_function",
      "type": "AsyncFunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Executes the 'edit_function' action using the CodeEditor.",
      "docstring": "Executes the 'edit_function' action using the CodeEditor.",
      "calls": [
        "FileNotFoundError",
        "PlanExecutionError",
        "add",
        "add_pending_write",
        "commit",
        "confirm_write",
        "dedent",
        "exists",
        "get_event_loop",
        "is_git_repo",
        "replace_symbol_in_code",
        "run_in_executor",
        "strip",
        "validate_code"
      ],
      "line_number": 324,
      "is_async": true,
      "parameters": [
        "self",
        "params"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.740139+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "df9e99690abb77fa164056e00dc548c3456e9598945ef0c1c803873ae751ebbe"
    },
    "src/agents/planner_agent.py::_execute_task": {
      "key": "src/agents/planner_agent.py::_execute_task",
      "name": "_execute_task",
      "type": "AsyncFunctionDef",
      "file": "src/agents/planner_agent.py",
      "domain": "agents",
      "agent": "planner_agent",
      "capability": "unassigned",
      "intent": "Dispatcher that executes a single task from a plan based on its action type.",
      "docstring": "Dispatcher that executes a single task from a plan based on its action type.",
      "calls": [
        "_execute_add_tag",
        "_execute_create_file",
        "_execute_edit_function",
        "_validate_task_params",
        "warning"
      ],
      "line_number": 354,
      "is_async": true,
      "parameters": [
        "self",
        "task"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.741578+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/planner_agent.py::PlannerAgent",
      "structural_hash": "6bdba87d6693facd69f26785c2d7f0bb5461d892c168fe889424d4396434d6a3"
    },
    "src/agents/models.py::TaskStatus": {
      "key": "src/agents/models.py::TaskStatus",
      "name": "TaskStatus",
      "type": "ClassDef",
      "file": "src/agents/models.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Enumeration of possible states for an ExecutionTask.",
      "docstring": "Enumeration of possible states for an ExecutionTask.",
      "calls": [],
      "line_number": 11,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "enum",
      "last_updated": "2025-08-11T16:43:38.742980+00:00",
      "is_class": true,
      "base_classes": [
        "Enum"
      ],
      "entry_point_justification": "enum_definition",
      "structural_hash": "3f35af37e4a402c877e5f20661774fe9ea58219ed768e5c166aa56cca41e769d"
    },
    "src/agents/models.py::ExecutionProgress": {
      "key": "src/agents/models.py::ExecutionProgress",
      "name": "ExecutionProgress",
      "type": "ClassDef",
      "file": "src/agents/models.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Represents the progress of a plan's execution.",
      "docstring": "Represents the progress of a plan's execution.",
      "calls": [],
      "line_number": 19,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.744063+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "71d705d70aada6975025e39b5802067af023d53b2b1d9198d0875b530d1b2fc1"
    },
    "src/agents/models.py::completion_percentage": {
      "key": "src/agents/models.py::completion_percentage",
      "name": "completion_percentage",
      "type": "FunctionDef",
      "file": "src/agents/models.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Calculates the completion percentage of the plan as a float, returning 0 if there are no tasks.",
      "docstring": "Calculates the completion percentage of the plan as a float, returning 0 if there are no tasks.",
      "calls": [],
      "line_number": 27,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.744760+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/agents/models.py::ExecutionProgress",
      "structural_hash": "2e185879694b400e85ad6ae02559a84632cdd03f58547c2c59f5dd16785826e7"
    },
    "src/agents/models.py::PlannerConfig": {
      "key": "src/agents/models.py::PlannerConfig",
      "name": "PlannerConfig",
      "type": "ClassDef",
      "file": "src/agents/models.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Configuration settings for the PlannerAgent's behavior.",
      "docstring": "Configuration settings for the PlannerAgent's behavior.",
      "calls": [],
      "line_number": 33,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.745460+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "6ace4d57fe33bdb00ebaad6b74b2728ee3fce59e3161ae977882986c90520a54"
    },
    "src/agents/models.py::TaskParams": {
      "key": "src/agents/models.py::TaskParams",
      "name": "TaskParams",
      "type": "ClassDef",
      "file": "src/agents/models.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Data model for the parameters of a single task in an execution plan.",
      "docstring": "Data model for the parameters of a single task in an execution plan.",
      "calls": [],
      "line_number": 42,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.746120+00:00",
      "is_class": true,
      "base_classes": [
        "BaseModel"
      ],
      "entry_point_justification": "pydantic_model",
      "structural_hash": "f4aff97a68663aeaf92000a2589554961f3bd1cdec488a231d52b61c9077bdb1"
    },
    "src/agents/models.py::ExecutionTask": {
      "key": "src/agents/models.py::ExecutionTask",
      "name": "ExecutionTask",
      "type": "ClassDef",
      "file": "src/agents/models.py",
      "domain": "agents",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Data model for a single, executable step in a plan.",
      "docstring": "Data model for a single, executable step in a plan.",
      "calls": [],
      "line_number": 49,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.746719+00:00",
      "is_class": true,
      "base_classes": [
        "BaseModel"
      ],
      "entry_point_justification": "pydantic_model",
      "structural_hash": "03e4378512aca46ae0ec1beb9f117c12310ae067b50214c49b492cbde5b1a5d5"
    },
    "src/shared/path_utils.py::get_repo_root": {
      "key": "src/shared/path_utils.py::get_repo_root",
      "name": "get_repo_root",
      "type": "FunctionDef",
      "file": "src/shared/path_utils.py",
      "domain": "shared",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Find and return the repository root by locating the .git directory, starting from the current directory or provided path.",
      "docstring": "Find and return the repository root by locating the .git directory, starting from the current directory or provided path.",
      "calls": [
        "Path",
        "RuntimeError",
        "cwd",
        "exists",
        "resolve"
      ],
      "line_number": 6,
      "is_async": false,
      "parameters": [
        "start_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.747962+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "2979d82634fc329fbff2dc3800c02fe2a3c14c252bcb4510c436fabaf3aaa03b"
    },
    "src/shared/config_loader.py::load_config": {
      "key": "src/shared/config_loader.py::load_config",
      "name": "load_config",
      "type": "FunctionDef",
      "file": "src/shared/config_loader.py",
      "domain": "shared",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Loads a JSON or YAML file into a dictionary, handling missing files, invalid formats, and parsing errors by returning an empty dict.",
      "docstring": "Loads a JSON or YAML file into a dictionary, handling missing files, invalid formats, and parsing errors by returning an empty dict.",
      "calls": [
        "Path",
        "error",
        "exists",
        "isinstance",
        "load",
        "lower",
        "open",
        "safe_load",
        "warning"
      ],
      "line_number": 11,
      "is_async": false,
      "parameters": [
        "file_path",
        "file_type"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.750931+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "5adc7503936339da7be5cea6b36673c3993f98880cf6455514a048522d0ba497"
    },
    "src/shared/config.py::Settings": {
      "key": "src/shared/config.py::Settings",
      "name": "Settings",
      "type": "ClassDef",
      "file": "src/shared/config.py",
      "domain": "shared",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "A Pydantic settings model that loads configuration from the environment.",
      "docstring": "A Pydantic settings model that loads configuration from the environment.\nIt provides typed, validated access to all system settings.",
      "calls": [
        "Path",
        "home"
      ],
      "line_number": 12,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.752601+00:00",
      "is_class": true,
      "base_classes": [
        "BaseSettings"
      ],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "25f4590e5be47c39b2a0418d6185c1cb2df3ac005c287006bfa07a7c135106e6"
    },
    "src/shared/config.py::Config": {
      "key": "src/shared/config.py::Config",
      "name": "Config",
      "type": "ClassDef",
      "file": "src/shared/config.py",
      "domain": "shared",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Defines Pydantic's behavior for the Settings model.",
      "docstring": "Defines Pydantic's behavior for the Settings model.",
      "calls": [],
      "line_number": 36,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.753179+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "e95a02178a8961d1e4c328480cd2f5690da7151f573f5fbc6f0d8ed6e8c1eba4"
    },
    "src/shared/logger.py::getLogger": {
      "key": "src/shared/logger.py::getLogger",
      "name": "getLogger",
      "type": "FunctionDef",
      "file": "src/shared/logger.py",
      "domain": "shared",
      "agent": "generic_agent",
      "capability": "system_logging",
      "intent": "Returns a pre-configured logger instance with the given name.",
      "docstring": "Returns a pre-configured logger instance with the given name.",
      "calls": [
        "getLogger"
      ],
      "line_number": 44,
      "is_async": false,
      "parameters": [
        "name"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.754357+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "7907d7ea4630bbe78fb56462e21cc38898aeccf14ddf808acee40a3fd5b102c5"
    },
    "src/system/guard/drift_detector.py::CapabilityMeta": {
      "key": "src/system/guard/drift_detector.py::CapabilityMeta",
      "name": "CapabilityMeta",
      "type": "ClassDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Minimal data container for capability metadata used in drift comparison.",
      "docstring": "Intent: Minimal data container for capability metadata used in drift comparison.",
      "calls": [
        "dataclass"
      ],
      "line_number": 27,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.760901+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "5408b6e0217089527a88993bda9c7375c851b57d5b226aa922a3df57b0bd9dff"
    },
    "src/system/guard/drift_detector.py::_parse_inline_meta": {
      "key": "src/system/guard/drift_detector.py::_parse_inline_meta",
      "name": "_parse_inline_meta",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Parse inline [key=value] metadata from trailing text and return as a dictionary.",
      "docstring": "Parse inline [key=value] metadata from trailing text and return as a dictionary.",
      "calls": [
        "findall",
        "group",
        "search"
      ],
      "line_number": 33,
      "is_async": false,
      "parameters": [
        "trailing"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.761850+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "a4043f43cef35c754adeb1c83b3c4b1eeaa9959bc4f109249e38b6166a58c5a9"
    },
    "src/system/guard/drift_detector.py::_try_import_kgb": {
      "key": "src/system/guard/drift_detector.py::_try_import_kgb",
      "name": "_try_import_kgb",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Attempt to access KnowledgeGraphBuilder without static cross-domain imports.",
      "docstring": "Intent: Attempt to access KnowledgeGraphBuilder without static cross-domain imports.",
      "calls": [
        "getattr",
        "import_module"
      ],
      "line_number": 42,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.762601+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "04affff91ef14b2814abe805b97cc81f43072e73ce3761583d98e16b284e36e8"
    },
    "src/system/guard/drift_detector.py::_find_manifest": {
      "key": "src/system/guard/drift_detector.py::_find_manifest",
      "name": "_find_manifest",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Locate the authoritative .intent manifest file.",
      "docstring": "Intent: Locate the authoritative .intent manifest file.",
      "calls": [
        "FileNotFoundError",
        "exists"
      ],
      "line_number": 50,
      "is_async": false,
      "parameters": [
        "start"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.763455+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "02a690e20b4df9af5e3b91d8ef7066d89ab7a1f12b52024492a60dd0c170969a"
    },
    "src/system/guard/drift_detector.py::_normalize_cap_list": {
      "key": "src/system/guard/drift_detector.py::_normalize_cap_list",
      "name": "_normalize_cap_list",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Normalize many shapes (list[str], list[dict], dict[str,dict]) into {cap: CapabilityMeta}.",
      "docstring": "Intent: Normalize many shapes (list[str], list[dict], dict[str,dict]) into {cap: CapabilityMeta}.",
      "calls": [
        "CapabilityMeta",
        "get",
        "isinstance",
        "items"
      ],
      "line_number": 59,
      "is_async": false,
      "parameters": [
        "items"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.765386+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "000accec56838a02aee24230dbb06b85894557b24cff9574652e7ae0a8714cd8"
    },
    "src/system/guard/drift_detector.py::_normalize_manifest_caps": {
      "key": "src/system/guard/drift_detector.py::_normalize_manifest_caps",
      "name": "_normalize_manifest_caps",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Normalize different manifest shapes into a {capability: CapabilityMeta} map.",
      "docstring": "Intent: Normalize different manifest shapes into a {capability: CapabilityMeta} map.\nLooks for keys commonly used in CORE manifests:\n  - capabilities\n  - required_capabilities\n  - expected_capabilities\n  - capability_map / capability_registry\n  - components[*].capabilities",
      "calls": [
        "_normalize_cap_list",
        "deque",
        "extend",
        "extract_from_node",
        "get",
        "isinstance",
        "popleft",
        "update",
        "values"
      ],
      "line_number": 81,
      "is_async": false,
      "parameters": [
        "raw"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.767781+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "151161bb55ec76b1d1d424f43a67b32a5de39e66595058274d9faca614cd8d62"
    },
    "src/system/guard/drift_detector.py::extract_from_node": {
      "key": "src/system/guard/drift_detector.py::extract_from_node",
      "name": "extract_from_node",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Extract capabilities from a node by checking common capability-related keys or nested components/modules/services.",
      "docstring": "Extract capabilities from a node by checking common capability-related keys or nested components/modules/services.",
      "calls": [
        "_normalize_cap_list",
        "get",
        "isinstance",
        "update"
      ],
      "line_number": 93,
      "is_async": false,
      "parameters": [
        "node"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.769282+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "e70c6aa4452dc8577225242af4a38c3e1efce9205ff77666187eb385a9014528"
    },
    "src/system/guard/drift_detector.py::load_manifest": {
      "key": "src/system/guard/drift_detector.py::load_manifest",
      "name": "load_manifest",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Load and parse the .intent manifest with PyYAML.",
      "docstring": "Intent: Load and parse the .intent manifest with PyYAML.",
      "calls": [
        "RuntimeError",
        "_find_manifest",
        "_normalize_manifest_caps",
        "open",
        "safe_load"
      ],
      "line_number": 124,
      "is_async": false,
      "parameters": [
        "root",
        "explicit_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.770748+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "f5f114172590c81ba4721a36562240bbc8ea21f094f179c237e6265c64f68e60"
    },
    "src/system/guard/drift_detector.py::_extract_cap_meta_from_node": {
      "key": "src/system/guard/drift_detector.py::_extract_cap_meta_from_node",
      "name": "_extract_cap_meta_from_node",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Extract capability/domain/owner from a variety of KG node shapes.",
      "docstring": "Intent: Extract capability/domain/owner from a variety of KG node shapes.",
      "calls": [
        "CapabilityMeta",
        "get",
        "group",
        "isinstance",
        "match",
        "str"
      ],
      "line_number": 133,
      "is_async": false,
      "parameters": [
        "node"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.772757+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "22a481b469db7b2e6ad4dcbe49ea14eb3ed0b6ebe9ff9c5b5514a4233d1111de"
    },
    "src/system/guard/drift_detector.py::_load_json_file": {
      "key": "src/system/guard/drift_detector.py::_load_json_file",
      "name": "_load_json_file",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Loads and parses a JSON file from the given path, returning its contents on success or None on failure.",
      "docstring": "Loads and parses a JSON file from the given path, returning its contents on success or None on failure.",
      "calls": [
        "loads",
        "read_text"
      ],
      "line_number": 157,
      "is_async": false,
      "parameters": [
        "path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.773682+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "e5b8dc6446d1af1f315425e62a0623a574872effad31ddb4031906ef31e69198"
    },
    "src/system/guard/drift_detector.py::_load_ndjson_file": {
      "key": "src/system/guard/drift_detector.py::_load_ndjson_file",
      "name": "_load_ndjson_file",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Loads and parses each non-empty line of an NDJSON file into a list of objects, skipping invalid lines and returning an empty list on file read errors.",
      "docstring": "Loads and parses each non-empty line of an NDJSON file into a list of objects, skipping invalid lines and returning an empty list on file read errors.",
      "calls": [
        "append",
        "loads",
        "read_text",
        "splitlines",
        "strip"
      ],
      "line_number": 164,
      "is_async": false,
      "parameters": [
        "path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.774697+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "6c3686fc95f810deedd4a59bf66fa5e1feba0be91ec17ba717cfc5d32c3bc40d"
    },
    "src/system/guard/drift_detector.py::_collect_from_kgb_artifact": {
      "key": "src/system/guard/drift_detector.py::_collect_from_kgb_artifact",
      "name": "_collect_from_kgb_artifact",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Read a previously emitted knowledge graph artifact from disk.",
      "docstring": "Intent: Read a previously emitted knowledge graph artifact from disk.\nAccepts common file names and shapes, returning a capability map.",
      "calls": [
        "_extract_cap_meta_from_node",
        "_load_json_file",
        "_load_ndjson_file",
        "exists",
        "get",
        "isinstance",
        "values"
      ],
      "line_number": 180,
      "is_async": false,
      "parameters": [
        "root"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.776807+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "b8c149e8f412f1d78f6c3275e10988516c909fc26af52f1b30c75f44591b427e"
    },
    "src/system/guard/drift_detector.py::_iter_source_files": {
      "key": "src/system/guard/drift_detector.py::_iter_source_files",
      "name": "_iter_source_files",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Yield repository files considered for direct CAPABILITY tag scanning.",
      "docstring": "Intent: Yield repository files considered for direct CAPABILITY tag scanning.",
      "calls": [
        "any",
        "is_file",
        "list",
        "match",
        "rglob",
        "wanted"
      ],
      "line_number": 218,
      "is_async": false,
      "parameters": [
        "root",
        "include_globs",
        "exclude_globs"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.778472+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "bf8906438a6c646d4a52c2baba2c66b6f2c25147ffabb71c57d330bd9c697e9c"
    },
    "src/system/guard/drift_detector.py::wanted": {
      "key": "src/system/guard/drift_detector.py::wanted",
      "name": "wanted",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Filter for include/exclude globs and typical source suffixes.",
      "docstring": "Intent: Filter for include/exclude globs and typical source suffixes.",
      "calls": [
        "any",
        "match"
      ],
      "line_number": 222,
      "is_async": false,
      "parameters": [
        "p"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.779396+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "4220ccd6e39fc1f11ffcf95f1692027fa63a5db31f8ac007644b7c82e168abf0"
    },
    "src/system/guard/drift_detector.py::_collect_from_grep": {
      "key": "src/system/guard/drift_detector.py::_collect_from_grep",
      "name": "_collect_from_grep",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Fallback discovery by scanning for '# CAPABILITY:' tags with optional inline metadata.",
      "docstring": "Intent: Fallback discovery by scanning for '# CAPABILITY:' tags with optional inline metadata.",
      "calls": [
        "CapabilityMeta",
        "_iter_source_files",
        "_parse_inline_meta",
        "get",
        "group",
        "match",
        "read_text",
        "splitlines",
        "strip"
      ],
      "line_number": 233,
      "is_async": false,
      "parameters": [
        "root",
        "include_globs",
        "exclude_globs"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.781028+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "88d045a129b2794465293fd74ee6974f55cf52b675067106e36ec20b45171b82"
    },
    "src/system/guard/drift_detector.py::_collect_from_kgb": {
      "key": "src/system/guard/drift_detector.py::_collect_from_kgb",
      "name": "_collect_from_kgb",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Use KnowledgeGraphBuilder (if present) to discover capabilities from the repo.",
      "docstring": "Intent: Use KnowledgeGraphBuilder (if present) to discover capabilities from the repo.",
      "calls": [
        "KGB",
        "_extract_cap_meta_from_node",
        "_try_import_kgb",
        "build",
        "get",
        "getattr",
        "hasattr",
        "isinstance",
        "str",
        "values"
      ],
      "line_number": 252,
      "is_async": false,
      "parameters": [
        "root"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.783438+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "c9d233a6c2aad1205b726312bbcc4a49f34e0f712b212629671de240f7977648"
    },
    "src/system/guard/drift_detector.py::collect_code_capabilities": {
      "key": "src/system/guard/drift_detector.py::collect_code_capabilities",
      "name": "collect_code_capabilities",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Unified discovery entrypoint respecting strict-intent when required.",
      "docstring": "Intent: Unified discovery entrypoint respecting strict-intent when required.",
      "calls": [
        "RuntimeError",
        "_collect_from_grep",
        "_collect_from_kgb",
        "_collect_from_kgb_artifact"
      ],
      "line_number": 300,
      "is_async": false,
      "parameters": [
        "root",
        "include_globs",
        "exclude_globs",
        "require_kgb"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.784907+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "483aa628f2dee2dafe4d5635b2961d044a72d1e9bbf42b1de6984255712ea2e9"
    },
    "src/system/guard/drift_detector.py::DriftReport": {
      "key": "src/system/guard/drift_detector.py::DriftReport",
      "name": "DriftReport",
      "type": "ClassDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Structured result for capability drift suitable for JSON emission and CI gating.",
      "docstring": "Intent: Structured result for capability drift suitable for JSON emission and CI gating.",
      "calls": [
        "sorted"
      ],
      "line_number": 315,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.785978+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "6d1d492460ffa7664cb9c67e0cb5ec92572ef9bd87dc2d02d43e760d45032255"
    },
    "src/system/guard/drift_detector.py::to_dict": {
      "key": "src/system/guard/drift_detector.py::to_dict",
      "name": "to_dict",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Convert the drift report into a stable JSON-serializable dict.",
      "docstring": "Intent: Convert the drift report into a stable JSON-serializable dict.",
      "calls": [
        "sorted"
      ],
      "line_number": 321,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.786666+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/guard/drift_detector.py::DriftReport",
      "structural_hash": "bf7d7caff963731b21d0f4785fe948223c719588267acea8c8e530c4f479a800"
    },
    "src/system/guard/drift_detector.py::detect_capability_drift": {
      "key": "src/system/guard/drift_detector.py::detect_capability_drift",
      "name": "detect_capability_drift",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Compute missing/undeclared/mismatched capability sets between manifest and code.",
      "docstring": "Intent: Compute missing/undeclared/mismatched capability sets between manifest and code.",
      "calls": [
        "DriftReport",
        "append",
        "keys",
        "set",
        "sorted"
      ],
      "line_number": 325,
      "is_async": false,
      "parameters": [
        "manifest_caps",
        "code_caps"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.788569+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "34ea6f8212c054e815fa4ae0416eb44ea9b151883e6ca85c75ecb5b5fd542e8a"
    },
    "src/system/guard/drift_detector.py::write_report": {
      "key": "src/system/guard/drift_detector.py::write_report",
      "name": "write_report",
      "type": "FunctionDef",
      "file": "src/system/guard/drift_detector.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Persist the drift report to disk under reports/ for evidence and CI.",
      "docstring": "Intent: Persist the drift report to disk under reports/ for evidence and CI.",
      "calls": [
        "dumps",
        "mkdir",
        "to_dict",
        "write_text"
      ],
      "line_number": 339,
      "is_async": false,
      "parameters": [
        "report_path",
        "report"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.789537+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "4036db64cd7bbe47f411d592822def62aa1c1e000240579410a282fd2d06f742"
    },
    "src/system/governance/constitutional_auditor.py::ConstitutionalAuditor": {
      "key": "src/system/governance/constitutional_auditor.py::ConstitutionalAuditor",
      "name": "ConstitutionalAuditor",
      "type": "ClassDef",
      "file": "src/system/governance/constitutional_auditor.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "alignment_checking",
      "intent": "Orchestrates the discovery and execution of all constitutional checks.",
      "docstring": "Orchestrates the discovery and execution of all constitutional checks.",
      "calls": [
        "AuditFinding",
        "AuditorContext",
        "Class",
        "Console",
        "IntentModel",
        "Panel",
        "Path",
        "Table",
        "_LoggingBridge",
        "_discover_checks",
        "_report_final_status",
        "add_row",
        "aggregate_manifests",
        "any",
        "append",
        "check_fn",
        "debug",
        "endswith",
        "error",
        "exists",
        "extend",
        "get",
        "get_repo_root",
        "getmembers",
        "glob",
        "import_module",
        "info",
        "len",
        "list",
        "load_config",
        "load_dotenv",
        "print",
        "sort",
        "startswith",
        "strip",
        "values",
        "warning"
      ],
      "line_number": 33,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.799909+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "d2645eeea8236f69647696b71e555ad61766c1adbdb02d143dd786203c3e1af1"
    },
    "src/system/governance/constitutional_auditor.py::_LoggingBridge": {
      "key": "src/system/governance/constitutional_auditor.py::_LoggingBridge",
      "name": "_LoggingBridge",
      "type": "ClassDef",
      "file": "src/system/governance/constitutional_auditor.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "A file-like object that redirects writes to the logger.",
      "docstring": "A file-like object that redirects writes to the logger.",
      "calls": [
        "info",
        "strip"
      ],
      "line_number": 36,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.800662+00:00",
      "is_class": true,
      "base_classes": [
        "StringIO"
      ],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "376b10bf9c9debcbdde0ec17b29f2be09e11fbe5c2723f3f8cd6e5a75bc01f65"
    },
    "src/system/governance/constitutional_auditor.py::write": {
      "key": "src/system/governance/constitutional_auditor.py::write",
      "name": "write",
      "type": "FunctionDef",
      "file": "src/system/governance/constitutional_auditor.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Redirects the write to the logger info stream.",
      "docstring": "Redirects the write to the logger info stream.",
      "calls": [
        "info",
        "strip"
      ],
      "line_number": 38,
      "is_async": false,
      "parameters": [
        "self",
        "s"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.801257+00:00",
      "is_class": false,
      "base_classes": [
        "StringIO",
        "StringIO",
        "StringIO"
      ],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/governance/constitutional_auditor.py::_LoggingBridge",
      "structural_hash": "44784f80c99de948a843e936c747ab36d54800d3091c6efbb2388a0316cf4919"
    },
    "src/system/governance/constitutional_auditor.py::__init__": {
      "key": "src/system/governance/constitutional_auditor.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/system/governance/constitutional_auditor.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Initializes the shared context for all audit checks.",
      "docstring": "Initializes the shared context for all audit checks.",
      "calls": [
        "IntentModel",
        "aggregate_manifests",
        "get",
        "list",
        "load_config",
        "values"
      ],
      "line_number": 70,
      "is_async": false,
      "parameters": [
        "self",
        "repo_root"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.805096+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/system/governance/constitutional_auditor.py::AuditorContext",
      "structural_hash": "abebf93c39a5643b78346a0a6c0f84539693bed2d10372564eecfafb3f82db00"
    },
    "src/system/governance/constitutional_auditor.py::AuditorContext": {
      "key": "src/system/governance/constitutional_auditor.py::AuditorContext",
      "name": "AuditorContext",
      "type": "ClassDef",
      "file": "src/system/governance/constitutional_auditor.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "A simple container for shared state that all checks can access.",
      "docstring": "A simple container for shared state that all checks can access.",
      "calls": [
        "IntentModel",
        "aggregate_manifests",
        "get",
        "list",
        "load_config",
        "values"
      ],
      "line_number": 68,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.804005+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "cdffc2a9d14d2a40a39e7d7f1a2427f6263f35daa325786fc9f3ca3b24c31598"
    },
    "src/system/governance/constitutional_auditor.py::_discover_checks": {
      "key": "src/system/governance/constitutional_auditor.py::_discover_checks",
      "name": "_discover_checks",
      "type": "FunctionDef",
      "file": "src/system/governance/constitutional_auditor.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Dynamically discovers check methods from modules in the 'checks' directory.",
      "docstring": "Dynamically discovers check methods from modules in the 'checks' directory.",
      "calls": [
        "Class",
        "Path",
        "append",
        "debug",
        "endswith",
        "error",
        "get",
        "getmembers",
        "glob",
        "import_module",
        "len",
        "sort",
        "startswith"
      ],
      "line_number": 87,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.807672+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/governance/constitutional_auditor.py::ConstitutionalAuditor",
      "structural_hash": "2c499bc27bff784b41670f456b6f427653a2905d8ceaba92a63d69ba9bd99472"
    },
    "src/system/governance/constitutional_auditor.py::run_full_audit": {
      "key": "src/system/governance/constitutional_auditor.py::run_full_audit",
      "name": "run_full_audit",
      "type": "FunctionDef",
      "file": "src/system/governance/constitutional_auditor.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Run all discovered validation phases and return overall status.",
      "docstring": "Run all discovered validation phases and return overall status.",
      "calls": [
        "AuditFinding",
        "Panel",
        "_report_final_status",
        "any",
        "append",
        "check_fn",
        "error",
        "extend",
        "info",
        "print",
        "warning"
      ],
      "line_number": 118,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.810153+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/governance/constitutional_auditor.py::ConstitutionalAuditor",
      "structural_hash": "202b834eec644f47db3ebae696cbedf6794b86d6dbbd2eb3e26373618e08020a"
    },
    "src/system/governance/constitutional_auditor.py::_report_final_status": {
      "key": "src/system/governance/constitutional_auditor.py::_report_final_status",
      "name": "_report_final_status",
      "type": "FunctionDef",
      "file": "src/system/governance/constitutional_auditor.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Prints the final audit summary to the console.",
      "docstring": "Prints the final audit summary to the console.",
      "calls": [
        "Panel",
        "Table",
        "add_row",
        "len",
        "print"
      ],
      "line_number": 140,
      "is_async": false,
      "parameters": [
        "self",
        "passed"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.812287+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/governance/constitutional_auditor.py::ConstitutionalAuditor",
      "structural_hash": "33be600348d1bb13399550f0f0d4828c3f994e7c8061daba8436dfacf4342064"
    },
    "src/system/governance/constitutional_auditor.py::main": {
      "key": "src/system/governance/constitutional_auditor.py::main",
      "name": "main",
      "type": "FunctionDef",
      "file": "src/system/governance/constitutional_auditor.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "CLI entry point for the Constitutional Auditor.",
      "docstring": "CLI entry point for the Constitutional Auditor.",
      "calls": [
        "ConstitutionalAuditor",
        "error",
        "exit",
        "load_dotenv",
        "run_full_audit"
      ],
      "line_number": 160,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "cli_entry_point",
      "last_updated": "2025-08-11T16:43:38.813673+00:00",
      "is_class": false,
      "base_classes": [],
      "structural_hash": "0512986f38fde3ebe727c4a8040ab129cb043d2cbdc1a6cd78242ecf659b7cd9"
    },
    "src/system/governance/models.py::AuditSeverity": {
      "key": "src/system/governance/models.py::AuditSeverity",
      "name": "AuditSeverity",
      "type": "ClassDef",
      "file": "src/system/governance/models.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Severity levels for audit findings.",
      "docstring": "Severity levels for audit findings.",
      "calls": [],
      "line_number": 9,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "enum",
      "last_updated": "2025-08-11T16:43:38.814633+00:00",
      "is_class": true,
      "base_classes": [
        "Enum"
      ],
      "entry_point_justification": "enum_definition",
      "structural_hash": "4e9c4d1c06d98ad21e0b44b0c5701a5d29a10efc8ffc6c837ff3f3c02a8dc7b5"
    },
    "src/system/governance/models.py::AuditFinding": {
      "key": "src/system/governance/models.py::AuditFinding",
      "name": "AuditFinding",
      "type": "ClassDef",
      "file": "src/system/governance/models.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Represents a single audit finding.",
      "docstring": "Represents a single audit finding.",
      "calls": [],
      "line_number": 16,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.815207+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "eb4ee3ee5de56a07d5d6c239b10bf3601ac8adf55ed605ad9696b345824b38cc"
    },
    "src/system/admin/fixer.py::register": {
      "key": "src/system/admin/fixer.py::register",
      "name": "register",
      "type": "FunctionDef",
      "file": "src/system/admin/fixer.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Register fixer commands under the admin CLI.",
      "docstring": "Intent: Register fixer commands under the admin CLI.",
      "calls": [
        "Typer",
        "add_typer",
        "command"
      ],
      "line_number": 9,
      "is_async": false,
      "parameters": [
        "app"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.816160+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "d119c5b236f246f150742a03612a188eafab5729006bc730807f45ebeea870dc"
    },
    "src/system/admin/scaffolder.py::new_project": {
      "key": "src/system/admin/scaffolder.py::new_project",
      "name": "new_project",
      "type": "FunctionDef",
      "file": "src/system/admin/scaffolder.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Scaffolds a new, constitutionally-governed \"Mind/Body\" application.",
      "docstring": "Scaffolds a new, constitutionally-governed \"Mind/Body\" application.",
      "calls": [
        "Argument",
        "Exit",
        "Option",
        "copy",
        "dump",
        "echo",
        "error",
        "exists",
        "format",
        "glob",
        "info",
        "is_dir",
        "mkdir",
        "read_text",
        "replace",
        "safe_load",
        "secho",
        "write_text"
      ],
      "line_number": 21,
      "is_async": false,
      "parameters": [
        "name",
        "profile",
        "dry_run"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.821993+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "6d72efce8e179e81549fd1b44b0da1e33a2341243b77054ea4893be090a77197"
    },
    "src/system/admin/scaffolder.py::register": {
      "key": "src/system/admin/scaffolder.py::register",
      "name": "register",
      "type": "FunctionDef",
      "file": "src/system/admin/scaffolder.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Register scaffolding commands under the admin CLI.",
      "docstring": "Intent: Register scaffolding commands under the admin CLI.",
      "calls": [
        "command"
      ],
      "line_number": 108,
      "is_async": false,
      "parameters": [
        "app"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.823172+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "98a02f61f0147d221e2de031f8dfbda197edfb760acee3919cfffb0b36db72ed"
    },
    "src/system/admin/migrator.py::register": {
      "key": "src/system/admin/migrator.py::register",
      "name": "register",
      "type": "FunctionDef",
      "file": "src/system/admin/migrator.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Register migration commands (manifest-migrator) under the admin CLI.",
      "docstring": "Register migration commands (manifest-migrator) under the admin CLI.",
      "calls": [
        "command"
      ],
      "line_number": 9,
      "is_async": false,
      "parameters": [
        "app"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.824022+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "ee9a8abfbb298a749103bd155c86721fec7987d013d5becda3e106fb4bade877"
    },
    "src/system/admin/utils.py::should_fail": {
      "key": "src/system/admin/utils.py::should_fail",
      "name": "should_fail",
      "type": "FunctionDef",
      "file": "src/system/admin/utils.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Determines if the CLI should exit with an error code based on the drift report and the specified fail condition (missing, undeclared, or any drift).",
      "docstring": "Determines if the CLI should exit with an error code based on the drift report and the specified fail condition (missing, undeclared, or any drift).",
      "calls": [
        "bool",
        "get"
      ],
      "line_number": 24,
      "is_async": false,
      "parameters": [
        "report",
        "fail_on"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.826164+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "b1c674a58b7314a9b247aa1057a5f0f14300b70dcac27f78d7a51e3b05fab535"
    },
    "src/system/admin/utils.py::load_yaml_file": {
      "key": "src/system/admin/utils.py::load_yaml_file",
      "name": "load_yaml_file",
      "type": "FunctionDef",
      "file": "src/system/admin/utils.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Load YAML for governance operations. Returns {} for empty documents.",
      "docstring": "Intent: Load YAML for governance operations. Returns {} for empty documents.",
      "calls": [
        "read_text",
        "safe_load"
      ],
      "line_number": 35,
      "is_async": false,
      "parameters": [
        "path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.826932+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "5a564a8a94624020ca0e40a9a24f12f7ac7d3076a3b58c9e8ebf7dc44de64648"
    },
    "src/system/admin/utils.py::save_yaml_file": {
      "key": "src/system/admin/utils.py::save_yaml_file",
      "name": "save_yaml_file",
      "type": "FunctionDef",
      "file": "src/system/admin/utils.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Persist YAML with stable ordering disabled to preserve human readability.",
      "docstring": "Intent: Persist YAML with stable ordering disabled to preserve human readability.",
      "calls": [
        "safe_dump",
        "write_text"
      ],
      "line_number": 41,
      "is_async": false,
      "parameters": [
        "path",
        "data"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.827677+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "396e43a31e3708070b0f283c8fb3a594575339632e811def2c3dd0619e7da1d8"
    },
    "src/system/admin/utils.py::generate_approval_token": {
      "key": "src/system/admin/utils.py::generate_approval_token",
      "name": "generate_approval_token",
      "type": "FunctionDef",
      "file": "src/system/admin/utils.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Produce a deterministic content-bound token for cryptographic proposal approvals.",
      "docstring": "Intent: Produce a deterministic content-bound token for cryptographic proposal approvals.",
      "calls": [
        "Hash",
        "SHA256",
        "encode",
        "finalize",
        "hex",
        "update"
      ],
      "line_number": 46,
      "is_async": false,
      "parameters": [
        "proposal_content"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.828539+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "4ee6fd44a30c5160810bdec931534a39a8fa715aaa87c9c2865adf294e2aed6e"
    },
    "src/system/admin/utils.py::load_private_key": {
      "key": "src/system/admin/utils.py::load_private_key",
      "name": "load_private_key",
      "type": "FunctionDef",
      "file": "src/system/admin/utils.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Load the operator's Ed25519 private key from the protected key store.",
      "docstring": "Intent: Load the operator's Ed25519 private key from the protected key store.",
      "calls": [
        "SystemExit",
        "error",
        "exists",
        "load_pem_private_key",
        "read_bytes"
      ],
      "line_number": 53,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.829508+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "e15a9023fa487ddc0be154c679571342a8e2b9eabb9831769d4e0728d6d8920f"
    },
    "src/system/admin/utils.py::archive_rollback_plan": {
      "key": "src/system/admin/utils.py::archive_rollback_plan",
      "name": "archive_rollback_plan",
      "type": "FunctionDef",
      "file": "src/system/admin/utils.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Persist a rollback plan snapshot for approved proposals under .intent/constitution/rollbacks/.",
      "docstring": "Intent: Persist a rollback plan snapshot for approved proposals under .intent/constitution/rollbacks/.",
      "calls": [
        "dumps",
        "get",
        "info",
        "mkdir",
        "strftime",
        "utcnow",
        "write_text"
      ],
      "line_number": 63,
      "is_async": false,
      "parameters": [
        "proposal_name",
        "proposal"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.831092+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "01a6e3cdb18120030b1eb3773ab19be13383b9dad8ef19407044bbb56d658897"
    },
    "src/system/admin/byor.py::initialize_repository": {
      "key": "src/system/admin/byor.py::initialize_repository",
      "name": "initialize_repository",
      "type": "FunctionDef",
      "file": "src/system/admin/byor.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Analyzes an external repository and scaffolds a minimal `.intent/` constitution.",
      "docstring": "Analyzes an external repository and scaffolds a minimal `.intent/` constitution.",
      "calls": [
        "Argument",
        "Exit",
        "KnowledgeGraphBuilder",
        "Option",
        "build",
        "dump",
        "echo",
        "error",
        "get",
        "info",
        "isinstance",
        "items",
        "len",
        "list",
        "mkdir",
        "read_text",
        "secho",
        "set",
        "sorted",
        "values",
        "write_text"
      ],
      "line_number": 22,
      "is_async": false,
      "parameters": [
        "path",
        "dry_run"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.836519+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "fed0aaf58b34f3db6c1ad3b18314c3f09dc21b4912eb0ed822e5e4c9be1cb08e"
    },
    "src/system/admin/byor.py::register": {
      "key": "src/system/admin/byor.py::register",
      "name": "register",
      "type": "FunctionDef",
      "file": "src/system/admin/byor.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Register BYOR commands (e.g., `byor-init`) under the admin CLI.",
      "docstring": "Register BYOR commands (e.g., `byor-init`) under the admin CLI.",
      "calls": [
        "command"
      ],
      "line_number": 117,
      "is_async": false,
      "parameters": [
        "app"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.837679+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "4dbcc180e75c303fa1d712cc87918be27fd25ca3d44f3d970b5d982761a41664"
    },
    "src/system/admin/proposals.py::register": {
      "key": "src/system/admin/proposals.py::register",
      "name": "register",
      "type": "FunctionDef",
      "file": "src/system/admin/proposals.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Register proposal lifecycle commands under the admin CLI.",
      "docstring": "Intent: Register proposal lifecycle commands under the admin CLI.",
      "calls": [
        "Argument",
        "ConstitutionalAuditor",
        "Exit",
        "Path",
        "TemporaryDirectory",
        "Typer",
        "add_typer",
        "any",
        "append",
        "archive_rollback_plan",
        "b64decode",
        "b64encode",
        "command",
        "decode",
        "encode",
        "endswith",
        "error",
        "exists",
        "generate_approval_token",
        "get",
        "glob",
        "info",
        "isinstance",
        "isoformat",
        "len",
        "load_pem_public_key",
        "load_private_key",
        "load_yaml_file",
        "mkdir",
        "prompt",
        "proposals_approve",
        "proposals_list",
        "proposals_sign",
        "run",
        "run_full_audit",
        "save_yaml_file",
        "setdefault",
        "sign",
        "sorted",
        "str",
        "strip",
        "unlink",
        "utcnow",
        "verify",
        "warning",
        "write_text"
      ],
      "line_number": 36,
      "is_async": false,
      "parameters": [
        "app"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.851878+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "f84277145add2c07d9c4cec1f0a7a08ce04cdc4061185c9c7e9b4e13c4a1f41e"
    },
    "src/system/admin/proposals.py::proposals_list": {
      "key": "src/system/admin/proposals.py::proposals_list",
      "name": "proposals_list",
      "type": "FunctionDef",
      "file": "src/system/admin/proposals.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "List pending constitutional proposals and display their justification, target path, and signature/quorum status.",
      "docstring": "List pending constitutional proposals and display their justification, target path, and signature/quorum status.",
      "calls": [
        "any",
        "command",
        "endswith",
        "get",
        "glob",
        "info",
        "len",
        "load_yaml_file",
        "mkdir",
        "sorted",
        "strip"
      ],
      "line_number": 39,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.854468+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "3f2580b6fee11b461cb6f0c98a933ada9d8e8674bdb7bf1d2d7efaa65b9fd62b"
    },
    "src/system/admin/proposals.py::proposals_sign": {
      "key": "src/system/admin/proposals.py::proposals_sign",
      "name": "proposals_sign",
      "type": "FunctionDef",
      "file": "src/system/admin/proposals.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Sign a proposal with the operator's private key (content-bound token).",
      "docstring": "Intent: Sign a proposal with the operator's private key (content-bound token).",
      "calls": [
        "Argument",
        "Exit",
        "append",
        "b64encode",
        "command",
        "decode",
        "encode",
        "error",
        "exists",
        "generate_approval_token",
        "get",
        "info",
        "isoformat",
        "load_private_key",
        "load_yaml_file",
        "prompt",
        "save_yaml_file",
        "setdefault",
        "sign",
        "utcnow"
      ],
      "line_number": 68,
      "is_async": false,
      "parameters": [
        "proposal_name"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.856861+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "67334960b9b08816bd27fdefea58d69811194ee00fbf399a3e0b56fb05709f9c"
    },
    "src/system/admin/proposals.py::proposals_approve": {
      "key": "src/system/admin/proposals.py::proposals_approve",
      "name": "proposals_approve",
      "type": "FunctionDef",
      "file": "src/system/admin/proposals.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Verify signatures/quorum, run a canary constitutional audit, then apply the proposal if valid.",
      "docstring": "Verify signatures/quorum, run a canary constitutional audit, then apply the proposal if valid.",
      "calls": [
        "Argument",
        "ConstitutionalAuditor",
        "Exit",
        "Path",
        "TemporaryDirectory",
        "any",
        "archive_rollback_plan",
        "b64decode",
        "command",
        "decode",
        "encode",
        "endswith",
        "error",
        "exists",
        "generate_approval_token",
        "get",
        "info",
        "isinstance",
        "load_pem_public_key",
        "load_yaml_file",
        "mkdir",
        "run",
        "run_full_audit",
        "str",
        "unlink",
        "verify",
        "warning",
        "write_text"
      ],
      "line_number": 101,
      "is_async": false,
      "parameters": [
        "proposal_name"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.863143+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "2f73765018b292716368099487b112640766245bbe1854ec304e57176456f329"
    },
    "src/system/admin/proposals.py::_group_list": {
      "key": "src/system/admin/proposals.py::_group_list",
      "name": "_group_list",
      "type": "FunctionDef",
      "file": "src/system/admin/proposals.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Group alias for proposals-list (namespaced UX).",
      "docstring": "Intent: Group alias for proposals-list (namespaced UX).",
      "calls": [
        "command",
        "proposals_list"
      ],
      "line_number": 204,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.864601+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "be676c7fcb85be2d60e86b995f6927f137c94cd2c49900902a647d9950f793db"
    },
    "src/system/admin/proposals.py::_group_sign": {
      "key": "src/system/admin/proposals.py::_group_sign",
      "name": "_group_sign",
      "type": "FunctionDef",
      "file": "src/system/admin/proposals.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Group alias for proposals-sign (namespaced UX).",
      "docstring": "Intent: Group alias for proposals-sign (namespaced UX).",
      "calls": [
        "command",
        "proposals_sign"
      ],
      "line_number": 209,
      "is_async": false,
      "parameters": [
        "proposal_name"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.865169+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "1ffed0e089f3ffc1cf72352e27badfd17a24e4cdb2ee12a61dfb1ffeeb850a7b"
    },
    "src/system/admin/proposals.py::_group_approve": {
      "key": "src/system/admin/proposals.py::_group_approve",
      "name": "_group_approve",
      "type": "FunctionDef",
      "file": "src/system/admin/proposals.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Group alias for proposals-approve (namespaced UX).",
      "docstring": "Intent: Group alias for proposals-approve (namespaced UX).",
      "calls": [
        "command",
        "proposals_approve"
      ],
      "line_number": 214,
      "is_async": false,
      "parameters": [
        "proposal_name"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.865758+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "c59db60b0344cc6dbeeb92a6c56bf5eac3321dddf247399b470e73137a50ea69"
    },
    "src/system/admin/guard.py::_find_manifest_path": {
      "key": "src/system/admin/guard.py::_find_manifest_path",
      "name": "_find_manifest_path",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the system domain.",
      "docstring": "",
      "calls": [
        "FileNotFoundError",
        "exists"
      ],
      "line_number": 36,
      "is_async": false,
      "parameters": [
        "root",
        "explicit"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.870314+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "e32348095e5b9828fb3244f44747078fc23b726f680e0eb982fbb0adf6879801"
    },
    "src/system/admin/guard.py::_load_raw_manifest": {
      "key": "src/system/admin/guard.py::_load_raw_manifest",
      "name": "_load_raw_manifest",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Loads and parses a YAML manifest file from the given root or explicit path, returning its contents as a dictionary.",
      "docstring": "Loads and parses a YAML manifest file from the given root or explicit path, returning its contents as a dictionary.",
      "calls": [
        "_find_manifest_path",
        "read_text",
        "safe_load"
      ],
      "line_number": 44,
      "is_async": false,
      "parameters": [
        "root",
        "explicit"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.871181+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "31df9fda9f8e60554f66a24794b202bbd107f66b69ec0c2d8df8879c04336bdb"
    },
    "src/system/admin/guard.py::_ux_defaults": {
      "key": "src/system/admin/guard.py::_ux_defaults",
      "name": "_ux_defaults",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the system domain.",
      "docstring": "",
      "calls": [
        "_load_raw_manifest",
        "bool",
        "get"
      ],
      "line_number": 50,
      "is_async": false,
      "parameters": [
        "root",
        "explicit"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.874297+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "4c684bc024b485fc6d927dba5b1556e9d68e864778d620e2ebc746ee6513ec90"
    },
    "src/system/admin/guard.py::_is_clean": {
      "key": "src/system/admin/guard.py::_is_clean",
      "name": "_is_clean",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the system domain.",
      "docstring": "",
      "calls": [
        "get"
      ],
      "line_number": 69,
      "is_async": false,
      "parameters": [
        "report"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.875282+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "af9737740143bfe33d0dab569efc15ac4ba0f7d8ad246ed48a3a5f5d5019d95c"
    },
    "src/system/admin/guard.py::_print_table": {
      "key": "src/system/admin/guard.py::_print_table",
      "name": "_print_table",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the system domain.",
      "docstring": "",
      "calls": [
        "Table",
        "_is_clean",
        "add_column",
        "add_row",
        "append",
        "fit",
        "get",
        "join",
        "row",
        "rprint"
      ],
      "line_number": 72,
      "is_async": false,
      "parameters": [
        "report_dict",
        "labels"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.879706+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "d38946831cbe0f2cd20a014e899d081136ebc7fbd9968d1194211b236341032b"
    },
    "src/system/admin/guard.py::row": {
      "key": "src/system/admin/guard.py::row",
      "name": "row",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Formats and adds a row to a table with a title and a list of items, highlighting empty lists in green and non-empty lists in yellow.",
      "docstring": "Formats and adds a row to a table with a title and a list of items, highlighting empty lists in green and non-empty lists in yellow.",
      "calls": [
        "add_row",
        "join"
      ],
      "line_number": 77,
      "is_async": false,
      "parameters": [
        "title",
        "items"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.881458+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "7a0e48cf5f4810d06c270e8041de6656253dd75bdb15ad03ee3196e0b4b827e6"
    },
    "src/system/admin/guard.py::_print_pretty": {
      "key": "src/system/admin/guard.py::_print_pretty",
      "name": "_print_pretty",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the system domain.",
      "docstring": "",
      "calls": [
        "Panel",
        "_is_clean",
        "_print_table",
        "get",
        "len",
        "rprint"
      ],
      "line_number": 105,
      "is_async": false,
      "parameters": [
        "report_dict",
        "labels"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.884280+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "970c77ce2837555a0d0dfeafe0b7e499116bafb5bc408a8de17ec4275f376193"
    },
    "src/system/admin/guard.py::register": {
      "key": "src/system/admin/guard.py::register",
      "name": "register",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the system domain.",
      "docstring": "",
      "calls": [
        "Exit",
        "Option",
        "Path",
        "Typer",
        "_print_pretty",
        "_print_table",
        "_ux_defaults",
        "add_typer",
        "bool",
        "collect_code_capabilities",
        "command",
        "detect_capability_drift",
        "dumps",
        "echo",
        "items",
        "len",
        "load_manifest",
        "lower",
        "mkdir",
        "rprint",
        "should_fail",
        "sorted",
        "to_dict",
        "write_report",
        "write_text"
      ],
      "line_number": 124,
      "is_async": false,
      "parameters": [
        "app"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.891746+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "26a10ce6edad71c2e37d05fc2b536efa6273929692e4e4bc000aa26e4c500a01"
    },
    "src/system/admin/guard.py::drift": {
      "key": "src/system/admin/guard.py::drift",
      "name": "drift",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Compare manifest vs code to detect capability drift; write JSON evidence for CI.",
      "docstring": "Intent: Compare manifest vs code to detect capability drift; write JSON evidence for CI.",
      "calls": [
        "Exit",
        "Option",
        "Path",
        "_print_pretty",
        "_print_table",
        "_ux_defaults",
        "bool",
        "collect_code_capabilities",
        "command",
        "detect_capability_drift",
        "dumps",
        "echo",
        "load_manifest",
        "lower",
        "should_fail",
        "to_dict",
        "write_report"
      ],
      "line_number": 129,
      "is_async": false,
      "parameters": [
        "root",
        "manifest_path",
        "output",
        "format",
        "fail_on",
        "include",
        "exclude",
        "strict_intent"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.896025+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "5be8b3155942deb85980ceec498cba2ab51083e40b02995758a3078042aa4f5f"
    },
    "src/system/admin/guard.py::kg_export": {
      "key": "src/system/admin/guard.py::kg_export",
      "name": "kg_export",
      "type": "FunctionDef",
      "file": "src/system/admin/guard.py",
      "domain": "system",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Intent: Emit a minimal knowledge-graph artifact with capability nodes.",
      "docstring": "Intent: Emit a minimal knowledge-graph artifact with capability nodes.\nFormat: { \"nodes\": [ {\"capability\": \"...\", \"domain\": \"x\", \"owner\": \"y\"}, ... ] }",
      "calls": [
        "Option",
        "Path",
        "collect_code_capabilities",
        "command",
        "dumps",
        "items",
        "len",
        "lower",
        "mkdir",
        "rprint",
        "sorted",
        "write_text"
      ],
      "line_number": 168,
      "is_async": false,
      "parameters": [
        "root",
        "output",
        "include",
        "exclude",
        "prefer"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.899713+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "094cc19d88b21f43e2f1814ca03e39a2b841f600297c394c65bf131921b11a01"
    },
    "src/system/admin/keys.py::register": {
      "key": "src/system/admin/keys.py::register",
      "name": "register",
      "type": "FunctionDef",
      "file": "src/system/admin/keys.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Register key management commands under the admin CLI.",
      "docstring": "Intent: Register key management commands under the admin CLI.",
      "calls": [
        "Argument",
        "NoEncryption",
        "chmod",
        "command",
        "confirm",
        "decode",
        "dump",
        "exists",
        "generate",
        "info",
        "mkdir",
        "print",
        "private_bytes",
        "public_bytes",
        "public_key",
        "write_bytes"
      ],
      "line_number": 20,
      "is_async": false,
      "parameters": [
        "app"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.904347+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "45b1bf1d9b8058d6b6d163fbe77f0a3f3247e146b2fda94ec421e4fdb0cd2d64"
    },
    "src/system/admin/keys.py::keygen": {
      "key": "src/system/admin/keys.py::keygen",
      "name": "keygen",
      "type": "FunctionDef",
      "file": "src/system/admin/keys.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Intent: Generate a new Ed25519 key pair and print an approver YAML block.",
      "docstring": "Intent: Generate a new Ed25519 key pair and print an approver YAML block.",
      "calls": [
        "Argument",
        "NoEncryption",
        "chmod",
        "command",
        "confirm",
        "decode",
        "dump",
        "exists",
        "generate",
        "info",
        "mkdir",
        "print",
        "private_bytes",
        "public_bytes",
        "public_key",
        "write_bytes"
      ],
      "line_number": 23,
      "is_async": false,
      "parameters": [
        "identity"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.906659+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "14fd3940cd7711b0bced8ef503a8d523385936748d6251684e23a8478149ca90"
    },
    "src/system/tools/change_log_updater.py::load_existing_log": {
      "key": "src/system/tools/change_log_updater.py::load_existing_log",
      "name": "load_existing_log",
      "type": "FunctionDef",
      "file": "src/system/tools/change_log_updater.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Loads the existing change log from disk or returns a new structure.",
      "docstring": "Loads the existing change log from disk or returns a new structure.",
      "calls": [
        "load_config"
      ],
      "line_number": 16,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.908407+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "045e84467919732fd82a3d8d4ca3e8605bc8f656d720c345b587d57e0312ffe8"
    },
    "src/system/tools/change_log_updater.py::append_change_entry": {
      "key": "src/system/tools/change_log_updater.py::append_change_entry",
      "name": "append_change_entry",
      "type": "FunctionDef",
      "file": "src/system/tools/change_log_updater.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Appends a new, structured entry to the metacode change log.",
      "docstring": "Appends a new, structured entry to the metacode change log.",
      "calls": [
        "append",
        "dumps",
        "info",
        "isoformat",
        "load_existing_log",
        "mkdir",
        "utcnow",
        "write_text"
      ],
      "line_number": 24,
      "is_async": false,
      "parameters": [
        "task",
        "step",
        "modified_files",
        "score",
        "violations"
      ],
      "entry_point_type": "cli_entry_point",
      "last_updated": "2025-08-11T16:43:38.909795+00:00",
      "is_class": false,
      "base_classes": [],
      "structural_hash": "c59edf7a035cfaf6190faeae846f49a4c3c34922515c4e64babf0800aa6797ab"
    },
    "src/system/tools/codegraph_builder.py::_strip_docstrings": {
      "key": "src/system/tools/codegraph_builder.py::_strip_docstrings",
      "name": "_strip_docstrings",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Recursively remove docstring nodes from an AST tree.",
      "docstring": "Recursively remove docstring nodes from an AST tree.",
      "calls": [
        "_strip_docstrings",
        "isinstance",
        "iter_child_nodes"
      ],
      "line_number": 17,
      "is_async": false,
      "parameters": [
        "node"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.918506+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "e3c656ea2e10afe46346850fd74a5afe1244dbbdab3e3a9566e0c17bb3135fe2"
    },
    "src/system/tools/codegraph_builder.py::FunctionInfo": {
      "key": "src/system/tools/codegraph_builder.py::FunctionInfo",
      "name": "FunctionInfo",
      "type": "ClassDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "A data structure holding all analyzed information about a single symbol (function or class).",
      "docstring": "A data structure holding all analyzed information about a single symbol (function or class).",
      "calls": [
        "field"
      ],
      "line_number": 30,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.919954+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "0834d2305e61040cca64fa89bcab2ff57afd635892ef582dc040723fb34040df"
    },
    "src/system/tools/codegraph_builder.py::ProjectStructureError": {
      "key": "src/system/tools/codegraph_builder.py::ProjectStructureError",
      "name": "ProjectStructureError",
      "type": "ClassDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Custom exception for when the project's root cannot be determined.",
      "docstring": "Custom exception for when the project's root cannot be determined.",
      "calls": [],
      "line_number": 54,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.920624+00:00",
      "is_class": true,
      "base_classes": [
        "Exception"
      ],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "864b1a1df341c21e17b30657d9044bd319c77d991c0135d673dd5e14e10c0cd3"
    },
    "src/system/tools/codegraph_builder.py::find_project_root": {
      "key": "src/system/tools/codegraph_builder.py::find_project_root",
      "name": "find_project_root",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Traverses upward from a starting path to find the project root, marked by 'pyproject.toml'.",
      "docstring": "Traverses upward from a starting path to find the project root, marked by 'pyproject.toml'.",
      "calls": [
        "ProjectStructureError",
        "exists",
        "resolve"
      ],
      "line_number": 58,
      "is_async": false,
      "parameters": [
        "start_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.921479+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "0b0564196641e7941214ad29e72ab4c10e1d22d852fdc72c05922945d647e25c"
    },
    "src/system/tools/codegraph_builder.py::FunctionCallVisitor": {
      "key": "src/system/tools/codegraph_builder.py::FunctionCallVisitor",
      "name": "FunctionCallVisitor",
      "type": "ClassDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "An AST visitor that collects the names of all functions being called within a node.",
      "docstring": "An AST visitor that collects the names of all functions being called within a node.",
      "calls": [
        "add",
        "generic_visit",
        "isinstance",
        "set"
      ],
      "line_number": 69,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.922622+00:00",
      "is_class": true,
      "base_classes": [
        "NodeVisitor"
      ],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "0645aa2e28bffc32027809f7b94a005ef6538a4295da20ad4e1dddd908eaa115"
    },
    "src/system/tools/codegraph_builder.py::__init__": {
      "key": "src/system/tools/codegraph_builder.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Initializes the builder, loading patterns and project configuration.",
      "docstring": "Initializes the builder, loading patterns and project configuration.",
      "calls": [
        "_get_cli_entry_points",
        "_get_domain_map",
        "_load_patterns",
        "resolve"
      ],
      "line_number": 107,
      "is_async": false,
      "parameters": [
        "self",
        "root_path",
        "exclude_patterns"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.946956+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "84dff06c76a72c2ce61aff340335153b24684af7d122c9554d1115ef23afe76a"
    },
    "src/system/tools/codegraph_builder.py::visit_Call": {
      "key": "src/system/tools/codegraph_builder.py::visit_Call",
      "name": "visit_Call",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Records function or method calls in `self.calls` and recursively visits child nodes.",
      "docstring": "Records function or method calls in `self.calls` and recursively visits child nodes.",
      "calls": [
        "add",
        "generic_visit",
        "isinstance"
      ],
      "line_number": 74,
      "is_async": false,
      "parameters": [
        "self",
        "node"
      ],
      "entry_point_type": "visitor_method",
      "last_updated": "2025-08-11T16:43:38.924098+00:00",
      "is_class": false,
      "base_classes": [
        "NodeVisitor"
      ],
      "entry_point_justification": "ast_visitor_method",
      "parent_class_key": "src/system/tools/codegraph_builder.py::FunctionCallVisitor",
      "structural_hash": "bab8953d0f192e3ad33731686353b006217f77d7d00d45c684d77c0b6d248dc7"
    },
    "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder": {
      "key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "name": "KnowledgeGraphBuilder",
      "type": "ClassDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "manifest_updating",
      "intent": "Builds a comprehensive JSON representation of the project's code structure and relationships.",
      "docstring": "Builds a comprehensive JSON representation of the project's code structure and relationships.",
      "calls": [
        "ContextAwareVisitor",
        "FunctionCallVisitor",
        "FunctionInfo",
        "Path",
        "_apply_entry_point_patterns",
        "_determine_domain",
        "_get_cli_entry_points",
        "_get_domain_map",
        "_get_entry_point_type",
        "_infer_agent_from_path",
        "_infer_domains_from_directory_structure",
        "_load_patterns",
        "_parse_metadata_comment",
        "_process_symbol_node",
        "_should_exclude_path",
        "_strip_docstrings",
        "any",
        "append",
        "as_posix",
        "asdict",
        "encode",
        "error",
        "exists",
        "extend",
        "findall",
        "generic_visit",
        "get",
        "get_docstring",
        "group",
        "hasattr",
        "hexdigest",
        "info",
        "is_dir",
        "isinstance",
        "isoformat",
        "items",
        "iterdir",
        "len",
        "list",
        "load_config",
        "lower",
        "match",
        "max",
        "now",
        "parse",
        "read_text",
        "relative_to",
        "resolve",
        "rglob",
        "scan_file",
        "search",
        "set",
        "sha256",
        "sorted",
        "split",
        "splitlines",
        "startswith",
        "str",
        "strip",
        "unparse",
        "update",
        "values",
        "visit",
        "walk",
        "warning"
      ],
      "line_number": 81,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.941134+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "76ea9fe903a3ab603154c3aeb3fbc43763f176ec22408f42b4be8e9131a20ecc"
    },
    "src/system/tools/codegraph_builder.py::ContextAwareVisitor": {
      "key": "src/system/tools/codegraph_builder.py::ContextAwareVisitor",
      "name": "ContextAwareVisitor",
      "type": "ClassDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "A stateful AST visitor that understands class context for methods.",
      "docstring": "A stateful AST visitor that understands class context for methods.",
      "calls": [
        "_process_symbol_node",
        "generic_visit"
      ],
      "line_number": 84,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.942846+00:00",
      "is_class": true,
      "base_classes": [
        "NodeVisitor"
      ],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "c27587d21b7bec2437ac70273878d45d96e7533e90df180be8a314f810a064b5"
    },
    "src/system/tools/codegraph_builder.py::visit_ClassDef": {
      "key": "src/system/tools/codegraph_builder.py::visit_ClassDef",
      "name": "visit_ClassDef",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the tooling domain.",
      "docstring": "",
      "calls": [
        "_process_symbol_node",
        "generic_visit"
      ],
      "line_number": 92,
      "is_async": false,
      "parameters": [
        "self",
        "node"
      ],
      "entry_point_type": "visitor_method",
      "last_updated": "2025-08-11T16:43:38.944357+00:00",
      "is_class": false,
      "base_classes": [
        "NodeVisitor"
      ],
      "entry_point_justification": "ast_visitor_method",
      "parent_class_key": "src/system/tools/codegraph_builder.py::ContextAwareVisitor",
      "structural_hash": "32840121166f4cad08c3e9f49084fd4153c870c694dd8d0a8ac5e696e0dfa829"
    },
    "src/system/tools/codegraph_builder.py::visit_FunctionDef": {
      "key": "src/system/tools/codegraph_builder.py::visit_FunctionDef",
      "name": "visit_FunctionDef",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the tooling domain.",
      "docstring": "",
      "calls": [
        "_process_symbol_node",
        "generic_visit"
      ],
      "line_number": 99,
      "is_async": false,
      "parameters": [
        "self",
        "node"
      ],
      "entry_point_type": "visitor_method",
      "last_updated": "2025-08-11T16:43:38.945007+00:00",
      "is_class": false,
      "base_classes": [
        "NodeVisitor"
      ],
      "entry_point_justification": "ast_visitor_method",
      "parent_class_key": "src/system/tools/codegraph_builder.py::ContextAwareVisitor",
      "structural_hash": "5575e92236a49f499b960b253498937dd49abf57b9bf3314aa7144639c90a6ac"
    },
    "src/system/tools/codegraph_builder.py::visit_AsyncFunctionDef": {
      "key": "src/system/tools/codegraph_builder.py::visit_AsyncFunctionDef",
      "name": "visit_AsyncFunctionDef",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the tooling domain.",
      "docstring": "",
      "calls": [
        "_process_symbol_node",
        "generic_visit"
      ],
      "line_number": 103,
      "is_async": false,
      "parameters": [
        "self",
        "node"
      ],
      "entry_point_type": "visitor_method",
      "last_updated": "2025-08-11T16:43:38.945687+00:00",
      "is_class": false,
      "base_classes": [
        "NodeVisitor"
      ],
      "entry_point_justification": "ast_visitor_method",
      "parent_class_key": "src/system/tools/codegraph_builder.py::ContextAwareVisitor",
      "structural_hash": "9eb4e07a0033dee3b46da9ce918059d5ed7d4f8205b913eef2dcb3ce18cfad5f"
    },
    "src/system/tools/codegraph_builder.py::_load_patterns": {
      "key": "src/system/tools/codegraph_builder.py::_load_patterns",
      "name": "_load_patterns",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Loads entry point detection patterns from the intent file.",
      "docstring": "Loads entry point detection patterns from the intent file.",
      "calls": [
        "exists",
        "get",
        "load_config",
        "warning"
      ],
      "line_number": 120,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.947957+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "93b538fcdb2347bab6a939b9818f3ffeeaf6ccf428452277abbf7e156068a179"
    },
    "src/system/tools/codegraph_builder.py::_get_cli_entry_points": {
      "key": "src/system/tools/codegraph_builder.py::_get_cli_entry_points",
      "name": "_get_cli_entry_points",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Parses pyproject.toml to find declared command-line entry points.",
      "docstring": "Parses pyproject.toml to find declared command-line entry points.",
      "calls": [
        "exists",
        "findall",
        "group",
        "read_text",
        "search",
        "set"
      ],
      "line_number": 128,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.949052+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "f611763e9c0c9f8e7c8e1b024cf3765e6929a7d36cde3481298d04934e55d09c"
    },
    "src/system/tools/codegraph_builder.py::_should_exclude_path": {
      "key": "src/system/tools/codegraph_builder.py::_should_exclude_path",
      "name": "_should_exclude_path",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Determines if a given path should be excluded from scanning.",
      "docstring": "Determines if a given path should be excluded from scanning.",
      "calls": [
        "any"
      ],
      "line_number": 136,
      "is_async": false,
      "parameters": [
        "self",
        "path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.949787+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "4b0556303a7101535defeeffda6781fa10a10e69da843d9a17ad36758d2f15a2"
    },
    "src/system/tools/codegraph_builder.py::_infer_domains_from_directory_structure": {
      "key": "src/system/tools/codegraph_builder.py::_infer_domains_from_directory_structure",
      "name": "_infer_domains_from_directory_structure",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "A heuristic to guess domains if source_structure.yaml is missing.",
      "docstring": "A heuristic to guess domains if source_structure.yaml is missing.",
      "calls": [
        "Path",
        "as_posix",
        "info",
        "is_dir",
        "iterdir",
        "len",
        "startswith",
        "warning"
      ],
      "line_number": 140,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.951245+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "f8b8ced07296e2c991e1ddda1f89a69950d11e36193309c54c198829041f6057"
    },
    "src/system/tools/codegraph_builder.py::_get_domain_map": {
      "key": "src/system/tools/codegraph_builder.py::_get_domain_map",
      "name": "_get_domain_map",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Loads the domain-to-path mapping from the constitution, with a fallback",
      "docstring": "Loads the domain-to-path mapping from the constitution, with a fallback\nto inferring domains from the directory structure.",
      "calls": [
        "Path",
        "_infer_domains_from_directory_structure",
        "as_posix",
        "get",
        "load_config"
      ],
      "line_number": 159,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.952506+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "59a16ffe86a33da2a7d39a5e250a38cd64604790c8ec39ed2e98e1c374695ab2"
    },
    "src/system/tools/codegraph_builder.py::_determine_domain": {
      "key": "src/system/tools/codegraph_builder.py::_determine_domain",
      "name": "_determine_domain",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Determines the logical domain for a file path based on the longest matching prefix.",
      "docstring": "Determines the logical domain for a file path based on the longest matching prefix.",
      "calls": [
        "as_posix",
        "get",
        "max",
        "startswith"
      ],
      "line_number": 173,
      "is_async": false,
      "parameters": [
        "self",
        "file_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.953444+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "4cb174d60ebdee3eff31ad4ad04373f054818349142764484d972eeaf4c92aea"
    },
    "src/system/tools/codegraph_builder.py::_infer_agent_from_path": {
      "key": "src/system/tools/codegraph_builder.py::_infer_agent_from_path",
      "name": "_infer_agent_from_path",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Infers the most likely responsible agent based on keywords in the file path.",
      "docstring": "Infers the most likely responsible agent based on keywords in the file path.",
      "calls": [
        "any",
        "lower",
        "str"
      ],
      "line_number": 179,
      "is_async": false,
      "parameters": [
        "self",
        "relative_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.954579+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "cdbd81d3f6f2175950361cd6fe94611efb4f0e24de5e81e9d5aee0ffc9f80917"
    },
    "src/system/tools/codegraph_builder.py::_parse_metadata_comment": {
      "key": "src/system/tools/codegraph_builder.py::_parse_metadata_comment",
      "name": "_parse_metadata_comment",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Parses the line immediately preceding a symbol definition for a '# CAPABILITY:' tag.",
      "docstring": "Parses the line immediately preceding a symbol definition for a '# CAPABILITY:' tag.",
      "calls": [
        "group",
        "search",
        "startswith",
        "strip"
      ],
      "line_number": 189,
      "is_async": false,
      "parameters": [
        "self",
        "node",
        "source_lines"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.955737+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "36610026f4ca2eb3d8fe96e1edfc0c1d4bba6b6194f1533cdbd723cb18b7087f"
    },
    "src/system/tools/codegraph_builder.py::_get_entry_point_type": {
      "key": "src/system/tools/codegraph_builder.py::_get_entry_point_type",
      "name": "_get_entry_point_type",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Identifies decorator or CLI-based entry points for a function.",
      "docstring": "Identifies decorator or CLI-based entry points for a function.",
      "calls": [
        "isinstance"
      ],
      "line_number": 198,
      "is_async": false,
      "parameters": [
        "self",
        "node"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.957201+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "556244c2de4cded3f191d20c2136fe0a36af8535e86d4fb6535204d855c9706a"
    },
    "src/system/tools/codegraph_builder.py::scan_file": {
      "key": "src/system/tools/codegraph_builder.py::scan_file",
      "name": "scan_file",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Scans a single Python file, parsing its AST to extract all symbols.",
      "docstring": "Scans a single Python file, parsing its AST to extract all symbols.",
      "calls": [
        "ContextAwareVisitor",
        "FunctionCallVisitor",
        "error",
        "isinstance",
        "parse",
        "read_text",
        "set",
        "splitlines",
        "str",
        "update",
        "visit",
        "walk"
      ],
      "line_number": 209,
      "is_async": false,
      "parameters": [
        "self",
        "filepath"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.959680+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "3a5d46ccf1b223a2a916c065fd33fec6dcf52d96d2e895381128eb18df3a6b19"
    },
    "src/system/tools/codegraph_builder.py::_process_symbol_node": {
      "key": "src/system/tools/codegraph_builder.py::_process_symbol_node",
      "name": "_process_symbol_node",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Extracts and stores metadata from a single function or class AST node.",
      "docstring": "Extracts and stores metadata from a single function or class AST node.",
      "calls": [
        "FunctionCallVisitor",
        "FunctionInfo",
        "_determine_domain",
        "_get_entry_point_type",
        "_infer_agent_from_path",
        "_parse_metadata_comment",
        "_strip_docstrings",
        "append",
        "as_posix",
        "encode",
        "get",
        "get_docstring",
        "hasattr",
        "hexdigest",
        "isinstance",
        "isoformat",
        "now",
        "parse",
        "relative_to",
        "sha256",
        "split",
        "strip",
        "unparse",
        "visit"
      ],
      "line_number": 231,
      "is_async": false,
      "parameters": [
        "self",
        "node",
        "filepath",
        "source_lines",
        "parent_key"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.962981+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "5a43fc9a730131ae5599722f300af6ce5d130fbfc53ea61c81f28afbdc04b2bd"
    },
    "src/system/tools/codegraph_builder.py::_apply_entry_point_patterns": {
      "key": "src/system/tools/codegraph_builder.py::_apply_entry_point_patterns",
      "name": "_apply_entry_point_patterns",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Applies declarative patterns to identify non-obvious entry points.",
      "docstring": "Applies declarative patterns to identify non-obvious entry points.",
      "calls": [
        "any",
        "extend",
        "get",
        "match",
        "values"
      ],
      "line_number": 268,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.965469+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "36fa393fb3ef9894a214e0e4cf3176ec01b78e215ed70799a45e4cddd8594dec"
    },
    "src/system/tools/codegraph_builder.py::build": {
      "key": "src/system/tools/codegraph_builder.py::build",
      "name": "build",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Orchestrates the full knowledge graph generation process.",
      "docstring": "Orchestrates the full knowledge graph generation process.",
      "calls": [
        "_apply_entry_point_patterns",
        "_should_exclude_path",
        "asdict",
        "info",
        "isoformat",
        "items",
        "len",
        "list",
        "now",
        "rglob",
        "scan_file",
        "sorted",
        "values"
      ],
      "line_number": 290,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.967913+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/tools/codegraph_builder.py::KnowledgeGraphBuilder",
      "structural_hash": "334afad9e374d7acaa5c681dab3e4a4174ff57f5c1f55fa496c07d443653ee3c"
    },
    "src/system/tools/codegraph_builder.py::main": {
      "key": "src/system/tools/codegraph_builder.py::main",
      "name": "main",
      "type": "FunctionDef",
      "file": "src/system/tools/codegraph_builder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "CLI entry point to run the knowledge graph builder and save the output.",
      "docstring": "CLI entry point to run the knowledge graph builder and save the output.",
      "calls": [
        "KnowledgeGraphBuilder",
        "build",
        "cwd",
        "dumps",
        "error",
        "find_project_root",
        "info",
        "len",
        "load_dotenv",
        "mkdir",
        "write_text"
      ],
      "line_number": 312,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "cli_entry_point",
      "last_updated": "2025-08-11T16:43:38.969640+00:00",
      "is_class": false,
      "base_classes": [],
      "structural_hash": "d7bec690f654cffb54e1cea5acd4e933f00ffd825867ad532b3d0470e21221ed"
    },
    "src/system/tools/manifest_migrator.py::migrate_manifest": {
      "key": "src/system/tools/manifest_migrator.py::migrate_manifest",
      "name": "migrate_manifest",
      "type": "FunctionDef",
      "file": "src/system/tools/manifest_migrator.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Reads the monolithic manifest and splits it into per-domain manifests.",
      "docstring": "Reads the monolithic manifest and splits it into per-domain manifests.",
      "calls": [
        "Exit",
        "Option",
        "all",
        "append",
        "dump",
        "echo",
        "enumerate",
        "error",
        "exists",
        "get",
        "info",
        "items",
        "loads",
        "mkdir",
        "next",
        "read_text",
        "safe_load",
        "secho",
        "values",
        "warning",
        "write_text"
      ],
      "line_number": 23,
      "is_async": false,
      "parameters": [
        "dry_run"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.975875+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "bea5f20466ad27f164b36e0c9e7d911fa1dc6e045cc6f9265c8bdc40f8b3e2a4"
    },
    "src/system/tools/docstring_adder.py::add_docstring_to_function_line_based": {
      "key": "src/system/tools/docstring_adder.py::add_docstring_to_function_line_based",
      "name": "add_docstring_to_function_line_based",
      "type": "FunctionDef",
      "file": "src/system/tools/docstring_adder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Surgically inserts a docstring into source code using a line-based method.",
      "docstring": "Surgically inserts a docstring into source code using a line-based method.\nPreserves comments and formatting.",
      "calls": [
        "error",
        "insert",
        "join",
        "len",
        "lstrip",
        "splitlines",
        "strip",
        "warning"
      ],
      "line_number": 25,
      "is_async": false,
      "parameters": [
        "source_code",
        "line_number",
        "docstring"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.980890+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "d72e9a2c564cb36b9ff44d485c1325732d95b6dc2d070af4a5ed78f48f520138"
    },
    "src/system/tools/docstring_adder.py::generate_and_apply_docstring": {
      "key": "src/system/tools/docstring_adder.py::generate_and_apply_docstring",
      "name": "generate_and_apply_docstring",
      "type": "AsyncFunctionDef",
      "file": "src/system/tools/docstring_adder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Generates and applies a docstring for a single function.",
      "docstring": "Generates and applies a docstring for a single function.",
      "calls": [
        "add_docstring_to_function_line_based",
        "error",
        "exists",
        "get",
        "get_docstring",
        "info",
        "isinstance",
        "make_request_async",
        "parse",
        "read_text",
        "replace",
        "secho",
        "strip",
        "unparse",
        "walk",
        "warning",
        "write_text"
      ],
      "line_number": 56,
      "is_async": true,
      "parameters": [
        "target",
        "generator",
        "dry_run"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.984925+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "fd679d58a58b2d5eb678d361ad7956490aec55ed96e7f03fd9dc733e2187f60e"
    },
    "src/system/tools/docstring_adder.py::_async_main": {
      "key": "src/system/tools/docstring_adder.py::_async_main",
      "name": "_async_main",
      "type": "AsyncFunctionDef",
      "file": "src/system/tools/docstring_adder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "The core asynchronous logic for finding and fixing docstrings.",
      "docstring": "The core asynchronous logic for finding and fixing docstrings.",
      "calls": [
        "GeneratorClient",
        "Semaphore",
        "as_completed",
        "error",
        "exists",
        "generate_and_apply_docstring",
        "get",
        "info",
        "len",
        "loads",
        "read_text",
        "str",
        "track",
        "values",
        "worker"
      ],
      "line_number": 129,
      "is_async": true,
      "parameters": [
        "dry_run"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.987596+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "904262bb53ca5588509bf4eb9f7ecf51c2e6efd4c684d04f4844dd4826b770dd"
    },
    "src/system/tools/docstring_adder.py::worker": {
      "key": "src/system/tools/docstring_adder.py::worker",
      "name": "worker",
      "type": "AsyncFunctionDef",
      "file": "src/system/tools/docstring_adder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "unassigned",
      "intent": "Processes the target asynchronously with a semaphore, generating and applying a docstring using the provided generator (dry run if specified).",
      "docstring": "Processes the target asynchronously with a semaphore, generating and applying a docstring using the provided generator (dry run if specified).",
      "calls": [
        "generate_and_apply_docstring"
      ],
      "line_number": 156,
      "is_async": true,
      "parameters": [
        "target"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.988360+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "0313c26fd75e5488a390b7dc961cfd40420d366cc475cd06ee299221b874f0f9"
    },
    "src/system/tools/docstring_adder.py::fix_missing_docstrings": {
      "key": "src/system/tools/docstring_adder.py::fix_missing_docstrings",
      "name": "fix_missing_docstrings",
      "type": "FunctionDef",
      "file": "src/system/tools/docstring_adder.py",
      "domain": "tooling",
      "agent": "tooling_agent",
      "capability": "add_missing_docstrings",
      "intent": "Finds all functions with missing docstrings and uses an LLM to generate them.",
      "docstring": "Finds all functions with missing docstrings and uses an LLM to generate them.",
      "calls": [
        "Option",
        "_async_main",
        "run"
      ],
      "line_number": 169,
      "is_async": false,
      "parameters": [
        "dry_run"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.989058+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "structural_hash": "83ebcd1b1903e76fcc721f4193ab7760916089fc6dbae3f6f3c2a749dd0ad4cd"
    },
    "src/system/governance/checks/quality_checks.py::QualityChecks": {
      "key": "src/system/governance/checks/quality_checks.py::QualityChecks",
      "name": "QualityChecks",
      "type": "ClassDef",
      "file": "src/system/governance/checks/quality_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Container for code quality constitutional checks.",
      "docstring": "Container for code quality constitutional checks.",
      "calls": [
        "AuditFinding",
        "append",
        "get",
        "set",
        "startswith",
        "update"
      ],
      "line_number": 6,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:38.993083+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "2b0d4043e357b1b636f833f610d100ac6f344d7e69451e91f4d8856f66809ad6"
    },
    "src/system/governance/checks/quality_checks.py::__init__": {
      "key": "src/system/governance/checks/quality_checks.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/quality_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Initializes the check with a shared auditor context.",
      "docstring": "Initializes the check with a shared auditor context.",
      "calls": [],
      "line_number": 9,
      "is_async": false,
      "parameters": [
        "self",
        "context"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:38.993646+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/system/governance/checks/quality_checks.py::QualityChecks",
      "structural_hash": "1dc0d14e79c72e94245288f7c44e6fac8589c4b05b4c310f37b47381e2ea41ea"
    },
    "src/system/governance/checks/quality_checks.py::check_docstrings_and_intents": {
      "key": "src/system/governance/checks/quality_checks.py::check_docstrings_and_intents",
      "name": "check_docstrings_and_intents",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/quality_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.docstrings",
      "intent": "Finds symbols missing docstrings or having generic intents.",
      "docstring": "Finds symbols missing docstrings or having generic intents.",
      "calls": [
        "AuditFinding",
        "append",
        "get"
      ],
      "line_number": 14,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.995460+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/quality_checks.py::QualityChecks",
      "structural_hash": "f40f71e160a1696a76c508fbb0d72e8da348ea77e5e2de0968ca611e23ac5789"
    },
    "src/system/governance/checks/quality_checks.py::check_for_dead_code": {
      "key": "src/system/governance/checks/quality_checks.py::check_for_dead_code",
      "name": "check_for_dead_code",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/quality_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.dead_code",
      "intent": "Detects unreferenced public symbols.",
      "docstring": "Detects unreferenced public symbols.",
      "calls": [
        "AuditFinding",
        "append",
        "get",
        "set",
        "startswith",
        "update"
      ],
      "line_number": 31,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:38.997278+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/quality_checks.py::QualityChecks",
      "structural_hash": "4abaedf091a82766f3e90e0aaafdaf80c041dce5008028f4ad54134032ae81a7"
    },
    "src/system/governance/checks/file_checks.py::FileChecks": {
      "key": "src/system/governance/checks/file_checks.py::FileChecks",
      "name": "FileChecks",
      "type": "ClassDef",
      "file": "src/system/governance/checks/file_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Container for file-based constitutional checks.",
      "docstring": "Container for file-based constitutional checks.",
      "calls": [
        "AuditFinding",
        "Path",
        "_get_known_files_from_meta",
        "_recursive_find_paths",
        "add",
        "any",
        "append",
        "exists",
        "extend",
        "glob",
        "is_file",
        "isinstance",
        "len",
        "list",
        "load_config",
        "read_text",
        "relative_to",
        "replace",
        "rglob",
        "set",
        "sorted",
        "str",
        "validate_code",
        "values"
      ],
      "line_number": 8,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.004866+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "26f4252f48775e4065186911d77aa24e5519f04c292f92cbd8b41d859cc87982"
    },
    "src/system/governance/checks/file_checks.py::__init__": {
      "key": "src/system/governance/checks/file_checks.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/file_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Initializes the check with a shared auditor context.",
      "docstring": "Initializes the check with a shared auditor context.",
      "calls": [],
      "line_number": 11,
      "is_async": false,
      "parameters": [
        "self",
        "context"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:39.005461+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/system/governance/checks/file_checks.py::FileChecks",
      "structural_hash": "1dc0d14e79c72e94245288f7c44e6fac8589c4b05b4c310f37b47381e2ea41ea"
    },
    "src/system/governance/checks/file_checks.py::check_required_files": {
      "key": "src/system/governance/checks/file_checks.py::check_required_files",
      "name": "check_required_files",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/file_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.required_files",
      "intent": "Verifies that all files declared in meta.yaml exist on disk.",
      "docstring": "Verifies that all files declared in meta.yaml exist on disk.",
      "calls": [
        "AuditFinding",
        "_get_known_files_from_meta",
        "append",
        "exists",
        "len",
        "list",
        "sorted"
      ],
      "line_number": 16,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.006985+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/file_checks.py::FileChecks",
      "structural_hash": "1d558fc4c97dd07e4659d870f61d359f6da2d9bc2cdb018f8f1b62ea92716870"
    },
    "src/system/governance/checks/file_checks.py::check_syntax": {
      "key": "src/system/governance/checks/file_checks.py::check_syntax",
      "name": "check_syntax",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/file_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.syntax",
      "intent": "Validates the syntax of all .intent YAML/JSON files (including proposals).",
      "docstring": "Validates the syntax of all .intent YAML/JSON files (including proposals).",
      "calls": [
        "AuditFinding",
        "append",
        "extend",
        "is_file",
        "len",
        "list",
        "read_text",
        "relative_to",
        "rglob",
        "str",
        "validate_code"
      ],
      "line_number": 42,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.008934+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/file_checks.py::FileChecks",
      "structural_hash": "46fa6c5267bbf18f04a97fc8a3a532a9ac2649f838025602eeb42bc1436f3091"
    },
    "src/system/governance/checks/file_checks.py::check_for_orphaned_intent_files": {
      "key": "src/system/governance/checks/file_checks.py::check_for_orphaned_intent_files",
      "name": "check_for_orphaned_intent_files",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/file_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.orphaned_intent_files",
      "intent": "Finds .intent files that are not referenced in meta.yaml.",
      "docstring": "Finds .intent files that are not referenced in meta.yaml.",
      "calls": [
        "AuditFinding",
        "_get_known_files_from_meta",
        "any",
        "append",
        "is_file",
        "list",
        "relative_to",
        "replace",
        "rglob",
        "sorted",
        "str"
      ],
      "line_number": 67,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.010806+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/file_checks.py::FileChecks",
      "structural_hash": "c577086d6a0a345ab40ff8a65cb0f2137646635d6768ca788ad32192f25a4604"
    },
    "src/system/governance/checks/file_checks.py::_get_known_files_from_meta": {
      "key": "src/system/governance/checks/file_checks.py::_get_known_files_from_meta",
      "name": "_get_known_files_from_meta",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/file_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Builds a set of all known intent files by reading .intent/meta.yaml.",
      "docstring": "Builds a set of all known intent files by reading .intent/meta.yaml.",
      "calls": [
        "Path",
        "_recursive_find_paths",
        "add",
        "exists",
        "glob",
        "isinstance",
        "load_config",
        "relative_to",
        "replace",
        "set",
        "str",
        "values"
      ],
      "line_number": 86,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.013004+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/governance/checks/file_checks.py::FileChecks",
      "structural_hash": "5d8cdc25ea5aa7895600393af6617167bc5aad4241a31c94070dd1a80c43b1ca"
    },
    "src/system/governance/checks/file_checks.py::_recursive_find_paths": {
      "key": "src/system/governance/checks/file_checks.py::_recursive_find_paths",
      "name": "_recursive_find_paths",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/file_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Recursively finds all file paths declared in the meta configuration.",
      "docstring": "Recursively finds all file paths declared in the meta configuration.",
      "calls": [
        "Path",
        "_recursive_find_paths",
        "add",
        "isinstance",
        "replace",
        "str",
        "values"
      ],
      "line_number": 94,
      "is_async": false,
      "parameters": [
        "data"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.014197+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/governance/checks/file_checks.py::FileChecks",
      "structural_hash": "538bc4a6f8ead1e6d26030e880c885a8805dfbe59e35976cca07b4ff9476024c"
    },
    "src/system/governance/checks/proposal_checks.py::ProposalChecks": {
      "key": "src/system/governance/checks/proposal_checks.py::ProposalChecks",
      "name": "ProposalChecks",
      "type": "ClassDef",
      "file": "src/system/governance/checks/proposal_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Container for proposal-related constitutional checks.",
      "docstring": "Container for proposal-related constitutional checks.",
      "calls": [
        "AuditFinding",
        "Draft7Validator",
        "ValueError",
        "_expected_token_for_content",
        "_load_proposal",
        "_proposal_paths",
        "append",
        "encode",
        "exists",
        "get",
        "glob",
        "hexdigest",
        "iter_errors",
        "join",
        "len",
        "list",
        "load_schema",
        "loads",
        "lower",
        "read_text",
        "relative_to",
        "safe_load",
        "sha256",
        "sorted",
        "str"
      ],
      "line_number": 16,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.023183+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "ddee93862c9ba4038940bc8763d369e5046ecda674b9d1fa5bdc2d1b518d104b"
    },
    "src/system/governance/checks/proposal_checks.py::__init__": {
      "key": "src/system/governance/checks/proposal_checks.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/proposal_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Initializes the check with a shared auditor context, setting `repo_root` and `proposals_dir` paths.",
      "docstring": "Initializes the check with a shared auditor context, setting `repo_root` and `proposals_dir` paths.",
      "calls": [],
      "line_number": 19,
      "is_async": false,
      "parameters": [
        "self",
        "context"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:39.023956+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/system/governance/checks/proposal_checks.py::ProposalChecks",
      "structural_hash": "c976dff4a3fed5398d62cfee4d4f593fa568780866806d4d8d2d6b858ec20536"
    },
    "src/system/governance/checks/proposal_checks.py::_proposal_paths": {
      "key": "src/system/governance/checks/proposal_checks.py::_proposal_paths",
      "name": "_proposal_paths",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/proposal_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Return all cr-* proposals (both YAML and JSON).",
      "docstring": "Return all cr-* proposals (both YAML and JSON).",
      "calls": [
        "exists",
        "glob",
        "list",
        "sorted"
      ],
      "line_number": 28,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.024932+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/governance/checks/proposal_checks.py::ProposalChecks",
      "structural_hash": "a246a33fc124b4c96eb9dbf11f96fcfdfb13e84643e0e69738685a5c4f1ce077"
    },
    "src/system/governance/checks/proposal_checks.py::_load_proposal": {
      "key": "src/system/governance/checks/proposal_checks.py::_load_proposal",
      "name": "_load_proposal",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/proposal_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Load proposal preserving its format.",
      "docstring": "Load proposal preserving its format.",
      "calls": [
        "ValueError",
        "loads",
        "lower",
        "read_text",
        "safe_load"
      ],
      "line_number": 39,
      "is_async": false,
      "parameters": [
        "self",
        "path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.026130+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/governance/checks/proposal_checks.py::ProposalChecks",
      "structural_hash": "c5ddbf6ffe774f94c6de5fb971495d377f916112bb542b8b4720fbbc76216a93"
    },
    "src/system/governance/checks/proposal_checks.py::_expected_token_for_content": {
      "key": "src/system/governance/checks/proposal_checks.py::_expected_token_for_content",
      "name": "_expected_token_for_content",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/proposal_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Mirror admin token format: 'core-proposal-v1:<sha256hex>'.",
      "docstring": "Mirror admin token format: 'core-proposal-v1:<sha256hex>'.",
      "calls": [
        "encode",
        "hexdigest",
        "sha256"
      ],
      "line_number": 50,
      "is_async": false,
      "parameters": [
        "content"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.026946+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "parent_class_key": "src/system/governance/checks/proposal_checks.py::ProposalChecks",
      "structural_hash": "9fce88da20cbd5c824350a85ed86bb5509f2bbbfedbb4c7f3bd750c28def1a90"
    },
    "src/system/governance/checks/proposal_checks.py::check_proposal_files_match_schema": {
      "key": "src/system/governance/checks/proposal_checks.py::check_proposal_files_match_schema",
      "name": "check_proposal_files_match_schema",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/proposal_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.proposals_schema",
      "intent": "Validate each cr-*.yaml/json proposal against proposal.schema.json.",
      "docstring": "Validate each cr-*.yaml/json proposal against proposal.schema.json.",
      "calls": [
        "AuditFinding",
        "Draft7Validator",
        "_load_proposal",
        "_proposal_paths",
        "append",
        "exists",
        "iter_errors",
        "join",
        "list",
        "load_schema",
        "relative_to",
        "str"
      ],
      "line_number": 59,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.029452+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/proposal_checks.py::ProposalChecks",
      "structural_hash": "f949078077927daa468297db92467250e19195b2cad0a507de1e46b2e528c85d"
    },
    "src/system/governance/checks/proposal_checks.py::check_signatures_match_content": {
      "key": "src/system/governance/checks/proposal_checks.py::check_signatures_match_content",
      "name": "check_signatures_match_content",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/proposal_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.proposals_drift",
      "intent": "Detect content/signature drift:",
      "docstring": "Detect content/signature drift:\n- warn if a proposal has no signatures\n- warn if any signature token does not match the current content",
      "calls": [
        "AuditFinding",
        "_expected_token_for_content",
        "_load_proposal",
        "_proposal_paths",
        "append",
        "get",
        "join",
        "len",
        "relative_to",
        "str"
      ],
      "line_number": 127,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.032448+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/proposal_checks.py::ProposalChecks",
      "structural_hash": "b9fd68f990cca281115e4052ff510391b9fce134aea1bc6cb4e451df66225c72"
    },
    "src/system/governance/checks/proposal_checks.py::list_pending_proposals": {
      "key": "src/system/governance/checks/proposal_checks.py::list_pending_proposals",
      "name": "list_pending_proposals",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/proposal_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.proposals_list",
      "intent": "Emit a friendly summary of pending proposals.",
      "docstring": "Emit a friendly summary of pending proposals.",
      "calls": [
        "AuditFinding",
        "_proposal_paths",
        "append",
        "exists",
        "relative_to",
        "str"
      ],
      "line_number": 198,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.034064+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/proposal_checks.py::ProposalChecks",
      "structural_hash": "978e7e6371c215f4d6251d974872f3016f9678f7b6e79b94e5b6a4a484d0c12a"
    },
    "src/system/governance/checks/architecture_checks.py::ArchitectureChecks": {
      "key": "src/system/governance/checks/architecture_checks.py::ArchitectureChecks",
      "name": "ArchitectureChecks",
      "type": "ClassDef",
      "file": "src/system/governance/checks/architecture_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Container for architectural integrity checks.",
      "docstring": "Container for architectural integrity checks.",
      "calls": [
        "AuditFinding",
        "append",
        "defaultdict",
        "get",
        "items",
        "join",
        "len"
      ],
      "line_number": 9,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.036886+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "2629fcdc233b11795446c924781df886fe2792e3178adff6e35b092ede84bc89"
    },
    "src/system/governance/checks/architecture_checks.py::__init__": {
      "key": "src/system/governance/checks/architecture_checks.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/architecture_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Initializes the check with a shared auditor context.",
      "docstring": "Initializes the check with a shared auditor context.",
      "calls": [],
      "line_number": 12,
      "is_async": false,
      "parameters": [
        "self",
        "context"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:39.037488+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/system/governance/checks/architecture_checks.py::ArchitectureChecks",
      "structural_hash": "1dc0d14e79c72e94245288f7c44e6fac8589c4b05b4c310f37b47381e2ea41ea"
    },
    "src/system/governance/checks/architecture_checks.py::check_for_structural_duplication": {
      "key": "src/system/governance/checks/architecture_checks.py::check_for_structural_duplication",
      "name": "check_for_structural_duplication",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/architecture_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.duplication",
      "intent": "Finds symbols with identical structural hashes, violating `dry_by_design`, using content-addressed knowledge graph for accurate duplication detection.",
      "docstring": "Finds symbols with identical structural hashes, violating `dry_by_design`, using content-addressed knowledge graph for accurate duplication detection.",
      "calls": [
        "AuditFinding",
        "append",
        "defaultdict",
        "get",
        "items",
        "join",
        "len"
      ],
      "line_number": 17,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.039319+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/architecture_checks.py::ArchitectureChecks",
      "structural_hash": "c14751da54071fc503aa083dbe5d566b22745f491dd37208cb1ea6dd588d5eb5"
    },
    "src/system/governance/checks/structure_checks.py::StructureChecks": {
      "key": "src/system/governance/checks/structure_checks.py::StructureChecks",
      "name": "StructureChecks",
      "type": "ClassDef",
      "file": "src/system/governance/checks/structure_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Container for structural constitutional checks.",
      "docstring": "Container for structural constitutional checks.",
      "calls": [
        "AuditFinding",
        "append",
        "exists",
        "get",
        "get_domain_permissions",
        "items",
        "joinpath",
        "len",
        "list",
        "load_config",
        "relative_to",
        "resolve_domain_for_path",
        "scan_imports_for_file",
        "set",
        "sorted",
        "split",
        "startswith",
        "validate_manifest_entry",
        "with_suffix"
      ],
      "line_number": 8,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.048329+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "3c1a6a1f14a41b65fac1d4e0e970cb6cba04db054f37532cbd1e81ac28246af7"
    },
    "src/system/governance/checks/structure_checks.py::__init__": {
      "key": "src/system/governance/checks/structure_checks.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/structure_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Initializes the check with a shared auditor context.",
      "docstring": "Initializes the check with a shared auditor context.",
      "calls": [],
      "line_number": 11,
      "is_async": false,
      "parameters": [
        "self",
        "context"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:39.048921+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/system/governance/checks/structure_checks.py::StructureChecks",
      "structural_hash": "1dc0d14e79c72e94245288f7c44e6fac8589c4b05b4c310f37b47381e2ea41ea"
    },
    "src/system/governance/checks/structure_checks.py::check_project_manifest": {
      "key": "src/system/governance/checks/structure_checks.py::check_project_manifest",
      "name": "check_project_manifest",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/structure_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.project_manifest",
      "intent": "Validates the integrity of project_manifest.yaml.",
      "docstring": "Validates the integrity of project_manifest.yaml.",
      "calls": [
        "AuditFinding",
        "append"
      ],
      "line_number": 16,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.050115+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/structure_checks.py::StructureChecks",
      "structural_hash": "47598ce1a8e620a4140c0826a51f3edda03959d6d6f8c64ff2e92638e07a6e1c"
    },
    "src/system/governance/checks/structure_checks.py::check_capability_coverage": {
      "key": "src/system/governance/checks/structure_checks.py::check_capability_coverage",
      "name": "check_capability_coverage",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/structure_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.capability_coverage",
      "intent": "Ensures all required capabilities are implemented.",
      "docstring": "Ensures all required capabilities are implemented.",
      "calls": [
        "AuditFinding",
        "append",
        "get",
        "list",
        "set",
        "sorted"
      ],
      "line_number": 31,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.051625+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/structure_checks.py::StructureChecks",
      "structural_hash": "19b8d23e4555fd690a9776f38aaa1d4f9f6d8bf338d78040a91d0cac069d0c49"
    },
    "src/system/governance/checks/structure_checks.py::check_capability_definitions": {
      "key": "src/system/governance/checks/structure_checks.py::check_capability_definitions",
      "name": "check_capability_definitions",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/structure_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.capability_definitions",
      "intent": "Ensures all implemented capabilities are valid.",
      "docstring": "Ensures all implemented capabilities are valid.",
      "calls": [
        "AuditFinding",
        "append",
        "get",
        "list",
        "load_config",
        "sorted"
      ],
      "line_number": 47,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.053357+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/structure_checks.py::StructureChecks",
      "structural_hash": "47f938a690dbeb4a63b0621bc0d6d8171257ae6dfaf680b4a4ba8a90aaedbbf3"
    },
    "src/system/governance/checks/structure_checks.py::check_knowledge_graph_schema": {
      "key": "src/system/governance/checks/structure_checks.py::check_knowledge_graph_schema",
      "name": "check_knowledge_graph_schema",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/structure_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.knowledge_graph_schema",
      "intent": "Validates all knowledge graph symbols against the schema.",
      "docstring": "Validates all knowledge graph symbols against the schema.",
      "calls": [
        "AuditFinding",
        "append",
        "items",
        "len",
        "validate_manifest_entry"
      ],
      "line_number": 66,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.054965+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/structure_checks.py::StructureChecks",
      "structural_hash": "7bcbe3ea7151332652bc185d29d0273e1cf161bb6a10c4dfa45c6d2326ef45bf"
    },
    "src/system/governance/checks/structure_checks.py::check_domain_integrity": {
      "key": "src/system/governance/checks/structure_checks.py::check_domain_integrity",
      "name": "check_domain_integrity",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/structure_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.domain_integrity",
      "intent": "Checks for domain mismatches and illegal imports.",
      "docstring": "Checks for domain mismatches and illegal imports.",
      "calls": [
        "AuditFinding",
        "append",
        "exists",
        "get",
        "get_domain_permissions",
        "joinpath",
        "relative_to",
        "resolve_domain_for_path",
        "scan_imports_for_file",
        "set",
        "split",
        "startswith",
        "with_suffix"
      ],
      "line_number": 84,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.058111+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/structure_checks.py::StructureChecks",
      "structural_hash": "a92728b73b17e96e47af742ae8a688e68a6326e89f4c40604d5fe7963529e6e5"
    },
    "src/system/governance/checks/environment_checks.py::EnvironmentChecks": {
      "key": "src/system/governance/checks/environment_checks.py::EnvironmentChecks",
      "name": "EnvironmentChecks",
      "type": "ClassDef",
      "file": "src/system/governance/checks/environment_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Container for environment and runtime configuration checks.",
      "docstring": "Container for environment and runtime configuration checks.",
      "calls": [
        "AuditFinding",
        "append",
        "exists",
        "get",
        "getenv",
        "load_config"
      ],
      "line_number": 6,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.061214+00:00",
      "is_class": true,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "6b024c0e90859e12afe8ec0853b994a793f02c31ddedf4521fff6cb812426a5e"
    },
    "src/system/governance/checks/environment_checks.py::__init__": {
      "key": "src/system/governance/checks/environment_checks.py::__init__",
      "name": "__init__",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/environment_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Initializes the check with a shared auditor context.",
      "docstring": "Initializes the check with a shared auditor context.",
      "calls": [],
      "line_number": 9,
      "is_async": false,
      "parameters": [
        "self",
        "context"
      ],
      "entry_point_type": "magic_method",
      "last_updated": "2025-08-11T16:43:39.061764+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "python_magic_method",
      "parent_class_key": "src/system/governance/checks/environment_checks.py::EnvironmentChecks",
      "structural_hash": "1dc0d14e79c72e94245288f7c44e6fac8589c4b05b4c310f37b47381e2ea41ea"
    },
    "src/system/governance/checks/environment_checks.py::check_runtime_environment": {
      "key": "src/system/governance/checks/environment_checks.py::check_runtime_environment",
      "name": "check_runtime_environment",
      "type": "FunctionDef",
      "file": "src/system/governance/checks/environment_checks.py",
      "domain": "system",
      "agent": "generic_agent",
      "capability": "audit.check.environment",
      "intent": "Verifies that required environment variables specified in runtime_requirements.yaml are set, returning a list of audit findings for missing variables or configuration issues.",
      "docstring": "Verifies that required environment variables specified in runtime_requirements.yaml are set, returning a list of audit findings for missing variables or configuration issues.",
      "calls": [
        "AuditFinding",
        "append",
        "exists",
        "get",
        "getenv",
        "load_config"
      ],
      "line_number": 14,
      "is_async": false,
      "parameters": [
        "self"
      ],
      "entry_point_type": "capability",
      "last_updated": "2025-08-11T16:43:39.063590+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "capability_implementation",
      "parent_class_key": "src/system/governance/checks/environment_checks.py::EnvironmentChecks",
      "structural_hash": "2797c7344f19a0c5c41afc6b3732d11c807fd725d9fc5feaea18718d1816aed8"
    },
    "src/core/cli/guard.py::ensure_cli_entrypoint": {
      "key": "src/core/cli/guard.py::ensure_cli_entrypoint",
      "name": "ensure_cli_entrypoint",
      "type": "FunctionDef",
      "file": "src/core/cli/guard.py",
      "domain": "core",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Provides functionality for the core domain.",
      "docstring": "Provides functionality for the core domain.",
      "calls": [
        "warning"
      ],
      "line_number": 12,
      "is_async": false,
      "parameters": [],
      "entry_point_type": "cli_entry_point",
      "last_updated": "2025-08-11T16:43:39.064674+00:00",
      "is_class": false,
      "base_classes": [],
      "structural_hash": "4da791dd1537cbe4dfbb0243540a0edc4249a708ac8fe669a17c2316679d73d1"
    },
    "src/shared/schemas/manifest_validator.py::load_schema": {
      "key": "src/shared/schemas/manifest_validator.py::load_schema",
      "name": "load_schema",
      "type": "FunctionDef",
      "file": "src/shared/schemas/manifest_validator.py",
      "domain": "shared",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Load a JSON schema from the .intent/schemas/ directory.",
      "docstring": "Load a JSON schema from the .intent/schemas/ directory.\n\nArgs:\n    schema_name (str): The filename of the schema (e.g., 'knowledge_graph_entry.schema.json').\n    \nReturns:\n    Dict[str, Any]: The loaded JSON schema.\n    \nRaises:\n    FileNotFoundError: If the schema file is not found.\n    json.JSONDecodeError: If the schema file is not valid JSON.",
      "calls": [
        "FileNotFoundError",
        "JSONDecodeError",
        "exists",
        "load",
        "open"
      ],
      "line_number": 13,
      "is_async": false,
      "parameters": [
        "schema_name"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.066608+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "75a3e049ba750ef453bcb196882ea2b1319162c6d42ce5ee57b96157e678286f"
    },
    "src/shared/schemas/manifest_validator.py::validate_manifest_entry": {
      "key": "src/shared/schemas/manifest_validator.py::validate_manifest_entry",
      "name": "validate_manifest_entry",
      "type": "FunctionDef",
      "file": "src/shared/schemas/manifest_validator.py",
      "domain": "shared",
      "agent": "validator_agent",
      "capability": "unassigned",
      "intent": "Validate a single manifest entry against a schema.",
      "docstring": "Validate a single manifest entry against a schema.\n\nArgs:\n    entry: The dictionary representing a single function/class entry.\n    schema_name: The filename of the schema to validate against.\n    \nReturns:\n    A tuple of (is_valid: bool, list_of_error_messages: List[str]).",
      "calls": [
        "Draft7Validator",
        "append",
        "iter_errors",
        "join",
        "load_schema",
        "str"
      ],
      "line_number": 38,
      "is_async": false,
      "parameters": [
        "entry",
        "schema_name"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.068222+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "7a12edc7bfad880292fb8f43c4c24c7a21d7fedbc14c2e5f8a80583fededc517"
    },
    "src/shared/utils/manifest_aggregator.py::aggregate_manifests": {
      "key": "src/shared/utils/manifest_aggregator.py::aggregate_manifests",
      "name": "aggregate_manifests",
      "type": "FunctionDef",
      "file": "src/shared/utils/manifest_aggregator.py",
      "domain": "shared",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Finds all domain-specific manifest.yaml files and merges them.",
      "docstring": "Finds all domain-specific manifest.yaml files and merges them.\n\nThis function is the heart of the modular manifest system. It reads the\nsource structure to find all domains, then searches for a manifest in each\ndomain's directory, aggregating their contents.\n\nArgs:\n    repo_root (Path): The absolute path to the repository root.\n\nReturns:\n    A dictionary representing the aggregated manifest, primarily focused\n    on compiling a unified list of 'required_capabilities'.",
      "calls": [
        "debug",
        "error",
        "exists",
        "extend",
        "get",
        "info",
        "list",
        "read_text",
        "safe_load",
        "set",
        "sorted"
      ],
      "line_number": 14,
      "is_async": false,
      "parameters": [
        "repo_root"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.071668+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "e328d65d3f4068a4f2dc7602f0d7d02c70915750d2c066dd17bb654cf717754b"
    },
    "src/shared/utils/import_scanner.py::scan_imports_for_file": {
      "key": "src/shared/utils/import_scanner.py::scan_imports_for_file",
      "name": "scan_imports_for_file",
      "type": "FunctionDef",
      "file": "src/shared/utils/import_scanner.py",
      "domain": "shared",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Parse a Python file and extract all imported module paths.",
      "docstring": "Parse a Python file and extract all imported module paths.\n\nArgs:\n    file_path (Path): Path to the file.\n\nReturns:\n    List[str]: List of imported module paths.",
      "calls": [
        "append",
        "isinstance",
        "parse",
        "read_text",
        "walk",
        "warning"
      ],
      "line_number": 17,
      "is_async": false,
      "parameters": [
        "file_path"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.073798+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "f1a07ab2523b8bbd0b09c1cd4de9ea522d6240c63d271ad9196321a5814c1ece"
    },
    "src/shared/utils/parsing.py::parse_write_blocks": {
      "key": "src/shared/utils/parsing.py::parse_write_blocks",
      "name": "parse_write_blocks",
      "type": "FunctionDef",
      "file": "src/shared/utils/parsing.py",
      "domain": "shared",
      "agent": "generic_agent",
      "capability": "unassigned",
      "intent": "Extracts all [[write:...]] blocks from LLM output.",
      "docstring": "Extracts all [[write:...]] blocks from LLM output.\n\nThis function is robust and handles both [[end]] and [[/write]] as valid terminators\nto accommodate different LLM habits.\n\nArgs:\n    llm_output (str): The raw text output from a language model.\n\nReturns:\n    A dictionary mapping file paths to their corresponding code content.",
      "calls": [
        "findall",
        "strip"
      ],
      "line_number": 8,
      "is_async": false,
      "parameters": [
        "llm_output"
      ],
      "entry_point_type": "data_model",
      "last_updated": "2025-08-11T16:43:39.075010+00:00",
      "is_class": false,
      "base_classes": [],
      "entry_point_justification": "dataclass_definition",
      "structural_hash": "0add4d2946891ed29ab7d8fcf5c560374c35d068110f7270b96bd5ec6af0d11d"
    }
  }
}
--- END OF FILE ./.intent/knowledge/knowledge_graph.json ---

--- START OF FILE ./.intent/knowledge/source_structure.yaml ---
# .intent/knowledge/source_structure.yaml
structure:
  - domain: core
    path: src/core
    description: Core logic for orchestration, routing, and CLI
    editable: true
    default_handler: python
    restricted_types: [python, yaml]
    allowed_imports: 
      - core
      - shared
      - system
      - agents
      # External & Standard Libs
      - fastapi
      - uvicorn
      - yaml
      - requests
      - dotenv
      - black
      - json
      - os
      - re
      - typing
      - pathlib
      - datetime
      - subprocess
      - contextlib
      - threading
      - uuid
      - platform
      - ast
      - tempfile

  - domain: agents
    path: src/agents
    description: Specialized AI actors (planners, reviewers, suggesters)
    editable: true
    default_handler: python
    known_files:
      - manifest.yaml
    allowed_imports: 
      - agents
      - shared
      - system
      - core
      # External & Standard Libs
      - json
      - re
      - textwrap
      - typing
      - pathlib

  - domain: system
    path: src/system
    description: Governance tooling, lifecycle setup, CLI utilities
    editable: true
    default_handler: python
    allowed_imports: 
      - system
      - shared
      - core
      - tooling
      # External & Standard Libs
      - json
      - ast
      - pathlib
      - typing
      - collections
      - rich
      - sys
      - yaml
      - re
      - logging
      - dataclasses

  - domain: shared
    path: src/shared
    description: Shared models, helpers, and config interfaces
    editable: true
    default_handler: python
    allowed_imports: 
      - shared
      # External & Standard Libs
      - json
      - yaml
      - pathlib
      - typing
      - jsonschema
      - os
      - ast
      
  - domain: features
    path: src/features
    description: Modular capabilities and extensions
    editable: true
    default_handler: python
    allowed_imports: [features, shared, data, services, integrations]

  - domain: tooling
    path: src/system/tools
    description: Internal introspection utilities
    editable: true
    default_handler: python
    # --- THIS IS THE FIX ---
    # We are adding 'core' to the list of allowed imports for the 'tooling' domain.
    allowed_imports: [tooling, system, shared, core, ast, json, logging, pathlib, typing, dataclasses]

  - domain: data
    path: src/data
    description: File access, storage backends, memory models
    editable: true
    default_handler: python
    allowed_imports: [data, shared]

  - domain: api
    path: src/api
    description: HTTP-facing endpoints
    editable: true
    default_handler: python
    allowed_imports: [api, shared, services]

  - domain: services
    path: src/services
    description: Business logic and orchestration
    editable: true
    default_handler: python
    allowed_imports: [services, shared, integrations, data]

  - domain: automation
    path: src/automation
    description: Task runners, schedulers, retry logic
    editable: true
    default_handler: python
    allowed_imports: [automation, shared, services]

  - domain: integrations
    path: src/integrations
    description: External service bridges (e.g., GitHub, remote LLMs)
    editable: true
    default_handler: python
    allowed_imports: [integrations, shared]

  - domain: mission
    path: mission
    description: CORE's declared beliefs, principles, and northstar
    editable: false
    restricted_types: [markdown, yaml]
    allowed_imports: []

  - domain: policies
    path: policies
    description: Governance rules and constraints
    editable: true
    default_handler: yaml
    allowed_imports: []

--- END OF FILE ./.intent/knowledge/source_structure.yaml ---

--- START OF FILE ./.intent/meta.yaml ---
version: "0.1.0"

# PURPOSE: This fulfills the evolvable_structure principle by establishing a clear
# index of all constitutional and governance files.
constitution:
  # Public keys of constitutional approvers
  approvers: "constitution/approvers.yaml"
  # NOTE: A full versioning system will be implemented in the future.

mission:
  northstar: "mission/northstar.yaml"
  manifesto: "mission/manifesto.md"
  principles: "mission/principles.yaml"

policies:
  intent_guard: "policies/intent_guard.yaml"
  safety_policies: "policies/safety_policies.yaml"
  security_intents: "policies/security_intents.yaml"

project:
  manifest_yaml: "project_manifest.yaml"

knowledge:
  source_structure: "knowledge/source_structure.yaml"
  codegraph: "knowledge/knowledge_graph.json" # This is a generated artifact, but good to list
  capability_tags: "knowledge/capability_tags.yaml"
  agent_roles: "knowledge/agent_roles.yaml"
  entry_point_patterns: "knowledge/entry_point_patterns.yaml"
  file_handlers: "knowledge/file_handlers.yaml"

evaluation:
  score_policy: "evaluation/score_policy.yaml"
  audit_checklist: "evaluation/audit_checklist.yaml"

config:
  local_mode: "config/local_mode.yaml"
  runtime_requirements: "config/runtime_requirements.yaml" 

schemas:
  # Schemas are currently discovered by the auditor via directory scan, not listed here.
  config: "schemas/config_schema.yaml"
  knowledge_graph_entry: "schemas/knowledge_graph_entry.schema.json"
--- END OF FILE ./.intent/meta.yaml ---

--- START OF FILE ./.intent/mission/manifesto.md ---
# CORE Manifesto

CORE is not a framework. It is not a plugin system.  
CORE is a system that evolves itself â€” governed by explicit declarations of purpose, constraints, and structure.

---

### Beliefs

- Software is never done, but it must always be explainable.
- Changes must serve a declared intent â€” or challenge it explicitly.
- LLMs are powerful, but must be contained by purpose, not left to guess.
- Governance is not bureaucracy; itâ€™s alignment between vision and action.

---

### CORE exists to:

- Understand its own capabilities
- Apply changes safely and transparently
- Validate every change against declared rules
- Learn from its outcomes and improve
- Inspire new systems to govern themselves

---

CORE does not pretend to be perfect.  
But it refuses to be mindless.

--- END OF FILE ./.intent/mission/manifesto.md ---

--- START OF FILE ./.intent/mission/northstar.yaml ---
# .intent/mission/northstar.yaml

# PURPOSE: This fulfills evolvable_structure and defines CORE's NorthStar.
name: CORE
version: v0.1.0
purpose: >
  CORE is a self-improving, intent-aware development system.
  Its purpose is to orchestrate safe, meaningful, and governed changes
  to its own codebase and related artifacts through intent bundles and introspective loops.

scope:
  - Planning and decomposition of tasks
  - Code generation via LLMs
  - Change validation and governance enforcement
  - Self-introspection and structural analysis
  - Knowledge management via manifests and graphs
  - Continuous self-evaluation and auditability

values:
  - Clarity over cleverness
  - Safety before speed
  - Traceability of every action
  - Alignment with declared purpose
  - Capability-driven reasoning

notes:
  - CORE evolves iteratively, but never silently.
  - All changes must fulfill a declared intent or generate a proposal to revise that intent.
--- END OF FILE ./.intent/mission/northstar.yaml ---

--- START OF FILE ./.intent/mission/principles.yaml ---
# .intent/mission/principles.yaml
#
# CORE's Constitution: clear, enforceable, and readable by humans and LLMs.
# Any agent (including future LLMs) must understand and obey these rules.

principles:

  - id: clarity_first
    description: >
      Every function must have:
        - A docstring explaining purpose
        - Clear parameter and return types
        - No nested logic deeper than 3 levels
      If a human cannot understand it in 30 seconds, it must be simplified.

  - id: safe_by_default
    description: >
      Every change must assume rollback or rejection unless explicitly validated.
      No file write, code execution, or intent update may proceed without confirmation.
      Rollback must be possible at every stage.

  - id: reason_with_purpose
    description: >
      Every planning step must include a comment:
        "PURPOSE: This fulfills <principle> from NorthStar."
      Example: "PURPOSE: This fulfills evolvable_structure."
      Actions without purpose tracing are invalid.

  - id: evolvable_structure
    description: >
      CORE may modify its own manifests only if:
        - The change is proposed via IntentBundle
        - It passes all policy checks
        - It is logged with a migration plan
      Self-modification without governance is forbidden.

  - id: no_orphaned_logic
    description: >
      No function, file, or rule may exist without a corresponding entry in the manifest.
      All code must be discoverable and auditable.
      If it's not in function_manifest.json, it does not exist.

  - id: use_intent_bundle
    description: >
      All executable capabilities must be declared and executed via a structured IntentBundle
      that reflects the 10-phase universal reasoning flow.
      No phase may be skipped.
    required_for:
      - all capabilities
      - all autonomous agents
      - all planning functions

  - id: minimalism_over_completeness
    description: >
      Prefer small, focused changes. Do not generate stubs, placeholders, or unused functions.
      If a capability is not actively used or tested, it must be removed.
      Empty implementations are technical debt.

  - id: dry_by_design
    description: >
      No logic may be duplicated. If a function, pattern, or decision exists in one place,
      it must be reused â€” not rewritten â€” anywhere else in the system.
      CORE must detect and reject duplication during self-modification.

  - id: single_source_of_truth
    description: >
      The project_manifest.yaml is the single source of truth for all capabilities, structure, and intent.
      All other files (e.g. codegraph.json, function_manifest.json) must be derived from it.
      Manual edits to derived files will be rejected.

  - id: separation_of_concerns
    description: >
      Each domain has a single responsibility:
        - core: orchestration, routing, safety
        - features: capabilities and extensions
        - system/tools: audit, manifest update, introspection
        - clients: external API interaction
      No file may mix logic across domains.
      Violations must be flagged during structural audits.

  - id: predictable_side_effects
    description: >
      Any file change must:
        - Be preceded by a log entry: "CHANGE: <description> â€” IntentBundle ID: <id>"
        - Be staged via FileHandler (not direct write)
        - Be reversible via Git diff or undo log
      Silent or unlogged changes are forbidden.

  - id: immutable_constitution
    description: >
      The files principles.yaml, manifesto.md, and northstar.yaml are immutable.
      CORE may propose changes via IntentBundle, but may not apply them directly.
      Human review is required for constitutional updates.

  - id: policy_change_requires_human_review
    description: >
      Any change to .intent/policies/*.yaml must be:
      - Proposed via IntentBundle
      - Logged with justification
      - Approved via CLI confirmation or Git merge
    enforcement: manual_review
--- END OF FILE ./.intent/mission/principles.yaml ---

--- START OF FILE ./.intent/policies/intent_guard.yaml ---
rules:
  - id: no_undocumented_change
    description: >
      CORE must not modify or create any file that is not declared in the function_manifest.
    enforcement: hard

  - id: must_match_intent
    description: >
      All changes must be traceable to a declared high-level intent in the mission or policies.
    enforcement: soft

  - id: deny_core_loop_edit
    description: >
      CORE cannot modify its own orchestration engine unless reviewed by a human.
    applies_to:
      paths: ["src/core/cli.py", "src/core/orchestrator.py"]
    enforcement: manual_review

  - id: require_file_path_comment
    description: >
      Every Python file must begin with a comment indicating its relative file path,
      using the format: '# src/<subfolder>/filename.py'. This enables accurate introspection,
      duplication detection, and auto-fix tracking.
    applies_to:
      patterns: ["src/**/*.py"]
    enforcement: hard

  - id: limit_rewrite_cycles
    description: >
      CORE may not rewrite the same file more than once per execution cycle
      without explicit validation or feedback input.
    enforcement: hard

  - id: require_tests_for_capabilities
    description: >
      All capabilities declared in the function_manifest must have at least one corresponding test in /tests.
    enforcement: soft

  - id: enforce_intent_bundle_usage
    description: >
      Any capability marked with `requires_intent_bundle: true` must be executed through an IntentBundle flow.
    enforcement: hard

  - id: manifest_file_existence
    description: >
      All file paths listed in function_manifest must exist on disk and be importable.
    enforcement: hard

  - id: require_manual_review_for_intent_updates
    description: >
      Any changes to files under .intent/ â€” including mission, policies, manifests, or evaluation criteria â€”
      must be manually reviewed and approved by a human before being written to disk.
    applies_to:
      paths:
        - ".intent/"
    enforcement: manual_review
  - id: immutable_constitution
    description: >
      The files principles.yaml, manifesto.md, and northstar.yaml are immutable.
      CORE may propose changes via IntentBundle, but may not apply them directly.
      Human review is required for constitutional updates.
    enforcement: manual_review
    applies_to:
      paths:
        - ".intent/mission/principles.yaml"
        - ".intent/mission/manifesto.md"
        - ".intent/mission/northstar.yaml"
    triggers:
      - on_write
      - on_generate
    action: require_human_approval
    feedback: |
      ðŸ”’ Constitutional file modification detected. Human review required.
--- END OF FILE ./.intent/policies/intent_guard.yaml ---

--- START OF FILE ./.intent/policies/safety_policies.yaml ---
# .intent/policies/safety_policies.yaml
meta:
  version: "0.2.1" # Version bump to reflect change
  last_updated: "2025-08-05T14:00:00Z" # Updated timestamp
  author: "CORE Constitution"
  description: >
    Safety policies governing code generation, execution, and modification.
    These rules are enforced at write-time and via IntentGuard.
    This file is part of the immutable constitution â€” changes require human review.

rules:
  # ===================================================================
  # RULE: Block dangerous execution primitives
  # ===================================================================
  - id: no_dangerous_execution
    description: >
      Generated or modified code must not contain calls to dangerous functions
      that enable arbitrary code execution, shell access, or unsafe deserialization.
    enforcement: hard
    scope:
      domains: [core, agents, features]
      exclude:
        - "tests/**"
        - "utils/safe_execution.py"
        - "tooling/**"
        - "src/core/git_service.py" # <-- ADDED THIS EXCEPTION
    triggers:
      - on_generate
      - on_write
    validator: semantic_checker
    method: content_scan
    detection:
      type: substring
      patterns:
        - "eval("
        - "exec("
        - "compile("
        - "os.system("
        - "os.popen("
        - "subprocess.run("
        - "subprocess.Popen("
        - "subprocess.call("
        - "shutil.rmtree("
        - "os.remove("
        - "os.rmdir("
    action: reject
    feedback: |
      âŒ Dangerous execution detected: '{{pattern}}' found in code.
      Use approved wrappers in `utils/safe_execution.py` instead.

  # ===================================================================
  # RULE: Block unsafe imports
  # ===================================================================
  - id: no_unsafe_imports
    description: >
      Prevent importing modules that enable network access, shell control,
      or unsafe serialization unless explicitly allowed.
    enforcement: hard
    scope:
      domains: [core, agents]
    triggers:
      - on_generate
      - on_write
    validator: ast_import_scanner
    method: ast
    detection:
      type: import_name
      forbidden:
        - "import socket"
        - "import telnetlib"
        - "import fcntl"  # Unix-specific unsafe ops
        - "import pickle"
        - "import shelve"
        - "from subprocess import"
        - "from os import system"
    action: reject
    feedback: |
      âŒ Unsafe import detected: '{{import}}'.
      Network and system-level imports are restricted in core domains.

  # ===================================================================
  # RULE: Require sandboxed file operations
  # ===================================================================
  - id: file_ops_must_be_sandboxed
    description: >
      All file operations must use the FileHandler or SafeIO wrapper.
      Direct use of open(), os.path, etc., is prohibited.
    enforcement: hard
    method: regex
    detection:
      patterns:
        - "open\\("
        - "os\\.path"
        - "os\\.makedirs"
        - "os\\.mkdir"
        - "os\\.chdir"
      exceptions:
        - "test_.*\\.py"
        - "utils/safe_io.py"
    action: reject
    feedback: |
      âŒ Raw file operation detected: '{{pattern}}'.
      Use FileHandler for all disk operations to ensure staging and rollback.

  # ===================================================================
  # RULE: Git checkpoint required before write
  # ===================================================================
  - id: git_checkpoint_required
    description: >
      CORE must create a Git stash or commit before writing any file.
      Ensures rollback is always possible.
    enforcement: hard
    triggers:
      - before_write
    validator: git_status_checker
    action: require_checkpoint
    feedback: |
      âŒ Uncommitted changes detected. Run `git stash` or `git commit` before proceeding.

  # ===================================================================
  # RULE: Tests must run if present
  # ===================================================================
  - id: run_tests_if_present
    description: >
      If test files exist for a modified component, tests must be run post-write.
    enforcement: soft
    triggers:
      - after_write
    action: warn_if_tests_skipped
    feedback: |
      âš  Test files detected but not run. Consider running `pytest` to verify behavior.

  # ===================================================================
  # RULE: No self-modification of core loop
  # ===================================================================
  - id: deny_core_loop_edit
    description: >
      CORE cannot modify its own orchestration engine without human review.
    enforcement: manual_review
    scope:
      paths:
        - "src/core/orchestrator.py"
        - "src/core/main.py"
        - "src/core/intent_guard.py"
        - ".intent/policies/intent_guard.yaml"
    action: require_human_approval
    feedback: |
      ðŸ”’ Core logic modification detected. Human review required before application.

  # ===================================================================
  # RULE: All changes must be logged
  # ===================================================================
  - id: change_must_be_logged
    description: >
      Every file change must be preceded by a log entry in .intent/change_log.json
      with IntentBundle ID and description.
    enforcement: hard
    triggers:
      - before_write
    validator: change_log_checker
    action: reject_if_unlogged
    feedback: |
      âŒ No prior log entry found for this change. Use CHANGE_LOG_PATH to register intent first.
--- END OF FILE ./.intent/policies/safety_policies.yaml ---

--- START OF FILE ./.intent/policies/security_intents.yaml ---
security_intents:
  - id: prompt_based_security
    description: "Security rules implemented as LLM prompts"
    enforcement: soft_prompt
    rules:
      - prompt: "Verify no subprocess, eval, or os.system calls"
      - prompt: "Check for safe file operations only"
      - prompt: "Validate no external network calls in core logic"

  - id: security_self_review
    description: "Security improves via self-reflection"
    process:
      - "Generate security concerns as intents"
      - "Review via LLM prompts"
      - "Update security_intents.yaml iteratively"

--- END OF FILE ./.intent/policies/security_intents.yaml ---

--- START OF FILE ./.intent/project_manifest.yaml ---
# .intent/project_manifest.yaml
#
# This is the single, canonical source of truth for the CORE project's
# high-level intent, capabilities, and configuration.
# All other manifests (e.g., function_manifest.json, codegraph.json) are
# derived from the information here and the source code itself.

name: "CORE"
version: "0.6.0" # Using the more recent version from the .json file
intent: "Build a self-improving AI coding assistant that can evolve itself through prompts, code, and validation."

# A comprehensive list of all capabilities the system is expected to have.
# This is the master list for all governance and validation checks.
required_capabilities:
  - prompt_interpretation
  - code_generation
  - syntax_validation
  - test_execution
  - introspection
  - alignment_checking
  - manifest_updating
  - self_correction
  - change_safety_enforcement
  - llm_orchestration
  - intent_guarding
  - semantic_validation
  - code_quality_analysis
  - system_logging
  - add_missing_docstrings
  - audit.check.required_files
  - audit.check.syntax
  - audit.check.project_manifest
  - audit.check.capability_coverage
  - audit.check.capability_definitions
  - audit.check.knowledge_graph_schema
  - audit.check.domain_integrity
  - audit.check.docstrings
  - audit.check.dead_code
  - audit.check.orphaned_intent_files
  - audit.check.environment
  - audit.check.proposals_schema
  - audit.check.proposals_drift
  - audit.check.proposals_list

# Defines the primary agents responsible for executing CORE's logic.
active_agents:
  - planner_agent
  - test_runner
  - validator_agent # Added for clarity

# Defines high-level roles for key directories.
folder_roles:
  "src/core": "Core logic and FastAPI services"
  "src/system": "Governance, introspection, and lifecycle tools"
  "src/agents": "Specialized AI actors (planners, reviewers, suggesters)"
  "tests": "Pytest-based validation for core behaviors"
  ".intent": "The 'brain' of the system: declarations, policies, and knowledge"

# Top-level configuration flags for system behavior.
configuration:
  allow_self_rewrites: true
  execution_mode: "auto" # 'auto' or 'manual_review'

# Metadata about the manifest file itself.
meta:
  created_by: "CORE v0.1 bootstrap"
  created_at: "2025-07-23T00:00:00Z"
  last_updated: "2025-08-05T12:00:00Z" # Using a placeholder for today

# --- Operator UX policy -------------------------------------------------------
operator_experience:
  guard:
    drift:
      # Default presentation when the operator runs the command with no flags:
      default_format: pretty        # options: pretty | table | json
      default_fail_on: any          # options: any | missing | undeclared
      strict_default: true          # require KG/artifact by default
      evidence_json: true           # always write JSON evidence to disk
      evidence_path: reports/drift_report.json
      labels:
        none: "NONE"                # how to label empty sections
        success: "âœ… No capability drift"
        failure: "ðŸš¨ Drift detected"  
--- END OF FILE ./.intent/project_manifest.yaml ---

--- START OF FILE ./.intent/proposals/cr-example.yaml ---
# Example proposal (copy, rename to cr-something.yaml, then edit)

target_path: ".intent/policies/safety_policies.yaml"
action: "replace_file"
justification: >
  Strengthen dangerous call coverage and align policies with current runtime.
  This improves safety and reduces false negatives in validation.

content: |
  # (paste the full new file content here)
  rules:
    - id: no_dangerous_execution
      description: Disallow dynamic eval/exec and OS command execution
      detection:
        patterns:
          - "eval("
          - "exec("
          - "os.system("
          - "subprocess.Popen("
      scope:
        exclude: []
    - id: no_unsafe_imports
      description: Forbid importing known-unsafe modules
      detection:
        forbidden:
          - "pickle"
          - "imp"
      scope:
        exclude: []

rollback_plan:
  description: "If this causes issues, restore the prior policy file from git history."
  previous_version_hint: "git show HEAD:.intent/policies/safety_policies.yaml"

# signatures are added by `core-admin proposals-sign`
signatures: []

--- END OF FILE ./.intent/proposals/cr-example.yaml ---

--- START OF FILE ./.intent/proposals/README.md ---
# Proposals

Create proposals here with filename pattern `cr-*.yaml`.

## Format
- `target_path`: repo-relative path (e.g., `.intent/policies/safety_policies.yaml`)
- `action`: currently only `replace_file`
- `justification`: why this is needed
- `content`: full new file contents (string)
- `rollback_plan` (optional): notes to revert
- `signatures`: added by `core-admin proposals-sign`

See `cr-example.yaml` for a starter.

--- END OF FILE ./.intent/proposals/README.md ---

--- START OF FILE ./.intent/schemas/config_schema.yaml ---
# .intent/schemas/config_schema.yaml
git:
  ignore_validation:
    type: boolean
    default: false
    description: >
      If true, skips Git pre-write checks. MUST be false in production or fallback modes
      to maintain rollback safety. Only for emergency recovery.
--- END OF FILE ./.intent/schemas/config_schema.yaml ---

--- START OF FILE ./.intent/schemas/knowledge_graph_entry.schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://core.system/schema/knowledge_graph_entry.json",
  "title": "Knowledge Graph Symbol Entry",
  "description": "Schema for a single symbol (function or class) in the knowledge_graph.json file.",
  "type": "object",
  "required": [
    "key",
    "name",
    "type",
    "file",
    "domain",
    "agent",
    "capability",
    "intent",
    "last_updated",
    "calls",
    "line_number",
    "is_async",
    "parameters",
    "is_class",
    "structural_hash"
  ],
  "properties": {
    "key": { "type": "string", "description": "The unique identifier for the symbol (e.g., 'path/to/file.py::MyClass')." },
    "name": { "type": "string", "description": "The name of the function or class." },
    "type": { "type": "string", "enum": ["FunctionDef", "ClassDef", "AsyncFunctionDef"] },
    "file": { "type": "string", "description": "The relative path to the source file." },
    "domain": { "type": "string", "description": "The logical domain from source_structure.yaml." },
    "agent": { "type": "string", "description": "The inferred agent responsible for this symbol's domain." },
    "capability": { "type": "string", "description": "The high-level capability this symbol provides, or 'unassigned'." },
    "intent": { "type": "string", "description": "A clear, concise statement of the symbol's purpose." },
    "docstring": { "type": ["string", "null"], "description": "The raw docstring from source code." },
    "calls": { "type": "array", "items": { "type": "string" }, "description": "List of other functions called by this one." },
    "line_number": { "type": "integer", "minimum": 0 },
    "is_async": { "type": "boolean" },
    "parameters": { "type": "array", "items": { "type": "string" } },
    "entry_point_type": { "type": ["string", "null"], "description": "Type of entry point if applicable (e.g., 'fastapi_route_post')." },
    "last_updated": { "type": "string", "format": "date-time" },
    "is_class": { "type": "boolean", "description": "True if the symbol is a class definition." },
    "base_classes": {
      "type": "array",
      "items": { "type": "string" },
      "description": "A list of base classes this symbol inherits from (if it is a class)."
    },
    "entry_point_justification": {
      "type": ["string", "null"],
      "description": "The name of the pattern that identified this symbol as an entry point."
    },
    "parent_class_key": {
      "type": ["string", "null"],
      "description": "The key of the parent class, if this symbol is a method."
    },
    "structural_hash": {
      "type": "string",
      "description": "A SHA256 hash of the symbol's structure, ignoring comments and docstrings."
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/schemas/knowledge_graph_entry.schema.json ---

--- START OF FILE ./.intent/schemas/proposal.schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://core.local/schemas/proposal.schema.json",
  "title": "CORE Proposal (v1)",
  "type": "object",
  "additionalProperties": false,
  "required": ["target_path", "action", "justification", "content"],
  "properties": {
    "target_path": {
      "type": "string",
      "description": "Repo-relative path to the Mind file to be replaced. Must live under .intent/, but not under .intent/proposals/.",
      "pattern": "^\\.intent\\/(?!proposals\\/)[\\w\\-\\.\\/]+$",
      "$comment": "Forward slashes only; prevents writing directly into .intent/proposals/."
    },
    "action": {
      "type": "string",
      "enum": ["replace_file"],
      "description": "Currently only full file replacement is supported."
    },
    "justification": {
      "type": "string",
      "minLength": 10,
      "description": "Human-readable rationale for the change.",
      "pattern": "\\S",
      "$comment": "Reject all-whitespace strings."
    },
    "content": {
      "type": "string",
      "minLength": 1,
      "description": "The full proposed content of the target file."
    },
    "rollback_plan": {
      "type": "object",
      "additionalProperties": false,
      "required": ["description"],
      "properties": {
        "description": { "type": "string", "minLength": 3 },
        "previous_version_hint": { "type": "string" }
      }
    },
    "metadata": {
      "type": "object",
      "additionalProperties": false,
      "description": "Optional authoring metadata (not used for security decisions).",
      "properties": {
        "created_by": { "type": "string", "format": "email" },
        "created_at": { "type": "string", "format": "date-time" },
        "content_mime": {
          "type": "string",
          "enum": ["application/yaml", "application/json", "text/markdown", "text/plain"]
        }
      }
    },
    "content_sha256": {
      "$ref": "#/$defs/sha256",
      "description": "Optional helper: precomputed SHA-256 of `content` for audit tooling."
    },
    "labels": {
      "type": "array",
      "items": { "type": "string", "minLength": 1, "maxLength": 64 },
      "maxItems": 16,
      "uniqueItems": true,
      "description": "Optional tags for filtering/search (e.g., ['governance','policy'])."
    },
    "signatures": {
      "type": "array",
      "description": "Optional array of signature objects. Quorum is enforced outside the schema.",
      "items": { "$ref": "#/$defs/signature" }
    }
  },
  "$defs": {
    "sha256": {
      "type": "string",
      "pattern": "^[a-f0-9]{64}$",
      "$comment": "Lowercase hex digest."
    },
    "base64": {
      "type": "string",
      "pattern": "^(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?$",
      "$comment": "RFC 4648 base64 without newlines."
    },
    "signature": {
      "type": "object",
      "additionalProperties": false,
      "required": ["identity", "signature_b64", "token", "timestamp"],
      "properties": {
        "identity": {
          "type": "string",
          "description": "Human identity for the signer (email preferred)."
        },
        "signature_b64": {
          "$ref": "#/$defs/base64",
          "description": "Ed25519 signature, base64-encoded."
        },
        "token": {
          "type": "string",
          "pattern": "^core-proposal-v1:[a-f0-9]{64}$",
          "description": "Approval token over proposal content. Must match admin CLI format."
        },
        "timestamp": {
          "type": "string",
          "format": "date-time",
          "description": "RFC 3339 timestamp when the signature was created."
        }
      }
    }
  },
  "examples": [
    {
      "target_path": ".intent/policies/safety_policies.yaml",
      "action": "replace_file",
      "justification": "Align policy wording with clarity_first; no behavior change.",
      "content": "# new YAML here",
      "metadata": {
        "created_by": "alice@example.com",
        "created_at": "2025-08-08T12:34:56Z",
        "content_mime": "application/yaml"
      },
      "labels": ["governance", "policy"]
    }
  ]
}

--- END OF FILE ./.intent/schemas/proposal.schema.json ---

--- START OF FILE ./LICENSE ---
MIT License

Copyright (c) 2024 Dariusz Newecki

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
--- END OF FILE ./LICENSE ---

--- START OF FILE ./Makefile ---
# Makefile for CORE â€“ Cognitive Orchestration Runtime Engine

SHELL := /bin/bash
.SHELLFLAGS := -eu -o pipefail -c
.DEFAULT_GOAL := help

# ---- Configurable knobs -----------------------------------------------------
POETRY  ?= python3 -m poetry
PYTHON  ?= python3
APP     ?= src.core.main:app
HOST    ?= 0.0.0.0
PORT    ?= 8000
RELOAD  ?= --reload
ENV_FILE ?= .env
PATHS   ?= .

.PHONY: help install lock run stop audit lint format test coverage check clean clean-logs distclean nuke

help:
	@echo "CORE Development Makefile"
	@echo "-------------------------"
	@echo "make install       - Install deps with Poetry"
	@echo "make lock          - Update poetry.lock"
	@echo "make run           - Start uvicorn ($(APP)) on $(HOST):$(PORT)"
	@echo "make stop          - Stop dev server reliably by killing process on port $(PORT)"
	@echo "make audit         - Run the full self-audit (KnowledgeGraph + Auditor)"
	@echo "make lint          - Ruff checks"
	@echo "make format        - Black + Ruff --fix"
	@echo "make test [ARGS=]  - Pytest (pass ARGS='-k expr -vv')"
	@echo "make coverage      - Pytest with coverage"
	@echo "make check         - Lint + Tests + Audit"
	@echo "make clean         - Remove caches, pending_writes, sandbox"
	@echo "make distclean     - Clean + venv/build leftovers"
	@echo "make nuke          - git clean -fdx (danger)"

install:
	@echo "ðŸ“¦ Installing dependencies..."
	$(POETRY) install

lock:
	@echo "ðŸ”’ Resolving and locking dependencies..."
	$(POETRY) lock

# Ensure we stop before run
run: stop
	@echo "ðŸš€ Starting FastAPI server at http://$(HOST):$(PORT)"
	$(POETRY) run uvicorn $(APP) --host $(HOST) --port $(PORT) $(RELOAD) --env-file $(ENV_FILE)

# --- THIS IS THE IMPROVED VERSION ---
stop:
	@echo "ðŸ›‘ Stopping any process on port $(PORT)..."
	@if command -v lsof >/dev/null 2>&1; then \
		PID=$$(lsof -t -i:$(PORT) || true); \
		if [ -n "$$PID" ]; then \
			echo "  -> Found process with PID: $$PID. Terminating..."; \
			kill $$PID || true; \
		else \
			echo "  -> No process found on port $(PORT)."; \
		fi; \
	else \
		echo "  -> 'lsof' not found. Trying 'pkill'. You might want to install 'lsof' for better reliability."; \
		pkill -f "uvicorn.*$(APP)" || true; \
	fi

audit:
	@echo "ðŸ§  Running constitutional self-audit..."
	$(POETRY) run python -m src.core.capabilities

lint:
	@echo "ðŸŽ¨ Checking code style with Ruff..."
	$(POETRY) run ruff check $(PATHS)

format:
	@echo "âœ¨ Formatting code with Black and Ruff..."
	$(POETRY) run black $(PATHS)
	$(POETRY) run ruff check $(PATHS) --fix

test:
	@echo "ðŸ§ª Running tests with pytest..."
	$(POETRY) run pytest $(ARGS)

coverage:
	@echo "ðŸ§® Running tests with coverage..."
	$(POETRY) run pytest --cov=src --cov-report=term-missing:skip-covered $(ARGS)

check: lint test audit

# ---- Clean targets ---------------------------------------------------------

clean:
	@echo "ðŸ§¹ Cleaning up temporary files and caches..."
	find . -type f -name '*.pyc' -delete
	find . -type d -name '__pycache__' -prune -exec rm -rf {} +
	rm -rf .pytest_cache .ruff_cache .mypy_cache .cache
	rm -f .coverage
	rm -rf htmlcov
	rm -rf build dist *.egg-info
	rm -rf pending_writes sandbox
	@echo "âœ… Clean complete."

distclean: clean
	@echo "ðŸ§¨ Distclean: removing virtual environments and build leftovers..."
	rm -rf .venv
	@echo "âœ… Distclean complete."

nuke:
	@echo "â˜¢ï¸  Running 'git clean -fdx' in 3s (CTRL+C to cancel)..."
	@sleep 3
	git clean -fdx
	@echo "âœ… Repo nuked (untracked files/dirs removed)."
--- END OF FILE ./Makefile ---

--- START OF FILE ./pyproject.toml ---
# pyproject.toml

[build-system]
# This part is correct: it tells Python to use Poetry to build the project.
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

# --- Poetry Project Metadata ---
# This [tool.poetry] section replaces the old [project] section.
# Poetry uses this to manage dependencies and package details.
[tool.poetry]
name = "core"
version = "0.1.0"
description = "CORE: A self-governing, intent-driven software development system."
authors = ["Dariusz Newecki <d.newecki@gmail.com>"]
license = "MIT"
readme = "README.md"
# --- MODIFICATION: Include all packages from the 'src' directory ---
packages = [
    { include = "core", from = "src" },
    { include = "agents", from = "src" },
    { include = "system", from = "src" },
    { include = "shared", from = "src" },
]

# --- Main Dependencies ---
# This replaces the old [project].dependencies array.
[tool.poetry.dependencies]
python = ">=3.9"
fastapi = ">=0.95.0"
uvicorn = ">=0.21.0"
pyyaml = ">=6.0"
requests = ">=2.28.0"
# --- THIS IS THE FIX ---
# We are adding 'httpx' which is an asynchronous HTTP client.
httpx = ">=0.25.0"
python-dotenv = ">=1.0.0"
pydantic = ">=2.0.0"
# --- ADDITION: Centralized settings management ---
pydantic-settings = "^2.0.0" 
cryptography = ">=42.0.0"
rich = "^13"
black = "^24"
jsonschema = "^4"
# --- ADDITION: A powerful CLI library ---
typer = {extras = ["rich"], version = "^0.9.0"}

# --- Development Dependencies ---
# This replaces the old [project.optional-dependencies].
# All dev/test tools go here. You install them with `poetry install --with dev`.
[tool.poetry.group.dev.dependencies]
pytest = ">=7.0,<8.0" # Pinned to avoid the conflict we saw
pytest-asyncio = "==0.21.0" # Pinned to the version we know works
ruff = ">=0.0.254"

# --- Command-Line Scripts ---
# This replaces the old [project.scripts] section.
# We will update this in a later step.
[tool.poetry.scripts]
core-admin = "system.admin_cli:app"

# --- Configuration for development tools (These do not change) ---
[tool.ruff]
line-length = 88
select = ["E", "W", "F", "I"]

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
pythonpath = ["src"]
addopts = ["-c", "pyproject.toml"]

--- END OF FILE ./pyproject.toml ---

--- START OF FILE ./README.md ---
# CORE â€” The Self-Improving System Architect

> **Where Intelligence Lives.**

[![Status: Architectural Prototype](https://img.shields.io/badge/status-architectural%20prototype-blue.svg)](#-project-status)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

CORE is a self-governing, constitutionally aligned AI development framework that can plan, write, validate, and evolve software systems â€” autonomously and safely. It is designed for environments where **trust, traceability, and governance matter**.

---

## ðŸ›ï¸ Project Status: Architectural Prototype

The core self-governance and constitutional amendment loop is complete and stable. The system can audit and modify its own constitution via a human-in-the-loop, cryptographically signed approval process.

The next phase, as outlined in our **[Strategic Plan](docs/StrategicPlan.md)**, is to expand agent capabilities so CORE can generate and manage entirely new applications based on user intent.

Weâ€™re making the project public now to invite collaboration on this foundational architecture.

---

## ðŸ§  What CORE *is*

* ðŸ§¾ Evolves itself through **declared intent**, not hidden assumptions
* ðŸ›¡ï¸ Enforces **constitutional rules**, **domain boundaries**, and **safety policies**
* ðŸ§© Uses a modular agent architecture with a clear separation of concerns
* ðŸ“š Ensures every decision is **documented, reversible, and introspectable**

---

## ðŸ¦® Key Concepts

| Concept                     | Description                                                                              |
| --------------------------- | ---------------------------------------------------------------------------------------- |
| **`.intent/`**              | The â€œmindâ€ of CORE: constitution, policies, capability maps, and self-knowledge.         |
| **`ConstitutionalAuditor`** | The â€œimmune system,â€ continuously verifying code aligns with the constitution.           |
| **`PlannerAgent`**          | Decomposes high-level goals into executable plans.                                       |
| **`core-admin` CLI**        | Human-in-the-loop tool for signing and ratifying constitutional changes.                 |
| **Canary Check**            | Applies proposed changes to an isolated copy and runs a full self-audit before approval. |
| **Knowledge Graph**         | Machine-readable map of symbols, roles, capabilities, and relationships.                 |
| **Git & Rollback**          | Everything is versioned; invalid changes can be safely rolled back.                      |

---

## âš™ï¸ Requirements

* Python **3.9+**
* [Poetry](https://python-poetry.org/) for dependency & venv management
* Optional: `ruff`, `black` (installed via Poetry), `uvicorn` for the API server

---

## ðŸš€ Getting Started

1. **Install dependencies**

```bash
poetry install
```

2. **Set up environment**

```bash
cp .env.example .env
# Edit .env with your keys/URLs. See .intent/config/runtime_requirements.yaml for required variables.
```

3. **Quick validation (governance)**

```bash
# Generate a knowledge-graph artifact (used by strict checks)
core-admin guard kg-export

# Detect capability drift between .intent manifest and code (strict mode)
core-admin guard drift --strict-intent --format pretty
# or JSON for machines/CI:
core-admin guard drift --strict-intent --format json
```

4. **Run the server (optional)**

```bash
make run
# FastAPI docs at: http://localhost:8000/docs
```

---

## ðŸ§‘â€âš–ï¸ Human-in-the-Loop (CLI)

Constitutional changes are proposed as files under `.intent/proposals/` and managed via the `core-admin` CLI.

```bash
# List pending proposals
core-admin proposals-list

# Sign a proposal with your generated key
core-admin proposals-sign cr-example.yaml

# Approve a proposal (runs canary self-audit in isolation)
core-admin proposals-approve cr-example.yaml

# Generate a new key pair for a new contributor
core-admin keygen "name@example.com"
```

> If `core-admin` isnâ€™t found, try: `poetry run core-admin ...`

---

## âœ… Validation Tools (Operator & CI)

### Capability Drift (source of truth = `.intent/`)

* Compares declared capabilities in `.intent/*manifest*.yaml` with capabilities discovered in code (via Knowledge Graph or tagged comments).
* **Strict mode** requires a KG artifact or live builder; it wonâ€™t â€œguessâ€.

**Typical flow**

```bash
core-admin guard kg-export
core-admin guard drift --strict-intent --format pretty
```

**Evidence**

* A machine-readable JSON report is written to `reports/drift_report.json`.

**Output UX defaults (human-friendly)**

* By default, youâ€™ll see a colored summary with **NONE** for empty sections and a clear âœ… or ðŸš¨ status.
* JSON remains available for CI.

These defaults are governed by `.intent` (see below).

---

## ðŸ“ UX Defaults Governed by `.intent/`

Place this block in `.intent/project_manifest.yaml` to make operator behavior explicit and predictable for anyone (human or agent):

```yaml
operator_experience:
  guard:
    drift:
      default_format: pretty        # pretty | table | json
      default_fail_on: any          # any | missing | undeclared
      strict_default: true          # require KG/artifact by default
      evidence_json: true           # always write JSON evidence
      evidence_path: reports/drift_report.json
      labels:
        none: "NONE"
        success: "âœ… No capability drift"
        failure: "ðŸš¨ Drift detected"
```

> Change `default_format` to `json` if you prefer raw JSON by default.

---

## ðŸ”ª What CORE *does*

* Plans improvements using AI agents
* Generates code, tests, and docstrings
* **Self-audits** to ensure constitutional alignment
* **Governs self-modification** via signed proposals + canary checks
* Self-corrects when validation fails
* Logs every step for transparency

---

## ðŸŒŒ North Star

COREâ€™s long-term aim is **A5 autonomy**: turn goals into governed code and running systems, safely and without human intervention in low-risk areas.
See **[NORTH\_STAR](docs/NORTH_STAR.md)** and **[BYOR](docs/05_BYOR.md)** for how CORE applies the same rules to any repo â€” including itself.

---

## ðŸ§° Troubleshooting

* **Strict mode error about missing capabilities**

  * Run `core-admin guard kg-export` first (or disable strict mode).
* **CLI not found**

  * Use `poetry run core-admin ...`
* **Drift report not in expected folder**

  * Reports are written under the repo root: `reports/`. You can change the path in `.intent` (`evidence_path`).

---

## ðŸ“Œ Why CORE is Different

* **Separation of duties** between agents and roles
* **Capability tags** (`# CAPABILITY:`) at the function/class level
* A **declared, auditable constitution** that governs behavior
* **Rollback, review, and cryptographic validation by default**
* Built for **governance-heavy** and **safety-critical** contexts

---

## ðŸŒ± Contributing

We welcome contributions from:

* AI engineers
* DevOps/GitOps pros
* Policy designers
* Governance/compliance experts

ðŸ‘‰ See **[`CONTRIBUTING.md`](CONTRIBUTING.md)** to get started.
ðŸ‘‰ Check the **[Strategic Plan](docs/StrategicPlan.md)** for where we're headed.

---

## ðŸ“„ License

Licensed under the **MIT License**. See **[LICENSE](LICENSE)**.

---

## ðŸ’¡ Inspiration

CORE was born from a simple but powerful idea:
**â€œSoftware should not only work â€” it should know *why* it works, and who itâ€™s working for.â€**

--- END OF FILE ./README.md ---

--- START OF FILE ./reports/drift_report.json ---
{
  "missing_in_code": [],
  "undeclared_in_manifest": [],
  "mismatched_mappings": []
}
--- END OF FILE ./reports/drift_report.json ---

--- START OF FILE ./reports/knowledge_graph.json ---
{
  "nodes": [
    {
      "capability": "add_missing_docstrings",
      "domain": null,
      "owner": null
    },
    {
      "capability": "alignment_checking",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.capability_coverage",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.capability_definitions",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.dead_code",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.docstrings",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.domain_integrity",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.environment",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.knowledge_graph_schema",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.orphaned_intent_files",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.project_manifest",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.proposals_drift",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.proposals_list",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.proposals_schema",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.required_files",
      "domain": null,
      "owner": null
    },
    {
      "capability": "audit.check.syntax",
      "domain": null,
      "owner": null
    },
    {
      "capability": "change_safety_enforcement",
      "domain": null,
      "owner": null
    },
    {
      "capability": "code_generation",
      "domain": null,
      "owner": null
    },
    {
      "capability": "code_quality_analysis",
      "domain": null,
      "owner": null
    },
    {
      "capability": "intent_guarding",
      "domain": null,
      "owner": null
    },
    {
      "capability": "introspection",
      "domain": null,
      "owner": null
    },
    {
      "capability": "llm_orchestration",
      "domain": null,
      "owner": null
    },
    {
      "capability": "manifest_updating",
      "domain": null,
      "owner": null
    },
    {
      "capability": "prompt_interpretation",
      "domain": null,
      "owner": null
    },
    {
      "capability": "self_correction",
      "domain": null,
      "owner": null
    },
    {
      "capability": "semantic_validation",
      "domain": null,
      "owner": null
    },
    {
      "capability": "syntax_validation",
      "domain": null,
      "owner": null
    },
    {
      "capability": "system_logging",
      "domain": null,
      "owner": null
    },
    {
      "capability": "test_execution",
      "domain": null,
      "owner": null
    }
  ]
}
--- END OF FILE ./reports/knowledge_graph.json ---

--- START OF FILE ./scripts/bootstrap_issues.sh ---
#!/usr/bin/env bash
# Create a visible, pragmatic roadmap as GitHub issues.
# Prereq: gh auth login  (or GH_TOKEN env)

set -euo pipefail

REPO="${1:-}" # optional: org/repo, otherwise uses current
LABELS_COMMON="roadmap,organizational"
MILESTONE="${2:-Organizational Pass}"

create_issue () {
  local title="$1"
  local body="$2"
  local labels="$3"
  if [[ -n "$REPO" ]]; then
    gh issue create --repo "$REPO" --title "$title" --body "$body" --label "$labels"
  else
    gh issue create --title "$title" --body "$body" --label "$labels"
  fi
}

# Ensure labels exist (idempotent)
ensure_label () {
  local name="$1"; local color="$2"; local desc="$3"
  gh label create "$name" --color "$color" --description "$desc" 2>/dev/null || true
}
ensure_label "roadmap" "0366d6" "Roadmap item"
ensure_label "organizational" "a2eeef" "Project organization"
ensure_label "ci" "7057ff" "CI/CD"
ensure_label "audit" "d73a4a" "Constitutional audit & governance"
ensure_label "docs" "0e8a16" "Documentation"

create_issue "Add JSON logging & request IDs" $'**Goal**: Switch logger to support LOG_FORMAT=json and add request id middleware in FastAPI.\n\n**Acceptance**\n- LOG_FORMAT=json writes structured logs\n- x-request-id is set/propagated\n- Docs updated in docs/CONVENTIONS.md' "$LABELS_COMMON,ci"

create_issue "Pre-commit hooks (Black, Ruff)" $'**Goal**: Add .pre-commit-config.yaml and wire to Make.\n\n**Acceptance**\n- pre-commit runs Black/Ruff locally\n- CI stays green' "$LABELS_COMMON,ci"

create_issue "Docs: CONVENTIONS.md & DEPENDENCIES.md" $'**Goal**: Codify folder map, import rules, capability tags, dependency policy.\n\n**Acceptance**\n- New contributors can place files w/o asking\n- Import discipline matrix documented' "$LABELS_COMMON,docs"

create_issue "Governance: proposal.schema.json + proposal_checks" $'**Goal**: Enforce schema & drift checks for .intent/proposals.\n\n**Acceptance**\n- Auditor shows schema pass/fail\n- Drift (token mismatch) â†’ warning\n- Example proposal present' "$LABELS_COMMON,audit"

create_issue "Modular manifests (aggregator + fallback)" $'**Goal**: Support src/*/manifest.yaml aggregated into .intent/knowledge/project_manifest_aggregated.yaml.\n\n**Acceptance**\n- Auditor prefers aggregated manifest\n- Backward-compatible with monolith' "$LABELS_COMMON"

create_issue "Pilot domain package (proposals)" $'**Goal**: Create src/domain/proposals/{models,services,schemas}.py and migrate only proposal-related code.\n\n**Acceptance**\n- No new audit failures\n- Clear import boundaries documented' "$LABELS_COMMON"

--- END OF FILE ./scripts/bootstrap_issues.sh ---

--- START OF FILE ./scripts/gh_status_report.sh ---
#!/usr/bin/env bash
set -euo pipefail
OWNER="${OWNER:-DariuszNewecki}"
REPO="${REPO:-CORE}"

has_jq() { command -v jq >/dev/null 2>&1; }

out="GH_STATUS.md"
echo "# GitHub Status Report â€” $OWNER/$REPO" > "$out"
echo "" >> "$out"
echo "Generated: $(date -u +"%Y-%m-%d %H:%M:%SZ")" >> "$out"
echo "" >> "$out"

echo "## Repository" >> "$out"
gh api repos/$OWNER/$REPO > /tmp/repo.json
if has_jq; then
  jq '{name,visibility,default_branch,open_issues_count,description}' /tmp/repo.json >> "$out"
else
  cat /tmp/repo.json >> "$out"
fi
echo "" >> "$out"

echo "## Milestones" >> "$out"
gh api repos/$OWNER/$REPO/milestones --paginate > /tmp/miles.json || echo "[]">/tmp/miles.json
if has_jq; then
  jq '.[] | {number,title,state,due_on,open_issues,closed_issues,description}' /tmp/miles.json >> "$out"
else
  cat /tmp/miles.json >> "$out"
fi
echo "" >> "$out"

echo "## Issues (all)" >> "$out"
gh issue list --repo $OWNER/$REPO --state all --limit 200 \
  --json number,title,state,labels,milestone,url,createdAt,closedAt > /tmp/issues.json
if has_jq; then
  jq '.[] | {number,title,state,milestone: .milestone.title,labels: [.labels[].name],url,createdAt,closedAt}' /tmp/issues.json >> "$out"
else
  cat /tmp/issues.json >> "$out"
fi
echo "" >> "$out"

echo "## Labels" >> "$out"
gh label list --repo $OWNER/$REPO --json name,color,description > /tmp/labels.json
if has_jq; then
  jq '.[] | {name,color,description}' /tmp/labels.json >> "$out"
else
  cat /tmp/labels.json >> "$out"
fi
echo "" >> "$out"

echo "## Projects (Projects v2)" >> "$out"
gh project list --owner $OWNER > /tmp/projects.txt || true
cat /tmp/projects.txt >> "$out"
echo "" >> "$out"
if grep -Eo '#[0-9]+' /tmp/projects.txt >/dev/null 2>&1; then
  while read -r num; do
    pnum="${num//#/}"
    echo "### Project $pnum" >> "$out"
    gh project view "$pnum" --owner $OWNER --format json >> "$out" || true
    echo "" >> "$out"
  done < <(grep -Eo '#[0-9]+' /tmp/projects.txt | sort -u)
fi

echo "## Releases" >> "$out"
gh release list --repo $OWNER/$REPO >> "$out" || true
echo "" >> "$out"

echo "Report written to $out"

--- END OF FILE ./scripts/gh_status_report.sh ---

--- START OF FILE ./src/agents/__init__.py ---
# src/agents/__init__.py
# Package marker for src/agents â€” contains CORE's agent implementations (e.g., PlannerAgent).
--- END OF FILE ./src/agents/__init__.py ---

--- START OF FILE ./src/agents/manifest.yaml ---
capabilities:
- code_generation
- llm_orchestration
description: Specialized AI actors (planners, reviewers, suggesters)
domain: agents

--- END OF FILE ./src/agents/manifest.yaml ---

--- START OF FILE ./src/agents/models.py ---
# src/agents/models.py
"""
Data models for the PlannerAgent and execution tasks.
Defines the structure of plans, tasks, and configurations.
"""
from typing import Optional, Literal
from dataclasses import dataclass
from enum import Enum
from pydantic import BaseModel

class TaskStatus(Enum):
    """Enumeration of possible states for an ExecutionTask."""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class ExecutionProgress:
    """Represents the progress of a plan's execution."""
    total_tasks: int
    completed_tasks: int
    current_task: Optional[str] = None
    status: TaskStatus = TaskStatus.PENDING
    
    @property
    def completion_percentage(self) -> float:
        """Calculates the completion percentage of the plan as a float, returning 0 if there are no tasks."""
        """Calculates the completion percentage of the plan."""
        return (self.completed_tasks / self.total_tasks) * 100 if self.total_tasks > 0 else 0

@dataclass
class PlannerConfig:
    """Configuration settings for the PlannerAgent's behavior."""
    max_retries: int = 3
    validation_enabled: bool = True
    auto_commit: bool = True
    rollback_on_failure: bool = True
    task_timeout: int = 300  # seconds

# --- THIS IS THE CORRECT, FLEXIBLE VERSION ---
class TaskParams(BaseModel):
    """Data model for the parameters of a single task in an execution plan."""
    file_path: str
    symbol_name: Optional[str] = None
    tag: Optional[str] = None
    code: Optional[str] = None

class ExecutionTask(BaseModel):
    """Data model for a single, executable step in a plan."""
    step: str
    action: Literal["add_capability_tag", "create_file", "edit_function"]
    params: TaskParams
--- END OF FILE ./src/agents/models.py ---

--- START OF FILE ./src/agents/planner_agent.py ---
# src/agents/planner_agent.py
"""
The primary agent responsible for decomposing high-level goals into executable plans.
"""

import json
import re
import textwrap
import asyncio
from typing import List, Dict, Tuple, Optional, Callable
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor
import contextvars
import atexit

from pydantic import ValidationError

from core.clients import OrchestratorClient, GeneratorClient
from core.file_handler import FileHandler
from core.git_service import GitService
from core.intent_guard import IntentGuard
from core.prompt_pipeline import PromptPipeline
from core.validation_pipeline import validate_code
from shared.utils.parsing import parse_write_blocks
from shared.logger import getLogger

from agents.models import ExecutionTask, ExecutionProgress, PlannerConfig, TaskParams, TaskStatus
from agents.utils import PlanExecutionContext, SymbolLocator, CodeEditor

log = getLogger(__name__)

# Context for structured logging
execution_context = contextvars.ContextVar('execution_context')

class PlanExecutionError(Exception):
    """Custom exception for failures during plan creation or execution."""
    def __init__(self, message, violations=None):
        super().__init__(message)
        self.violations = violations or []

# CAPABILITY: code_generation
class PlannerAgent:
    """
    The primary agent responsible for decomposing high-level goals into executable plans.
    It orchestrates the generation, validation, and commitment of code changes.
    """
    
    def __init__(self,
                 orchestrator_client: OrchestratorClient,
                 generator_client: GeneratorClient,
                 file_handler: FileHandler,
                 git_service: GitService,
                 intent_guard: IntentGuard,
                 config: Optional[PlannerConfig] = None):
        """Initializes the PlannerAgent with all necessary service dependencies."""
        self.orchestrator = orchestrator_client
        self.generator = generator_client
        self.file_handler = file_handler
        self.git_service = git_service
        self.intent_guard = intent_guard
        self.config = config or PlannerConfig()
        self.repo_path = self.file_handler.repo_path
        self.prompt_pipeline = PromptPipeline(self.repo_path)
        self.symbol_locator = SymbolLocator()
        self.code_editor = CodeEditor()
        
        self._executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="planner_agent")
        atexit.register(self._cleanup_resources)

    def _cleanup_resources(self):
        """Clean up resources on shutdown."""
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=True)

    def __del__(self):
        """Ensure resources are cleaned up when the agent is garbage collected."""
        self._cleanup_resources()

    def _setup_logging_context(self, goal: str, plan_id: str):
        """Setup structured logging context for better observability."""
        execution_context.set({
            'goal': goal,
            'plan_id': plan_id,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })
    
    # --- THIS IS THE NEW, MORE ROBUST FUNCTION ---
    def _extract_json_from_response(self, text: str) -> Optional[Dict]:
        """
        Extract JSON with multiple strategies and better error handling.
        """
        # Strategy 1: Look for a markdown code block with 'json'
        match = re.search(r'```json\s*(\{.*\}|\[.*\])\s*```', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                log.warning("Found a JSON markdown block, but it contained invalid JSON.")

        # Strategy 2: Look for any JSON-like string (starts with { or [)
        match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                log.warning("Found a JSON-like string, but it was invalid.")

        log.error(f"Failed to extract any valid JSON from the LLM response.")
        return None
    # --- END OF NEW FUNCTION ---

    def _log_plan_summary(self, plan: List[ExecutionTask]) -> None:
        """Log a readable summary of the execution plan."""
        log.info(f"ðŸ“‹ Execution Plan Summary ({len(plan)} tasks):")
        for i, task in enumerate(plan, 1):
            log.info(f"  {i}. [{task.action}] {task.step}")
    
    def _validate_task_params(self, task: ExecutionTask):
        """Validates that a task has all the logically required parameters for its action."""
        params = task.params
        if task.action == "add_capability_tag":
            if not all([params.file_path, params.symbol_name, params.tag]):
                raise PlanExecutionError(f"Task '{task.step}' is missing required parameters for 'add_capability_tag'.")
        elif task.action == "create_file":
            if not params.file_path:
                raise PlanExecutionError(f"Task '{task.step}' is missing required parameter 'file_path' for 'create_file'.")
        elif task.action == "edit_function":
             if not all([params.file_path, params.symbol_name]):
                raise PlanExecutionError(f"Task '{task.step}' is missing required parameters for 'edit_function'.")

    # CAPABILITY: llm_orchestration
    def create_execution_plan(self, high_level_goal: str) -> List[ExecutionTask]:
        """Creates a high-level, code-agnostic execution plan."""
        plan_id = f"plan_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
        self._setup_logging_context(high_level_goal, plan_id)
        
        log.info(f"ðŸ§  Step 1: Decomposing goal into a high-level plan...")
        
        prompt_template = textwrap.dedent("""
            You are a hyper-competent, meticulous system architect AI. Your task is to decompose a high-level goal into a JSON execution plan.
            Your entire output MUST be a single, valid JSON array of objects.

            **High-Level Goal:**
            "{goal}"

            **Reasoning Framework:**
            1.  Analyze the Goal: What is the user's core intent?
            2.  Choose the Right Tool: Select the correct action from the list below.
                - To CREATE a NEW file, use the `create_file` action.
                - To MODIFY an EXISTING file/function, use `edit_function`.
                - To ADD a #CAPABILITY tag to an EXISTING function, use `add_capability_tag`.
            3.  Construct the Plan: Build a JSON object for each step. **DO NOT generate any code content.** Just define the actions and targets.

            **Available Actions & Required Parameters (for this step):**
            - Action: `create_file` -> Params: `{{ "file_path": "<path_to_new_file>" }}`
            - Action: `edit_function` -> Params: `{{ "file_path": "<path_to_existing_file>", "symbol_name": "<function_to_edit>" }}`
            - Action: `add_capability_tag` -> Params: `{{ "file_path": "<path_to_existing_file>", "symbol_name": "<function_to_tag>", "tag": "<tag_name>" }}`
            
            **CRITICAL RULE:**
            - Every task object MUST include a `"step"` and `"action"` key.
            - The `"params"` object should ONLY contain the parameters listed above. DO NOT include a `"code"` parameter.

            Generate the complete, code-free JSON plan now.
        """).strip()

        final_prompt = prompt_template.format(goal=high_level_goal)
        enriched_prompt = self.prompt_pipeline.process(final_prompt)
        
        for attempt in range(self.config.max_retries):
            try:
                response_text = self.orchestrator.make_request(enriched_prompt, user_id="planner_agent_architect")
                parsed_json = self._extract_json_from_response(response_text)
                if not parsed_json: raise ValueError("No valid JSON found in response")
                if isinstance(parsed_json, dict): parsed_json = [parsed_json]
                
                validated_plan = [ExecutionTask(**task) for task in parsed_json]
                self._log_plan_summary(validated_plan)
                return validated_plan
                
            except (ValueError, json.JSONDecodeError, ValidationError) as e:
                log.warning(f"High-level plan creation attempt {attempt + 1} failed: {e}")
                if attempt == self.config.max_retries - 1:
                    log.error("FATAL: Failed to create valid high-level plan after max retries.")
                    raise PlanExecutionError("Failed to create a valid high-level plan.")
        return []

    async def _generate_code_for_task(self, task: ExecutionTask, goal: str) -> str:
        """Generates the code content for a single task."""
        log.info(f"âœï¸ Step 2: Generating code for task: '{task.step}'...")

        if task.action not in ["create_file", "edit_function"]:
            return ""

        prompt_template = textwrap.dedent("""
            You are an expert Python programmer. Your task is to generate a single block of Python code to fulfill a specific step in a larger plan.

            **Overall Goal:** {goal}
            **Current Task:** {step}
            **Target File:** {file_path}

            **Instructions:**
            - Your output MUST be ONLY the raw Python code.
            - Do not wrap the code in markdown blocks (```python ... ```).
            - Do not add any conversational text or explanations.
            - Ensure the code is complete, correct, and ready to be written to a file.
            - If editing a function, you MUST provide the complete, new version of that function, including its decorator, signature, and docstring.
            
            Generate the code now.
        """).strip()

        final_prompt = prompt_template.format(
            goal=goal,
            step=task.step,
            file_path=task.params.file_path,
        )
        enriched_prompt = self.prompt_pipeline.process(final_prompt)
        
        return self.generator.make_request(enriched_prompt, user_id="planner_agent_coder")

    async def _find_symbol_line_async(self, file_path: str, symbol_name: str) -> Optional[int]:
        """Async version using shared thread pool for file I/O."""
        loop = asyncio.get_event_loop()
        full_path = self.repo_path / file_path
        return await loop.run_in_executor(
            self._executor, self.symbol_locator.find_symbol_line, full_path, symbol_name
        )

    async def _execute_task_with_timeout(self, task: ExecutionTask, timeout: int = None) -> None:
        """Execute task with timeout protection."""
        timeout = timeout or self.config.task_timeout
        try:
            await asyncio.wait_for(self._execute_task(task), timeout=timeout)
        except asyncio.TimeoutError:
            raise PlanExecutionError(f"Task '{task.step}' timed out after {timeout}s")

    async def execute_plan(self, high_level_goal: str) -> Tuple[bool, str]:
        """Creates and executes a plan in a two-step (Plan -> Generate -> Execute) process."""
        try:
            plan = self.create_execution_plan(high_level_goal)
        except PlanExecutionError as e:
            return False, str(e)

        if not plan: return False, "Plan is empty or invalid."
        
        progress = ExecutionProgress(total_tasks=len(plan), completed_tasks=0)
        
        with PlanExecutionContext(self):
            for i, task in enumerate(plan):
                log.info(f"--- Executing Step {i + 1}/{len(plan)}: {task.step} ---")
                try:
                    if task.action in ["create_file", "edit_function"]:
                        generated_code = await self._generate_code_for_task(task, high_level_goal)
                        if not generated_code:
                            raise PlanExecutionError("Code generation failed for this step.")
                        task.params.code = generated_code

                    await self._execute_task_with_timeout(task)
                    progress.completed_tasks += 1
                except Exception as e:
                    error_detail = str(e)
                    log.error(f"Step failed with error: {error_detail}", exc_info=True)
                    if hasattr(e, 'violations') and e.violations:
                        log.error("Violations found:")
                        for v in e.violations:
                            log.error(f"  - [{v.get('rule')}] L{v.get('line')}: {v.get('message')}")
                    return False, f"Plan failed at step {i + 1} ('{task.step}'): {error_detail}"
        
        return True, "âœ… Plan executed successfully."

    async def _execute_add_tag(self, params: TaskParams):
        """Executes the surgical 'add_capability_tag' action."""
        file_path, symbol_name, tag = params.file_path, params.symbol_name, params.tag
        line_number = await self._find_symbol_line_async(file_path, symbol_name)
        if not line_number: raise PlanExecutionError(f"Could not find symbol '{symbol_name}' in '{file_path}'.")
        
        full_path = self.repo_path / file_path
        if not full_path.exists():
            raise PlanExecutionError(f"File '{file_path}' does not exist.")
            
        lines = full_path.read_text(encoding='utf-8').splitlines()
        
        insertion_index = line_number - 1
        if insertion_index > 0 and f"# CAPABILITY: {tag}" in lines[insertion_index - 1]:
            log.info(f"Tag '{tag}' already exists for '{symbol_name}'. Skipping.")
            return

        indentation = len(lines[insertion_index]) - len(lines[insertion_index].lstrip(' '))
        lines.insert(insertion_index, f"{' ' * indentation}# CAPABILITY: {tag}")
        modified_code = "\n".join(lines)

        validation_result = validate_code(file_path, modified_code)
        if validation_result["status"] == "dirty":
            raise PlanExecutionError(f"Surgical modification for '{file_path}' failed validation.", violations=validation_result["violations"])
            
        pending_id = self.file_handler.add_pending_write(
            prompt=f"Goal: add tag to {symbol_name}", suggested_path=file_path, code=validation_result["code"]
        )
        self.file_handler.confirm_write(pending_id)

        if self.config.auto_commit and self.git_service.is_git_repo():
            self.git_service.add(file_path)
            self.git_service.commit(f"refactor(capability): Add '{tag}' tag to {symbol_name}")

    async def _execute_create_file(self, params: TaskParams):
        """Executes the 'create_file' action."""
        file_path, code = params.file_path, params.code
        full_path = self.repo_path / file_path
        if full_path.exists():
            raise FileExistsError(f"File '{file_path}' already exists. Use 'edit_function' or another edit action instead.")

        validation_result = validate_code(file_path, code)
        if validation_result["status"] == "dirty":
            raise PlanExecutionError(f"Generated code for new file '{file_path}' failed validation.", violations=validation_result["violations"])

        pending_id = self.file_handler.add_pending_write(
            prompt=f"Goal: create file {file_path}", suggested_path=file_path, code=validation_result["code"]
        )
        self.file_handler.confirm_write(pending_id)

        if self.config.auto_commit and self.git_service.is_git_repo():
            self.git_service.add(file_path)
            self.git_service.commit(f"feat: Create new file {file_path}")
            
    async def _execute_edit_function(self, params: TaskParams):
        """Executes the 'edit_function' action using the CodeEditor."""
        file_path, symbol_name, new_code = params.file_path, params.symbol_name, params.code
        full_path = self.repo_path / file_path

        if not full_path.exists():
            raise FileNotFoundError(f"Cannot edit function, file not found: '{file_path}'")

        loop = asyncio.get_event_loop()
        original_code = await loop.run_in_executor(self._executor, full_path.read_text, "utf-8")

        function_only = textwrap.dedent(new_code).strip()
        validation_result = validate_code(file_path, function_only)
        if validation_result["status"] == "dirty":
            raise PlanExecutionError(f"Modified code for '{file_path}' failed validation.", violations=validation_result["violations"])

        try:
            final_code = self.code_editor.replace_symbol_in_code(original_code, symbol_name, validation_result["code"])
        except ValueError as e:
            raise PlanExecutionError(f"Failed to edit code in '{file_path}': {e}")

        pending_id = self.file_handler.add_pending_write(
            prompt=f"Goal: edit function {symbol_name} in {file_path}", suggested_path=file_path, code=final_code
        )
        self.file_handler.confirm_write(pending_id)

        if self.config.auto_commit and self.git_service.is_git_repo():
            self.git_service.add(file_path)
            self.git_service.commit(f"feat: Modify function {symbol_name} in {file_path}")

    async def _execute_task(self, task: ExecutionTask) -> None:
        """Dispatcher that executes a single task from a plan based on its action type."""
        self._validate_task_params(task)

        if task.action == "add_capability_tag":
            await self._execute_add_tag(task.params)
        elif task.action == "create_file":
            await self._execute_create_file(task.params)
        elif task.action == "edit_function":
            await self._execute_edit_function(task.params)
        else:
            log.warning(f"Skipping task: Unknown action '{task.action}'.")

--- END OF FILE ./src/agents/planner_agent.py ---

--- START OF FILE ./src/agents/utils.py ---
# src/agents/utils.py
"""
Utility classes and functions for CORE agents.
"""
import ast
import textwrap
from pathlib import Path
from typing import Optional, Union, Tuple

from shared.logger import getLogger

log = getLogger(__name__)

# --- REFACTORED CLASS: CodeEditor (Corrected & Robust Implementation) ---
# This version uses a line-based replacement strategy guided by the AST,
# which is more robust for preserving comments and file structure.
class CodeEditor:
    """Provides capabilities to surgically edit code files."""

    def _get_symbol_start_end_lines(self, tree: ast.AST, symbol_name: str) -> Optional[Tuple[int, int]]:
        """Finds the 1-based start and end line numbers of a symbol."""
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                if node.name == symbol_name:
                    # The end_lineno attribute is available in Python 3.8+ and is inclusive.
                    if hasattr(node, 'end_lineno') and node.end_lineno is not None:
                        return node.lineno, node.end_lineno
        return None

    def replace_symbol_in_code(self, original_code: str, symbol_name: str, new_code_str: str) -> str:
        """Error: Could not connect to LLM endpoint. Details: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out. (read timeout=180)"""
        """
        Replaces a function/method in code with a new version using a line-based strategy.
        """
        try:
            original_tree = ast.parse(original_code)
        except SyntaxError as e:
            raise ValueError(f"Could not parse original code due to syntax error: {e}")

        symbol_location = self._get_symbol_start_end_lines(original_tree, symbol_name)
        if not symbol_location:
            raise ValueError(f"Symbol '{symbol_name}' not found in the original code.")

        start_line, end_line = symbol_location
        # Convert to 0-based indices for list slicing
        start_index = start_line - 1
        end_index = end_line

        lines = original_code.splitlines()
        
        # Determine the indentation of the original symbol
        original_line = lines[start_index]
        indentation = len(original_line) - len(original_line.lstrip(' '))
        
        # Prepare the new code block
        clean_new_code = textwrap.dedent(new_code_str).strip()
        new_code_lines = clean_new_code.splitlines()
        indented_new_code_lines = [f"{' ' * indentation}{line}" for line in new_code_lines]

        # Reconstruct the file content
        code_before = lines[:start_index]
        code_after = lines[end_index:]
        
        final_lines = code_before + indented_new_code_lines + code_after
        # Join with newline to create the final string
        return "\n".join(final_lines)


class SymbolLocator:
    """Dedicated class for finding symbols in code files."""
    
    @staticmethod
    def find_symbol_line(file_path: Path, symbol_name: str) -> Optional[int]:
        """Finds the line number of a function, async function, or class definition matching `symbol_name` in the file at `file_path`, or None if not found."""
        """Finds the line number of a function or class definition in a file."""
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        try:
            code = file_path.read_text(encoding='utf-8')
            tree = ast.parse(code)
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                    if node.name == symbol_name:
                        return node.lineno
        except (SyntaxError, UnicodeDecodeError) as e:
            raise RuntimeError(f"Failed to parse {file_path}: {e}")
        return None

class PlanExecutionContext:
    """Context manager for safe plan execution with rollback."""
    
    def __init__(self, planner_agent):
        """Initializes the context with a reference to the calling agent."""
        self.planner = planner_agent
        self.initial_commit = None
        
    def __enter__(self):
        """Sets up the execution context, capturing the initial git commit hash."""
        if self.planner.git_service.is_git_repo():
            try:
                self.initial_commit = self.planner.git_service.get_current_commit()
            except Exception as e:
                log.warning(f"Could not get current commit for rollback: {e}")
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Cleans up and handles rollback on failure."""
        if exc_type and self.initial_commit and self.planner.config.rollback_on_failure:
            log.warning("Rolling back to initial state due to failure")
            try:
                self.planner.git_service.reset_to_commit(self.initial_commit)
            except Exception as e:
                log.error(f"Failed to rollback: {e}")
--- END OF FILE ./src/agents/utils.py ---

--- START OF FILE ./src/core/black_formatter.py ---
# src/core/black_formatter.py
"""
Formats Python code using Black before it's written to disk.
"""
import black

# --- MODIFICATION: The function now returns only the formatted code on success ---
# --- and raises a specific exception on failure, simplifying its contract. ---
def format_code_with_black(code: str) -> str:
    """Formats the given Python code using Black, raising `black.InvalidInput` for syntax errors or `Exception` for other formatting issues."""
    """
    Attempts to format the given Python code using Black.

    Args:
        code: The Python source code to format.

    Returns:
        The formatted code as a string.

    Raises:
        black.InvalidInput: If the code contains a syntax error that Black cannot handle.
        Exception: For other unexpected Black formatting errors.
    """
    try:
        mode = black.FileMode()
        formatted_code = black.format_str(code, mode=mode)
        return formatted_code
    except black.InvalidInput as e:
        # Re-raise with a clear message for the pipeline to catch.
        raise black.InvalidInput(f"Black could not format the code due to a syntax error: {e}")
    except Exception as e:
        # Catch any other unexpected errors from Black.
        raise Exception(f"An unexpected error occurred during Black formatting: {e}")
--- END OF FILE ./src/core/black_formatter.py ---

--- START OF FILE ./src/core/capabilities.py ---
# src/core/capabilities.py
"""
CORE Capability Registry
This file is the high-level entry point for the system's self-awareness loop.
It defines the `introspection` capability, which orchestrates the system's tools
to perform a full self-analysis.
"""
import subprocess
import sys
from pathlib import Path
from dotenv import load_dotenv

from shared.logger import getLogger

log = getLogger(__name__)

# CAPABILITY: introspection
def introspection():
    """
    Runs a full self-analysis cycle to inspect system structure and health.
    This orchestrates the execution of the system's own introspection tools
    as separate, governed processes.
    """
    log.info("ðŸ” Starting introspection cycle...")
    
    project_root = Path(__file__).resolve().parents[2]
    python_executable = sys.executable

    tools_to_run = [
        ("Knowledge Graph Builder", "system.tools.codegraph_builder"),
        ("Constitutional Auditor", "system.governance.constitutional_auditor"),
    ]

    all_passed = True
    for name, module in tools_to_run:
        log.info(f"Running {name}...")
        try:
            result = subprocess.run(
                [python_executable, "-m", module],
                cwd=project_root,
                capture_output=True,
                text=True,
                check=True 
            )
            # --- THIS IS THE FIX ---
            # If the process was successful, print its standard output.
            # This gives us the detailed report from the auditor.
            if result.stdout:
                # We use print() directly here so the rich formatting from the
                # auditor's console is preserved perfectly.
                print(result.stdout)
            
            if result.stderr:
                log.warning(f"{name} stderr:\n{result.stderr}")
            log.info(f"âœ… {name} completed successfully.")
        except subprocess.CalledProcessError as e:
            log.error(f"âŒ {name} failed with exit code {e.returncode}.")
            # Print the output on failure so we can see the full error report.
            if e.stdout:
                print(e.stdout)
            if e.stderr:
                print(e.stderr)
            all_passed = False
        except Exception as e:
            log.error(f"ðŸ’¥ An unexpected error occurred while running {name}: {e}", exc_info=True)
            all_passed = False
            
    log.info("ðŸ§  Introspection cycle completed.")
    return all_passed

if __name__ == "__main__":
    load_dotenv()
    # Allows running the full introspection cycle directly from the CLI.
    if not introspection():
        sys.exit(1)
    sys.exit(0)

--- END OF FILE ./src/core/capabilities.py ---

--- START OF FILE ./src/core/clients.py ---
# src/core/clients.py
"""
Clients for communicating with the different LLMs in the CORE ecosystem.
This version is updated to use the modern "Chat Completions" API format,
and uses the 'httpx' library for robust, asynchronous network requests.
"""
import httpx
import requests
import json
from typing import Dict, Any

from shared.logger import getLogger
from shared.config import settings

log = getLogger(__name__)


class BaseLLMClient:
    """
    Base class for LLM clients, handling common request logic for Chat APIs.
    Provides shared initialization and error handling for all LLM clients.
    """

    def __init__(self, api_url: str, api_key: str, model_name: str):
        """
        Initialize the LLM client with API credentials and endpoint.

        Args:
            api_url (str): Base URL for the LLM's chat completions API.
            api_key (str): Authentication token for the API.
            model_name (str): Name of the model to use (e.g., 'gpt-4', 'deepseek-coder').
        """
        if not api_url or not api_key:
            raise ValueError(f"{self.__class__.__name__} requires both API_URL and API_KEY.")
        
        if not api_url.endswith("/v1/chat/completions"):
            self.api_url = api_url.rstrip("/") + "/v1/chat/completions"
        else:
            self.api_url = api_url

        self.model_name = model_name
        self.headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        # --- THIS IS THE UPGRADE (Part 1 of 2) ---
        # We add an async client for concurrent operations, while keeping the sync client for now.
        self.async_client = httpx.AsyncClient(timeout=180.0)

    def make_request(self, prompt: str, user_id: str = "core_system") -> str:
        """
        Sends a prompt to the configured Chat Completions API. (Synchronous)
        """
        payload = {"model": self.model_name, "messages": [{"role": "user", "content": prompt}], "user": user_id}

        try:
            log.debug(f"Sending request to {self.api_url} for model {self.model_name}...")
            response = requests.post(self.api_url, headers=self.headers, json=payload, timeout=180)
            response.raise_for_status()
            response_data = response.json()
            content = response_data["choices"][0]["message"]["content"]
            log.debug("Successfully received and parsed LLM response.")
            return content if content is not None else ""
        except requests.exceptions.RequestException as e:
            log.error(f"Network error during LLM request to {self.api_url}: {e}", exc_info=True)
            return f"Error: Could not connect to LLM endpoint. Details: {e}"
        except (KeyError, IndexError):
            log.error(f"Error parsing LLM response. Full response: {response.text}", exc_info=True)
            return "Error: Could not parse response from API."

    # --- THIS IS THE UPGRADE (Part 2 of 2) ---
    # A new, async version of the make_request method.
    async def make_request_async(self, prompt: str, user_id: str = "core_system") -> str:
        """
        Sends a prompt asynchronously to the configured Chat Completions API.
        """
        payload = {"model": self.model_name, "messages": [{"role": "user", "content": prompt}], "user": user_id}
        try:
            log.debug(f"Sending async request to {self.api_url} for model {self.model_name}...")
            response = await self.async_client.post(self.api_url, headers=self.headers, json=payload)
            response.raise_for_status()
            response_data = response.json()
            content = response_data["choices"][0]["message"]["content"]
            log.debug("Successfully received and parsed async LLM response.")
            return content if content is not None else ""
        except httpx.ReadTimeout:
            log.error(f"Network timeout during async LLM request to {self.api_url}.")
            return f"Error: Request timed out."
        except httpx.RequestError as e:
            log.error(f"Network error during async LLM request to {self.api_url}: {e}", exc_info=True)
            return f"Error: Could not connect to LLM endpoint. Details: {e}"
        except (KeyError, IndexError, json.JSONDecodeError):
            log.error(f"Error parsing async LLM response. Full response: {response.text}", exc_info=True)
            return "Error: Could not parse response from API."

class OrchestratorClient(BaseLLMClient):
    """
    Client for the Orchestrator LLM (e.g., GPT-4, Claude 3).
    Responsible for high-level planning and intent interpretation.
    """
    def __init__(self):
        super().__init__(
            api_url=settings.ORCHESTRATOR_API_URL, api_key=settings.ORCHESTRATOR_API_KEY,
            model_name=settings.ORCHESTRATOR_MODEL_NAME,
        )
        log.info(f"OrchestratorClient initialized for model '{self.model_name}'.")


class GeneratorClient(BaseLLMClient):
    """
    Client for the Generator LLM (e.g., a specialized coding model).
    Responsible for code generation and detailed implementation.
    """
    def __init__(self):
        """Initialize the LLM client with API URL, key, and model name, setting up headers and async client."""
        super().__init__(
            api_url=settings.GENERATOR_API_URL, api_key=settings.GENERATOR_API_KEY,
            model_name=settings.GENERATOR_MODEL_NAME,
        )
        log.info(f"GeneratorClient initialized for model '{self.model_name}'.")
--- END OF FILE ./src/core/clients.py ---

--- START OF FILE ./src/core/cli/guard.py ---
# src/core/cli/guard.py
"""
This module is deprecated and will be removed.
The `core-admin` CLI provided by the `system` domain is now the primary
entry point for all governance and operational commands.
"""

from shared.logger import getLogger

log = getLogger(__name__)

def ensure_cli_entrypoint():
    """Provides functionality for the core domain."""
    log.warning("The `core-admin guard` command is deprecated and will be removed.")
    log.warning("Please use the main `core-admin` command from the `system` domain.")
    log.warning("For example: `poetry run core-admin guard drift`")

if __name__ == "__main__":
    ensure_cli_entrypoint()

--- END OF FILE ./src/core/cli/guard.py ---

--- START OF FILE ./src/core/file_handler.py ---
# src/core/file_handler.py
"""
Backend File Handling Module (Refactored)

Handles staging and writing file changes. It supports traceable, auditable
operations. All writes go through a pending stage to enable review and rollback.
"""

import json
import threading
from datetime import datetime, timezone
from uuid import uuid4
from pathlib import Path
from typing import Dict, Optional, Any
from shared.logger import getLogger

log = getLogger(__name__)

class FileHandler:
    """
    Central class for safe, auditable file operations in CORE.
    All writes are staged first and require confirmation. Validation is handled
    by the calling agent via the validation_pipeline.
    """

    def __init__(self, repo_path: str):
        """
        Initialize FileHandler with repository root.
        """
        self.repo_path = Path(repo_path).resolve()
        if not self.repo_path.is_dir():
            raise ValueError(f"Invalid repository path provided: {repo_path}")
        
        # --- THIS IS THE FIX ---
        # All operational directories are now relative to the repo_path
        # that the handler was initialized with. This makes the handler
        # safe to use in different contexts (like our integration test).
        self.log_dir = self.repo_path / "logs"
        self.pending_dir = self.repo_path / "pending_writes"
        self.undo_log = self.log_dir / "undo_log.jsonl"

        self.log_dir.mkdir(exist_ok=True)
        self.pending_dir.mkdir(exist_ok=True)
        # --- END OF FIX ---
        
        self.pending_writes: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.Lock()

    def add_pending_write(self, prompt: str, suggested_path: str, code: str) -> str:
        """
        Stages a pending write operation for later confirmation.
        """
        pending_id = str(uuid4())
        rel_path = Path(suggested_path).as_posix()
        entry = {
            "id": pending_id,
            "prompt": prompt,
            "path": rel_path,
            "code": code,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

        with self._lock:
            self.pending_writes[pending_id] = entry

        pending_file = self.pending_dir / f"{pending_id}.json"
        pending_file.write_text(json.dumps(entry, indent=2), encoding="utf-8")
        return pending_id

    def confirm_write(self, pending_id: str) -> Dict[str, str]:
        """
        Confirms and applies a pending write to disk. Assumes content has been validated.
        """
        with self._lock:
            pending_op = self.pending_writes.pop(pending_id, None)

        pending_file = self.pending_dir / f"{pending_id}.json"
        if pending_file.exists():
            pending_file.unlink(missing_ok=True)

        if not pending_op:
            return {"status": "error", "message": f"Pending write ID '{pending_id}' not found or already processed."}

        file_rel_path = pending_op["path"]
        
        try:
            abs_file_path = self.repo_path / file_rel_path
            
            if not abs_file_path.resolve().is_relative_to(self.repo_path.resolve()):
                 raise ValueError(f"Attempted to write outside of repository boundary: {file_rel_path}")

            abs_file_path.parent.mkdir(parents=True, exist_ok=True)
            abs_file_path.write_text(pending_op["code"], encoding="utf-8")
            
            log.info(f"Wrote to {file_rel_path}")
            return {
                "status": "success",
                "message": f"Wrote to {file_rel_path}",
                "file_path": file_rel_path
            }
        except Exception as e:
            if pending_op:
                with self._lock:
                    self.pending_writes[pending_id] = pending_op
                pending_file.write_text(json.dumps(pending_op, indent=2), encoding="utf-8")
            return {"status": "error", "message": f"Failed to write file: {str(e)}"}
--- END OF FILE ./src/core/file_handler.py ---

--- START OF FILE ./src/core/git_service.py ---
# src/core/git_service.py
"""
GitService â€” CORE's Git Integration Layer

Provides safe, auditable Git operations:
- add, commit, rollback
- status checks
- branch management

Ensures all changes are tracked and reversible.
Used by main.py and self-correction engine.
"""

import subprocess
from pathlib import Path
from typing import Optional

from shared.logger import getLogger

log = getLogger(__name__)

class GitService:
    """
    Encapsulates Git operations for the CORE system.
    Ensures all file changes are committed with traceable messages.
    """

    def __init__(self, repo_path: str):
        """Initialize GitService with the resolved absolute path to the Git repository; raises ValueError if path is not a valid Git repo."""
        self.repo_path = Path(repo_path).resolve()
        if not self.is_git_repo():
            raise ValueError(f"Invalid Git repository: {repo_path}")
        log.info(f"GitService initialized for repo at {self.repo_path}")

    # CAPABILITY: change_safety_enforcement
    def _run_command(self, command: list) -> str:
        """
        Run a Git command and return stdout.

        Args:
            command (list): Git command as a list (e.g., ['git', 'status']).

        Returns:
            str: Command output, or raises RuntimeError on failure.
        """
        try:
            log.debug(f"Running git command: {' '.join(command)}")
            result = subprocess.run(
                command, cwd=self.repo_path, capture_output=True, text=True, check=True
            )
            return result.stdout.strip()
        except subprocess.CalledProcessError as e:
            log.error(f"Git command failed: {e.stderr}")
            raise RuntimeError(f"Git command failed: {e.stderr}") from e

    def add(self, file_path: str = "."):
        """
        Stage a file or directory for commit.

        Args:
            file_path (str): Path to stage. Defaults to '.' (all changes).
        """
        abs_path = (self.repo_path / file_path).resolve()
        if self.repo_path not in abs_path.parents and abs_path != self.repo_path:
            raise ValueError(f"Cannot stage file outside repo: {file_path}")
        self._run_command(["git", "add", file_path])

    def commit(self, message: str):
        """
        Commit staged changes with a message.
        If there are no changes to commit, this operation is a no-op and will not raise an error.

        Args:
            message (str): Commit message explaining the change.
        """
        try:
            # --- THIS IS THE FIX ---
            # First, check if there are any staged changes.
            status_output = self._run_command(["git", "status", "--porcelain"])
            if not status_output:
                log.info("No changes to commit.")
                return

            self._run_command(["git", "commit", "-m", message])
            log.info(f"Committed changes with message: {message}")
        except RuntimeError as e:
            # It's possible for a race condition, or for the status check to be insufficient.
            # We specifically check for the "nothing to commit" message from Git.
            if "nothing to commit" in str(e).lower():
                log.info("No changes to commit.")
            else:
                # Re-raise any other unexpected error.
                raise e
            # --- END OF FIX ---

    def is_git_repo(self) -> bool:
        """
        Check if the configured path is a valid Git repository.

        Returns:
            bool: True if it's a Git repo, False otherwise.
        """
        git_dir = self.repo_path / ".git"
        return git_dir.is_dir()

    def get_current_commit(self) -> str:
        """
        Gets the full SHA hash of the current commit (HEAD).
        """
        return self._run_command(["git", "rev-parse", "HEAD"])

    def reset_to_commit(self, commit_hash: str):
        """
        Performs a hard reset to a specific commit hash.
        This will discard all current changes.
        """
        log.warning(f"Performing hard reset to commit {commit_hash}...")
        self._run_command(["git", "reset", "--hard", commit_hash])
        log.info(f"Repository reset to {commit_hash}.")

--- END OF FILE ./src/core/git_service.py ---

--- START OF FILE ./src/core/__init__.py ---
# src/core/__init__.py
# Package marker for src/core â€” central module for CORE's intent-driven engine.
--- END OF FILE ./src/core/__init__.py ---

--- START OF FILE ./src/core/intent_guard.py ---
# src/core/intent_guard.py
"""
IntentGuard â€” CORE's Constitutional Enforcement Module

Enforces safety, structure, and intent alignment for all file changes.
Loads governance rules from .intent/policies/*.yaml and prevents unauthorized
self-modifications of the CORE constitution.
"""

import json
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional
from shared.config_loader import load_config
from shared.logger import getLogger

log = getLogger(__name__)

# CAPABILITY: intent_guarding
class IntentGuard:
    """
    Central enforcement engine for CORE's safety and governance policies.
    Ensures all proposed file changes comply with declared rules and classifications.
    """

    def __init__(self, repo_path: Path):
        """
        Initialize IntentGuard with repository path and load all policies.
        """
        self.repo_path = Path(repo_path).resolve()
        self.intent_path = self.repo_path / ".intent"
        self.proposals_path = self.intent_path / "proposals"
        self.policies_path = self.intent_path / "policies"
        self.rules: List[Dict] = []
        
        self._load_policies()
        self.source_code_manifest = self._load_source_manifest()
        
        log.info(f"IntentGuard initialized. {len(self.rules)} rules loaded. Watching {len(self.source_code_manifest)} source files.")

    def _load_policies(self):
        """Load rules from all YAML files in the `.intent/policies/` directory."""
        if not self.policies_path.is_dir():
            return
        for policy_file in self.policies_path.glob("*.yaml"):
            content = load_config(policy_file, "yaml")
            if content and "rules" in content and isinstance(content["rules"], list):
                self.rules.extend(content["rules"])

    def _load_source_manifest(self) -> List[str]:
        """
        Load the list of all known source files from the knowledge graph.
        """
        manifest_file = self.intent_path / "knowledge" / "knowledge_graph.json"
        if not manifest_file.exists():
            return []
        try:
            manifest_data = json.loads(manifest_file.read_text(encoding="utf-8"))
            symbols = manifest_data.get("symbols", {})
            # Use a set to get unique file paths, then convert to a sorted list.
            unique_files = {entry.get("file") for entry in symbols.values() if entry.get("file")}
            return sorted(list(unique_files))
        except (json.JSONDecodeError, TypeError):
            return []

    # --- THIS IS THE FIX ---
    # The method now correctly resolves paths relative to the repository root.
    def check_transaction(self, proposed_paths: List[str]) -> Tuple[bool, List[str]]:
        """
        Check if a proposed set of file changes complies with all active rules.
        This is the primary enforcement point for constitutional integrity.
        """
        violations = []
        
        # Rule: Prevent direct writes to the .intent directory, except for proposals.
        for path_str in proposed_paths:
            # Resolve the path relative to the repository root, not the current working directory.
            # This makes the check robust regardless of where the script is executed from.
            path = (self.repo_path / path_str).resolve()
            
            # Check if the path is within the .intent directory
            if self.intent_path.resolve() in path.parents:
                # If it is, check if it's also within the allowed proposals directory
                if self.proposals_path.resolve() not in path.parents and path.parent != self.proposals_path.resolve():
                    violations.append(
                        f"Rule Violation (immutable_intent): Direct write to '{path_str}' is forbidden. "
                        "All changes to the constitution must go through '.intent/proposals/'."
                    )
        
        # Placeholder for future, more sophisticated rule checks
        # for rule in self.rules:
        #    ...

        return not violations, violations

--- END OF FILE ./src/core/intent_guard.py ---

--- START OF FILE ./src/core/intent_model.py ---
# src/core/intent_model.py

"""
CORE Intent Structure Loader
============================

Provides a normalized interface to the declared domain structure in:
.intent/knowledge/source_structure.yaml

Used to enforce boundaries, access rules, and governance alignment
without hardcoding anything.
"""

import yaml
from pathlib import Path
from typing import Dict, List, Optional


class IntentModel:
    """
    Loads and provides an queryable interface to the source code structure
    defined in .intent/knowledge/source_structure.yaml.
    """
    def __init__(self, repo_root: Optional[Path] = None):
        """Initializes the model by loading the source structure definition from the repository, inferring the root if not provided."""
        """
        Initializes the model by loading the source structure definition.

        Args:
            repo_root (Optional[Path]): The root of the repository. Inferred if not provided.
        """
        self.repo_root = repo_root or Path(__file__).resolve().parents[2]
        self.structure_path = self.repo_root / ".intent" / "knowledge" / "source_structure.yaml"
        self.structure: Dict[str, dict] = self._load_structure()

    """Load the domain structure from .intent/knowledge/source_structure.yaml and return a mapping of domain names to metadata (path, permissions, etc.)."""
    def _load_structure(self) -> Dict[str, dict]:
        """
        Load the domain structure from .intent/knowledge/source_structure.yaml.

        Returns:
            Dict[str, dict]: Mapping of domain names to metadata (path, permissions, etc.).
        """
        if not self.structure_path.exists():
            raise FileNotFoundError(f"Missing: {self.structure_path}")

        data = yaml.safe_load(self.structure_path.read_text(encoding="utf-8"))

        if not isinstance(data, dict) or "structure" not in data:
            raise ValueError(
                f"Invalid source_structure.yaml: missing top-level 'structure' key"
            )

        return {entry["domain"]: entry for entry in data["structure"]}

    def resolve_domain_for_path(self, file_path: Path) -> Optional[str]:
        """
        Given an absolute or relative path, determine which domain it belongs to.
        Prefers deeper (more specific) paths over shorter ones.
        """
        # --- THIS IS THE FIX ---
        # Ensure the path is resolved relative to THIS model's root, not the CWD.
        full_path = (self.repo_root / file_path).resolve()
        
        sorted_domains = sorted(
            self.structure.items(),
            key=lambda item: len((self.repo_root / item[1]["path"]).parts),
            reverse=True,
        )
        for domain, entry in sorted_domains:
            domain_root = (self.repo_root / entry["path"]).resolve()
            # Check if the domain_root is the same as the path or one of its parents.
            if domain_root == full_path or domain_root in full_path.parents:
                return domain
        return None

    def get_domain_permissions(self, domain: str) -> List[str]:
        """
        Return a list of allowed domains that the given domain can import from.

        Args:
            domain (str): The domain to query.

        Returns:
            List[str]: List of allowed domain names, or empty list if not defined.
        """
        entry = self.structure.get(domain, {})
        allowed = entry.get("allowed_imports", [])
        return allowed if isinstance(allowed, list) else []
--- END OF FILE ./src/core/intent_model.py ---

--- START OF FILE ./src/core/main.py ---
# src/core/main.py
"""
main.py â€” CORE's API Gateway and Execution Engine

Implements the FastAPI server that handles:
- Goal submission
- Write confirmation
- Test execution
- System status

Integrates all core capabilities into a unified interface.
"""
from typing import Dict
from fastapi import FastAPI, HTTPException, Request, status as http_status
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from dotenv import load_dotenv
from pathlib import Path
from pydantic import BaseModel

from core.clients import OrchestratorClient, GeneratorClient
from core.file_handler import FileHandler
from core.git_service import GitService
from core.intent_guard import IntentGuard
from agents.planner_agent import PlannerAgent, PlanExecutionError
from core.capabilities import introspection
from shared.logger import getLogger

log = getLogger(__name__)
load_dotenv()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan handler â€” runs startup and shutdown logic."""
    log.info("ðŸš€ Starting CORE system...")
    
    log.info("ðŸ§  Performing startup introspection...")
    if not introspection():
        log.warning("âš ï¸ Introspection cycle completed with errors. System may be unstable.")
    else:
        log.info("âœ… Introspection complete. System state is constitutionally valid.")
    
    # Initialize services that are safe to be singletons and store them in the app state
    log.info("ðŸ› ï¸  Initializing shared services...")
    app.state.orchestrator_client = OrchestratorClient()
    app.state.generator_client = GeneratorClient()
    app.state.git_service = GitService(".")
    app.state.intent_guard = IntentGuard(Path(".")) # Corrected initialization
    log.info("âœ… CORE system is online and ready.")
    yield
    log.info("ðŸ›‘ CORE system shutting down.")

app = FastAPI(lifespan=lifespan)

class GoalRequest(BaseModel):
    """Defines the request body for the /execute_goal endpoint."""
    goal: str

@app.post("/execute_goal")
async def execute_goal(request_data: GoalRequest, request: Request):
    """Execute a high-level goal by planning and generating code."""
    goal = request_data.goal
    log.info(f"ðŸŽ¯ Received new goal: '{goal}'")
    
    # --- THIS IS THE FIX (Part 3 of 3) ---
    # We now create a new, request-specific instance of the FileHandler and PlannerAgent.
    # This prevents state from leaking between different API calls.
    try:
        file_handler = FileHandler(".")
        planner = PlannerAgent(
            orchestrator_client=request.app.state.orchestrator_client,
            generator_client=request.app.state.generator_client,
            file_handler=file_handler,
            git_service=request.app.state.git_service,
            intent_guard=request.app.state.intent_guard
        )
        
        success, message = await planner.execute_plan(goal)
        
        if success:
            log.info(f"âœ… Goal executed successfully. Message: {message}")
            return JSONResponse(
                content={"status": "success", "message": message},
                status_code=http_status.HTTP_200_OK
            )
        else:
            log.error(f"âŒ Goal execution failed. Reason: {message}")
            raise HTTPException(status_code=500, detail=f"Goal execution failed: {message}")

    except Exception as e:
        log.error(f"ðŸ’¥ An unexpected error occurred during goal execution: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {str(e)}")

@app.get("/")
async def root():
    """Root endpoint â€” returns system status."""
    return {"message": "CORE system is online and self-governing."}

--- END OF FILE ./src/core/main.py ---

--- START OF FILE ./src/core/manifest.yaml ---
capabilities:
- prompt_interpretation
- syntax_validation
- test_execution
- introspection
- self_correction
- change_safety_enforcement
- intent_guarding
- semantic_validation
- code_quality_analysis
description: Core logic for orchestration, routing, and CLI
domain: core

--- END OF FILE ./src/core/manifest.yaml ---

--- START OF FILE ./src/core/prompt_pipeline.py ---
# src/core/prompt_pipeline.py
"""
PromptPipeline â€” CORE's Unified Directive Processor

A single pipeline that processes all [[directive:...]] blocks in a user prompt.
Responsible for:
- Injecting context (e.g., file contents)
- Expanding includes
- Adding analysis from introspection tools
- Enriching with manifest data

This is the central "pre-processor" for all LLM interactions.
"""

import re
import yaml
from pathlib import Path
from typing import Dict

# --- FIX: Define a constant for a reasonable file size limit (1MB) ---
MAX_FILE_SIZE_BYTES = 1 * 1024 * 1024

class PromptPipeline:
    """
    Processes and enriches user prompts by resolving directives like [[include:...]] and [[analysis:...]].
    Ensures the LLM receives full context before generating code.
    """

    def __init__(self, repo_path: Path):
        """
        Initialize PromptPipeline with repository root.

        Args:
            repo_path (Path): Root path of the repository.
        """
        self.repo_path = Path(repo_path).resolve()

        # Regex patterns for directive matching
        self.context_pattern = re.compile(r"\[\[context:(.+?)\]\]")
        self.include_pattern = re.compile(r"\[\[include:(.+?)\]\]")
        self.analysis_pattern = re.compile(r"\[\[analysis:(.+?)\]\]")
        self.manifest_pattern = re.compile(r"\[\[manifest:(.+?)\]\]")

    def _replace_context_match(self, match: re.Match) -> str:
        """Dynamically replaces a [[context:...]] regex match with file content or an error message if the file is missing, unreadable, or exceeds size limits."""
        """Dynamically replaces a [[context:...]] regex match with file content or an error message."""
        file_path = match.group(1).strip()
        abs_path = self.repo_path / file_path
        if abs_path.exists() and abs_path.is_file():
            # --- FIX: Add file size check to prevent memory bloat ---
            if abs_path.stat().st_size > MAX_FILE_SIZE_BYTES:
                return f"\nâŒ Could not include {file_path}: File size exceeds 1MB limit.\n"
            try:
                return f"\n--- CONTEXT: {file_path} ---\n{abs_path.read_text(encoding='utf-8')}\n--- END CONTEXT ---\n"
            except Exception as e:
                return f"\nâŒ Could not read {file_path}: {str(e)}\n"
        return f"\nâŒ File not found: {file_path}\n"

    def _inject_context(self, prompt: str) -> str:
        """Replaces [[context:file.py]] directives with actual file content."""
        return self.context_pattern.sub(self._replace_context_match, prompt)

    """Dynamically replaces an [[include:...]] regex match with the corresponding file's content or an error message if the file is missing, unreadable, or exceeds size limits."""
    def _replace_include_match(self, match: re.Match) -> str:
        """Dynamically replaces an [[include:...]] regex match with file content or an error message."""
        file_path = match.group(1).strip()
        abs_path = self.repo_path / file_path
        if abs_path.exists() and abs_path.is_file():
            # --- FIX: Add file size check to prevent memory bloat ---
            if abs_path.stat().st_size > MAX_FILE_SIZE_BYTES:
                return f"\nâŒ Could not include {file_path}: File size exceeds 1MB limit.\n"
            try:
                return f"\n--- INCLUDED: {file_path} ---\n{abs_path.read_text(encoding='utf-8')}\n--- END INCLUDE ---\n"
            except Exception as e:
                return f"\nâŒ Could not read {file_path}: {str(e)}\n"
        return f"\nâŒ File not found: {file_path}\n"

    def _inject_includes(self, prompt: str) -> str:
        """Replaces [[include:file.py]] directives with file content."""
        return self.include_pattern.sub(self._replace_include_match, prompt)

    def _replace_analysis_match(self, match: re.Match) -> str:
        """Dynamically replaces an [[analysis:...]] regex match with a placeholder analysis message for the given file path."""
        """Dynamically replaces an [[analysis:...]] regex match with a placeholder analysis message."""
        file_path = match.group(1).strip()
        # This functionality is a placeholder.
        return f"\n--- ANALYSIS FOR {file_path} (DEFERRED) ---\n"

    def _inject_analysis(self, prompt: str) -> str:
        """Replaces [[analysis:file.py]] directives with code analysis."""
        return self.analysis_pattern.sub(self._replace_analysis_match, prompt)

    def _replace_manifest_match(self, match: re.Match) -> str:
        """Dynamically replaces a [[manifest:...]] regex match with manifest data or an error."""
        manifest_path = self.repo_path / ".intent" / "project_manifest.yaml"
        if not manifest_path.exists():
            return f"\nâŒ Manifest file not found at {manifest_path}\n"

        try:
            manifest = yaml.safe_load(manifest_path.read_text(encoding="utf-8"))
        except Exception:
            return f"\nâŒ Could not parse manifest file at {manifest_path}\n"

        field = match.group(1).strip()
        value = manifest
        # Improved logic for nested key access
        for key in field.split("."):
            value = value.get(key) if isinstance(value, dict) else None
            if value is None:
                break
        
        if value is None:
            return f"\nâŒ Manifest field not found: {field}\n"
        
        # Pretty print for better context
        value_str = yaml.dump(value, indent=2) if isinstance(value, (dict, list)) else str(value)
        return f"\n--- MANIFEST: {field} ---\n{value_str}\n--- END MANIFEST ---\n"

    def _inject_manifest(self, prompt: str) -> str:
        """Replaces [[manifest:field]] directives with data from project_manifest.yaml."""
        return self.manifest_pattern.sub(self._replace_manifest_match, prompt)

    # CAPABILITY: prompt_interpretation
    def process(self, prompt: str) -> str:
        """
        Processes the full prompt by sequentially resolving all directives.
        This is the main entry point for prompt enrichment.
        """
        prompt = self._inject_context(prompt)
        prompt = self._inject_includes(prompt)
        prompt = self._inject_analysis(prompt)
        prompt = self._inject_manifest(prompt)
        return prompt
--- END OF FILE ./src/core/prompt_pipeline.py ---

--- START OF FILE ./src/core/ruff_linter.py ---
# src/core/ruff_linter.py
"""
Runs Ruff lint checks on generated Python code before it's staged.
Returns a success flag and an optional linting message.
"""
import subprocess
import tempfile
import os
import json
from typing import Tuple, List, Dict, Any

from shared.logger import getLogger

log = getLogger(__name__)
Violation = Dict[str, Any]

# --- MODIFICATION: Complete refactor to use Ruff's JSON output. ---
# --- The function now returns the fixed code and a list of structured violations. ---
def fix_and_lint_code_with_ruff(code: str, display_filename: str = "<code>") -> Tuple[str, List[Violation]]:
    """
    Fix and lint the provided Python code using Ruff's JSON output format.

    Args:
        code (str): Source code to fix and lint.
        display_filename (str): Optional display name for readable error messages.

    Returns:
        A tuple containing:
        - The potentially fixed code as a string.
        - A list of structured violation dictionaries for any remaining issues.
    """
    violations = []
    with tempfile.NamedTemporaryFile(suffix=".py", mode="w+", delete=False, encoding="utf-8") as tmp_file:
        tmp_file.write(code)
        tmp_file_path = tmp_file.name

    try:
        # Step 1: Run Ruff with --fix to apply safe fixes. This modifies the temp file.
        subprocess.run(
            ["ruff", "check", tmp_file_path, "--fix", "--exit-zero", "--quiet"],
            capture_output=True, text=True, check=False
        )

        # Step 2: Read the potentially modified code back from the file.
        with open(tmp_file_path, "r", encoding="utf-8") as f:
            fixed_code = f.read()

        # Step 3: Run Ruff again without fix, but with JSON output to get remaining violations.
        result = subprocess.run(
            ["ruff", "check", tmp_file_path, "--format", "json", "--exit-zero"],
            capture_output=True, text=True, check=False
        )

        # Parse the JSON output for any remaining violations.
        if result.stdout:
            ruff_violations = json.loads(result.stdout)
            for v in ruff_violations:
                violations.append({
                    "rule": v.get("code", "RUFF-UNKNOWN"),
                    "message": v.get("message", "Unknown Ruff error"),
                    "line": v.get("location", {}).get("row", 0),
                    "severity": "warning" # Assume all ruff issues are warnings for now
                })
        
        return fixed_code, violations

    except FileNotFoundError:
        log.error("Ruff is not installed or not in your PATH. Please install it.")
        # Return a critical violation if the tool itself is missing.
        tool_missing_violation = {
            "rule": "tooling.missing",
            "message": "Ruff is not installed or not in your PATH.",
            "line": 0,
            "severity": "error"
        }
        return code, [tool_missing_violation]
    except json.JSONDecodeError:
        log.error("Failed to parse Ruff's JSON output.")
        return code, [] # Return empty if we can't parse, to avoid crashing.
    except Exception as e:
        log.error(f"An unexpected error occurred during Ruff execution: {e}")
        return code, []
    finally:
        if os.path.exists(tmp_file_path):
            os.remove(tmp_file_path)
--- END OF FILE ./src/core/ruff_linter.py ---

--- START OF FILE ./src/core/self_correction_engine.py ---
# src/core/self_correction_engine.py
"""
Self-Correction Engine
This module takes failure context (from validation or test failure)
and attempts to repair the issue using a structured LLM prompt,
then stages the corrected version via the file handler.
"""
import json
from pathlib import Path
from core.prompt_pipeline import PromptPipeline
from core.clients import GeneratorClient
from core.validation_pipeline import validate_code
from shared.utils.parsing import parse_write_blocks
from core.file_handler import FileHandler

REPO_PATH = Path(".").resolve()
pipeline = PromptPipeline(repo_path=REPO_PATH)
file_handler = FileHandler(repo_path=REPO_PATH)

# CAPABILITY: self_correction
def attempt_correction(failure_context: dict) -> dict:
    """Attempts to fix a failed validation or test result by generating corrected code via an LLM prompt based on the provided failure context."""
    """
    Attempts to fix a failed validation or test result using an enriched LLM prompt.
    """
    generator = GeneratorClient()
    file_path = failure_context.get("file_path")
    code = failure_context.get("code")
    # --- MODIFICATION: The key is now "violations", not "error_type" or "details" ---
    violations = failure_context.get("violations", [])
    base_prompt = failure_context.get("original_prompt", "")

    if not file_path or not code or not violations:
        return {"status": "error", "message": "Missing required failure context fields."}

    # --- MODIFICATION: The prompt is updated to send structured violation data to the LLM ---
    correction_prompt = (
        f"You are CORE's self-correction agent.\n\nA recent code generation attempt failed validation.\n"
        f"Please analyze the violations and fix the code below.\n\nFile: {file_path}\n\n"
        f"[[violations]]\n{json.dumps(violations, indent=2)}\n[[/violations]]\n\n"
        f"[[code]]\n{code.strip()}\n[[/code]]\n\n"
        f"Respond with the full, corrected code in a single write block:\n[[write:{file_path}]]\n<corrected code here>\n[[/write]]"
    )

    final_prompt = pipeline.process(correction_prompt)
    llm_output = generator.make_request(final_prompt, user_id="auto_repair")
    
    write_blocks = parse_write_blocks(llm_output)

    if not write_blocks:
        return {"status": "error", "message": "LLM did not produce a valid correction in a write block."}

    # Assuming one write block for self-correction
    path, fixed_code = list(write_blocks.items())[0]

    validation_result = validate_code(path, fixed_code)
    # --- MODIFICATION: Check for 'error' severity in the new violations list ---
    if validation_result["status"] == "dirty":
        return {
            "status": "correction_failed_validation",
            "message": "The corrected code still fails validation.",
            "violations": validation_result["violations"],
        }

    pending_id = file_handler.add_pending_write(prompt=final_prompt, suggested_path=path, code=validation_result["code"])
    return {
        "status": "retry_staged",
        "pending_id": pending_id,
        "file_path": path,
        "message": "Corrected code staged for approval.",
    }
--- END OF FILE ./src/core/self_correction_engine.py ---

--- START OF FILE ./src/core/syntax_checker.py ---
# src/core/syntax_checker.py
"""
A simple syntax checker utility.
Validates the syntax of Python code before it's staged for write/commit.
"""
import ast
from typing import List, Dict, Any

Violation = Dict[str, Any]

# --- MODIFICATION: The function now returns a list of structured violation dictionaries. ---
# CAPABILITY: syntax_validation
def check_syntax(file_path: str, code: str) -> List[Violation]:
    """Checks the given Python code for syntax errors and returns a list of violations, if any."""
    """
    Checks whether the given code has valid Python syntax.

    Args:
        file_path (str): File name (used to detect .py files).
        code (str): Source code string.

    Returns:
        A list of violation dictionaries. An empty list means the syntax is valid.
    """
    if not file_path.endswith(".py"):
        return []

    try:
        ast.parse(code)
        return []
    except SyntaxError as e:
        error_line = e.text.strip() if e.text else "<source unavailable>"
        return [{
            "rule": "E999", # Ruff's code for syntax errors
            "message": f"Invalid Python syntax: {e.msg} near '{error_line}'",
            "line": e.lineno,
            "severity": "error"
        }]
--- END OF FILE ./src/core/syntax_checker.py ---

--- START OF FILE ./src/core/test_runner.py ---
# src/core/test_runner.py
"""
Runs pytest against the local /tests directory and captures results.
This provides the core `test_execution` capability, allowing the system
to verify its own integrity after making changes.
"""
import subprocess
import os
import json
import datetime
from typing import Dict
from pathlib import Path
from shared.logger import getLogger

log = getLogger(__name__)

LOG_DIR = Path("logs")
LOG_FILE = LOG_DIR / "test_results.log"
FAILURE_FILE = LOG_DIR / "test_failures.json"
LOG_DIR.mkdir(exist_ok=True)

# CAPABILITY: test_execution
def run_tests(silent: bool = True) -> Dict[str, str]:
    """Executes pytest on the tests/ directory, capturing stdout, stderr, exit code, and a summary, returning results as a structured dict."""
    """
    Executes pytest on the tests/ directory and returns a structured result.
    This function captures stdout, stderr, and the exit code, providing a
    comprehensive summary of the test run for agents to act upon.
    """
    log.info("ðŸ§ª Running tests with pytest...")
    result = {
        "exit_code": "-1",
        "stdout": "",
        "stderr": "",
        "summary": "âŒ Unknown error",
        "timestamp": datetime.datetime.utcnow().isoformat()
    }

    repo_root = Path(__file__).resolve().parents[2]
    tests_path = repo_root / "tests"
    cmd = ["pytest", str(tests_path), "--tb=short", "-q"]

    timeout = os.getenv("TEST_RUNNER_TIMEOUT")
    try:
        timeout_val = int(timeout) if timeout else None
    except ValueError:
        timeout_val = None

    try:
        proc = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False,
            timeout=timeout_val,
        )
        result["exit_code"] = str(proc.returncode)
        result["stdout"] = proc.stdout.strip()
        result["stderr"] = proc.stderr.strip()
        result["summary"] = _summarize(proc.stdout)

        if not silent:
            log.info(f"Pytest stdout:\n{proc.stdout}")
            if proc.stderr:
                log.warning(f"Pytest stderr:\n{proc.stderr}")

    except subprocess.TimeoutExpired:
        result["stderr"] = "Test run timed out."
        result["summary"] = "â° Timeout"
        log.error("Pytest run timed out.")
    except FileNotFoundError:
        result["stderr"] = "pytest is not installed or not found in PATH."
        result["summary"] = "âŒ Pytest not available"
        log.error("Pytest command not found. Is it installed in the environment?")
    except Exception as e:
        result["stderr"] = str(e)
        result["summary"] = "âŒ Test run error"
        log.error(f"An unexpected error occurred during test run: {e}", exc_info=True)

    _log_test_result(result)
    _store_failure_if_any(result)
    
    log.info(f"ðŸ Test run complete. Summary: {result['summary']}")
    return result

def _summarize(output: str) -> str:
    """Error: Could not connect to LLM endpoint. Details: HTTPSConnectionPool(host='api.deepseek.com', port=443): Read timed out. (read timeout=180)"""
    """Parses pytest output to find the final summary line."""
    lines = output.strip().splitlines()
    for line in reversed(lines):
        if "passed" in line or "failed" in line or "error" in line:
            return line.strip()
    return "No test summary found."

def _log_test_result(data: Dict[str, str]):
    """Appends a JSON record of a test run to the persistent log file."""
    try:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(data) + "\n")
    except Exception as e:
        log.warning(f"Failed to write to persistent test log file: {e}", exc_info=True)

    """Saves test failure details to FAILURE_FILE if exit_code is non-zero, otherwise removes the file if it exists."""
def _store_failure_if_any(data: Dict[str, str]):
    """Saves the details of a failed test run to a dedicated file for easy access."""
    try:
        if data.get("exit_code") != "0":
            with open(FAILURE_FILE, "w", encoding="utf-8") as f:
                json.dump({
                    "summary": data.get("summary"),
                    "stdout": data.get("stdout"),
                    "timestamp": data.get("timestamp")
                }, f, indent=2)
        elif os.path.exists(FAILURE_FILE):
            os.remove(FAILURE_FILE)
    except Exception as e:
        log.warning(f"Could not save test failure data: {e}", exc_info=True)
--- END OF FILE ./src/core/test_runner.py ---

--- START OF FILE ./src/core/validation_pipeline.py ---
# src/core/validation_pipeline.py
"""
A context-aware validation pipeline that applies different validation steps
based on the type of file being processed. This is the single source of truth
for all code and configuration validation.
"""
import ast
import yaml
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import black
from core.black_formatter import format_code_with_black
from core.ruff_linter import fix_and_lint_code_with_ruff
from core.syntax_checker import check_syntax
from shared.config_loader import load_config
from shared.path_utils import get_repo_root
from shared.logger import getLogger

log = getLogger(__name__)
Violation = Dict[str, Any]

# --- Policy-Aware Validation ---

_safety_policies_cache: Optional[List[Dict]] = None

def _load_safety_policies() -> List[Dict]:
    """Loads and caches the safety policies from the .intent directory."""
    global _safety_policies_cache
    if _safety_policies_cache is None:
        repo_root = get_repo_root()
        policies_path = repo_root / ".intent" / "policies" / "safety_policies.yaml"
        policy_data = load_config(policies_path, "yaml")
        _safety_policies_cache = policy_data.get("rules", [])
    return _safety_policies_cache

def _get_full_attribute_name(node: ast.Attribute) -> str:
    """Recursively builds the full name of an attribute call (e.g., 'os.path.join')."""
    parts = []
    current = node
    while isinstance(current, ast.Attribute):
        parts.insert(0, current.attr)
        current = current.value
    if isinstance(current, ast.Name):
        parts.insert(0, current.id)
    return ".".join(parts)

def _find_dangerous_patterns(tree: ast.AST, file_path: str) -> List[Violation]:
    """Scans the AST for calls and imports forbidden by safety policies."""
    violations: List[Violation] = []
    rules = _load_safety_policies()
    
    forbidden_calls = set()
    forbidden_imports = set()

    for rule in rules:
        is_excluded = any(Path(file_path).match(p) for p in rule.get("scope", {}).get("exclude", []))
        if is_excluded:
            continue

        if rule.get("id") == "no_dangerous_execution":
            patterns = {p.replace('(', '') for p in rule.get("detection", {}).get("patterns", [])}
            forbidden_calls.update(patterns)
        elif rule.get("id") == "no_unsafe_imports":
            patterns = {imp.split(' ')[-1] for imp in rule.get("detection", {}).get("forbidden", [])}
            forbidden_imports.update(patterns)
            
    for node in ast.walk(tree):
        # Check for dangerous function calls
        if isinstance(node, ast.Call):
            full_call_name = ""
            if isinstance(node.func, ast.Name): full_call_name = node.func.id
            elif isinstance(node.func, ast.Attribute): full_call_name = _get_full_attribute_name(node.func)
            
            if full_call_name in forbidden_calls:
                violations.append({
                    "rule": "safety.dangerous_call",
                    "message": f"Use of forbidden call: '{full_call_name}'",
                    "line": node.lineno,
                    "severity": "error"
                })
        # Check for forbidden imports
        elif isinstance(node, ast.Import):
            for alias in node.names:
                if alias.name.split(".")[0] in forbidden_imports:
                    violations.append({
                        "rule": "safety.forbidden_import",
                        "message": f"Import of forbidden module: '{alias.name}'",
                        "line": node.lineno,
                        "severity": "error"
                    })
        elif isinstance(node, ast.ImportFrom):
            if node.module and node.module.split(".")[0] in forbidden_imports:
                 violations.append({
                    "rule": "safety.forbidden_import",
                    "message": f"Import from forbidden module: '{node.module}'",
                    "line": node.lineno,
                    "severity": "error"
                })
    return violations

def _check_for_todo_comments(code: str) -> List[Violation]:
    """Scans source code for TODO/FIXME comments and returns them as violations."""
    violations: List[Violation] = []
    for i, line in enumerate(code.splitlines(), 1):
        if '#' in line:
            comment = line.split('#', 1)[1]
            if 'TODO' in comment or 'FIXME' in comment:
                violations.append({
                    "rule": "clarity.no_todo_comments",
                    "message": f"Unresolved '{comment.strip()}' on line {i}",
                    "line": i,
                    "severity": "warning"
                })
    return violations

# CAPABILITY: semantic_validation
def _check_semantics(code: str, file_path: str) -> List[Violation]:
    """Runs all policy-aware semantic checks on a string of Python code."""
    try:
        tree = ast.parse(code)
    except SyntaxError:
        # Syntax errors are caught by check_syntax, so we can ignore them here.
        return []
    return _find_dangerous_patterns(tree, file_path)

def _validate_python_code(path_hint: str, code: str) -> Tuple[str, List[Violation]]:
    """
    Internal pipeline for Python code validation.
    Returns the final code and a list of all found violations.
    """
    all_violations: List[Violation] = []
    
    # 1. Format with Black. This can fail on major syntax errors.
    try:
        formatted_code = format_code_with_black(code)
    except (black.InvalidInput, Exception) as e:
        # If Black fails, the code is fundamentally broken.
        all_violations.append({"rule": "tooling.black_failure", "message": str(e), "line": 0, "severity": "error"})
        # Return the original code since formatting failed.
        return code, all_violations

    # 2. Lint with Ruff (which also fixes).
    fixed_code, ruff_violations = fix_and_lint_code_with_ruff(formatted_code, path_hint)
    all_violations.extend(ruff_violations)
    
    # 3. Check syntax on the post-Ruff code.
    syntax_violations = check_syntax(path_hint, fixed_code)
    all_violations.extend(syntax_violations)
    # If there's a syntax error, no further checks are reliable.
    if any(v['severity'] == 'error' for v in syntax_violations):
        return fixed_code, all_violations
    
    # 4. Perform semantic and clarity checks on the valid code.
    all_violations.extend(_check_semantics(fixed_code, path_hint))
    all_violations.extend(_check_for_todo_comments(fixed_code))
    
    return fixed_code, all_violations

def _validate_yaml(code: str) -> Tuple[str, List[Violation]]:
    """Internal pipeline for YAML validation."""
    violations = []
    try:
        yaml.safe_load(code)
    except yaml.YAMLError as e:
        violations.append({
            "rule": "syntax.yaml",
            "message": f"Invalid YAML format: {e}",
            "line": e.problem_mark.line + 1 if e.problem_mark else 0,
            "severity": "error"
        })
    return code, violations

def _get_file_classification(file_path: str) -> str:
    """Determines the file type based on its extension."""
    suffix = Path(file_path).suffix.lower()
    if suffix == ".py": return "python"
    if suffix in [".yaml", ".yml"]: return "yaml"
    if suffix in [".md", ".txt", ".json"]: return "text"
    return "unknown"

# CAPABILITY: code_quality_analysis
def validate_code(file_path: str, code: str, quiet: bool = False) -> Dict[str, Any]:
    """Validate a file's code by routing it to the appropriate validation pipeline based on its file type, returning a standardized dictionary with status, violations, and processed code."""
    """
    The main entry point for validation. It determines the file type
    and routes it to the appropriate validation pipeline, returning a
    standardized dictionary.
    """
    classification = _get_file_classification(file_path)
    if not quiet:
        log.debug(f"Validation: Classifying '{file_path}' as '{classification}'.")
    
    final_code = code
    violations: List[Violation] = []

    if classification == "python":
        final_code, violations = _validate_python_code(file_path, code)
    elif classification == "yaml":
        final_code, violations = _validate_yaml(code)
    
    # Determine final status. "dirty" if there are any 'error' severity violations.
    is_dirty = any(v.get("severity") == "error" for v in violations)
    status = "dirty" if is_dirty else "clean"

    return {"status": status, "violations": violations, "code": final_code}
--- END OF FILE ./src/core/validation_pipeline.py ---

--- START OF FILE ./src/shared/config_loader.py ---
# src/shared/config_loader.py

import json
import yaml
from pathlib import Path
from typing import Dict, Any
from shared.logger import getLogger

log = getLogger(__name__)

def load_config(file_path: Path, file_type: str = "auto") -> Dict[str, Any]:
    """Loads a JSON or YAML file into a dictionary, handling missing files, invalid formats, and parsing errors by returning an empty dict."""
    """
    Loads a JSON or YAML file into a dictionary with consistent error handling.

    Args:
        file_path (Path): Path to the file to load.
        file_type (str): 'json', 'yaml', or 'auto' to infer from extension.

    Returns:
        Dict[str, Any]: Parsed file content or empty dict if file is missing/invalid.
    """
    file_path = Path(file_path)
    if not file_path.exists():
        log.warning(f"Configuration file not found at {file_path}, returning empty dict.")
        return {}

    # Determine file type if 'auto'
    if file_type == "auto":
        suffix = file_path.suffix.lower()
        file_type = "json" if suffix == ".json" else "yaml" if suffix in (".yaml", ".yml") else None

    if file_type not in ("json", "yaml"):
        log.error(f"Unsupported file type for {file_path}, cannot load.")
        return {}

    try:
        with file_path.open(encoding="utf-8") as f:
            if file_type == "json":
                data = json.load(f)
                return data if isinstance(data, dict) else {}
            else:  # yaml
                data = yaml.safe_load(f)
                return data if isinstance(data, dict) else {}
    except (json.JSONDecodeError, yaml.YAMLError) as e:
        log.error(f"Error parsing {file_path}: {e}", exc_info=True)
        return {}
--- END OF FILE ./src/shared/config_loader.py ---

--- START OF FILE ./src/shared/config.py ---
# src/shared/config.py
"""
Centralized Pydantic-based settings management for CORE.

This module defines a `Settings` class that automatically loads configuration
from environment variables and .env files. It provides a single, typed source
of truth for all configuration parameters.
"""
from pathlib import Path
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    """
    A Pydantic settings model that loads configuration from the environment.
    It provides typed, validated access to all system settings.
    """
    # --- Path Configuration ---
    # These have sensible defaults but can be overridden by the .env file.
    MIND: Path = Path(".intent")
    BODY: Path = Path("src")
    REPO_PATH: Path = Path(".")

    # --- Orchestrator LLM Configuration ---
    ORCHESTRATOR_API_URL: str
    ORCHESTRATOR_API_KEY: str
    ORCHESTRATOR_MODEL_NAME: str = "deepseek-chat"

    # --- Generator LLM Configuration ---
    GENERATOR_API_URL: str
    GENERATOR_API_KEY: str
    GENERATOR_MODEL_NAME: str = "deepseek-coder"
    
    # --- CLI & Governance Configuration ---
    KEY_STORAGE_DIR: Path = Path.home() / ".config" / "core"

    class Config:
        """Defines Pydantic's behavior for the Settings model."""
        # This tells Pydantic to load variables from a .env file if it exists.
        env_file = ".env"
        env_file_encoding = 'utf-8'

# Create a single, reusable instance of the settings for other modules to import.
settings = Settings()
--- END OF FILE ./src/shared/config.py ---

--- START OF FILE ./src/shared/constants.py ---
"""
Centralized location for system-wide constant values.
"""

# Maximum allowed file size for system operations (1MB)
MAX_FILE_SIZE_BYTES = 1 * 1024 * 1024

--- END OF FILE ./src/shared/constants.py ---

--- START OF FILE ./src/shared/logger.py ---
# src/shared/logger.py
"""
CORE's Unified Logging System.

This module provides a single, pre-configured logger instance for the entire
application. It uses the 'rich' library to ensure all output is consistent,
beautifully formatted, and informative.

All other modules should import `getLogger` from this file instead of using
print() or configuring their own loggers.
"""
import logging
import sys
from rich.logging import RichHandler

# --- Configuration ---
LOG_LEVEL = "INFO"
LOG_FORMAT = "%(message)s"
LOG_DATE_FORMAT = "[%X]" # e.g., [14:30:55]

# --- Prevent duplicate handlers if this module is reloaded ---
# This is crucial for environments like Uvicorn's reloader.
logging.getLogger().handlers = []

# --- Create and configure the handler ---
# The RichHandler will format the output beautifully.
handler = RichHandler(
    rich_tracebacks=True,
    show_time=True,
    show_level=True,
    show_path=False, # Can be enabled for deeper debugging
    log_time_format=LOG_DATE_FORMAT
)

# --- Configure the root logger ---
# All loggers created with logging.getLogger(name) will inherit this config.
logging.basicConfig(
    level=LOG_LEVEL,
    format=LOG_FORMAT,
    handlers=[handler]
)

# CAPABILITY: system_logging
def getLogger(name: str) -> logging.Logger:
    """Returns a pre-configured logger instance with the given name."""
    """
    Returns a pre-configured logger instance.

    Args:
        name (str): The name of the logger, typically __name__ of the calling module.
    
    Returns:
        logging.Logger: The configured logger.
    """
    return logging.getLogger(name)

# Example of a root-level logger if needed directly
log = getLogger("core_root")
--- END OF FILE ./src/shared/logger.py ---

--- START OF FILE ./src/shared/manifest.yaml ---
capabilities:
- system_logging
description: Shared models, helpers, and config interfaces
domain: shared

--- END OF FILE ./src/shared/manifest.yaml ---

--- START OF FILE ./src/shared/path_utils.py ---
# src/shared/path_utils.py

from pathlib import Path
from typing import Optional

def get_repo_root(start_path: Optional[Path] = None) -> Path:
    """Find and return the repository root by locating the .git directory, starting from the current directory or provided path."""
    """
    Find and return the repository root by locating the .git directory.
    Starts from current directory or provided path.
    
    Returns:
        Path: Absolute path to repo root.
    
    Raises:
        RuntimeError: If no .git directory is found.
    """
    current = Path(start_path or Path.cwd()).resolve()
    
    # Traverse upward until .git is found
    for parent in [current, *current.parents]:
        if (parent / ".git").exists():
            return parent
    
    raise RuntimeError("Not a git repository: could not find .git directory")
--- END OF FILE ./src/shared/path_utils.py ---

--- START OF FILE ./src/shared/schemas/manifest_validator.py ---
# src/shared/schemas/manifest_validator.py
"""Shared utilities for validating manifest files against schemas."""
import json
import jsonschema
from pathlib import Path
from typing import Dict, Any, Tuple, List

from shared.path_utils import get_repo_root

# The single source of truth for the location of constitutional schemas.
SCHEMA_DIR = get_repo_root() / ".intent" / "schemas"

def load_schema(schema_name: str) -> Dict[str, Any]:
    """
    Load a JSON schema from the .intent/schemas/ directory.
    
    Args:
        schema_name (str): The filename of the schema (e.g., 'knowledge_graph_entry.schema.json').
        
    Returns:
        Dict[str, Any]: The loaded JSON schema.
        
    Raises:
        FileNotFoundError: If the schema file is not found.
        json.JSONDecodeError: If the schema file is not valid JSON.
    """
    schema_path = SCHEMA_DIR / schema_name
    
    if not schema_path.exists():
        raise FileNotFoundError(f"Schema file not found: {schema_path}")
        
    try:
        with open(schema_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        raise json.JSONDecodeError(f"Invalid JSON in schema file {schema_path}: {e.msg}", e.doc, e.pos)

def validate_manifest_entry(entry: Dict[str, Any], schema_name: str = "knowledge_graph_entry.schema.json") -> Tuple[bool, List[str]]:
    """
    Validate a single manifest entry against a schema.
    
    Args:
        entry: The dictionary representing a single function/class entry.
        schema_name: The filename of the schema to validate against.
        
    Returns:
        A tuple of (is_valid: bool, list_of_error_messages: List[str]).
    """
    try:
        schema = load_schema(schema_name)
    except Exception as e:
        return False, [f"Failed to load schema '{schema_name}': {e}"]

    # Use Draft7Validator for compatibility with our schema definition.
    validator = jsonschema.Draft7Validator(schema)
    errors = []
    
    for error in validator.iter_errors(entry):
        # Create a user-friendly error message
        path = ".".join(str(p) for p in error.absolute_path) or "<root>"
        errors.append(f"Validation error at '{path}': {error.message}")
        
    is_valid = not errors
    return is_valid, errors
--- END OF FILE ./src/shared/schemas/manifest_validator.py ---

--- START OF FILE ./src/shared/utils/import_scanner.py ---
# src/shared/utils/import_scanner.py

"""
Import Scanner Utility
======================

Scans a Python file for top-level import statements.
"""

import ast
from pathlib import Path
from typing import List
from shared.logger import getLogger

log = getLogger(__name__)

def scan_imports_for_file(file_path: Path) -> List[str]:
    """
    Parse a Python file and extract all imported module paths.

    Args:
        file_path (Path): Path to the file.

    Returns:
        List[str]: List of imported module paths.
    """
    imports = []
    try:
        source = file_path.read_text(encoding="utf-8")
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)

    except Exception as e:
        log.warning(f"Failed to scan imports for {file_path}: {e}", exc_info=True)

    return imports
--- END OF FILE ./src/shared/utils/import_scanner.py ---

--- START OF FILE ./src/shared/utils/__init__.py ---
[EMPTY FILE]

--- END OF FILE ./src/shared/utils/__init__.py ---

--- START OF FILE ./src/shared/utils/manifest_aggregator.py ---
# src/shared/utils/manifest_aggregator.py
"""
A utility to discover and aggregate domain-specific manifests into a single,
unified view of the system's constitution.
"""
from pathlib import Path
import yaml
from typing import Dict, Any, List

from shared.logger import getLogger

log = getLogger("manifest_aggregator")

def aggregate_manifests(repo_root: Path) -> Dict[str, Any]:
    """
    Finds all domain-specific manifest.yaml files and merges them.

    This function is the heart of the modular manifest system. It reads the
    source structure to find all domains, then searches for a manifest in each
    domain's directory, aggregating their contents.

    Args:
        repo_root (Path): The absolute path to the repository root.

    Returns:
        A dictionary representing the aggregated manifest, primarily focused
        on compiling a unified list of 'required_capabilities'.
    """
    log.info("ðŸ” Starting manifest aggregation...")
    source_structure_path = repo_root / ".intent" / "knowledge" / "source_structure.yaml"
    if not source_structure_path.exists():
        log.error("âŒ Cannot aggregate manifests: source_structure.yaml not found.")
        return {}

    source_structure = yaml.safe_load(source_structure_path.read_text())
    
    all_capabilities = []
    domains_found = 0

    for domain_entry in source_structure.get("structure", []):
        domain_path_str = domain_entry.get("path")
        if not domain_path_str:
            continue

        manifest_path = repo_root / domain_path_str / "manifest.yaml"
        if manifest_path.exists():
            domains_found += 1
            log.debug(f"   -> Found manifest for domain '{domain_entry.get('domain')}' at {manifest_path}")
            domain_manifest = yaml.safe_load(manifest_path.read_text())
            if domain_manifest and "capabilities" in domain_manifest:
                all_capabilities.extend(domain_manifest["capabilities"])

    log.info(f"   -> Aggregated capabilities from {domains_found} domain manifests.")

    # We also keep some top-level info from the original monolithic manifest for now
    # to ensure a smooth transition.
    monolith_path = repo_root / ".intent" / "project_manifest.yaml"
    monolith_data = {}
    if monolith_path.exists():
        monolith_data = yaml.safe_load(monolith_path.read_text())

    aggregated_manifest = {
        "name": monolith_data.get("name", "CORE"),
        "intent": monolith_data.get("intent", "No intent provided."),
        "active_agents": monolith_data.get("active_agents", []),
        "required_capabilities": sorted(list(set(all_capabilities))),
    }

    return aggregated_manifest

--- END OF FILE ./src/shared/utils/manifest_aggregator.py ---

--- START OF FILE ./src/shared/utils/parsing.py ---
# src/shared/utils/parsing.py
"""
Parsing utility functions for the CORE system.
"""
import re
from typing import Dict

def parse_write_blocks(llm_output: str) -> Dict[str, str]:
    """
    Extracts all [[write:...]] blocks from LLM output.

    This function is robust and handles both [[end]] and [[/write]] as valid terminators
    to accommodate different LLM habits.

    Args:
        llm_output (str): The raw text output from a language model.

    Returns:
        A dictionary mapping file paths to their corresponding code content.
    """

    pattern = r"\[\[write:\s*(.+?)\]\](.*?)(?:\[\[end\]\]|\[\[/write\]\])"
    matches = re.findall(pattern, llm_output, re.DOTALL)
    return {path.strip(): code.strip() for path, code in matches}

#def extract_json_from_response(text: str) -> str:
#    """
#    Extracts a JSON object or array from a raw text response.
#    Handles both markdown ```json code blocks and raw JSON strings.#
#        Args:
#        text (str): The raw text output from a language model.#
#    Returns:
#        A string containing the extracted JSON, or an empty string if not found.
#    """
#    match = re.search(r"```json\n([\s\S]*?)\n```", text, re.DOTALL)
#    if match:
#        return match.group(1).strip()
#    match = re.search(r'\[\s*\{[\s\S]*?\}\s*\]', text)
#    if match:
#        return match.group(0).strip()
#    return ""
--- END OF FILE ./src/shared/utils/parsing.py ---

--- START OF FILE ./src/system/admin/byor.py ---
# src/system/admin/byor.py
"""
Intent: Implements the 'byor-init' command for the CORE Admin CLI.

This command is the entry point for the "Bring Your Own Repo" capability.
It analyzes an external repository and proposes a minimal `.intent/`
scaffold to begin governing it with CORE.
"""

from pathlib import Path
import typer
import yaml
from shared.logger import getLogger
from system.tools.codegraph_builder import KnowledgeGraphBuilder

log = getLogger("core_admin.byor")
# --- THIS IS THE CHANGE (Part 1 of 3) ---
# We now know where our templates are located.
CORE_ROOT = Path(__file__).resolve().parents[2]
TEMPLATES_DIR = CORE_ROOT / "system" / "templates"

def initialize_repository(
    path: Path = typer.Argument(
        ...,
        help="The path to the external repository to analyze.",
        exists=True,
        file_okay=False,
        dir_okay=True,
        resolve_path=True,
    ),
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--write",
        help="Show the proposed .intent/ scaffold without writing files. Use --write to apply.",
    ),
):
    """
    Analyzes an external repository and scaffolds a minimal `.intent/` constitution.
    """
    log.info(f"ðŸš€ Starting analysis of repository at: {path}")

    # Step 1: Build the Knowledge Graph.
    log.info("   -> Step 1: Building Knowledge Graph of the target repository...")
    try:
        builder = KnowledgeGraphBuilder(root_path=path)
        graph = builder.build()
        total_symbols = len(graph.get("symbols", {}))
        log.info(f"   -> âœ… Knowledge Graph built successfully. Found {total_symbols} symbols.")
    except Exception as e:
        log.error(f"   -> âŒ Failed to build Knowledge Graph: {e}", exc_info=True)
        raise typer.Exit(code=1)

    # Step 2: Generate the content for the new constitutional files.
    log.info("   -> Step 2: Generating starter constitution from analysis...")
    
    # --- THIS IS THE CHANGE (Part 2 of 3) ---
    # We now create a dictionary of all the files we intend to generate.
    
    # File 1: source_structure.yaml
    domains = builder.domain_map
    source_structure_content = {
        "structure": [
            { "domain": name, "path": path_str, "description": f"Domain for '{name}' inferred by CORE.", "allowed_imports": [name, "shared"], }
            for path_str, name in domains.items()
        ]
    }
    
    # File 2: project_manifest.yaml
    discovered_capabilities = sorted(list(set(
        s["capability"] for s in graph.get("symbols", {}).values() if s.get("capability") != "unassigned"
    )))
    project_manifest_content = {
        "name": path.name, "version": "0.1.0-core-scaffold", "intent": "A high-level description of what this project is intended to do.",
        "required_capabilities": discovered_capabilities,
    }

    # File 3: capability_tags.yaml (dynamically populated)
    capability_tags_content = {
        "tags": [
            {"name": cap, "description": "A clear explanation of what this capability does."}
            for cap in discovered_capabilities
        ]
    }

    # The files we will create and their content.
    files_to_generate = {
        ".intent/knowledge/source_structure.yaml": source_structure_content,
        ".intent/project_manifest.yaml": project_manifest_content,
        ".intent/knowledge/capability_tags.yaml": capability_tags_content,
        ".intent/mission/principles.yaml": (TEMPLATES_DIR / "principles.yaml.template").read_text(),
        ".intent/policies/safety_policies.yaml": (TEMPLATES_DIR / "safety_policies.yaml.template").read_text(),
    }

    # Step 3: Write the files or display the dry run.
    if dry_run:
        log.info("\nðŸ’§ Dry Run Mode: No files will be written.")
        for rel_path, content in files_to_generate.items():
            typer.secho(f"\nðŸ“„ Proposed `{rel_path}`:", fg=typer.colors.YELLOW)
            if isinstance(content, dict):
                typer.echo(yaml.dump(content, indent=2))
            else:
                typer.echo(content)
    else:
        log.info("\nðŸ’¾ **Write Mode:** Applying changes to disk.")
        for rel_path, content in files_to_generate.items():
            target_path = path / rel_path
            target_path.parent.mkdir(parents=True, exist_ok=True)
            if isinstance(content, dict):
                target_path.write_text(yaml.dump(content, indent=2))
            else:
                target_path.write_text(content)
            typer.secho(f"   -> âœ… Wrote starter file to {target_path}", fg=typer.colors.GREEN)

    log.info("\nðŸŽ‰ BYOR initialization complete.")


def register(app: typer.Typer) -> None:
    """Register BYOR commands (e.g., `byor-init`) under the admin CLI."""
    """Intent: Register BYOR commands under the admin CLI."""
    app.command("byor-init")(initialize_repository)
--- END OF FILE ./src/system/admin/byor.py ---

--- START OF FILE ./src/system/admin_cli.py ---
# src/system/admin_cli.py
"""
Intent: Stable public entrypoint for the CORE Admin CLI.
Re-exports the Typer application without exposing internal wiring.
"""
from system.admin import app

__all__ = ["app"]

--- END OF FILE ./src/system/admin_cli.py ---

--- START OF FILE ./src/system/admin/fixer.py ---
# src/system/admin/fixer.py
"""
Intent: Registers self-healing and code-fixing tools with the CORE Admin CLI.
"""

import typer
from system.tools.docstring_adder import fix_missing_docstrings

def register(app: typer.Typer) -> None:
    """Intent: Register fixer commands under the admin CLI."""
    fixer_app = typer.Typer(help="Self-healing and code quality tools.")
    app.add_typer(fixer_app, name="fix")
    
    fixer_app.command("docstrings")(fix_missing_docstrings)

--- END OF FILE ./src/system/admin/fixer.py ---

--- START OF FILE ./src/system/admin/guard.py ---
# src/system/admin/guard.py
"""
Intent: Governance/validation guard commands exposed to the operator.
- `drift`: compare .intent manifest vs discovered code capabilities.
- `kg-export`: emit a knowledge-graph artifact so `--strict-intent` has a stable source.

This command honors UX defaults declared in .intent/project_manifest.yaml under:
  operator_experience.guard.drift
If absent, it falls back to sensible defaults (JSON output, non-strict).
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import List, Optional, Dict, Any

import typer
import yaml
from rich import print as rprint
from rich.panel import Panel
from rich.table import Table
from shared.logger import getLogger
from system.guard.drift_detector import (
    collect_code_capabilities,
    detect_capability_drift,
    load_manifest,
    write_report,
)
# --- THIS IS THE FIX ---
# We now import our single source of truth.
from system.admin.utils import should_fail

log = getLogger("core_admin")

def _find_manifest_path(root: Path, explicit: Optional[Path]) -> Path:
    if explicit:
        return explicit
    for p in (root / ".intent" / "project_manifest.yaml", root / ".intent" / "manifest.yaml"):
        if p.exists():
            return p
    raise FileNotFoundError("No manifest found (.intent/project_manifest.yaml or .intent/manifest.yaml)")

def _load_raw_manifest(root: Path, explicit: Optional[Path]) -> Dict[str, Any]:
    """Loads and parses a YAML manifest file from the given root or explicit path, returning its contents as a dictionary."""
    path = _find_manifest_path(root, explicit)
    data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    return data

def _ux_defaults(root: Path, explicit: Optional[Path]) -> Dict[str, Any]:
    raw = _load_raw_manifest(root, explicit)
    ux = (((raw.get("operator_experience") or {})
           .get("guard") or {})
           .get("drift") or {})
    return {
        "default_format": ux.get("default_format", "json"),
        "default_fail_on": ux.get("default_fail_on", "any"),
        "strict_default": bool(ux.get("strict_default", False)),
        "evidence_json": bool(ux.get("evidence_json", True)),
        "evidence_path": ux.get("evidence_path", "reports/drift_report.json"),
        "labels": {
            "none": (ux.get("labels") or {}).get("none", "NONE"),
            "success": (ux.get("labels") or {}).get("success", "âœ… No capability drift"),
            "failure": (ux.get("labels") or {}).get("failure", "ðŸš¨ Drift detected"),
        },
    }

    """Determines whether a report is clean by checking for missing, undeclared, or mismatched entries."""
def _is_clean(report: dict) -> bool:
    return not (report.get("missing_in_code") or report.get("undeclared_in_manifest") or report.get("mismatched_mappings"))

def _print_table(report_dict: dict, labels: Dict[str, str]) -> None:
    table = Table(show_header=True, header_style="bold", title="Capability Drift")
    table.add_column("Section", style="bold")
    table.add_column("Values")

    def row(title: str, items: List[str]):
        """Formats and adds a row to a table with a title and a list of items, highlighting empty lists in green and non-empty lists in yellow."""
        if not items:
            table.add_row(title, f"[bold green]{labels['none']}[/bold green]")
        else:
            joined = "\n".join(f"- {it}" for it in items)
            table.add_row(title, f"[yellow]{joined}[/yellow]")

    row("Missing in code", report_dict.get("missing_in_code", []))
    row("Undeclared in manifest", report_dict.get("undeclared_in_manifest", []))

    mism = report_dict.get("mismatched_mappings", [])
    if not mism:
        table.add_row("Mismatched mappings", f"[bold green]{labels['none']}[/bold green]")
    else:
        lines = []
        for m in mism:
            man, cod = m.get("manifest", {}), m.get("code", {})
            lines.append(
                f"- {m.get('capability')}: "
                f"manifest(domain={man.get('domain')}, owner={man.get('owner')}) "
                f"!= code(domain={cod.get('domain')}, owner={cod.get('owner')})"
            )
        table.add_row("Mismatched mappings", "[yellow]" + "\n".join(lines) + "[/yellow]")

    status = "[bold green]" + labels["success"] + "[/bold green]" if _is_clean(report_dict) else "[bold red]" + labels["failure"] + "[/bold red]"
    rprint(Panel.fit(table, title=status))

def _print_pretty(report_dict: dict, labels: Dict[str, str]) -> None:
    clean = _is_clean(report_dict)
    if clean:
        summary = (
            f"[bold green]{labels['success']}[/bold green]\n"
            f"[green]missing_in_code: {labels['none']}[/green]\n"
            f"[green]undeclared_in_manifest: {labels['none']}[/green]\n"
            f"[green]mismatched_mappings: {labels['none']}[/green]"
        )
    else:
        summary = (
            f"[bold red]{labels['failure']}[/bold red]\n"
            f"[yellow]missing_in_code: {len(report_dict.get('missing_in_code', []))}[/yellow]\n"
            f"[yellow]undeclared_in_manifest: {len(report_dict.get('undeclared_in_manifest', []))}[/yellow]\n"
            f"[yellow]mismatched_mappings: {len(report_dict.get('mismatched_mappings', []))}[/yellow]"
        )
    rprint(Panel(summary, title="Capability Drift", border_style="green" if clean else "red"))
    _print_table(report_dict, labels)

def register(app: typer.Typer) -> None:
    guard = typer.Typer(help="Governance/validation guards")
    app.add_typer(guard, name="guard")

    @guard.command("drift")
    def drift(
        root: Path = typer.Option(Path("."), help="Repository root (default: .)"),
        manifest_path: Optional[Path] = typer.Option(None, help="Explicit manifest path"),
        output: Optional[Path] = typer.Option(None, help="Path for JSON evidence report"),
        format: Optional[str] = typer.Option(None, help="json|table|pretty (defaults come from manifest)"),
        fail_on: Optional[str] = typer.Option(None, help="any|missing|undeclared (default from manifest)"),
        include: Optional[List[str]] = typer.Option(None, help="Include globs"),
        exclude: Optional[List[str]] = typer.Option(None, help="Exclude globs"),
        strict_intent: Optional[bool] = typer.Option(None, help="Default from manifest (strict_default)"),
    ) -> None:
        """Intent: Compare manifest vs code to detect capability drift; write JSON evidence for CI."""
        ux = _ux_defaults(root, manifest_path)
        fmt = (format or ux["default_format"]).lower()
        fail_policy = (fail_on or ux["default_fail_on"]).lower()
        strict = bool(ux["strict_default"] if strict_intent is None else strict_intent)

        manifest_caps = load_manifest(root, explicit_path=manifest_path)
        code_caps = collect_code_capabilities(root, include_globs=include or [], exclude_globs=exclude or [], require_kgb=strict)
        report = detect_capability_drift(manifest_caps, code_caps)
        report_dict = report.to_dict()

        if ux["evidence_json"]:
            evidence_path = output or (root / ux["evidence_path"])
            write_report(evidence_path, report)

        labels = ux["labels"]
        if fmt == "json":
            typer.echo(json.dumps(report_dict, indent=2))
        elif fmt == "table":
            _print_table(report_dict, labels)
        elif fmt == "pretty":
            _print_pretty(report_dict, labels)
        else:
            typer.echo(json.dumps(report_dict, indent=2))

        if should_fail(report_dict, fail_policy):
            raise typer.Exit(code=2)

    @guard.command("kg-export")
    def kg_export(
        root: Path = typer.Option(Path("."), help="Repository root (default: .)"),
        output: Optional[Path] = typer.Option(None, help="Artifact file (default: <root>/reports/knowledge_graph.json)"),
        include: Optional[List[str]] = typer.Option(None, help="Include globs"),
        exclude: Optional[List[str]] = typer.Option(None, help="Exclude globs"),
        prefer: str = typer.Option("auto", case_sensitive=False, help="auto|kgb|grep"),
    ) -> None:
        """
        Intent: Emit a minimal knowledge-graph artifact with capability nodes.
        Format: { "nodes": [ {"capability": "...", "domain": "x", "owner": "y"}, ... ] }
        """
        require_kgb = prefer.lower() == "kgb"
        caps = collect_code_capabilities(root, include_globs=include or [], exclude_globs=exclude or [], require_kgb=require_kgb)
        nodes = [{"capability": k, "domain": v.domain, "owner": v.owner} for k, v in sorted(caps.items())]

        out_path = output or (root / "reports" / "knowledge_graph.json")
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(json.dumps({"nodes": nodes}, indent=2), encoding="utf-8")
        rprint(f"[bold green]âœ…[/bold green] Wrote knowledge-graph artifact with [bold]{len(nodes)}[/bold] capability nodes -> {out_path}")
--- END OF FILE ./src/system/admin/guard.py ---

--- START OF FILE ./src/system/admin/__init__.py ---
# src/system/admin/__init__.py
"""
Intent: Modular CORE Admin CLI root. Wires subcommand groups (keys, proposals, guard)
without changing the public console script target (system.admin_cli:app).
"""

import typer

app = typer.Typer(
    rich_markup_mode="markdown",
    help="""
    ðŸ›ï¸  **CORE Admin CLI**

    The command-line interface for the CORE Human Operator.
    Provides safe, governed commands for managing the system's constitution.
    """,
    no_args_is_help=True,
)

# Register command groups
from system.admin import keys as _keys  # noqa: E402
from system.admin import proposals as _proposals  # noqa: E402
from system.admin import guard as _guard  # noqa: E402
from system.admin import migrator as _migrator # noqa: E402
from system.admin import fixer as _fixer # noqa: E402
from system.admin import byor as _byor # noqa: E402
from system.admin import scaffolder as _scaffolder # noqa: E402

_keys.register(app)
_proposals.register(app)
_guard.register(app)
_migrator.register(app)
_fixer.register(app)
_byor.register(app)
_scaffolder.register(app)


__all__ = ["app"]

--- END OF FILE ./src/system/admin/__init__.py ---

--- START OF FILE ./src/system/admin/keys.py ---
# src/system/admin/keys.py
"""
Intent: Key management commands for the CORE Admin CLI.
Provides Ed25519 key generation and helper output for approver configuration.
"""

from __future__ import annotations

import os
import yaml
import typer
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ed25519
from shared.config import settings
from shared.logger import getLogger

log = getLogger("core_admin")


def register(app: typer.Typer) -> None:
    """Intent: Register key management commands under the admin CLI."""
    @app.command("keygen")
    def keygen(
        identity: str = typer.Argument(help="Identity for the key pair (e.g., 'your.name@example.com').")
    ) -> None:
        """Intent: Generate a new Ed25519 key pair and print an approver YAML block."""
        log.info(f"ðŸ”‘ Generating new key pair for identity: {identity}")
        settings.KEY_STORAGE_DIR.mkdir(parents=True, exist_ok=True)
        private_key_path = settings.KEY_STORAGE_DIR / "private.key"

        if private_key_path.exists():
            typer.confirm(
                "âš ï¸ A private key already exists. Overwriting it will invalidate your old identity. Continue?",
                abort=True,
            )

        private_key = ed25519.Ed25519PrivateKey.generate()
        public_key = private_key.public_key()

        pem_private = private_key.private_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PrivateFormat.PKCS8,
            encryption_algorithm=serialization.NoEncryption(),
        )
        private_key_path.write_bytes(pem_private)
        os.chmod(private_key_path, 0o600)

        pem_public = public_key.public_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PublicFormat.SubjectPublicKeyInfo,
        )

        log.info(f"\nâœ… Private key saved securely to: {private_key_path}")
        log.info("\nðŸ“‹ Add the following YAML block to '.intent/constitution/approvers.yaml' under 'approvers':\n")

        approver_yaml = yaml.dump(
            [
                {
                    "identity": identity,
                    "public_key": pem_public.decode("utf-8"),
                    "role": "maintainer",
                    "description": "Primary maintainer",
                }
            ],
            indent=2,
        )
        print(approver_yaml)

--- END OF FILE ./src/system/admin/keys.py ---

--- START OF FILE ./src/system/admin/migrator.py ---
# src/system/admin/migrator.py
"""
Intent: Registers the manifest migration tool with the CORE Admin CLI.
"""

import typer
from system.tools.manifest_migrator import migrate_manifest

def register(app: typer.Typer) -> None:
    """Register migration commands (manifest-migrator) under the admin CLI."""
    """Intent: Register migration commands under the admin CLI."""
    app.command("manifest-migrator")(migrate_manifest)
--- END OF FILE ./src/system/admin/migrator.py ---

--- START OF FILE ./src/system/admin/proposals.py ---
# src/system/admin/proposals.py
"""
Intent: Proposal lifecycle commands (list, sign, approve) for constitution-governed changes.
Integrates with the ConstitutionalAuditor via a canary workspace before applying changes.
"""

from __future__ import annotations

import base64
import shutil
import tempfile
import subprocess # <<< MODIFICATION: We need this to run git commands.
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

import typer
from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ed25519
from shared.config import settings
from shared.logger import getLogger
from system.governance.constitutional_auditor import ConstitutionalAuditor
from system.admin.utils import (
    archive_rollback_plan,
    generate_approval_token,
    load_private_key,
    load_yaml_file,
    save_yaml_file,
)

log = getLogger("core_admin")


def register(app: typer.Typer) -> None:
    """Intent: Register proposal lifecycle commands under the admin CLI."""
    @app.command("proposals-list")
    def proposals_list() -> None:
        """List pending constitutional proposals and display their justification, target path, and signature/quorum status."""
        log.info("ðŸ” Finding pending constitutional proposals...")
        proposals_dir = settings.MIND / "proposals"
        proposals_dir.mkdir(exist_ok=True)
        proposals = sorted(proposals_dir.glob("cr-*.yaml"))

        if not proposals:
            log.info("âœ… No pending proposals found.")
            return

        log.info(f"Found {len(proposals)} pending proposal(s):")
        approvers_config = load_yaml_file(settings.MIND / "constitution" / "approvers.yaml")

        for prop_path in proposals:
            config = load_yaml_file(prop_path)
            justification = config.get("justification", "No justification provided.")
            is_critical = any(
                config.get("target_path", "").endswith(p) for p in approvers_config.get("critical_paths", [])
            )
            required = approvers_config.get("quorum", {}).get("critical" if is_critical else "standard", 1)
            current = len(config.get("signatures", []))
            status = "âœ… Ready" if current >= required else f"â³ {current}/{required} sigs"

            log.info(f"\n  - **{prop_path.name}**: {justification.strip()}")
            log.info(f"    Target: {config.get('target_path')}")
            log.info(f"    Status: {status} ({'Critical' if is_critical else 'Standard'})")

    @app.command("proposals-sign")
    def proposals_sign(
        proposal_name: str = typer.Argument(help="Filename of the proposal to sign (e.g., 'cr-new-policy.yaml').")
    ) -> None:
        """Intent: Sign a proposal with the operator's private key (content-bound token)."""
        log.info(f"âœï¸ Signing proposal: {proposal_name}")
        proposal_path = settings.MIND / "proposals" / proposal_name
        if not proposal_path.exists():
            log.error(f"âŒ Proposal '{proposal_name}' not found.")
            raise typer.Exit(code=1)

        proposal = load_yaml_file(proposal_path)
        private_key = load_private_key()

        token = generate_approval_token(proposal.get("content", ""))
        signature = private_key.sign(token.encode("utf-8"))

        identity = typer.prompt("Enter your identity (e.g., name@domain.com) to associate with this signature")

        proposal.setdefault("signatures", [])
        proposal["signatures"] = [s for s in proposal["signatures"] if s.get("identity") != identity]
        proposal["signatures"].append(
            {
                "identity": identity,
                "signature_b64": base64.b64encode(signature).decode("utf-8"),
                "token": token,
                "timestamp": datetime.utcnow().isoformat() + "Z",
            }
        )

        save_yaml_file(proposal_path, proposal)
        log.info("âœ… Signature added to proposal file.")

    @app.command("proposals-approve")
    def proposals_approve(
        proposal_name: str = typer.Argument(help="Filename of the proposal to approve.")
    ) -> None:
        """Verify signatures/quorum, run a canary constitutional audit, then apply the proposal if valid."""
        log.info(f"ðŸš€ Attempting to approve proposal: {proposal_name}")
        proposal_path = settings.MIND / "proposals" / proposal_name
        if not proposal_path.exists():
            log.error(f"âŒ Proposal '{proposal_name}' not found.")
            raise typer.Exit(code=1)

        proposal = load_yaml_file(proposal_path)
        target_rel_path = proposal.get("target_path")
        if not target_rel_path:
            log.error("âŒ Proposal is invalid: missing 'target_path'.")
            raise typer.Exit(code=1)

        log.info("ðŸ” Verifying cryptographic signatures...")
        approvers_config = load_yaml_file(settings.MIND / "constitution" / "approvers.yaml")
        approver_keys = {a["identity"]: a["public_key"] for a in approvers_config.get("approvers", [])}

        valid_signatures = 0
        for sig in proposal.get("signatures", []):
            identity = sig.get("identity")
            pem = approver_keys.get(identity)
            if not pem:
                continue
            try:
                pub_key = serialization.load_pem_public_key(pem.encode("utf-8"))
                if not isinstance(pub_key, ed25519.Ed25519PublicKey):
                    log.warning(f"   âš ï¸ Key for '{identity}' is not a valid Ed25519 signing key. Skipping.")
                    continue

                pub_key.verify(base64.b64decode(sig["signature_b64"]), sig["token"].encode("utf-8"))
                if sig["token"] == generate_approval_token(proposal.get("content", "")):
                    log.info(f"   âœ… Valid signature from '{identity}'.")
                    valid_signatures += 1
                else:
                    log.warning(f"   âš ï¸ Signature from '{identity}' is for outdated content.")
            except (InvalidSignature, ValueError, TypeError):
                log.warning(f"   âš ï¸ Invalid signature for '{identity}'.")

        is_critical = any(str(target_rel_path).endswith(p) for p in approvers_config.get("critical_paths", []))
        required = approvers_config.get("quorum", {}).get("critical" if is_critical else "standard", 1)

        if valid_signatures < required:
            log.error(f"âŒ Approval failed: Quorum not met. Have {valid_signatures}/{required} valid signatures.")
            raise typer.Exit(code=1)

        log.info("\nðŸ§  Generating fresh Knowledge Graph before canary validation...")
        try:
            subprocess.run(
                [sys.executable, "-m", "src.system.tools.codegraph_builder"],
                cwd=settings.REPO_PATH, check=True, capture_output=True,
            )
            log.info("   -> Knowledge Graph regenerated successfully.")
        except subprocess.CalledProcessError as e:
            log.error(f"âŒ Failed to regenerate Knowledge Graph. Aborting. Stderr: {e.stderr.decode()}")
            raise typer.Exit(code=1)

        log.info("\nðŸ¦ Spinning up canary environment for validation...")
        with tempfile.TemporaryDirectory() as tmp:
            tmp_path = Path(tmp)
            
            # --- THIS IS THE FIX ---
            # Instead of copying the current (potentially dirty) directory,
            # we create a fresh, clean clone of the repository's current state.
            log.info(f"   -> Creating a clean clone of the repository at {tmp_path}...")
            try:
                subprocess.run(
                    ["git", "clone", str(settings.REPO_PATH), "."],
                    cwd=tmp_path, check=True, capture_output=True
                )
            except subprocess.CalledProcessError as e:
                log.error(f"âŒ Failed to create clean git clone for canary. Aborting. Stderr: {e.stderr.decode()}")
                raise typer.Exit(code=1)
            # --- END OF FIX ---
            
            # Apply proposed change in the clean canary environment
            canary_target_path = tmp_path / target_rel_path
            canary_target_path.parent.mkdir(parents=True, exist_ok=True)
            canary_target_path.write_text(proposal.get("content", ""), encoding="utf-8")

            log.info("ðŸ”¬ Commanding canary to perform a self-audit...")
            auditor = ConstitutionalAuditor(repo_root_override=tmp_path)
            success = auditor.run_full_audit()

            if success:
                log.info("âœ… Canary audit PASSED. Change is constitutionally valid.")
                archive_rollback_plan(proposal_name, proposal)
                live_target_path = settings.REPO_PATH / target_rel_path
                live_target_path.parent.mkdir(parents=True, exist_ok=True)
                live_target_path.write_text(proposal.get("content", ""), encoding="utf-8")
                proposal_path.unlink()
                log.info(f"âœ… Successfully approved and applied '{proposal_name}'.")
            else:
                log.error("âŒ Canary audit FAILED. Proposal rejected; live system untouched.")
                raise typer.Exit(code=1)

    # Optional ergonomic namespace, keeps old single-word commands intact
    proposals = typer.Typer(help="Work with constitutional proposals")
    app.add_typer(proposals, name="proposals")

    @proposals.command("list")
    def _group_list() -> None:
        """Intent: Group alias for proposals-list (namespaced UX)."""
        proposals_list()

    @proposals.command("sign")
    def _group_sign(proposal_name: str) -> None:
        """Intent: Group alias for proposals-sign (namespaced UX)."""
        proposals_sign(proposal_name)

    @proposals.command("approve")
    def _group_approve(proposal_name: str) -> None:
        """Intent: Group alias for proposals-approve (namespaced UX)."""
        proposals_approve(proposal_name)


--- END OF FILE ./src/system/admin/proposals.py ---

--- START OF FILE ./src/system/admin/scaffolder.py ---
# src/system/admin/scaffolder.py
"""
Intent: Implements the 'new' command for scaffolding new CORE-native projects.

This command is the entry point for the "CORE-fication" pipeline, responsible
for creating new Mind/Body applications from scratch, complete with a starter
constitution and CI/CD wiring.
"""

import shutil
from pathlib import Path
import typer
import yaml
from shared.logger import getLogger

log = getLogger("core_admin.scaffolder")
CORE_ROOT = Path(__file__).resolve().parents[2]
STARTER_KITS_DIR = CORE_ROOT / "system" / "starter_kits"
WORKSPACE_DIR = CORE_ROOT / "work"

def new_project(
    name: str = typer.Argument(
        ...,
        help="The name of the new CORE-governed application to create.",
    ),
    profile: str = typer.Option(
        "default",
        "--profile",
        help="The starter kit profile to use for the new project's constitution.",
    ),
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--write",
        help="Show what will be created without writing files. Use --write to apply.",
    ),
):
    """
    Scaffolds a new, constitutionally-governed "Mind/Body" application.
    """
    log.info(f"ðŸš€ Scaffolding new CORE application: '{name}' using '{profile}' profile.")
    
    project_root = WORKSPACE_DIR / name
    starter_kit_path = STARTER_KITS_DIR / profile

    if not starter_kit_path.is_dir():
        log.error(f"âŒ Starter kit profile '{profile}' not found at {starter_kit_path}.")
        raise typer.Exit(code=1)

    # --- THIS IS THE NEW, CLEAN LOGIC ---
    # The scaffolder no longer contains hardcoded templates. It only reads files.
    if dry_run:
        log.info("\nðŸ’§ Dry Run Mode: No files will be written.")
        typer.secho(f"Would create project '{name}' in '{WORKSPACE_DIR}/' with the '{profile}' starter kit.", fg=typer.colors.YELLOW)
    else:
        log.info(f"\nðŸ’¾ **Write Mode:** Creating project structure at {project_root}...")
        if project_root.exists():
            log.error(f"âŒ Directory '{project_root}' already exists. Aborting.")
            raise typer.Exit(code=1)
        
        # Create the basic structure
        project_root.mkdir(parents=True, exist_ok=True)
        (project_root / "src").mkdir()
        (project_root / "reports").mkdir()

        # Copy and process all template files from the starter kit
        for template_path in starter_kit_path.glob("*.template"):
            content = template_path.read_text().format(project_name=name)
            
            # Remove '.template' and handle special case for '.gitignore'
            if template_path.name == "gitignore.template":
                target_name = ".gitignore"
            else:
                target_name = template_path.name.replace(".template", "")
                
            target_path = project_root / target_name
            target_path.write_text(content)
            typer.secho(f"   -> âœ… Created file:      {target_path}", fg=typer.colors.GREEN)
            
        # Copy constitutional files into the .intent directory
        intent_dir = project_root / ".intent"
        intent_dir.mkdir()
        
        constitutional_files = [
            "principles.yaml", "project_manifest.yaml", "safety_policies.yaml", "source_structure.yaml"
        ]
        # Also copy the intent README
        shutil.copy(starter_kit_path / "intent_README.md.template", intent_dir / "README.md")

        for f in constitutional_files:
             shutil.copy(starter_kit_path / f, intent_dir / f)

        typer.secho(f"   -> âœ… Populated .intent/ from '{profile}' starter kit", fg=typer.colors.GREEN)

        # Dynamically update the project name in the new manifest
        manifest_path = intent_dir / "project_manifest.yaml"
        if manifest_path.exists():
            manifest_data = yaml.safe_load(manifest_path.read_text())
            manifest_data["name"] = name
            manifest_path.write_text(yaml.dump(manifest_data, indent=2))
            typer.secho(f"   -> âœ… Customized project name in manifest", fg=typer.colors.GREEN)

    log.info(f"\nðŸŽ‰ Scaffolding for '{name}' complete.")
    typer.secho("\nNext Steps:", bold=True)
    typer.echo(f"1. Navigate into your new project: `cd work/{name}`")
    typer.echo("2. Run `poetry install` to set up the environment.")
    typer.echo(f"3. From the CORE directory, run `core-admin byor-init work/{name}` to perform the first audit.")

def register(app: typer.Typer) -> None:
    """Intent: Register scaffolding commands under the admin CLI."""
    app.command("new")(new_project)

--- END OF FILE ./src/system/admin/scaffolder.py ---

--- START OF FILE ./src/system/admin/utils.py ---
# src/system/admin/utils.py
"""
Intent: Shared admin utilities used by CLI commands. These helpers are small,
documented, and domain-aligned so they can be safely referenced by the auditor.
"""

from __future__ import annotations

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict

import yaml
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import ed25519
from shared.config import settings
from shared.logger import getLogger

log = getLogger("core_admin")

# --- THIS IS THE NEW, SHARED FUNCTION ---
def should_fail(report: dict, fail_on: str) -> bool:
    """Determines if the CLI should exit with an error code based on the drift report and the specified fail condition (missing, undeclared, or any drift)."""
    """Determines if the CLI should exit with an error code based on the drift report."""
    if fail_on == "missing":
        return bool(report.get("missing_in_code"))
    if fail_on == "undeclared":
        return bool(report.get("undeclared_in_manifest"))
    # Default to 'any'
    return bool(report.get("missing_in_code") or report.get("undeclared_in_manifest") or report.get("mismatched_mappings"))
# --- END OF NEW FUNCTION ---

def load_yaml_file(path: Path) -> Dict[str, Any]:
    """Intent: Load YAML for governance operations. Returns {} for empty documents."""
    return yaml.safe_load(path.read_text(encoding="utf-8")) or {}


    """Save the given data as a YAML file at the specified path without sorting keys for readability."""
def save_yaml_file(path: Path, data: Dict[str, Any]) -> None:
    """Intent: Persist YAML with stable ordering disabled to preserve human readability."""
    path.write_text(yaml.safe_dump(data, sort_keys=False), encoding="utf-8")


def generate_approval_token(proposal_content: str) -> str:
    """Intent: Produce a deterministic content-bound token for cryptographic proposal approvals."""
    digest = hashes.Hash(hashes.SHA256())
    digest.update((proposal_content or "").encode("utf-8"))
    return f"core-proposal-v1:{digest.finalize().hex()}"


def load_private_key() -> ed25519.Ed25519PrivateKey:
    """Intent: Load the operator's Ed25519 private key from the protected key store."""
    key_path = settings.KEY_STORAGE_DIR / "private.key"
    if not key_path.exists():
        log.error("âŒ Private key not found. Please run 'core-admin keygen' to create one.")
        raise SystemExit(1)
    return serialization.load_pem_private_key(key_path.read_bytes(), password=None)

    """Persist a rollback plan snapshot for approved proposals under .intent/constitution/rollbacks/ as a timestamped JSON file."""

def archive_rollback_plan(proposal_name: str, proposal: Dict[str, Any]) -> None:
    """Intent: Persist a rollback plan snapshot for approved proposals under .intent/constitution/rollbacks/."""
    rollback_plan = proposal.get("rollback_plan")
    if not rollback_plan:
        return
    rollbacks_dir = settings.MIND / "constitution" / "rollbacks"
    rollbacks_dir.mkdir(parents=True, exist_ok=True)
    archive_path = rollbacks_dir / f"{datetime.utcnow().strftime('%Y%m%d%H%M%S')}-{proposal_name}.json"
    archive_path.write_text(
        json.dumps(
            {
                "proposal_name": proposal_name,
                "target_path": proposal.get("target_path"),
                "justification": proposal.get("justification"),
                "rollback_plan": rollback_plan,
            },
            indent=2,
        ),
        encoding="utf-8",
    )
    log.info(f"ðŸ“– Rollback plan archived to {archive_path}")
--- END OF FILE ./src/system/admin/utils.py ---

--- START OF FILE ./src/system/governance/checks/architecture_checks.py ---
# src/system/governance/checks/architecture_checks.py
"""
Auditor checks for higher-level architectural principles and smells,
forming the basis of CORE's "architectural conscience."
"""
from collections import defaultdict
from system.governance.models import AuditFinding, AuditSeverity

class ArchitectureChecks:
    """Container for architectural integrity checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        self.context = context

    # CAPABILITY: audit.check.duplication
    def check_for_structural_duplication(self) -> list[AuditFinding]:
        """Finds symbols with identical structural hashes, violating `dry_by_design`, using content-addressed knowledge graph for accurate duplication detection."""
        """
        Finds symbols with identical structural hashes, violating `dry_by_design`.
        This check uses the content-addressed nature of the knowledge graph to
        detect code duplication with perfect accuracy.
        """
        findings = []
        check_name = "Architectural Integrity: Code Duplication"
        
        hashes = defaultdict(list)
        for symbol in self.context.symbols_list:
            # We only care about functions and classes, not their methods for now.
            if symbol.get("structural_hash") and not symbol.get("parent_class_key"):
                hashes[symbol["structural_hash"]].append(symbol["key"])
        
        duplicates_found = False
        for structural_hash, keys in hashes.items():
            if len(keys) > 1:
                duplicates_found = True
                locations = ", ".join(f"'{key}'" for key in keys)
                message = (
                    f"Structural duplication detected. The following symbols are "
                    f"identical: {locations}. This may violate the 'dry_by_design' principle."
                )
                findings.append(AuditFinding(AuditSeverity.WARNING, message, check_name))
        
        # --- THIS IS THE FIX ---
        # We explicitly add a success message to the findings list if no duplicates are found.
        if not duplicates_found:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, "No structural code duplication found.", check_name))
            
        return findings
--- END OF FILE ./src/system/governance/checks/architecture_checks.py ---

--- START OF FILE ./src/system/governance/checks/environment_checks.py ---
# src/system/governance/checks/environment_checks.py
"""Auditor checks related to the system's runtime environment."""
import os
from system.governance.models import AuditFinding, AuditSeverity

class EnvironmentChecks:
    """Container for environment and runtime configuration checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        self.context = context

    # CAPABILITY: audit.check.environment
    def check_runtime_environment(self) -> list[AuditFinding]:
        """Verifies that required environment variables specified in runtime_requirements.yaml are set, returning a list of audit findings for missing variables or configuration issues."""
        """Verifies that required environment variables are set."""
        findings = []
        check_name = "Runtime Environment Validation"
        
        requirements_path = self.context.intent_dir / "config" / "runtime_requirements.yaml"
        if not requirements_path.exists():
            findings.append(AuditFinding(AuditSeverity.WARNING, "runtime_requirements.yaml not found; cannot validate environment.", check_name))
            return findings

        requirements = self.context.load_config(requirements_path, "yaml")
        required_vars = requirements.get("required_environment_variables", [])
        
        missing_vars = []
        for var in required_vars:
            if var.get("required") and not os.getenv(var.get("name")):
                missing_vars.append(var)

        if missing_vars:
            for var in missing_vars:
                msg = f"Required environment variable '{var.get('name')}' is not set. Description: {var.get('description')}"
                findings.append(AuditFinding(AuditSeverity.ERROR, msg, check_name))
        else:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, "All required environment variables are set.", check_name))
            
        return findings
--- END OF FILE ./src/system/governance/checks/environment_checks.py ---

--- START OF FILE ./src/system/governance/checks/file_checks.py ---
# src/system/governance/checks/file_checks.py
"""Auditor checks related to file existence, format, and structure."""

from pathlib import Path
from system.governance.models import AuditFinding, AuditSeverity
from core.validation_pipeline import validate_code

class FileChecks:
    """Container for file-based constitutional checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        self.context = context

    # CAPABILITY: audit.check.required_files
    def check_required_files(self) -> list[AuditFinding]:
        """Verifies that all files declared in meta.yaml exist on disk."""
        findings = []
        check_name = "Required Intent File Existence"
        
        required_files = self._get_known_files_from_meta()
        
        if not required_files:
            findings.append(AuditFinding(AuditSeverity.WARNING, "meta.yaml is empty or missing; cannot check for required files.", check_name))
            return findings

        missing_count = 0
        for file_rel_path in sorted(list(required_files)):
            full_path = self.context.repo_root / file_rel_path
            if not full_path.exists():
                missing_count += 1
                findings.append(AuditFinding(AuditSeverity.ERROR, f"Missing constitutionally-required file: '{file_rel_path}'", check_name))

        if missing_count == 0:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, f"All {len(required_files)} constitutionally-required files are present.", check_name))
            
        return findings

    # --- THIS IS THE FIX ---
    # Restore the missing capability tag.
    # CAPABILITY: audit.check.syntax
    def check_syntax(self) -> list[AuditFinding]:
        """Validates the syntax of all .intent YAML/JSON files (including proposals)."""
        findings = []
        check_name = "YAML/JSON Syntax Validity"
        error_findings = []

        files_to_check = list(self.context.intent_dir.rglob("*.yaml")) + list(self.context.intent_dir.rglob("*.json"))
        for file_path in files_to_check:
            if file_path.is_file():
                result = validate_code(str(file_path), file_path.read_text(encoding='utf-8'), quiet=True)
                if result["status"] == "dirty":
                    for violation in result["violations"]:
                        error_findings.append(AuditFinding(
                            AuditSeverity.ERROR,
                            f"Syntax Error: {violation['message']}",
                            check_name,
                            str(file_path.relative_to(self.context.repo_root))
                        ))

        if not error_findings:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, f"Validated syntax for {len(files_to_check)} YAML/JSON files.", check_name))
        findings.extend(error_findings)
        return findings

    # CAPABILITY: audit.check.orphaned_intent_files
    def check_for_orphaned_intent_files(self) -> list[AuditFinding]:
        """Finds .intent files that are not referenced in meta.yaml."""
        findings = []
        check_name = "Orphaned Intent Files"
        known_files = self._get_known_files_from_meta()
        if not known_files: return []

        ignore_patterns = [".bak", "proposals", ".example"]
        physical_files = {str(p.relative_to(self.context.repo_root)).replace("\\", "/") for p in self.context.intent_dir.rglob("*") if p.is_file() and not any(pat in str(p) for pat in ignore_patterns)}
        
        orphaned_files = sorted(list(physical_files - known_files))
        
        if orphaned_files:
            for orphan in orphaned_files:
                findings.append(AuditFinding(AuditSeverity.WARNING, f"Orphaned intent file: '{orphan}' is not a recognized system file.", check_name))
        else:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, "No orphaned or unrecognized intent files found.", check_name))
        return findings

    def _get_known_files_from_meta(self) -> set:
        """Builds a set of all known intent files by reading .intent/meta.yaml."""
        meta_file_path = self.context.intent_dir / "meta.yaml"
        if not meta_file_path.exists(): return set()

        meta_config = self.context.load_config(meta_file_path, "yaml")
        known_files = set()

        def _recursive_find_paths(data):
            """Recursively finds all file paths declared in the meta configuration."""
            if isinstance(data, dict):
                for value in data.values(): _recursive_find_paths(value)
            elif isinstance(data, list):
                for item in data: _recursive_find_paths(item)
            elif isinstance(data, str) and ('.' in data and '/' in data):
                full_path_str = str(Path(".intent") / data)
                known_files.add(full_path_str.replace("\\", "/"))

        _recursive_find_paths(meta_config)
        
        known_files.add(".intent/meta.yaml")
        known_files.add(".intent/project_manifest.yaml")
        known_files.add(".intent/knowledge/knowledge_graph.json")
        
        schema_dir = self.context.intent_dir / "schemas"
        if schema_dir.exists():
            for schema_file in schema_dir.glob("*.json"):
                known_files.add(str(schema_file.relative_to(self.context.repo_root)).replace("\\", "/"))
        
        return known_files

--- END OF FILE ./src/system/governance/checks/file_checks.py ---

--- START OF FILE ./src/system/governance/checks/__init__.py ---
[EMPTY FILE]

--- END OF FILE ./src/system/governance/checks/__init__.py ---

--- START OF FILE ./src/system/governance/checks/proposal_checks.py ---
# src/system/governance/checks/proposal_checks.py
"""Auditor checks for proposal formats and drift in .intent/proposals/."""

from __future__ import annotations

from pathlib import Path
import json
import hashlib
import yaml
import jsonschema

from system.governance.models import AuditFinding, AuditSeverity
from shared.schemas.manifest_validator import load_schema


class ProposalChecks:
    """Container for proposal-related constitutional checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context, setting `repo_root` and `proposals_dir` paths."""
        """Initializes the check with a shared auditor context."""
        self.context = context
        self.repo_root: Path = context.repo_root
        self.proposals_dir: Path = self.repo_root / ".intent" / "proposals"

    # --- helpers -------------------------------------------------------------

    def _proposal_paths(self) -> list[Path]:
        """Return all cr-* proposals (both YAML and JSON)."""
        if not self.proposals_dir.exists():
            return []
        return sorted(
            list(self.proposals_dir.glob("cr-*.yaml"))
            + list(self.proposals_dir.glob("cr-*.yml"))
            + list(self.proposals_dir.glob("cr-*.json"))
        )

    """Loads a proposal from a JSON or YAML file at the given path, returning an empty dict on parse failure or empty content."""
    def _load_proposal(self, path: Path) -> dict:
        """Load proposal preserving its format."""
        try:
            text = path.read_text(encoding="utf-8")
            if path.suffix.lower() == ".json":
                return json.loads(text) or {}
            return yaml.safe_load(text) or {}
        except Exception as e:  # surface upstream with path context
            raise ValueError(f"parse error: {e}") from e

    @staticmethod
    def _expected_token_for_content(content: str) -> str:
        """Mirror admin token format: 'core-proposal-v1:<sha256hex>'."""
        digest = hashlib.sha256(content.encode("utf-8")).hexdigest()
        return f"core-proposal-v1:{digest}"

    # --- checks --------------------------------------------------------------

    """Validate each cr-*.yaml/json proposal against proposal.schema.json, returning a list of AuditFindings for compliance or errors."""
    # CAPABILITY: audit.check.proposals_schema
    def check_proposal_files_match_schema(self) -> list[AuditFinding]:
        """Validate each cr-*.yaml/json proposal against proposal.schema.json."""
        findings: list[AuditFinding] = []
        check_name = "Proposals: Schema Compliance"

        paths = self._proposal_paths()
        if not paths:
            if not self.proposals_dir.exists():
                findings.append(
                    AuditFinding(
                        AuditSeverity.SUCCESS,
                        "No proposals directory found; nothing to validate.",
                        check_name,
                    )
                )
            else:
                findings.append(
                    AuditFinding(
                        AuditSeverity.SUCCESS,
                        "No pending proposals found.",
                        check_name,
                    )
                )
            return findings

        schema = load_schema("proposal.schema.json")
        validator = jsonschema.Draft7Validator(schema)

        for path in paths:
            rel = str(path.relative_to(self.repo_root))
            try:
                data = self._load_proposal(path)
            except ValueError as e:
                findings.append(
                    AuditFinding(
                        AuditSeverity.ERROR,
                        f"{path.name}: {e}",
                        check_name,
                        rel,
                    )
                )
                continue

            errors = list(validator.iter_errors(data))
            if errors:
                for err in errors:
                    loc = ".".join(str(p) for p in err.absolute_path) or "<root>"
                    findings.append(
                        AuditFinding(
                            AuditSeverity.ERROR,
                            f"{path.name}: {loc} -> {err.message}",
                            check_name,
                            rel,
                        )
                    )
            else:
                findings.append(
                    AuditFinding(
                        AuditSeverity.SUCCESS,
                        f"{path.name} conforms to proposal.schema.json",
                        check_name,
                        rel,
                    )
                )

        return findings

    # CAPABILITY: audit.check.proposals_drift
    def check_signatures_match_content(self) -> list[AuditFinding]:
        """
        Detect content/signature drift:
        - warn if a proposal has no signatures
        - warn if any signature token does not match the current content
        """
        findings: list[AuditFinding] = []
        check_name = "Proposals: Signature â†” Content Drift"

        for path in self._proposal_paths():
            rel = str(path.relative_to(self.repo_root))
            try:
                data = self._load_proposal(path)
            except ValueError as e:
                findings.append(
                    AuditFinding(
                        AuditSeverity.ERROR,
                        f"{path.name}: {e}",
                        check_name,
                        rel,
                    )
                )
                continue

            content = data.get("content", "")
            expected = self._expected_token_for_content(content)
            signatures = data.get("signatures", [])

            if not signatures:
                findings.append(
                    AuditFinding(
                        AuditSeverity.WARNING,
                        f"{path.name}: no signatures present.",
                        check_name,
                        rel,
                    )
                )
                continue

            mismatches = [
                s for s in signatures if s.get("token") != expected
            ]
            if mismatches:
                identities = ", ".join(s.get("identity", "<unknown>") for s in mismatches)
                findings.append(
                    AuditFinding(
                        AuditSeverity.WARNING,
                        f"{path.name}: {len(mismatches)} signature(s) do not match current content "
                        f"(likely edited after signing). Identities: {identities}",
                        check_name,
                        rel,
                    )
                )
            else:
                findings.append(
                    AuditFinding(
                        AuditSeverity.SUCCESS,
                        f"{path.name}: all signatures match current content.",
                        check_name,
                        rel,
                    )
                )

        if not findings and not self._proposal_paths():
            # nothing to report if there are no proposals
            return []

        return findings
        """Return a list of AuditFinding objects summarizing pending proposals, including their paths and severity."""

    # CAPABILITY: audit.check.proposals_list
    def list_pending_proposals(self) -> list[AuditFinding]:
        """Emit a friendly summary of pending proposals."""
        findings: list[AuditFinding] = []
        check_name = "Proposals: Pending Summary"

        paths = self._proposal_paths()
        if not paths:
            if self.proposals_dir.exists():
                findings.append(
                    AuditFinding(
                        AuditSeverity.SUCCESS,
                        "No pending proposals.",
                        check_name,
                    )
                )
            return findings

        for path in paths:
            findings.append(
                AuditFinding(
                    AuditSeverity.WARNING,  # warning to make it visible in audit output
                    f"Pending proposal: {path.name}",
                    check_name,
                    str(path.relative_to(self.repo_root)),
                )
            )
        return findings
--- END OF FILE ./src/system/governance/checks/proposal_checks.py ---

--- START OF FILE ./src/system/governance/checks/quality_checks.py ---
# src/system/governance/checks/quality_checks.py
"""Auditor checks related to code quality and conventions."""

from system.governance.models import AuditFinding, AuditSeverity

class QualityChecks:
    """Container for code quality constitutional checks."""
    
    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        self.context = context

    # CAPABILITY: audit.check.docstrings
    def check_docstrings_and_intents(self) -> list[AuditFinding]:
        """Finds symbols missing docstrings or having generic intents."""
        findings = []
        check_name = "Docstring & Intent Presence"
        warnings_found = False
        for entry in self.context.symbols_list:
            if entry.get("type") != "ClassDef" and not entry.get("docstring"):
                warnings_found = True
                findings.append(AuditFinding(AuditSeverity.WARNING, f"Missing Docstring in '{entry.get('file')}': Symbol '{entry.get('name')}'", check_name))
            if "Provides functionality for the" in entry.get("intent", ""):
                 warnings_found = True
                 findings.append(AuditFinding(AuditSeverity.WARNING, f"Generic Intent in '{entry.get('file')}': Symbol '{entry.get('name')}' has a weak intent statement.", check_name))
        if not warnings_found:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, "All symbols have docstrings and specific intents.", check_name))
        return findings

    # CAPABILITY: audit.check.dead_code
    def check_for_dead_code(self) -> list[AuditFinding]:
        """Detects unreferenced public symbols."""
        findings = []
        check_name = "Dead Code (Unreferenced Symbols)"
        all_called_symbols = set()
        for symbol in self.context.symbols_list:
            all_called_symbols.update(symbol.get("calls", []))
        
        warnings_found = False
        for symbol in self.context.symbols_list:
            name = symbol["name"]
            if name.startswith(('_', 'test_')): continue
            if name in all_called_symbols: continue
            if symbol.get("entry_point_type"): continue
            warnings_found = True
            findings.append(AuditFinding(AuditSeverity.WARNING, f"Potentially dead code: Symbol '{name}' in '{symbol['file']}' is unreferenced.", check_name))
            
        if not warnings_found:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, "No unreferenced public symbols found.", check_name))
        return findings
--- END OF FILE ./src/system/governance/checks/quality_checks.py ---

--- START OF FILE ./src/system/governance/checks/structure_checks.py ---
# src/system/governance/checks/structure_checks.py
"""Auditor checks related to the system's declared structure and relationships."""

from system.governance.models import AuditFinding, AuditSeverity
from shared.schemas.manifest_validator import validate_manifest_entry
from shared.utils.import_scanner import scan_imports_for_file

class StructureChecks:
    """Container for structural constitutional checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        self.context = context

    # CAPABILITY: audit.check.project_manifest
    def check_project_manifest(self) -> list[AuditFinding]:
        """Validates the integrity of project_manifest.yaml."""
        findings = []
        check_name = "Project Manifest Integrity"
        required_keys = ["name", "intent", "required_capabilities", "active_agents"]
        errors_found = False
        for key in required_keys:
            if key not in self.context.project_manifest:
                errors_found = True
                findings.append(AuditFinding(AuditSeverity.ERROR, f"project_manifest.yaml missing required key: '{key}'", check_name))
        if not errors_found:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, "project_manifest.yaml contains all required keys.", check_name))
        return findings

    # CAPABILITY: audit.check.capability_coverage
    def check_capability_coverage(self) -> list[AuditFinding]:
        """Ensures all required capabilities are implemented."""
        findings = []
        check_name = "Capability Coverage"
        required_caps = set(self.context.project_manifest.get("required_capabilities", []))
        implemented_caps = {f.get("capability") for f in self.context.symbols_list if f.get("capability") != "unassigned"}
        missing = sorted(list(required_caps - implemented_caps))
        
        for cap in missing:
            findings.append(AuditFinding(AuditSeverity.ERROR, f"Missing capability implementation for: {cap}", check_name))
        
        if not missing:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, "All required capabilities are implemented.", check_name))
        return findings

    # CAPABILITY: audit.check.capability_definitions
    def check_capability_definitions(self) -> list[AuditFinding]:
        """Ensures all implemented capabilities are valid."""
        findings = []
        check_name = "Capability Definitions"
        capability_tags_path = self.context.intent_dir / "knowledge" / "capability_tags.yaml"
        defined_tags_data = self.context.load_config(capability_tags_path, "yaml")
        defined_tags = {tag['name'] for tag in defined_tags_data.get('tags', [])}
        
        implemented_caps = {f.get("capability") for f in self.context.symbols_list if f.get("capability") != "unassigned"}
        
        undefined = sorted(list(implemented_caps - defined_tags))
        for cap in undefined:
            findings.append(AuditFinding(AuditSeverity.ERROR, f"Unconstitutional capability: '{cap}' is implemented in the code but not defined in capability_tags.yaml.", check_name))

        if not undefined:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, "All implemented capabilities are constitutionally defined.", check_name))
        return findings

    # CAPABILITY: audit.check.knowledge_graph_schema
    def check_knowledge_graph_schema(self) -> list[AuditFinding]:
        """Validates all knowledge graph symbols against the schema."""
        findings = []
        check_name = "Knowledge Graph Schema Compliance"
        error_count = 0
        for key, entry in self.context.symbols_map.items():
            is_valid, validation_errors = validate_manifest_entry(entry, "knowledge_graph_entry.schema.json")
            if not is_valid:
                error_count += 1
                for err in validation_errors:
                    findings.append(AuditFinding(AuditSeverity.ERROR, f"Knowledge Graph entry '{key}' schema error: {err}", check_name))
        if error_count == 0:
            findings.append(AuditFinding(AuditSeverity.SUCCESS, f"All {len(self.context.symbols_map)} symbols in knowledge graph pass schema validation.", check_name))
        return findings
        
    # --- THIS IS THE FIX ---
    # Restore the missing capability tag.
    # CAPABILITY: audit.check.domain_integrity
    def check_domain_integrity(self) -> list[AuditFinding]:
        """Checks for domain mismatches and illegal imports."""
        findings = []
        check_name = "Domain Integrity (Location & Imports)"
        errors_found = False
        for entry in self.context.symbols_list:
            file_path = self.context.repo_root / entry.get("file", "")
            if not file_path.exists():
                findings.append(AuditFinding(AuditSeverity.WARNING, f"File '{entry.get('file')}' from knowledge graph not found on disk.", check_name))
                continue
            declared_domain = entry.get("domain")
            actual_domain = self.context.intent_model.resolve_domain_for_path(file_path.relative_to(self.context.repo_root))
            if declared_domain != actual_domain:
                errors_found = True
                findings.append(AuditFinding(AuditSeverity.ERROR, f"Domain Mismatch for '{entry.get('key')}': Declared='{declared_domain}', Actual='{actual_domain}'", check_name))
            
            allowed = set(self.context.intent_model.get_domain_permissions(actual_domain)) | {actual_domain}
            imports = scan_imports_for_file(file_path)
            for imp in imports:
                if imp.startswith("src."): imp = imp[4:]
                if imp.startswith(("core.", "shared.", "system.", "agents.")):
                    imp_path_parts = imp.split('.')
                    potential_path = self.context.src_dir.joinpath(*imp_path_parts)
                    check_path = potential_path.with_suffix(".py")
                    if not check_path.exists(): check_path = potential_path 
                    imp_domain = self.context.intent_model.resolve_domain_for_path(check_path)
                    if imp_domain and imp_domain not in allowed:
                         errors_found = True
                         findings.append(AuditFinding(AuditSeverity.ERROR, f"Forbidden Import in '{entry.get('file')}': Domain '{actual_domain}' cannot import '{imp}' from forbidden domain '{imp_domain}'", check_name))
        if not errors_found:
             findings.append(AuditFinding(AuditSeverity.SUCCESS, "Domain locations and import boundaries are valid.", check_name))
        return findings

--- END OF FILE ./src/system/governance/checks/structure_checks.py ---

--- START OF FILE ./src/system/governance/constitutional_auditor.py ---
# src/system/governance/constitutional_auditor.py
"""
CORE Constitutional Auditor Orchestrator
=======================================
Discovers and runs modular checks to validate the system's integrity.
"""
import sys
import io
import inspect
import importlib
from dotenv import load_dotenv
from pathlib import Path
from typing import List, Optional, Callable, Tuple

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from shared.path_utils import get_repo_root
from shared.config_loader import load_config
from core.intent_model import IntentModel
from shared.logger import getLogger
from system.governance.models import AuditFinding, AuditSeverity
# --- THIS IS THE CHANGE (Part 1 of 2) ---
# We now import our new aggregator tool.
from shared.utils.manifest_aggregator import aggregate_manifests


log = getLogger(__name__)

# CAPABILITY: introspection
# CAPABILITY: alignment_checking
class ConstitutionalAuditor:
    """Orchestrates the discovery and execution of all constitutional checks."""
    
    class _LoggingBridge(io.StringIO):
        """A file-like object that redirects writes to the logger."""
        def write(self, s: str):
            """Redirects the write to the logger info stream."""
            cleaned_s = s.strip()
            if cleaned_s:
                log.info(cleaned_s)

    def __init__(self, repo_root_override: Optional[Path] = None):
        """
        Initializes the auditor, loading all necessary configuration and knowledge files.
        
        Args:
            repo_root_override (Optional[Path]): 
                If provided, the auditor will run against this directory as its root.
                This is crucial for the 'canary' validation process.
        """
        self.repo_root = repo_root_override or get_repo_root()

        if repo_root_override:
            dotenv_path = self.repo_root / ".env"
            if dotenv_path.exists():
                load_dotenv(dotenv_path=dotenv_path, override=True)
                log.info(f"   -> Canary auditor loaded environment from {dotenv_path}")
        
        # Create a shared context for all checks
        self.context = self.AuditorContext(self.repo_root)

        self.console = Console(file=self._LoggingBridge(), force_terminal=True, color_system="auto")
        self.findings: List[AuditFinding] = []
        self.checks: List[Tuple[str, Callable]] = self._discover_checks()

    class AuditorContext:
        """A simple container for shared state that all checks can access."""
        def __init__(self, repo_root):
            """Initializes the shared context for all audit checks."""
            self.repo_root = repo_root
            self.intent_dir = self.repo_root / ".intent"
            self.src_dir = self.repo_root / "src"
            self.intent_model = IntentModel(self.repo_root)
            
            # --- THIS IS THE CHANGE (Part 2 of 2) ---
            # Instead of loading the monolithic manifest, we now call our aggregator.
            # This makes the Auditor aware of the new modular system.
            self.project_manifest = aggregate_manifests(self.repo_root)
            
            self.knowledge_graph = load_config(self.intent_dir / "knowledge/knowledge_graph.json", "json")
            self.symbols_map = self.knowledge_graph.get("symbols", {})
            self.symbols_list = list(self.symbols_map.values())
            self.load_config = load_config

    def _discover_checks(self) -> List[Tuple[str, Callable]]:
        """Dynamically discovers check methods from modules in the 'checks' directory."""
        discovered_checks = []
        checks_dir = Path(__file__).parent / "checks"
        
        for check_file in checks_dir.glob("*.py"):
            if check_file.name.startswith("__"): continue
            
            module_name = f"system.governance.checks.{check_file.stem}"
            try:
                module = importlib.import_module(module_name)
                for class_name, Class in inspect.getmembers(module, inspect.isclass):
                    if not class_name.endswith("Checks"):
                        continue

                    check_instance = Class(self.context)
                    for method_name, method in inspect.getmembers(check_instance, inspect.ismethod):
                        if method_name.startswith("_"): continue
                        
                        symbol_key = f"src/system/governance/checks/{check_file.name}::{method_name}"
                        symbol_data = self.context.symbols_map.get(symbol_key, {})
                        if symbol_data.get("capability", "").startswith("audit.check."):
                            check_name = symbol_data.get("intent", method_name)
                            discovered_checks.append((check_name, method))
            except ImportError as e:
                log.error(f"Failed to import check module {module_name}: {e}")

        log.debug(f"Discovered {len(discovered_checks)} audit checks.")
        discovered_checks.sort(key=lambda item: item[0] != "Ensures all implemented capabilities are valid.")
        return discovered_checks

    def run_full_audit(self) -> bool:
        """Run all discovered validation phases and return overall status."""
        self.console.print(Panel("ðŸ§  CORE Constitutional Integrity Audit", style="bold blue", expand=False))
        
        for name, check_fn in self.checks:
            log.info(f"ðŸ” [bold]Running Check:[/bold] {name}")
            try:
                findings = check_fn()
                if findings:
                    self.findings.extend(findings)
                    for finding in findings:
                        if finding.severity == AuditSeverity.ERROR: log.error(f"âŒ {finding.message}")
                        elif finding.severity == AuditSeverity.WARNING: log.warning(f"âš ï¸ {finding.message}")
                        elif finding.severity == AuditSeverity.SUCCESS: log.info(f"âœ… {finding.message}")
            except Exception as e:
                log.error(f"ðŸ’¥ Check '{name}' failed with an unexpected error: {e}", exc_info=True)
                self.findings.append(AuditFinding(AuditSeverity.ERROR, f"Check failed: {e}", name))
        
        all_passed = not any(f.severity == AuditSeverity.ERROR for f in self.findings)
        self._report_final_status(all_passed)
        return all_passed

    def _report_final_status(self, passed: bool):
        """Prints the final audit summary to the console."""
        errors = [f for f in self.findings if f.severity == AuditSeverity.ERROR]
        warnings = [f for f in self.findings if f.severity == AuditSeverity.WARNING]

        if passed:
            msg = f"âœ… ALL CHECKS PASSED ({len(warnings)} warnings)"
            self.console.print(Panel(msg, style="bold green", expand=False))
        else:
            msg = f"âŒ AUDIT FAILED: {len(errors)} error(s) and {len(warnings)} warning(s) found."
            self.console.print(Panel(msg, style="bold red", expand=False))
        if errors:
            error_table = Table("ðŸš¨ Critical Errors", style="red", show_header=True, header_style="bold red")
            for err in errors: error_table.add_row(err.message)
            self.console.print(error_table)
        if warnings:
            warning_table = Table("âš ï¸ Warnings", style="yellow", show_header=True, header_style="bold yellow")
            for warn in warnings: warning_table.add_row(warn.message)
            self.console.print(warning_table)

def main():
    """CLI entry point for the Constitutional Auditor."""
    load_dotenv()
    auditor = ConstitutionalAuditor()
    try:
        success = auditor.run_full_audit()
        sys.exit(0 if success else 1)
    except FileNotFoundError as e:
        log.error(f"A required file was not found: {e}. Try running the introspection cycle.", exc_info=True)
        sys.exit(1)
    except Exception as e:
        log.error(f"An unexpected error occurred during the audit: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()

--- END OF FILE ./src/system/governance/constitutional_auditor.py ---

--- START OF FILE ./src/system/governance/__init__.py ---
[EMPTY FILE]

--- END OF FILE ./src/system/governance/__init__.py ---

--- START OF FILE ./src/system/governance/models.py ---
# src/system/governance/models.py
"""
Data models for the Constitutional Auditor.
"""
from dataclasses import dataclass
from enum import Enum
from typing import Optional

class AuditSeverity(Enum):
    """Severity levels for audit findings."""
    ERROR = "error"
    WARNING = "warning"
    SUCCESS = "success"

@dataclass
class AuditFinding:
    """Represents a single audit finding."""
    severity: AuditSeverity
    message: str
    check_name: str
    file_path: Optional[str] = None
--- END OF FILE ./src/system/governance/models.py ---

--- START OF FILE ./src/system/guard/drift_detector.py ---
"""
Intent: Provide an intent-aligned capability drift detector that compares the .intent
manifest to capabilities discovered in code, producing a machine-readable report
for governance. This module avoids static imports from forbidden domains.

Strict mode behavior:
- Prefer live KnowledgeGraphBuilder (KGB) if available.
- If KGB yields nothing, also look for an on-disk knowledge graph artifact.
- Only if both are absent will strict mode fail.
"""
from __future__ import annotations
import importlib
import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional
try:
    import yaml
except Exception:
    yaml = None
_CAPABILITY_RE = re.compile('^\\s*#\\s*CAPABILITY:\\s*([A-Za-z0-9_.\\-:/]+)(.*)$')
_INLINE_KV_RE = re.compile('\\[\\s*([^\\]]+)\\s*\\]')
_KV_PAIR_RE = re.compile('([A-Za-z0-9_.\\-:/]+)\\s*=\\s*([^\\s,;]+)')

@dataclass(frozen=True)
class CapabilityMeta:
    """Intent: Minimal data container for capability metadata used in drift comparison."""
    capability: str
    domain: Optional[str] = None
    owner: Optional[str] = None

def _parse_inline_meta(trailing: str) -> Dict[str, str]:
    """Parse inline [key=value] metadata from trailing text and return as a dictionary."""
    """Intent: Parse inline [key=value] metadata next to CAPABILITY tags."""
    m = _INLINE_KV_RE.search(trailing or '')
    if not m:
        return {}
    body = m.group(1)
    return {k: v for k, v in _KV_PAIR_RE.findall(body)}

def _try_import_kgb():
    """Intent: Attempt to access KnowledgeGraphBuilder without static cross-domain imports."""
    try:
        mod = importlib.import_module('system.tools.codegraph_builder')
        return getattr(mod, 'KnowledgeGraphBuilder', None)
    except Exception:
        return None

def _find_manifest(start: Path) -> Path:
    """Intent: Locate the authoritative .intent manifest file."""
    candidates = [start / '.intent' / 'project_manifest.yaml', start / '.intent' / 'manifest.yaml']
    for p in candidates:
        if p.exists():
            return p
    raise FileNotFoundError('No manifest found in .intent/ (looked for project_manifest.yaml, manifest.yaml)')

    """Normalize various input shapes (list[str], list[dict], dict[str,dict]) into a dictionary of {capability_name: CapabilityMeta} objects."""
def _normalize_cap_list(items: Any) -> Dict[str, CapabilityMeta]:
    """Intent: Normalize many shapes (list[str], list[dict], dict[str,dict]) into {cap: CapabilityMeta}."""
    out: Dict[str, CapabilityMeta] = {}
    if isinstance(items, dict):
        for cap, meta in items.items():
            if not isinstance(cap, str):
                continue
            if isinstance(meta, dict):
                out[cap] = CapabilityMeta(capability=cap, domain=meta.get('domain'), owner=meta.get('owner'))
            else:
                out[cap] = CapabilityMeta(capability=cap)
        return out
    if isinstance(items, list):
        for it in items:
            if isinstance(it, str):
                out[it] = CapabilityMeta(it)
            elif isinstance(it, dict):
                cap = it.get('id') or it.get('key') or it.get('name') or it.get('capability')
                if cap:
                    out[cap] = CapabilityMeta(capability=cap, domain=it.get('domain'), owner=it.get('owner'))
    return out

def _normalize_manifest_caps(raw: dict) -> Dict[str, CapabilityMeta]:
    """
    Intent: Normalize different manifest shapes into a {capability: CapabilityMeta} map.
    Looks for keys commonly used in CORE manifests:
      - capabilities
      - required_capabilities
      - expected_capabilities
      - capability_map / capability_registry
      - components[*].capabilities
    """
    from collections import deque

    def extract_from_node(node: Any) -> Optional[Dict[str, CapabilityMeta]]:
        """Extract capabilities from a node by checking common capability-related keys or nested components/modules/services."""
        if not isinstance(node, dict):
            return None
        for key in ('capabilities', 'required_capabilities', 'expected_capabilities', 'capability_map', 'capability_registry'):
            if key in node:
                got = _normalize_cap_list(node[key])
                if got:
                    return got
        comps = node.get('components') or node.get('modules') or node.get('services')
        if isinstance(comps, list):
            merged: Dict[str, CapabilityMeta] = {}
            for c in comps:
                if isinstance(c, dict) and 'capabilities' in c:
                    merged.update(_normalize_cap_list(c['capabilities']))
            if merged:
                return merged
        return None
    q = deque([raw])
    while q:
        node = q.popleft()
        got = extract_from_node(node)
        if got:
            return got
        if isinstance(node, dict):
            q.extend(node.values())
        elif isinstance(node, list):
            q.extend(node)
    return {}
    """Loads and parses a YAML manifest file, normalizes its capabilities, and returns a dictionary mapping capability names to their metadata."""

def load_manifest(root: Path, explicit_path: Optional[Path]=None) -> Dict[str, CapabilityMeta]:
    """Intent: Load and parse the .intent manifest with PyYAML."""
    if yaml is None:
        raise RuntimeError('PyYAML is required. Install with `pip install pyyaml`.')
    path = explicit_path if explicit_path else _find_manifest(root)
    with path.open('r', encoding='utf-8') as f:
        data = yaml.safe_load(f) or {}
    return _normalize_manifest_caps(data)

def _extract_cap_meta_from_node(node: Dict[str, Any]) -> Optional[CapabilityMeta]:
    """Intent: Extract capability/domain/owner from a variety of KG node shapes."""
    cap = node.get('capability')
    domain = node.get('domain')
    owner = node.get('owner')
    if cap is None:
        attrs = node.get('attrs') or node.get('meta') or {}
        if isinstance(attrs, dict):
            cap = attrs.get('capability') or attrs.get('CAPABILITY') or cap
            domain = attrs.get('domain', domain)
            owner = attrs.get('owner', owner)
    if cap is None:
        tags = node.get('tags') or node.get('labels')
        if isinstance(tags, list):
            for t in tags:
                if isinstance(t, str):
                    m = re.match('^\\s*CAPABILITY:\\s*([A-Za-z0-9_.\\-:/]+)\\s*$', t)
                    if m:
                        cap = m.group(1)
                        break
    if cap:
        return CapabilityMeta(str(cap), str(domain) if domain else None, str(owner) if owner else None)
    return None

def _load_json_file(path: Path) -> Optional[Any]:
    """Loads and parses a JSON file from the given path, returning its contents on success or None on failure."""
    try:
        return json.loads(path.read_text(encoding='utf-8'))
    except Exception:
        return None

def _load_ndjson_file(path: Path) -> List[Any]:
    """Loads and parses each non-empty line of an NDJSON file into a list of objects, skipping invalid lines and returning an empty list on file read errors."""
    items: List[Any] = []
    try:
        for line in path.read_text(encoding='utf-8').splitlines():
            line = line.strip()
            if not line:
                continue
            try:
                items.append(json.loads(line))
            except Exception:
                continue
    except Exception:
        return []
    return items

def _collect_from_kgb_artifact(root: Path) -> Dict[str, CapabilityMeta]:
    """
    Intent: Read a previously emitted knowledge graph artifact from disk.
    Accepts common file names and shapes, returning a capability map.
    """
    candidates = [root / 'reports' / 'knowledge_graph.json', root / 'reports' / 'codegraph.json', root / 'reports' / 'knowledge_graph.ndjson', root / 'artifacts' / 'knowledge_graph.json', root / '.intent' / 'knowledge_graph.json']
    caps: Dict[str, CapabilityMeta] = {}
    for p in candidates:
        if not p.exists():
            continue
        data: Any
        if p.suffix == '.ndjson':
            data = _load_ndjson_file(p)
        else:
            data = _load_json_file(p)
        if data is None:
            continue
        nodes_iter: Iterable[Any] = []
        if isinstance(data, dict) and 'nodes' in data:
            nodes = data.get('nodes')
            if isinstance(nodes, dict):
                nodes_iter = nodes.values()
            elif isinstance(nodes, list):
                nodes_iter = nodes
        elif isinstance(data, list):
            nodes_iter = data
        else:
            continue
        for n in nodes_iter:
            if not isinstance(n, dict):
                continue
            meta = _extract_cap_meta_from_node(n)
            if meta:
                caps[meta.capability] = meta
        if caps:
            break
    return caps

def _iter_source_files(root: Path, include_globs: List[str], exclude_globs: List[str]) -> Iterable[Path]:
    """Intent: Yield repository files considered for direct CAPABILITY tag scanning."""
    all_files = list(root.rglob('*'))

    def wanted(p: Path) -> bool:
        """Intent: Filter for include/exclude globs and typical source suffixes."""
        if any((p.match(g) for g in exclude_globs)):
            return False
        if include_globs:
            return any((p.match(g) for g in include_globs))
        return p.suffix in {'.py', '.ts', '.tsx', '.js'}
    for p in all_files:
        if p.is_file() and wanted(p):
            yield p

def _collect_from_grep(root: Path, include_globs: List[str], exclude_globs: List[str]) -> Dict[str, CapabilityMeta]:
    """Intent: Fallback discovery by scanning for '# CAPABILITY:' tags with optional inline metadata."""
    caps: Dict[str, CapabilityMeta] = {}
    for file in _iter_source_files(root, include_globs, exclude_globs):
        try:
            for line in file.read_text(encoding='utf-8', errors='ignore').splitlines():
                m = _CAPABILITY_RE.match(line)
                if not m:
                    continue
                cap = m.group(1).strip()
                kv = {}
                trailing = m.group(2) or ''
                if trailing:
                    kv = _parse_inline_meta(trailing)
                caps[cap] = CapabilityMeta(capability=cap, domain=kv.get('domain'), owner=kv.get('owner'))
        except Exception:
            continue
    return caps

def _collect_from_kgb(root: Path) -> Dict[str, CapabilityMeta]:
    """Intent: Use KnowledgeGraphBuilder (if present) to discover capabilities from the repo."""
    KGB = _try_import_kgb()
    if not KGB:
        return {}
    try:
        builder = KGB(root=str(root))
        graph = builder.build()
        caps: Dict[str, CapabilityMeta] = {}
        if hasattr(graph, 'nodes'):
            try:
                for node in getattr(graph, 'nodes'):
                    attrs = {}
                    try:
                        attrs = graph.nodes[node]
                    except Exception:
                        pass
                    meta = _extract_cap_meta_from_node(attrs if isinstance(attrs, dict) else {})
                    if meta:
                        caps[meta.capability] = meta
                return caps
            except Exception:
                pass
        if isinstance(graph, dict):
            nodes = graph.get('nodes')
            if isinstance(nodes, dict):
                iterable = nodes.values()
            elif isinstance(nodes, list):
                iterable = nodes
            else:
                iterable = []
            for n in iterable:
                if isinstance(n, dict):
                    meta = _extract_cap_meta_from_node(n)
                    if meta:
                        caps[meta.capability] = meta
            return caps
        if isinstance(graph, list):
            for n in graph:
                if isinstance(n, dict):
                    meta = _extract_cap_meta_from_node(n)
                    if meta:
                        caps[meta.capability] = meta
            return caps
        return caps
    except Exception:
        return {}

def collect_code_capabilities(root: Path, include_globs: Optional[List[str]]=None, exclude_globs: Optional[List[str]]=None, require_kgb: bool=False) -> Dict[str, CapabilityMeta]:
    """Intent: Unified discovery entrypoint respecting strict-intent when required."""
    include_globs = include_globs or []
    exclude_globs = exclude_globs or ['**/.git/**', '**/.venv/**', '**/__pycache__/**', '**/.pytest_cache/**', '**/.ruff_cache/**', 'logs/**', 'sandbox/**', 'pending_writes/**']
    caps = _collect_from_kgb(root)
    if caps:
        return caps
    artifact_caps = _collect_from_kgb_artifact(root)
    if artifact_caps:
        return artifact_caps
    if require_kgb:
        raise RuntimeError('Strict intent mode: No capabilities found from KnowledgeGraphBuilder or artifacts. Run `core-admin guard kg-export` first (or rerun without --strict-intent).')
    return _collect_from_grep(root, include_globs, exclude_globs)

@dataclass
class DriftReport:
    """Intent: Structured result for capability drift suitable for JSON emission and CI gating."""
    missing_in_code: List[str]
    undeclared_in_manifest: List[str]
    mismatched_mappings: List[Dict[str, Dict[str, Optional[str]]]]

    def to_dict(self) -> dict:
        """Intent: Convert the drift report into a stable JSON-serializable dict."""
        return {'missing_in_code': sorted(self.missing_in_code), 'undeclared_in_manifest': sorted(self.undeclared_in_manifest), 'mismatched_mappings': self.mismatched_mappings}

def detect_capability_drift(manifest_caps: Dict[str, CapabilityMeta], code_caps: Dict[str, CapabilityMeta]) -> DriftReport:
    """Intent: Compute missing/undeclared/mismatched capability sets between manifest and code."""
    m_keys = set(manifest_caps.keys())
    c_keys = set(code_caps.keys())
    missing = sorted(m_keys - c_keys)
    undeclared = sorted(c_keys - m_keys)
    mismatches: List[Dict[str, Dict[str, Optional[str]]]] = []
    for k in sorted(m_keys & c_keys):
        m = manifest_caps[k]
        c = code_caps[k]
        if (m.domain or c.domain) and m.domain != c.domain or ((m.owner or c.owner) and m.owner != c.owner):
            mismatches.append({'capability': k, 'manifest': {'domain': m.domain, 'owner': m.owner}, 'code': {'domain': c.domain, 'owner': c.owner}})
    return DriftReport(missing, undeclared, mismatches)

def write_report(report_path: Path, report: DriftReport) -> None:
    """Intent: Persist the drift report to disk under reports/ for evidence and CI."""
    report_path.parent.mkdir(parents=True, exist_ok=True)
    report_path.write_text(json.dumps(report.to_dict(), indent=2), encoding='utf-8')
--- END OF FILE ./src/system/guard/drift_detector.py ---

--- START OF FILE ./src/system/manifest.yaml ---
capabilities:
- alignment_checking
- audit.check.required_files
- audit.check.syntax
- audit.check.project_manifest
- audit.check.capability_coverage
- audit.check.capability_definitions
- audit.check.knowledge_graph_schema
- audit.check.domain_integrity
- audit.check.docstrings
- audit.check.dead_code
- audit.check.orphaned_intent_files
- audit.check.environment
- audit.check.proposals_schema
- audit.check.proposals_drift
- audit.check.proposals_list
description: Governance tooling, lifecycle setup, CLI utilities
domain: system

--- END OF FILE ./src/system/manifest.yaml ---

--- START OF FILE ./src/system/starter_kits/default/gitignore.template ---
# Python
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.so
.venv/
.env

# CORE-specific artifacts
.intent/knowledge/knowledge_graph.json
reports/
logs/
sandbox/
pending_writes/

# IDE / Editor
.vscode/
.idea/

--- END OF FILE ./src/system/starter_kits/default/gitignore.template ---

--- START OF FILE ./src/system/starter_kits/default/.gitkeep ---
[EMPTY FILE]

--- END OF FILE ./src/system/starter_kits/default/.gitkeep ---

--- START OF FILE ./src/system/starter_kits/default/principles.yaml ---
# .intent/mission/principles.yaml
principles:
  - id: clarity_first
    description: >
      Every function must have a docstring explaining its purpose.
      If a human cannot understand a piece of code in 30 seconds, it must be simplified.
  - id: safe_by_default
    description: >
      Every change must assume rollback or rejection unless explicitly validated.
      No file write or code execution may proceed without confirmation or a safety check.
  - id: no_orphaned_logic
    description: >
      All code must be discoverable and auditable. No function or file should exist
      without being traceable to a manifest or a clear purpose.
  - id: single_source_of_truth
    description: >
      The `.intent/` directory is the single source of truth for the project's
      capabilities, structure, and intent. All governance is derived from these files.

--- END OF FILE ./src/system/starter_kits/default/principles.yaml ---

--- START OF FILE ./src/system/starter_kits/default/project_manifest.yaml ---
# .intent/project_manifest.yaml
name: "new-core-project"
version: "0.1.0"
intent: "A new project, ready to be guided by a clear intent."
required_capabilities: []

--- END OF FILE ./src/system/starter_kits/default/project_manifest.yaml ---

--- START OF FILE ./src/system/starter_kits/default/pyproject.toml.template ---
[tool.poetry]
name = "{project_name}"
version = "0.1.0"
description = "A new project governed by CORE."
authors = ["Your Name <you@example.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.9"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

--- END OF FILE ./src/system/starter_kits/default/pyproject.toml.template ---

--- START OF FILE ./src/system/starter_kits/default/README.md ---
# Welcome to Your New CORE Constitution

This `.intent/` directory is the "Mind" of your new application. It contains the complete, machine-readable definition of your project's goals, rules, and structure. It is the single source of truth that governs the behavior of any AI agents working on this codebase.

This starter kit was generated by CORE to provide a balanced, best-practice foundation for your project.

## Your First Steps

1.  **Review `mission/principles.yaml`**: These are the high-level values of your project. You should edit them to match your own philosophy.
2.  **Review `project_manifest.yaml`**: This file lists the capabilities your application is expected to have. As you add `# CAPABILITY:` tags to your code, this list should grow.
3.  **Run Your First Audit**: Use CORE's tools to run a constitutional audit. This will tell you if your code is in alignment with the rules defined here.

This constitution is now yours. Evolve it, amend it, and use it to guide your project's growth with clarity and purpose.

--- END OF FILE ./src/system/starter_kits/default/README.md ---

--- START OF FILE ./src/system/starter_kits/default/README.md.template ---
# {project_name}

A new project governed by CORE.

--- END OF FILE ./src/system/starter_kits/default/README.md.template ---

--- START OF FILE ./src/system/starter_kits/default/safety_policies.yaml ---
# .intent/policies/safety_policies.yaml
rules:
  - id: no_dangerous_execution
    description: >
      Generated or modified code must not contain calls to dangerous functions
      that enable arbitrary code execution or shell access.
    enforcement: hard
    detection:
      type: substring
      patterns:
        - "eval("
        - "exec("
        - "os.system("
        - "subprocess.run("
        - "subprocess.Popen("
    action: reject
    feedback: "âŒ Dangerous execution call detected. This is forbidden by safety policies."
  - id: no_unsafe_imports
    description: >
      Prevent importing modules that enable dangerous operations unless explicitly allowed.
    enforcement: hard
    detection:
      type: import_name
      forbidden:
        - "import pickle"
        - "from subprocess import"
    action: reject
    feedback: "âŒ Unsafe import detected. This is forbidden by safety policies."

--- END OF FILE ./src/system/starter_kits/default/safety_policies.yaml ---

--- START OF FILE ./src/system/starter_kits/default/source_structure.yaml ---
# .intent/knowledge/source_structure.yaml
structure:
  - domain: main
    path: src/main
    description: "The primary domain for this application's core logic."
    allowed_imports: [main, shared]
  - domain: shared
    path: src/shared
    description: "Shared utilities and data models."
    allowed_imports: [shared]

--- END OF FILE ./src/system/starter_kits/default/source_structure.yaml ---

--- START OF FILE ./src/system/templates/capability_tags.yaml.template ---
# .intent/knowledge/capability_tags.yaml
#
# This is the canonical dictionary of all valid capability tags for this project.
# The CORE ConstitutionalAuditor will verify that any # CAPABILITY tag used in
# the source code is defined in this file.
#
# This file was seeded by CORE's analysis of your repository. You should add
# a 'description' for each capability to clarify its purpose.

tags:
  # - name: your_capability_1
  #   description: "A clear explanation of what this capability does."
  # - name: your_capability_2
  #   description: "Another clear explanation."

--- END OF FILE ./src/system/templates/capability_tags.yaml.template ---

--- START OF FILE ./src/system/templates/principles.yaml.template ---
# .intent/mission/principles.yaml
#
# This is a starter set of principles for your new CORE-governed project.
# These principles guide the behavior of any AI agents operating on this codebase.
# You should review, edit, and expand these to match your project's specific goals.

principles:

  - id: clarity_first
    description: >
      Every function must have a docstring explaining its purpose.
      If a human cannot understand a piece of code in 30 seconds, it must be simplified.

  - id: safe_by_default
    description: >
      Every change must assume rollback or rejection unless explicitly validated.
      No file write or code execution may proceed without confirmation or a safety check.

  - id: no_orphaned_logic
    description: >
      All code must be discoverable and auditable. No function or file should exist
      without being traceable to a manifest or a clear purpose.

  - id: single_source_of_truth
    description: >
      The `.intent/` directory is the single source of truth for the project's
      capabilities, structure, and intent. All governance is derived from these files.

--- END OF FILE ./src/system/templates/principles.yaml.template ---

--- START OF FILE ./src/system/templates/safety_policies.yaml.template ---
# .intent/policies/safety_policies.yaml
#
# A starter set of safety policies for your CORE-governed project.
# These rules prevent AI agents from generating or executing dangerous code.

rules:
  - id: no_dangerous_execution
    description: >
      Generated or modified code must not contain calls to dangerous functions
      that enable arbitrary code execution or shell access.
    enforcement: hard
    detection:
      type: substring
      patterns:
        - "eval("
        - "exec("
        - "os.system("
        - "subprocess.run("
        - "subprocess.Popen("
    action: reject
    feedback: "âŒ Dangerous execution call detected. This is forbidden by safety policies."

  - id: no_unsafe_imports
    description: >
      Prevent importing modules that enable dangerous operations unless explicitly allowed.
    enforcement: hard
    detection:
      type: import_name
      forbidden:
        - "import pickle"
        - "from subprocess import"
    action: reject
    feedback: "âŒ Unsafe import detected. This is forbidden by safety policies."

--- END OF FILE ./src/system/templates/safety_policies.yaml.template ---

--- START OF FILE ./src/system/tools/change_log_updater.py ---
# src/system/tools/change_log_updater.py

import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict
from shared.config_loader import load_config
from shared.logger import getLogger

log = getLogger(__name__)

CHANGE_LOG_PATH = Path(".intent/knowledge/meta_code_change_log.json")
SCHEMA_VERSION = "1.0.0"


def load_existing_log() -> Dict:
    """Loads the existing change log from disk or returns a new structure."""
    data = load_config(CHANGE_LOG_PATH, "json")
    if not data:
        return {"schema_version": SCHEMA_VERSION, "changes": []}
    return data


def append_change_entry(task: str, step: str, modified_files: List[str], score: float, violations: List[Dict]):
    """Appends a new, structured entry to the metacode change log."""
    log_data = load_existing_log()
    timestamp = datetime.utcnow().isoformat() + "Z"

    log_data["changes"].append({
        "timestamp": timestamp,
        "task": task,
        "step": step,
        "modified_files": modified_files,
        "score": score,
        "violations": violations,
        "source": "orchestrator"
    })

    CHANGE_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
    CHANGE_LOG_PATH.write_text(json.dumps(log_data, indent=2), encoding="utf-8")
    log.info(f"Appended change log entry at {timestamp}.")


if __name__ == "__main__":
    # Example usage for testing
    append_change_entry(
        task="Add intent guard integration",
        step="Check manifest before file write",
        modified_files=["src/core/cli.py", "src/core/intent_guard.py"],
        score=0.85,
        violations=[]
    )
--- END OF FILE ./src/system/tools/change_log_updater.py ---

--- START OF FILE ./src/system/tools/codegraph_builder.py ---
import ast
import json
import re
import hashlib # <<< MODIFICATION: Import hashlib for hashing
from dotenv import load_dotenv
from pathlib import Path
from typing import Dict, Set, Optional, List, Any
from dataclasses import dataclass, asdict, field
from datetime import datetime, timezone

from shared.config_loader import load_config
from shared.logger import getLogger

log = getLogger(__name__)

# --- THIS IS THE FIX (Part 1 of 2): A helper to remove docstrings for accurate hashing ---
def _strip_docstrings(node):
    """Recursively remove docstring nodes from an AST tree."""
    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module)):
        if node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Constant):
            # In Python 3.8+, docstrings are ast.Constant. In older versions, ast.Str.
            # This handles both.
            if isinstance(node.body[0].value.value, str):
                node.body = node.body[1:]
    for child_node in ast.iter_child_nodes(node):
        _strip_docstrings(child_node)
    return node

@dataclass
class FunctionInfo:
    """A data structure holding all analyzed information about a single symbol (function or class)."""
    key: str
    name: str
    type: str
    file: str
    domain: str
    agent: str
    capability: str
    intent: str
    docstring: Optional[str]
    calls: Set[str] = field(default_factory=set)
    line_number: int = 0
    is_async: bool = False
    parameters: List[str] = field(default_factory=list)
    entry_point_type: Optional[str] = None
    last_updated: str = ""
    is_class: bool = False
    base_classes: List[str] = field(default_factory=list)
    entry_point_justification: Optional[str] = None
    parent_class_key: Optional[str] = None
    # --- THIS IS THE FIX (Part 2 of 2): Restore the missing structural_hash field ---
    structural_hash: str = ""

class ProjectStructureError(Exception):
    """Custom exception for when the project's root cannot be determined."""
    pass

def find_project_root(start_path: Path) -> Path:
    """
    Traverses upward from a starting path to find the project root, marked by 'pyproject.toml'.
    """
    current_path = start_path.resolve()
    while current_path != current_path.parent:
        if (current_path / "pyproject.toml").exists():
            return current_path
        current_path = current_path.parent
    raise ProjectStructureError("Could not find 'pyproject.toml'.")

class FunctionCallVisitor(ast.NodeVisitor):
    """An AST visitor that collects the names of all functions being called within a node."""
    def __init__(self):
        self.calls: Set[str] = set()

    def visit_Call(self, node: ast.Call):
        """Records function or method calls in `self.calls` and recursively visits child nodes."""
        if isinstance(node.func, ast.Name): self.calls.add(node.func.id)
        elif isinstance(node.func, ast.Attribute): self.calls.add(node.func.attr)
        self.generic_visit(node)

# CAPABILITY: manifest_updating
class KnowledgeGraphBuilder:
    """Builds a comprehensive JSON representation of the project's code structure and relationships."""

    class ContextAwareVisitor(ast.NodeVisitor):
        """A stateful AST visitor that understands class context for methods."""
        def __init__(self, builder, filepath: Path, source_lines: List[str]):
            self.builder = builder
            self.filepath = filepath
            self.source_lines = source_lines
            self.current_class_key: Optional[str] = None

        def visit_ClassDef(self, node: ast.ClassDef):
            class_key = self.builder._process_symbol_node(node, self.filepath, self.source_lines, None)
            outer_class_key = self.current_class_key
            self.current_class_key = class_key
            self.generic_visit(node)
            self.current_class_key = outer_class_key

        def visit_FunctionDef(self, node: ast.FunctionDef):
            self.builder._process_symbol_node(node, self.filepath, self.source_lines, self.current_class_key)
            self.generic_visit(node)

        def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
            self.builder._process_symbol_node(node, self.filepath, self.source_lines, self.current_class_key)
            self.generic_visit(node)

    def __init__(self, root_path: Path, exclude_patterns: Optional[List[str]] = None):
        """Initializes the builder, loading patterns and project configuration."""
        self.root_path = root_path.resolve()
        self.src_root = self.root_path / "src"
        self.exclude_patterns = exclude_patterns or ["venv", ".venv", "__pycache__", ".git", "tests"]
        self.functions: Dict[str, FunctionInfo] = {}
        self.files_scanned = 0
        self.files_failed = 0
        self.cli_entry_points = self._get_cli_entry_points()
        self.patterns = self._load_patterns()
        self.domain_map = self._get_domain_map()
        self.fastapi_app_name: Optional[str] = None

    def _load_patterns(self) -> List[Dict]:
        """Loads entry point detection patterns from the intent file."""
        patterns_path = self.root_path / ".intent/knowledge/entry_point_patterns.yaml"
        if not patterns_path.exists():
            log.warning("entry_point_patterns.yaml not found.")
            return []
        return load_config(patterns_path, "yaml").get("patterns", [])

    def _get_cli_entry_points(self) -> Set[str]:
        """Parses pyproject.toml to find declared command-line entry points."""
        pyproject_path = self.root_path / "pyproject.toml"
        if not pyproject_path.exists(): return set()
        content = pyproject_path.read_text(encoding="utf-8")
        match = re.search(r"\[tool\.poetry\.scripts\]([^\[]*)", content, re.DOTALL)
        return set(re.findall(r'=\s*"[^"]+:(\w+)"', match.group(1))) if match else set()

    def _should_exclude_path(self, path: Path) -> bool:
        """Determines if a given path should be excluded from scanning."""
        return any(p in path.parts for p in self.exclude_patterns)

    def _infer_domains_from_directory_structure(self) -> Dict[str, str]:
        """
        A heuristic to guess domains if source_structure.yaml is missing.
        """
        log.warning("source_structure.yaml not found. Falling back to directory-based domain inference.")
        if not self.src_root.is_dir():
            log.warning("`src` directory not found. Cannot infer domains.")
            return {}
        
        domain_map = {}
        for item in self.src_root.iterdir():
            if item.is_dir() and not item.name.startswith(("_", ".")):
                domain_name = item.name
                domain_path = Path("src") / domain_name
                domain_map[domain_path.as_posix()] = domain_name
        
        log.info(f"   -> Inferred {len(domain_map)} domains from `src/` directory structure.")
        return domain_map

    def _get_domain_map(self) -> Dict[str, str]:
        """
        Loads the domain-to-path mapping from the constitution, with a fallback
        to inferring domains from the directory structure.
        """
        path = self.root_path / ".intent/knowledge/source_structure.yaml"
        data = load_config(path, "yaml")
        structure = data.get("structure")

        if not structure:
            return self._infer_domains_from_directory_structure()

        return {Path(e["path"]).as_posix(): e["domain"] for e in structure if "path" in e and "domain" in e}

    def _determine_domain(self, file_path: Path) -> str:
        """Determines the logical domain for a file path based on the longest matching prefix."""
        file_posix = file_path.as_posix()
        best = max((p for p in self.domain_map if file_posix.startswith(p)), key=len, default="")
        return self.domain_map.get(best, "unassigned")

    def _infer_agent_from_path(self, relative_path: Path) -> str:
        """Infers the most likely responsible agent based on keywords in the file path."""
        path = str(relative_path).lower()
        if "planner" in path: return "planner_agent"
        if "generator" in path: return "generator_agent"
        if any(x in path for x in ["validator", "guard", "audit"]): return "validator_agent"
        if "core" in path: return "core_agent"
        if "tool" in path: return "tooling_agent"
        return "generic_agent"

    def _parse_metadata_comment(self, node: ast.AST, source_lines: List[str]) -> Dict[str, str]:
        """Parses the line immediately preceding a symbol definition for a '# CAPABILITY:' tag."""
        if node.lineno > 1:
            line = source_lines[node.lineno - 2].strip()
            if line.startswith('#'):
                match = re.search(r'CAPABILITY:\s*(\S+)', line, re.IGNORECASE)
                if match: return {'capability': match.group(1).strip()}
        return {}

    def _get_entry_point_type(self, node: ast.FunctionDef | ast.AsyncFunctionDef) -> Optional[str]:
        """Identifies decorator or CLI-based entry points for a function."""
        for decorator in node.decorator_list:
            if isinstance(decorator, ast.Call) and isinstance(decorator.func, ast.Attribute) and isinstance(decorator.func.value, ast.Name) and decorator.func.value.id == self.fastapi_app_name:
                return f"fastapi_route_{decorator.func.attr}"
            elif isinstance(decorator, ast.Name) and decorator.id == "asynccontextmanager":
                return "context_manager"
        if self.fastapi_app_name and node.name == "lifespan": return "fastapi_lifespan"
        if node.name in self.cli_entry_points: return "cli_entry_point"
        return None

    def scan_file(self, filepath: Path) -> bool:
        """Scans a single Python file, parsing its AST to extract all symbols."""
        try:
            content = filepath.read_text(encoding="utf-8")
            source_lines = content.splitlines()
            tree = ast.parse(content, filename=str(filepath))
            
            main_block_entries, self.fastapi_app_name = set(), None
            for node in ast.walk(tree):
                if isinstance(node, ast.Assign) and isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name) and node.value.func.id == 'FastAPI' and isinstance(node.targets[0], ast.Name):
                    self.fastapi_app_name = node.targets[0].id
                elif isinstance(node, ast.If) and isinstance(node.test, ast.Compare) and isinstance(node.test.left, ast.Name) and node.test.left.id == '__name__' and isinstance(node.test.comparators[0], ast.Constant) and node.test.comparators[0].value == '__main__':
                    visitor = FunctionCallVisitor(); visitor.visit(node); main_block_entries.update(visitor.calls)
            self.cli_entry_points.update(main_block_entries)

            visitor = self.ContextAwareVisitor(self, filepath, source_lines)
            visitor.visit(tree)
            return True
        except Exception as e:
            log.error(f"Error scanning {filepath}: {e}", exc_info=False)
            return False

    def _process_symbol_node(self, node: ast.AST, filepath: Path, source_lines: List[str], parent_key: Optional[str]) -> Optional[str]:
        """Extracts and stores metadata from a single function or class AST node."""
        if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)): return None
        
        # --- HASHING LOGIC ---
        node_for_hashing = _strip_docstrings(ast.parse(ast.unparse(node)))
        structural_string = ast.unparse(node_for_hashing)
        structural_hash = hashlib.sha256(structural_string.encode('utf-8')).hexdigest()
        # --- END HASHING LOGIC ---

        visitor = FunctionCallVisitor(); visitor.visit(node)
        key = f"{filepath.relative_to(self.root_path).as_posix()}::{node.name}"
        doc = ast.get_docstring(node) or ""
        domain = self._determine_domain(filepath.relative_to(self.root_path))
        is_class = isinstance(node, ast.ClassDef)

        base_classes = []
        if is_class:
            for base in node.bases:
                if isinstance(base, ast.Name): base_classes.append(base.id)
                elif isinstance(base, ast.Attribute): base_classes.append(base.attr)
        
        func_info = FunctionInfo(
            key=key, name=node.name, type=node.__class__.__name__, file=filepath.relative_to(self.root_path).as_posix(),
            calls=visitor.calls, line_number=node.lineno, is_async=isinstance(node, ast.AsyncFunctionDef),
            docstring=doc, parameters=[arg.arg for arg in node.args.args] if hasattr(node, 'args') else [],
            entry_point_type=self._get_entry_point_type(node) if not is_class else None,
            domain=domain, agent=self._infer_agent_from_path(filepath.relative_to(self.root_path)),
            capability=self._parse_metadata_comment(node, source_lines).get("capability", "unassigned"),
            intent=doc.split('\n')[0].strip() or f"Provides functionality for the {domain} domain.",
            last_updated=datetime.now(timezone.utc).isoformat(), is_class=is_class,
            base_classes=base_classes, parent_class_key=parent_key,
            structural_hash=structural_hash
        )
        self.functions[key] = func_info
        return key

    def _apply_entry_point_patterns(self):
        """Applies declarative patterns to identify non-obvious entry points."""
        all_base_classes = {base for info in self.functions.values() for base in info.base_classes}
        for info in self.functions.values():
            if info.entry_point_type: continue
            for pattern in self.patterns:
                rules, is_match = pattern.get("match", {}), True
                
                if rules.get("has_capability_tag") and info.capability == "unassigned": is_match = False
                if rules.get("is_base_class") and (not info.is_class or info.name not in all_base_classes): is_match = False
                if "name_regex" in rules and not re.match(rules["name_regex"], info.name): is_match = False
                
                if "base_class_includes" in rules:
                    parent_bases = info.base_classes
                    if info.parent_class_key and info.parent_class_key in self.functions:
                        parent_bases.extend(self.functions[info.parent_class_key].base_classes)
                    if not any(b == rules["base_class_includes"] for b in parent_bases): is_match = False

                if is_match:
                    info.entry_point_type, info.entry_point_justification = pattern["entry_point_type"], pattern["name"]
                    break

    def build(self) -> Dict[str, Any]:
        """Orchestrates the full knowledge graph generation process."""
        log.info(f"Building knowledge graph for directory: {self.src_root}")
        py_files = [f for f in self.src_root.rglob("*.py") if f.name != "__init__.py" and not self._should_exclude_path(f)]
        log.info(f"Found {len(py_files)} Python files to scan in src/")
        
        for pyfile in py_files:
            if self.scan_file(pyfile): self.files_scanned += 1
            else: self.files_failed += 1
        
        log.info(f"Scanned {self.files_scanned} files ({self.files_failed} failed). Applying declarative patterns...")
        self._apply_entry_point_patterns()

        serializable_functions = {key: asdict(info, dict_factory=lambda x: {k: v for (k, v) in x if v is not None}) for key, info in self.functions.items()}
        for data in serializable_functions.values(): data["calls"] = sorted(list(data["calls"]))
        
        return {
            "schema_version": "2.0.0",
            "metadata": {"files_scanned": self.files_scanned, "total_symbols": len(self.functions), "timestamp_utc": datetime.now(timezone.utc).isoformat()},
            "symbols": serializable_functions
        }

def main():
    """CLI entry point to run the knowledge graph builder and save the output."""
    load_dotenv()
    try:
        root = find_project_root(Path.cwd())
        builder = KnowledgeGraphBuilder(root)
        graph = builder.build()
        out_path = root / ".intent/knowledge/knowledge_graph.json"
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(json.dumps(graph, indent=2))
        log.info(f"âœ… Knowledge graph generated! Scanned {builder.files_scanned} files, found {len(graph['symbols'])} symbols.")
        log.info(f"   -> Saved to {out_path}")
    except Exception as e:
        log.error(f"An error occurred: {e}", exc_info=True)

if __name__ == "__main__":
    main()
--- END OF FILE ./src/system/tools/codegraph_builder.py ---

--- START OF FILE ./src/system/tools/docstring_adder.py ---
# src/system/tools/docstring_adder.py
"""
A tool that finds and adds missing docstrings to the codebase, fulfilling
the 'clarity_first' principle. This is a core capability for CORE's
self-healing and self-improvement loop.
"""
import ast
import json
from pathlib import Path
import asyncio
from typing import Dict, Optional, Any

import typer
from rich.progress import track

from core.clients import GeneratorClient
from shared.logger import getLogger

# --- Constants & Setup ---
log = getLogger("docstring_adder")
REPO_ROOT = Path(__file__).resolve().parents[3]
KNOWLEDGE_GRAPH_PATH = REPO_ROOT / ".intent" / "knowledge" / "knowledge_graph.json"
CONCURRENCY_LIMIT = 10

def add_docstring_to_function_line_based(
    source_code: str, line_number: int, docstring: str
) -> str:
    """
    Surgically inserts a docstring into source code using a line-based method.
    Preserves comments and formatting.
    """
    lines = source_code.splitlines()
    if not lines or line_number < 1 or line_number > len(lines):
        log.error(f"Invalid line number {line_number} for source code")
        return source_code

    target_line_index = line_number - 1
    target_line = lines[target_line_index]
    indentation = len(target_line) - len(target_line.lstrip())
    docstring_indent = " " * (indentation + 4)

    docstring = docstring.strip()
    if not docstring:
        log.warning("Empty docstring received")
        return source_code
        
    formatted_docstring = f'{docstring_indent}"""{docstring}"""'

    if target_line_index + 1 < len(lines) and '"""' in lines[target_line_index + 1]:
        log.warning(f"Docstring already exists at line {line_number}")
        return source_code

    lines.insert(target_line_index + 1, formatted_docstring)
    return "\n".join(lines)

async def generate_and_apply_docstring(
    target: Dict[str, Any], 
    generator: GeneratorClient, 
    dry_run: bool
) -> None:
    """Generates and applies a docstring for a single function."""
    func_name = target.get("name")
    
    # --- THIS IS THE FIX (Part 1 of 2): Add defensive checks ---
    # This prevents the TypeError by ensuring the 'file' key is a valid string.
    file_rel_path = target.get("file")
    if not isinstance(file_rel_path, str):
        log.error(f"Invalid KG entry for '{func_name}': 'file' key is not a string. Entry: {target}")
        return

    file_path = REPO_ROOT / file_rel_path
    line_num = target.get("line_number")

    try:
        if not file_path.exists():
            log.error(f"File not found: {file_path}")
            return

        source_code = file_path.read_text(encoding="utf-8")
        tree = ast.parse(source_code)
        
        # This walk is now more robust for finding methods inside classes.
        node_to_update = None
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)) and node.name == func_name and node.lineno == line_num:
                node_to_update = node
                break
        
        if not node_to_update:
            log.warning(f"Function `{func_name}` at line {line_num} not found in {file_path}. It might be nested.")
            return

        # Check if docstring already exists
        if ast.get_docstring(node_to_update):
            log.info(f"Function `{func_name}` already has a docstring. Skipping.")
            return

        function_source = ast.unparse(node_to_update)
        prompt = (
            f"You are an expert Python programmer specializing in documentation.\n"
            f"Write a clear, concise, single-line docstring for the following function. "
            f"Your response must be ONLY the docstring text itself, without quotes.\n\n"
            f"Function:\n```python\n{function_source}\n```"
        )
        
        generated_docstring = await generator.make_request_async(prompt)
        generated_docstring = generated_docstring.strip().replace('"', "")

        if "Error:" in generated_docstring:
            log.warning(f"LLM failed for `{func_name}` in {file_path}: {generated_docstring}")
            return

        if dry_run:
            typer.secho(f"\nðŸ“„ In {file_path.name}, would add to function `{func_name}`:", fg=typer.colors.YELLOW)
            typer.secho(f'   """{generated_docstring}"""', fg=typer.colors.CYAN)
        else:
            new_source_code = add_docstring_to_function_line_based(
                source_code, line_num, generated_docstring
            )
            if new_source_code != source_code:
                file_path.write_text(new_source_code)
                log.info(f"Added docstring to `{func_name}` in {file_path.name}")

    except Exception as e:
        log.error(f"Failed to process `{func_name}` in {file_path}: {e}", exc_info=True)

# --- THIS IS THE FIX (Part 2 of 2): Un-nest the main logic ---
# This makes the functions top-level and easier for tools to find and reason about.
async def _async_main(dry_run: bool):
    """The core asynchronous logic for finding and fixing docstrings."""
    log.info("ðŸ©º Starting self-documentation cycle...")
    
    if not KNOWLEDGE_GRAPH_PATH.exists():
        log.error(f"Knowledge graph not found at {KNOWLEDGE_GRAPH_PATH}")
        return

    try:
        kg_data = json.loads(KNOWLEDGE_GRAPH_PATH.read_text())
    except json.JSONDecodeError as e:
        log.error(f"Invalid knowledge graph JSON: {str(e)}")
        return

    generator = GeneratorClient()
    symbols = kg_data.get("symbols", {}).values()
    # Find all functions and methods that are missing a docstring.
    targets = [s for s in symbols if not s.get("docstring") and s.get("type") in ["FunctionDef", "AsyncFunctionDef"]]

    if not targets:
        log.info("âœ… No functions with missing docstrings found. Codebase is fully documented.")
        return

    log.info(f"Found {len(targets)} functions requiring docstrings. Generating concurrently...")

    semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)

    async def worker(target):
        """Processes the target asynchronously with a semaphore, generating and applying a docstring using the provided generator (dry run if specified)."""
        async with semaphore:
            await generate_and_apply_docstring(target, generator, dry_run)

    tasks = [worker(target) for target in targets]
    
    for f in track(asyncio.as_completed(tasks), description="Generating docstrings...", total=len(tasks)):
        await f

    log.info("ðŸŽ‰ Self-documentation cycle complete.")

# CAPABILITY: add_missing_docstrings
def fix_missing_docstrings(
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--write",
        help="Show what docstrings would be added without writing any files. Use --write to apply.",
    )
):
    """Finds all functions with missing docstrings and uses an LLM to generate them."""
    asyncio.run(_async_main(dry_run))

if __name__ == "__main__":
    typer.run(fix_missing_docstrings)
--- END OF FILE ./src/system/tools/docstring_adder.py ---

--- START OF FILE ./src/system/tools/__init__.py ---
# src/system/tools/__init__.py
# Package marker for src/system/tools â€” contains CORE's introspection and audit tools.
--- END OF FILE ./src/system/tools/__init__.py ---

--- START OF FILE ./src/system/tools/manifest_migrator.py ---
# src/system/tools/manifest_migrator.py
"""
A CLI tool to migrate the monolithic project_manifest.yaml into domain-specific
manifests, as per the modular manifest architecture.
"""
import json
from pathlib import Path

import typer
import yaml

from shared.logger import getLogger

# --- Constants & Setup ---
log = getLogger("manifest_migrator")
REPO_ROOT = Path(__file__).resolve().parents[3]
INTENT_DIR = REPO_ROOT / ".intent"
MONOLITHIC_MANIFEST_PATH = INTENT_DIR / "project_manifest.yaml"
SOURCE_STRUCTURE_PATH = INTENT_DIR / "knowledge" / "source_structure.yaml"
KNOWLEDGE_GRAPH_PATH = INTENT_DIR / "knowledge" / "knowledge_graph.json"


def migrate_manifest(
    dry_run: bool = typer.Option(
        True,  # Default to True for safety
        "--dry-run/--write",
        help="Show what changes would be made without writing any files. Use --write to apply changes.",
    )
):
    """
    Reads the monolithic manifest and splits it into per-domain manifests.
    """
    log.info("ðŸš€ Starting manifest migration...")

    # Step 1: Load all necessary constitutional and knowledge files.
    log.info("   -> Loading source files...")
    if not all(
        p.exists()
        for p in [
            MONOLITHIC_MANIFEST_PATH,
            SOURCE_STRUCTURE_PATH,
            KNOWLEDGE_GRAPH_PATH,
        ]
    ):
        log.error("âŒ Critical file missing. Ensure manifest, source structure, and knowledge graph exist.")
        raise typer.Exit(code=1)

    monolith = yaml.safe_load(MONOLITHIC_MANIFEST_PATH.read_text())
    source_structure = yaml.safe_load(SOURCE_STRUCTURE_PATH.read_text())
    knowledge_graph = json.loads(KNOWLEDGE_GRAPH_PATH.read_text())

    # Step 2: Build a map to determine which domain "owns" each capability.
    # The Knowledge Graph is the source of truth for this.
    log.info("   -> Building capability-to-domain map from Knowledge Graph...")
    cap_to_domain = {
        symbol["capability"]: symbol["domain"]
        for symbol in knowledge_graph.get("symbols", {}).values()
        if symbol.get("capability") != "unassigned"
    }

    # Step 3: Prepare the new domain-specific manifests in memory.
    log.info("   -> Preparing new domain-specific manifests...")
    domain_manifests = {}
    for entry in source_structure.get("structure", []):
        domain_name = entry.get("domain")
        if domain_name:
            domain_manifests[domain_name] = {
                "domain": domain_name,
                "description": entry.get("description", "No description provided."),
                "capabilities": [],
            }

    # Step 4: Distribute capabilities from the old manifest to the new ones.
    log.info("   -> Distributing capabilities to their respective domains...")
    all_capabilities = monolith.get("required_capabilities", [])
    for cap in all_capabilities:
        domain = cap_to_domain.get(cap)
        if domain and domain in domain_manifests:
            domain_manifests[domain]["capabilities"].append(cap)
        else:
            log.warning(f"   -> Capability '{cap}' has no known domain. Skipping.")

    # Step 5: Perform the write or dry run.
    if dry_run:
        log.info("\nðŸ’§ **Dry Run Mode:** No files will be written.")
        for domain, content in domain_manifests.items():
            if not content["capabilities"]:  # Don't create empty manifests
                continue
            domain_path = source_structure["structure"][
                [i for i, d in enumerate(source_structure["structure"]) if d["domain"] == domain][0]
            ]["path"]
            target_path = REPO_ROOT / domain_path / "manifest.yaml"
            typer.secho(f"\nðŸ“„ Would write to: {target_path}", fg=typer.colors.YELLOW)
            typer.echo(yaml.dump(content, indent=2))
    else:
        log.info("\nðŸ’¾ **Write Mode:** Applying changes to disk.")
        for domain, content in domain_manifests.items():
            if not content["capabilities"]:
                continue
            domain_path_str = next(
                (d["path"] for d in source_structure["structure"] if d["domain"] == domain), None
            )
            if domain_path_str:
                target_path = REPO_ROOT / domain_path_str / "manifest.yaml"
                target_path.parent.mkdir(parents=True, exist_ok=True)
                target_path.write_text(yaml.dump(content, indent=2))
                typer.secho(f"   -> âœ… Wrote manifest for domain '{domain}' to {target_path}", fg=typer.colors.GREEN)

    log.info("\nðŸŽ‰ Migration process complete.")


if __name__ == "__main__":
    typer.run(migrate_manifest)
--- END OF FILE ./src/system/tools/manifest_migrator.py ---

--- START OF FILE ./src/system/tools/manifest.yaml ---
capabilities:
- manifest_updating
- add_missing_docstrings
description: Internal introspection utilities
domain: tooling

--- END OF FILE ./src/system/tools/manifest.yaml ---

--- START OF FILE ./tests/admin/test_guard_drift_cli.py ---
# tests/admin/test_guard_drift_cli.py
from __future__ import annotations

import json
from pathlib import Path

from typer.testing import CliRunner
from system.admin import app  # uses core-admin entrypoint

runner = CliRunner()


def write(p: Path, text: str) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(text, encoding="utf-8")


def manifest_yaml(capabilities):
    import yaml  # already a project dep
    # Prefer a top-level 'capabilities' key; adjust if your manifest uses a different key.
    data = {"capabilities": []}
    for c in capabilities:
        data["capabilities"].append(c)
    return yaml.safe_dump(data, sort_keys=False)


def test_guard_drift_clean_repo(tmp_path: Path):
    # Arrange: manifest and code agree (no meta mismatch)
    write(tmp_path / ".intent" / "project_manifest.yaml", manifest_yaml(["alpha.cap", "beta.cap"]))
    write(tmp_path / "src" / "mod.py", "# CAPABILITY: alpha.cap\n# CAPABILITY: beta.cap\n")
    out = tmp_path / "reports" / "drift_report.json"

    # Act
    result = runner.invoke(
        app, ["guard", "drift", "--root", str(tmp_path), "--format", "json", "--output", str(out)]
    )

    # Assert
    assert result.exit_code == 0, result.output
    assert out.exists()
    report = json.loads(out.read_text(encoding="utf-8"))
    assert report["missing_in_code"] == []
    assert report["undeclared_in_manifest"] == []
    assert report["mismatched_mappings"] == []


def test_guard_drift_detects_undeclared(tmp_path: Path):
    # Arrange: code has extra capability not in manifest
    write(tmp_path / ".intent" / "project_manifest.yaml", manifest_yaml(["alpha.cap"]))
    write(tmp_path / "src" / "mod.py", "# CAPABILITY: alpha.cap\n# CAPABILITY: ghost.cap\n")
    out = tmp_path / "reports" / "drift_report.json"

    # Act
    result = runner.invoke(
        app,
        ["guard", "drift", "--root", str(tmp_path), "--format", "json", "--fail-on", "any", "--output", str(out)],
    )

    # Assert
    assert result.exit_code == 2  # drift should fail the command
    assert out.exists()
    report = json.loads(out.read_text(encoding="utf-8"))
    assert "ghost.cap" in report["undeclared_in_manifest"]


def test_guard_drift_strict_requires_kg(tmp_path: Path):
    # Arrange: no KG present; strict should fail fast instead of grepping
    write(tmp_path / ".intent" / "project_manifest.yaml", manifest_yaml([]))

    # Act
    result = runner.invoke(
        app, ["guard", "drift", "--root", str(tmp_path), "--strict-intent", "--format", "json"]
    )

    # Assert
    assert result.exit_code != 0
    # Typer doesn't print the exception to stdout; check the exception message instead.
    assert result.exception is not None
    assert "Strict intent mode" in str(result.exception)


def test_guard_drift_mismatched_mappings(tmp_path: Path):
    # Arrange: same capability both sides, but code has meta that manifest does not -> mismatch
    write(tmp_path / ".intent" / "project_manifest.yaml", manifest_yaml(["beta.cap"]))
    write(tmp_path / "src" / "mod.py", "# CAPABILITY: beta.cap [domain=governance owner=ops]\n")
    out = tmp_path / "reports" / "drift_report.json"

    # Act
    result = runner.invoke(
        app, ["guard", "drift", "--root", str(tmp_path), "--format", "json", "--output", str(out)]
    )

    # Assert: default --fail-on any -> should fail due to mismatched_mappings
    assert result.exit_code == 2
    assert out.exists()
    report = json.loads(out.read_text(encoding="utf-8"))
    assert report["missing_in_code"] == []
    assert report["undeclared_in_manifest"] == []
    assert any(m.get("capability") == "beta.cap" for m in report["mismatched_mappings"])

--- END OF FILE ./tests/admin/test_guard_drift_cli.py ---

--- START OF FILE ./tests/core/test_intent_model.py ---
# tests/core/test_intent_model.py

import pytest
from pathlib import Path
from core.intent_model import IntentModel

# Use a more specific fixture to get the project root
@pytest.fixture(scope="module")
def project_root() -> Path:
    """Fixture to provide the absolute path to the project root."""
    # This assumes the tests are run from the project root, which pytest does.
    return Path.cwd().resolve()

@pytest.fixture(scope="module")
def intent_model(project_root: Path) -> IntentModel:
    """Fixture to provide a loaded IntentModel instance."""
    return IntentModel(project_root)

def test_intent_model_loads_structure(intent_model: IntentModel):
    """Verify that the intent model loads the structure data without crashing."""
    assert intent_model.structure is not None
    assert "core" in intent_model.structure
    assert "agents" in intent_model.structure
    assert isinstance(intent_model.structure["core"], dict)

def test_resolve_domain_for_path_core(intent_model: IntentModel, project_root: Path):
    """Test that a path within the 'core' domain resolves correctly."""
    # Create a dummy path that would exist in the core domain
    core_file_path = project_root / "src" / "core" / "main.py"
    domain = intent_model.resolve_domain_for_path(core_file_path)
    assert domain == "core"

def test_resolve_domain_for_path_agents(intent_model: IntentModel, project_root: Path):
    """Test that a path within the 'agents' domain resolves correctly."""
    agents_file_path = project_root / "src" / "agents" / "planner_agent.py"
    domain = intent_model.resolve_domain_for_path(agents_file_path)
    assert domain == "agents"

def test_resolve_domain_for_path_unassigned(intent_model: IntentModel, project_root: Path):
    """Test that a path outside any defined domain resolves to None."""
    # A path that doesn't fall into any defined source structure domain
    other_file_path = project_root / "README.md"
    domain = intent_model.resolve_domain_for_path(other_file_path)
    # The current implementation might resolve to None or a default.
    # Based on the code, it should be None as it's outside 'src'.
    assert domain is None

def test_get_domain_permissions_core(intent_model: IntentModel):
    """Check the permissions for a domain that has defined allowed_imports."""
    core_permissions = intent_model.get_domain_permissions("core")
    assert isinstance(core_permissions, list)
    assert "shared" in core_permissions
    assert "agents" in core_permissions

def test_get_domain_permissions_unrestricted(intent_model: IntentModel):
    """Check that a domain without 'allowed_imports' returns an empty list."""
    # Assuming a domain 'policies' might not have explicit imports defined
    # in source_structure.yaml. This may need adjustment if that file changes.
    policy_permissions = intent_model.get_domain_permissions("policies")
    assert isinstance(policy_permissions, list)
    assert policy_permissions == []
--- END OF FILE ./tests/core/test_intent_model.py ---

--- START OF FILE ./tests/governance/test_local_mode_governance.py ---
# tests/governance/test_local_mode_governance.py
"""
Tests to ensure that CORE's governance principles are correctly
reflected in its configuration files.
"""
from pathlib import Path
from shared.config_loader import load_config
from shared.path_utils import get_repo_root

def test_local_fallback_requires_git_checkpoint():
    """Ensure local_mode.yaml correctly enforces Git validation."""
    repo_root = get_repo_root()
    config_path = repo_root / ".intent" / "config" / "local_mode.yaml"
    
    # Check that the file actually exists before testing its content
    assert config_path.exists(), "local_mode.yaml configuration file is missing."

    config = load_config(config_path)
    
    # This is a critical safety check: local mode must not bypass Git commits.
    ignore_validation = config.get("apis", {}).get("git", {}).get("ignore_validation")
    assert ignore_validation is False, "CRITICAL: local_mode.yaml is configured to ignore Git validation."
--- END OF FILE ./tests/governance/test_local_mode_governance.py ---

--- START OF FILE ./tests/integration/test_full_run.py ---
# tests/integration/test_full_run.py
"""
An end-to-end integration test for the CORE system.
This test simulates a real user request and verifies the entire
Plan -> Generate -> Validate -> Write -> Commit cycle.
"""

import pytest
import json
from pathlib import Path
from unittest.mock import MagicMock

from fastapi.testclient import TestClient
from core.main import app

# Use pytest-mock to simplify mocking
@pytest.fixture
def mock_llm_clients(mocker):
    """Mocks both the Orchestrator and Generator LLM clients."""
    # Mock the Orchestrator to return a valid plan
    mock_orchestrator = mocker.patch('core.main.OrchestratorClient', autospec=True)
    plan_json = json.dumps([{
        "step": "Create a simple Python file.",
        "action": "create_file",
        "params": { "file_path": "src/hello.py" }
    }])
    mock_orchestrator.return_value.make_request.return_value = f"```json\n{plan_json}\n```"

    # Mock the Generator to return valid Python code
    mock_generator = mocker.patch('core.main.GeneratorClient', autospec=True)
    # --- THIS IS THE FIX (Part 1 of 2) ---
    # We now mock the unformatted code, just like a real LLM would produce.
    mock_generator.return_value.make_request.return_value = "print('Hello from CORE!')"
    
    return mock_orchestrator, mock_generator

@pytest.fixture
def test_git_repo(tmp_path: Path):
    """Creates a temporary, valid Git repository for the test to run in."""
    import subprocess
    # Initialize a git repo in the temp directory
    subprocess.run(["git", "init"], cwd=tmp_path, check=True)
    # Create the necessary src directory
    (tmp_path / "src").mkdir()
    return tmp_path

def test_execute_goal_end_to_end(mock_llm_clients, test_git_repo, monkeypatch, mocker):
    """
    Tests the entire /execute_goal endpoint flow.
    - Mocks LLM responses.
    - Runs against a real temporary file system with a Git repo.
    - Asserts that the file is created with the correct content.
    - Asserts that a Git commit was made.
    """
    # Use monkeypatch to change the current working directory for the test
    monkeypatch.chdir(test_git_repo)
    
    # Mock the GitService to track commit calls
    mock_git_service_instance = MagicMock()
    mock_git_service_instance.is_git_repo.return_value = True
    mocker.patch('core.main.GitService', return_value=mock_git_service_instance)
    
    with TestClient(app) as client:
        # 1. Make the API call
        response = client.post("/execute_goal", json={"goal": "Create a hello world script"})
    
        # 2. Assert the API response is successful
        assert response.status_code == 200
        assert response.json()["status"] == "success"
        
        # 3. Assert the file was created correctly
        expected_file = test_git_repo / "src" / "hello.py"
        assert expected_file.exists()
        
        # --- THIS IS THE FIX (Part 2 of 2) ---
        # Assert that the file content matches what the code formatter (black)
        # would produce: double quotes and a trailing newline.
        expected_content = 'print("Hello from CORE!")\n'
        assert expected_file.read_text() == expected_content
        
        # 4. Assert that the Git commit was made
        mock_git_service_instance.commit.assert_called_once()
        commit_message = mock_git_service_instance.commit.call_args[0][0]
        assert "Create new file src/hello.py" in commit_message


--- END OF FILE ./tests/integration/test_full_run.py ---

--- START OF FILE ./tests/unit/test_agent_utils.py ---
# tests/unit/test_agent_utils.py
import pytest
import textwrap
import re
from agents.utils import CodeEditor

@pytest.fixture
def code_editor():
    """Provides an instance of the CodeEditor."""
    return CodeEditor()

@pytest.fixture
def sample_code():
    """Provides a sample Python code snippet for testing."""
    return textwrap.dedent("""
        # A sample file
        import os

        class MyClass:
            def method_one(self):
                \"\"\"This is the first method.\"\"\" 
                return 1

        def top_level_function(a, b):
            \"\"\"A function at the top level.\"\"\" 
            return a + b
    """)

def test_replace_simple_function(code_editor, sample_code):
    """Tests replacing a top-level function with a new version."""
    new_function_code = textwrap.dedent("""
        def top_level_function(a, b):
            \"\"\"A modified function.\"\"\" 
            # Added a comment
            return a * b  # Changed the operation
    """)
    modified_code = code_editor.replace_symbol_in_code(sample_code, "top_level_function", new_function_code)
    
    assert "return a * b" in modified_code
    assert "return a + b" not in modified_code
    assert "class MyClass:" in modified_code
    assert "method_one" in modified_code
    assert "# A sample file" in modified_code  # Check that comments are preserved

def test_replace_method_in_class(code_editor, sample_code):
    """Tests replacing a method within a class."""
    new_method_code = textwrap.dedent("""
        def method_one(self):
            \"\"\"A new docstring for the method.\"\"\" 
            return 100
    """)
    modified_code = code_editor.replace_symbol_in_code(sample_code, "method_one", new_method_code)
    
    assert "return 100" in modified_code
    # Ensure there's no standalone "return 1" line (avoid substring false positives)
    assert not re.search(r'(?m)^\s*return\s+1\s*$', modified_code)
    assert "top_level_function" in modified_code
    # Crucially, check that the class definition is still present
    assert "class MyClass:" in modified_code

def test_replace_symbol_not_found_raises_error(code_editor, sample_code):
    """Tests that a ValueError is raised if the target symbol doesn't exist."""
    new_code = "def new_func(): return None"
    with pytest.raises(ValueError, match="Symbol 'non_existent_function' not found"):
        code_editor.replace_symbol_in_code(sample_code, "non_existent_function", new_code)

def test_replace_with_invalid_original_syntax_raises_error(code_editor):
    """Tests that a ValueError is raised if the original code has a syntax error."""
    invalid_original_code = "def top_level_function(a, b) return a + b"
    new_code = "def top_level_function(a,b): return a*b"
    with pytest.raises(ValueError, match="Could not parse original code due to syntax error"):
        code_editor.replace_symbol_in_code(invalid_original_code, "top_level_function", new_code)

--- END OF FILE ./tests/unit/test_agent_utils.py ---

--- START OF FILE ./tests/unit/test_clients.py ---
# tests/unit/test_clients.py
import pytest
import requests
from unittest.mock import MagicMock
from core.clients import OrchestratorClient

@pytest.fixture
def set_orchestrator_env(monkeypatch):
    monkeypatch.setenv("ORCHESTRATOR_API_URL", "http://fake-orchestrator.com/api/v1")
    monkeypatch.setenv("ORCHESTRATOR_API_KEY", "fake_orch_key")
    # --- THIS IS THE FIX ---
    # The test now expects the model name to be the new default from config.py.
    monkeypatch.setenv("ORCHESTRATOR_MODEL_NAME", "deepseek-chat")

def test_make_request_sends_correct_chat_payload(set_orchestrator_env, mocker):
    mock_post = mocker.patch("requests.post")
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        "choices": [{"message": {"content": "This is a mock chat response."}}]
    }
    mock_post.return_value = mock_response

    client = OrchestratorClient()
    prompt_text = "Analyze this user request."
    response_text = client.make_request(prompt_text, user_id="test_user")

    mock_post.assert_called_once()
    call_kwargs = mock_post.call_args.kwargs
    
    sent_payload = call_kwargs["json"]
    # The test will now correctly assert the model name.
    assert sent_payload["model"] == "deepseek-chat"
--- END OF FILE ./tests/unit/test_clients.py ---

--- START OF FILE ./tests/unit/test_git_service.py ---
# tests/unit/test_git_service.py
import pytest
from unittest.mock import MagicMock, call
from pathlib import Path
from core.git_service import GitService

@pytest.fixture
def mock_git_service(mocker, tmp_path):
    """Creates a GitService instance with a mocked subprocess.run."""
    (tmp_path / ".git").mkdir()
    
    mock_run = mocker.patch("subprocess.run")
    
    # Configure mock for multiple calls: first for status, then for commit
    mock_run.side_effect = [
        MagicMock(stdout=" M my_file.py", stderr="", returncode=0), # For git status
        MagicMock(stdout="commit success", stderr="", returncode=0) # For git commit
    ]
    
    service = GitService(repo_path=str(tmp_path))
    return service, mock_run

def test_git_add(mock_git_service):
    """Tests that the add method calls subprocess.run with the correct arguments."""
    service, mock_run = mock_git_service
    # Reset side_effect for this simple, single-call test
    mock_run.side_effect = None
    mock_run.return_value = MagicMock(stdout="", stderr="", returncode=0)

    file_to_add = "src/core/main.py"
    service.add(file_to_add)

    mock_run.assert_called_once_with(
        ['git', 'add', file_to_add],
        cwd=service.repo_path, capture_output=True, text=True, check=True
    )

def test_git_commit(mock_git_service):
    """Tests that the commit method calls subprocess.run with status and then commit."""
    service, mock_run = mock_git_service
    commit_message = "feat(agent): Test commit"

    service.commit(commit_message)

    # --- THIS IS THE FIX ---
    # Assert that run was called twice
    assert mock_run.call_count == 2
    
    # Check the calls were made in the correct order with correct arguments
    expected_calls = [
        call(['git', 'status', '--porcelain'], cwd=service.repo_path, capture_output=True, text=True, check=True),
        call(['git', 'commit', '-m', commit_message], cwd=service.repo_path, capture_output=True, text=True, check=True)
    ]
    mock_run.assert_has_calls(expected_calls)

def test_is_git_repo_true(tmp_path):
    """Tests that is_git_repo returns True when a .git directory exists."""
    (tmp_path / ".git").mkdir()
    service = GitService(repo_path=str(tmp_path))
    assert service.is_git_repo() is True

def test_is_git_repo_false(tmp_path):
    """Tests that GitService raises an error if .git directory is missing on init."""
    with pytest.raises(ValueError):
        GitService(repo_path=str(tmp_path))
--- END OF FILE ./tests/unit/test_git_service.py ---

--- START OF FILE ./tests/unit/test_planner_agent.py ---
# tests/unit/test_planner_agent.py
import json
import pytest
import textwrap
from agents.planner_agent import PlannerAgent, ExecutionTask, PlannerConfig, TaskParams, PlanExecutionError
from unittest.mock import MagicMock, patch, AsyncMock
from pathlib import Path
from pydantic import ValidationError

@pytest.fixture
def mock_dependencies():
    """Mocks all external dependencies for the PlannerAgent."""
    return {
        "orchestrator_client": MagicMock(),
        "generator_client": MagicMock(),
        "file_handler": MagicMock(repo_path=Path("/fake/repo")),
        "git_service": MagicMock(),
        "intent_guard": MagicMock(),
        "config": PlannerConfig(auto_commit=True)
    }

def test_create_execution_plan_success(mock_dependencies):
    """Tests that the planner can successfully parse a valid high-level plan."""
    agent = PlannerAgent(**mock_dependencies)
    goal = "Test goal"
    
    plan_json = json.dumps([{
        "step": "A valid step",
        "action": "create_file",
        "params": { "file_path": "src/test.py" }
    }])
    mock_dependencies["orchestrator_client"].make_request.return_value = f"```json\n{plan_json}\n```"

    with patch('core.prompt_pipeline.PromptPipeline.process', return_value="enriched_prompt"):
        plan = agent.create_execution_plan(goal)

    assert len(plan) == 1
    assert isinstance(plan[0], ExecutionTask)
    assert plan[0].action == "create_file"

def test_create_execution_plan_fails_on_invalid_action(mock_dependencies):
    """Tests that the planner fails if the plan contains an invalid action."""
    agent = PlannerAgent(**mock_dependencies)
    goal = "Test goal"
    
    invalid_plan_json = json.dumps([{"step": "Invalid action", "action": "make_coffee", "params": {}}])
    mock_dependencies["orchestrator_client"].make_request.return_value = f"```json\n{invalid_plan_json}\n```"

    with patch('core.prompt_pipeline.PromptPipeline.process', return_value="enriched_prompt"):
        with pytest.raises(PlanExecutionError):
            agent.create_execution_plan(goal)

@pytest.mark.asyncio
async def test_execute_task_fails_with_missing_params(mock_dependencies):
    """Tests that the pre-flight validation catches logically incomplete tasks."""
    agent = PlannerAgent(**mock_dependencies)
    
    incomplete_task = ExecutionTask(
        step="Incomplete tag task",
        action="add_capability_tag",
        params=TaskParams(file_path="src/test.py", symbol_name="test_func") # Missing 'tag'
    )
    
    with pytest.raises(PlanExecutionError, match="missing required parameters"):
        await agent._execute_task(incomplete_task)

@pytest.mark.asyncio
async def test_execute_plan_full_flow(mock_dependencies):
    """Tests the new two-step execute_plan flow."""
    agent = PlannerAgent(**mock_dependencies)
    goal = "Create a hello world file"

    plan_json = json.dumps([{"step": "Create the file", "action": "create_file", "params": {"file_path": "hello.py"}}])
    agent.orchestrator.make_request.return_value = f"```json\n{plan_json}\n```"
    agent.generator.make_request.return_value = "print('Hello, World!')"

    agent._execute_create_file = AsyncMock()
    
    success, message = await agent.execute_plan(goal)

    assert success is True
    assert message == "âœ… Plan executed successfully."
    agent._execute_create_file.assert_awaited_once()
    call_args = agent._execute_create_file.call_args[0][0]
    assert call_args.code == "print('Hello, World!')"

@pytest.mark.asyncio
@patch('agents.planner_agent.validate_code')
async def test_execute_create_file_success(mock_validate_code, mock_dependencies, tmp_path):
    """Happy Path: Verifies that a valid 'create_file' task succeeds."""
    mock_validate_code.return_value = {"status": "clean", "code": "print('hello world')", "violations": []}
    agent = PlannerAgent(**mock_dependencies)
    agent.repo_path = tmp_path
    params = TaskParams(file_path="src/new_feature.py", code="print('hello world')")
    
    await agent._execute_create_file(params)
    
    mock_validate_code.assert_called_once_with("src/new_feature.py", "print('hello world')")
    agent.file_handler.add_pending_write.assert_called_once()
    agent.git_service.commit.assert_called_once()

@pytest.mark.asyncio
@patch('agents.planner_agent.validate_code')
async def test_execute_create_file_fails_on_validation_error(mock_validate_code, mock_dependencies):
    """Sad Path: Verifies that the task fails if the code has 'error' severity violations."""
    mock_validate_code.return_value = {
        "status": "dirty",
        "violations": [{"rule": "E999", "message": "Syntax error!", "line": 1, "severity": "error"}],
        "code": "print("
    }
    agent = PlannerAgent(**mock_dependencies)
    params = TaskParams(file_path="src/bad_file.py", code="print(")
    
    with pytest.raises(PlanExecutionError, match="failed validation") as excinfo:
        await agent._execute_create_file(params)
    
    assert len(excinfo.value.violations) == 1
    assert excinfo.value.violations[0]["rule"] == "E999"

@pytest.mark.asyncio
async def test_execute_create_file_fails_if_file_exists(mock_dependencies, tmp_path):
    """Sad Path: Verifies that the task fails if the target file already exists."""
    agent = PlannerAgent(**mock_dependencies)
    agent.repo_path = tmp_path

    existing_file_path = tmp_path / "src/already_exists.py"
    existing_file_path.parent.mkdir(exist_ok=True)
    existing_file_path.touch()
    
    params = TaskParams(file_path="src/already_exists.py", code="print('overwrite?')")
    
    with pytest.raises(FileExistsError):
        await agent._execute_create_file(params)

@pytest.mark.asyncio
@patch('agents.planner_agent.validate_code')
async def test_execute_edit_function_success(mock_validate_code, mock_dependencies, tmp_path):
    """Happy Path: Verifies that a valid 'edit_function' task succeeds."""
    agent = PlannerAgent(**mock_dependencies)
    agent.repo_path = tmp_path
    
    original_code = textwrap.dedent("""
        def my_func():
            return 1
    """)
    target_file = tmp_path / "src/feature.py"
    target_file.parent.mkdir(exist_ok=True)
    target_file.write_text(original_code)
    
    new_function_code = textwrap.dedent("""
        def my_func():
            # A new comment
            return 2
    """)
    
    # --- MODIFICATION: This is the exact string our new line-based editor will produce ---
    expected_final_code = "def my_func():\n    # A new comment\n    return 2"
    
    params = TaskParams(
        file_path="src/feature.py",
        symbol_name="my_func",
        code=new_function_code
    )

    mock_validate_code.return_value = {"status": "clean", "code": expected_final_code, "violations": []}
    
    await agent._execute_edit_function(params)

    mock_validate_code.assert_called_once_with("src/feature.py", expected_final_code)
    agent.file_handler.add_pending_write.assert_called_once()
    agent.git_service.commit.assert_called_once_with("feat: Modify function my_func in src/feature.py")
--- END OF FILE ./tests/unit/test_planner_agent.py ---

