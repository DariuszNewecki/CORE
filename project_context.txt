--- START OF FILE ./check.txt ---
🎨 Checking code style with Black and Ruff...
python3 -m poetry run black --check src tests
python3 -m poetry run ruff check src tests
All checks passed!
🧪 Running tests with pytest...
python3 -m poetry run pytest 
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.4, pluggy-1.6.0
rootdir: /opt/dev/CORE
configfile: pyproject.toml
testpaths: tests
plugins: cov-4.1.0, mock-3.14.1, anyio-4.9.0, asyncio-0.21.0
asyncio: mode=Mode.AUTO

----------------------------- live log collection ------------------------------
INFO     core_api.errors:errors.py:52 Registered global exception handlers.
collected 30 items

tests/admin/test_agent_cli.py::test_scaffold_new_application_logic_success 
-------------------------------- live log call ---------------------------------
INFO     core_admin.agent:agent.py:116 🌱 Starting to scaffold new application 'test-app'...
INFO     core_admin.agent:agent.py:143    -> LLM planned a structure with 2 files.
INFO     core_admin.agent:agent.py:153    -> Adding starter test and CI workflow...
PASSED                                                                   [  3%]
tests/admin/test_agent_cli.py::test_scaffold_new_application_handles_llm_failure 
-------------------------------- live log call ---------------------------------
INFO     core_admin.agent:agent.py:116 🌱 Starting to scaffold new application 'test-app'...
ERROR    core_admin.agent:agent.py:194 LLM response was not valid JSON. Preview: 'this is not json'
PASSED                                                                   [  6%]
tests/admin/test_guard_drift_cli.py::test_guard_drift_clean_repo PASSED  [ 10%]
tests/admin/test_guard_drift_cli.py::test_guard_drift_detects_undeclared PASSED [ 13%]
tests/admin/test_guard_drift_cli.py::test_guard_drift_detects_mismatched_domain PASSED [ 16%]
tests/core/test_intent_model.py::test_intent_model_loads_structure PASSED [ 20%]
tests/core/test_intent_model.py::test_resolve_domain_for_path_core PASSED [ 23%]
tests/core/test_intent_model.py::test_resolve_domain_for_path_agents PASSED [ 26%]
tests/core/test_intent_model.py::test_resolve_domain_for_path_unassigned PASSED [ 30%]
tests/core/test_intent_model.py::test_get_domain_permissions_core FAILED [ 33%]
tests/core/test_intent_model.py::test_get_domain_permissions_unrestricted PASSED [ 36%]
tests/governance/test_local_mode_governance.py::test_local_fallback_requires_git_checkpoint PASSED [ 40%]
tests/integration/test_full_run.py::test_execute_goal_end_to_end PASSED  [ 43%]
tests/unit/test_agent_utils.py::test_replace_simple_function PASSED      [ 46%]
tests/unit/test_agent_utils.py::test_replace_method_in_class PASSED      [ 50%]
tests/unit/test_agent_utils.py::test_replace_symbol_not_found_raises_error PASSED [ 53%]
tests/unit/test_agent_utils.py::test_replace_with_invalid_original_syntax_raises_error PASSED [ 56%]
tests/unit/test_clients.py::test_make_request_sends_correct_chat_payload PASSED [ 60%]
tests/unit/test_execution_agent.py::test_execute_plan_success PASSED     [ 63%]
tests/unit/test_execution_agent.py::test_execute_plan_fails_on_code_generation_failure PASSED [ 66%]
tests/unit/test_execution_agent.py::test_execute_plan_handles_executor_failure PASSED [ 70%]
tests/unit/test_git_service.py::test_git_add PASSED                      [ 73%]
tests/unit/test_git_service.py::test_git_commit PASSED                   [ 76%]
tests/unit/test_git_service.py::test_is_git_repo_true PASSED             [ 80%]
tests/unit/test_git_service.py::test_is_git_repo_false PASSED            [ 83%]
tests/unit/test_planner_agent.py::test_create_execution_plan_success PASSED [ 86%]
tests/unit/test_planner_agent.py::test_create_execution_plan_fails_on_invalid_action PASSED [ 90%]
tests/unit/test_test_runner.py::test_run_tests_success PASSED            [ 93%]
tests/unit/test_test_runner.py::test_run_tests_failure PASSED            [ 96%]
tests/unit/test_test_runner.py::test_run_tests_pytest_not_found FAILED   [100%]

=================================== FAILURES ===================================
_______________________ test_get_domain_permissions_core _______________________

intent_model = <core.intent_model.IntentModel object at 0x7422fe19d5e0>

    def test_get_domain_permissions_core(intent_model: IntentModel):
        """Check the permissions for a domain that has defined allowed_imports."""
        core_permissions = intent_model.get_domain_permissions("core")
        assert isinstance(core_permissions, list)
        assert "shared" in core_permissions
>       assert "agents" in core_permissions
E       AssertionError: assert 'agents' in ['core', 'shared']

tests/core/test_intent_model.py:64: AssertionError
_______________________ test_run_tests_pytest_not_found ________________________

mock_subprocess_run = <MagicMock name='run' id='127693623605824'>

    def test_run_tests_pytest_not_found(mock_subprocess_run):
        """
        Verify that run_tests handles the case where pytest is not installed.
        """
        # Arrange: Configure the mock to simulate a FileNotFoundError
        mock_subprocess_run.side_effect = FileNotFoundError(
            "No such file or directory: 'pytest'"
        )
    
        # Act
        result = run_tests()
    
        # Assert
>       assert result["summary"] == "❌ Pytest not available"
E       AssertionError: assert '❌ pytest not found' == '❌ Pytest not available'
E         - ❌ Pytest not available
E         + ❌ pytest not found

tests/unit/test_test_runner.py:72: AssertionError
----------------------------- Captured stderr call -----------------------------
INFO:core.test_runner: 🧪 Running tests with pytest...
------------------------------ Captured log call -------------------------------
INFO     core.test_runner:test_runner.py:29 🧪 Running tests with pytest...
=========================== short test summary info ============================
FAILED tests/core/test_intent_model.py::test_get_domain_permissions_core - As...
FAILED tests/unit/test_test_runner.py::test_run_tests_pytest_not_found - Asse...
========================= 2 failed, 28 passed in 1.87s =========================

--- END OF FILE ./check.txt ---

--- START OF FILE ./CONTRIBUTING.md ---
# Contributing to CORE

> **First principle:** The constitution lives in `.intent/`. Touch **.intent first, code second**.

Thanks for helping improve CORE! This guide explains how to propose changes safely and predictably.

---

## 1) Prereqs

* Python **3.9+** (3.11 recommended)
* [Poetry](https://python-poetry.org/) installed
* Install deps: `poetry install`

---

## 2) Branch & Commit Rules

**Branch names**

* `feat/<short-purpose>` — new features
* `fix/<short-purpose>` — bug fixes
* `chore/<short-purpose>` — non-functional changes
* `docs/<short-purpose>` — docs-only changes

**Conventional commits**

* `feat(core): add capability router`
* `fix(system): handle empty drift file`
* `chore(shared): bump pydantic floor`
* `docs(governance): add waiver example`

**Scopes** should map to domains (`core`, `agents`, `system`, `shared`, `data`) or docs (`docs`, `governance`).

---

## 3) Adding or Changing Capabilities

1. **Pick the right domain** in `.intent/knowledge/source_structure.yaml`.
2. Edit the domain manifest in `.intent/manifests/<domain>.manifest.json`:

   * Capabilities must be **unique** and **domain-prefixed** (e.g., `core:task-router`).
3. Run:

   ```bash
   make migrate              # scaffold + validate + duplicate check
   make guard-check          # enforce import boundaries
   make drift                # writes reports/drift_report.json
   ```
4. Implement code **inside that domain only**. Cross-domain calls go through **core interfaces**.
5. **Update docs** when behavior or rules change.

---

## 4) Dependency Policy

* Use **httpx** for HTTP. Do **not** use `requests` (forbidden by policy).
* Add third-party libs only if justified by architecture. Reflect allow-lists in:

  * `.intent/policies/intent_guard.yaml` → `rules.libraries.allowed`
  * `.intent/knowledge/source_structure.yaml` → domain `allowed_imports`

**Checklist for new deps**

* [ ] Added to `pyproject.toml`
* [ ] Allowed in policy + source map (only where needed)
* [ ] No violations from `make guard-check`

---

## 5) Temporary Waivers (rare)

If you must defer a fix:

1. Add a **time-boxed waiver** in `.intent/policies/intent_guard.yaml` under `enforcement.waivers`:

```yaml
waivers:
  - path: "^src/system/legacy/.*\\.py$"
    reason: "Temporary while refactoring bootstrap"
    expires: "2025-12-31"
```

2. Open an issue linking to the waiver.
3. Plan a follow-up PR to remove it. **Goal: zero waivers.**

---

## 6) Local Checks (run before every PR)

```bash
# Governance
make migrate FAIL_ON_CONFLICTS=1
make guard-check
make drift

# Quality
make fast-check           # lint + tests
```

If you’re iterating fast and don’t want a failure to stop you:

```bash
poetry run core-admin guard check --no-fail
```

---

## 7) Definition of Done (DoD)

A PR is ready when:

* `.intent/` updates (if any) are consistent and validated
* `make migrate FAIL_ON_CONFLICTS=1` passes
* `make guard-check` shows no violations
* `make drift` generated **0** validation errors and **0** duplicate capabilities
* `make fast-check` is green (lint + tests)
* Docs updated (`README` or `docs/`), including rationale for governance-relevant changes

---

## 8) PR Review Expectations

Reviewers will check:

* Constitution alignment (no cycles, correct domain imports)
* Capability uniqueness and accurate domain placement
* Library policy compliance (**httpx** over `requests`)
* Tests & lint
* Clear rationale in the PR description

---

## 9) CI Overview

Every push/PR runs:

* Manifest migration & validation (fails on duplicate capabilities)
* Intent Guard (import boundaries & library policy)
* Lint + tests

**Workflow:** `.github/workflows/guard-and-drift.yml`
**Artifacts:** `reports/drift_report.json` is attached to the run.

---

## 10) Security

Report vulnerabilities privately (see `SECURITY.md`). Do **not** include secrets in PRs.

---

## 11) Questions

Open a GitHub Discussion or issue. For governance questions, reference:

* `.intent/policies/intent_guard.yaml`
* `.intent/knowledge/source_structure.yaml`
* `docs/03_GOVERNANCE.md`

--- END OF FILE ./CONTRIBUTING.md ---

--- START OF FILE ./coverage.xml ---
<?xml version="1.0" ?>
<coverage version="7.10.4" timestamp="1755506949717" lines-valid="3282" lines-covered="1899" line-rate="0.5786" branches-covered="0" branches-valid="0" branch-rate="0" complexity="0">
	<!-- Generated by coverage.py: https://coverage.readthedocs.io/en/7.10.4 -->
	<!-- Based on https://raw.githubusercontent.com/cobertura/web/master/htdocs/xml/coverage-04.dtd -->
	<sources>
		<source>/opt/dev/CORE/src</source>
	</sources>
	<packages>
		<package name="agents" line-rate="0.6562" branch-rate="0" complexity="0">
			<classes>
				<class name="__init__.py" filename="agents/__init__.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines/>
				</class>
				<class name="execution_agent.py" filename="agents/execution_agent.py" complexity="0" line-rate="0.8913" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="17" hits="1"/>
						<line number="20" hits="1"/>
						<line number="23" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="35" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="0"/>
						<line number="41" hits="1"/>
						<line number="52" hits="1"/>
						<line number="58" hits="1"/>
						<line number="59" hits="1"/>
						<line number="63" hits="1"/>
						<line number="70" hits="1"/>
						<line number="71" hits="0"/>
						<line number="73" hits="1"/>
						<line number="74" hits="1"/>
						<line number="75" hits="1"/>
						<line number="76" hits="1"/>
						<line number="77" hits="1"/>
						<line number="79" hits="1"/>
						<line number="80" hits="1"/>
						<line number="81" hits="1"/>
						<line number="82" hits="1"/>
						<line number="83" hits="1"/>
						<line number="84" hits="1"/>
						<line number="85" hits="1"/>
						<line number="86" hits="1"/>
						<line number="87" hits="1"/>
						<line number="88" hits="1"/>
						<line number="89" hits="1"/>
						<line number="90" hits="1"/>
						<line number="93" hits="1"/>
						<line number="94" hits="0"/>
						<line number="95" hits="0"/>
						<line number="98" hits="0"/>
					</lines>
				</class>
				<class name="models.py" filename="agents/models.py" complexity="0" line-rate="0.9429" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="10" hits="1"/>
						<line number="13" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="28" hits="1"/>
						<line number="29" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="37" hits="0"/>
						<line number="38" hits="0"/>
						<line number="45" hits="1"/>
						<line number="46" hits="1"/>
						<line number="49" hits="1"/>
						<line number="50" hits="1"/>
						<line number="51" hits="1"/>
						<line number="52" hits="1"/>
						<line number="53" hits="1"/>
						<line number="57" hits="1"/>
						<line number="60" hits="1"/>
						<line number="61" hits="1"/>
						<line number="62" hits="1"/>
						<line number="63" hits="1"/>
						<line number="66" hits="1"/>
						<line number="69" hits="1"/>
						<line number="70" hits="1"/>
						<line number="71" hits="1"/>
					</lines>
				</class>
				<class name="plan_executor.py" filename="agents/plan_executor.py" complexity="0" line-rate="0.3111" branch-rate="0">
					<methods/>
					<lines>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="18" hits="1"/>
						<line number="21" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="29" hits="1"/>
						<line number="32" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="1"/>
						<line number="42" hits="1"/>
						<line number="44" hits="1"/>
						<line number="46" hits="0"/>
						<line number="47" hits="0"/>
						<line number="48" hits="0"/>
						<line number="50" hits="1"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="54" hits="0"/>
						<line number="55" hits="0"/>
						<line number="56" hits="0"/>
						<line number="58" hits="1"/>
						<line number="60" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="68" hits="0"/>
						<line number="70" hits="1"/>
						<line number="72" hits="0"/>
						<line number="73" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="84" hits="0"/>
						<line number="85" hits="0"/>
						<line number="86" hits="0"/>
						<line number="88" hits="0"/>
						<line number="90" hits="0"/>
						<line number="91" hits="0"/>
						<line number="94" hits="0"/>
						<line number="95" hits="0"/>
						<line number="97" hits="0"/>
						<line number="98" hits="0"/>
						<line number="99" hits="0"/>
						<line number="104" hits="0"/>
						<line number="109" hits="0"/>
						<line number="111" hits="0"/>
						<line number="112" hits="0"/>
						<line number="113" hits="0"/>
						<line number="117" hits="1"/>
						<line number="119" hits="0"/>
						<line number="120" hits="0"/>
						<line number="121" hits="0"/>
						<line number="122" hits="0"/>
						<line number="126" hits="0"/>
						<line number="127" hits="0"/>
						<line number="128" hits="0"/>
						<line number="133" hits="0"/>
						<line number="138" hits="0"/>
						<line number="140" hits="0"/>
						<line number="141" hits="0"/>
						<line number="142" hits="0"/>
						<line number="144" hits="1"/>
						<line number="146" hits="0"/>
						<line number="151" hits="0"/>
						<line number="153" hits="0"/>
						<line number="154" hits="0"/>
						<line number="158" hits="0"/>
						<line number="160" hits="0"/>
						<line number="161" hits="0"/>
						<line number="162" hits="0"/>
						<line number="167" hits="0"/>
						<line number="168" hits="0"/>
						<line number="169" hits="0"/>
						<line number="172" hits="0"/>
						<line number="173" hits="0"/>
						<line number="175" hits="0"/>
						<line number="180" hits="0"/>
						<line number="182" hits="0"/>
						<line number="183" hits="0"/>
						<line number="184" hits="0"/>
					</lines>
				</class>
				<class name="planner_agent.py" filename="agents/planner_agent.py" complexity="0" line-rate="0.7284" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="24" hits="1"/>
						<line number="27" hits="1"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="38" hits="1"/>
						<line number="40" hits="1"/>
						<line number="48" hits="1"/>
						<line number="50" hits="1"/>
						<line number="53" hits="1"/>
						<line number="54" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="0"/>
						<line number="57" hits="0"/>
						<line number="60" hits="0"/>
						<line number="61" hits="0"/>
						<line number="62" hits="0"/>
						<line number="63" hits="0"/>
						<line number="64" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="68" hits="0"/>
						<line number="69" hits="0"/>
						<line number="70" hits="0"/>
						<line number="71" hits="0"/>
						<line number="72" hits="0"/>
						<line number="74" hits="1"/>
						<line number="76" hits="1"/>
						<line number="77" hits="1"/>
						<line number="78" hits="1"/>
						<line number="80" hits="1"/>
						<line number="82" hits="1"/>
						<line number="83" hits="1"/>
						<line number="84" hits="1"/>
						<line number="85" hits="0"/>
						<line number="86" hits="1"/>
						<line number="87" hits="1"/>
						<line number="88" hits="0"/>
						<line number="89" hits="0"/>
						<line number="90" hits="1"/>
						<line number="91" hits="0"/>
						<line number="96" hits="1"/>
						<line number="98" hits="1"/>
						<line number="99" hits="1"/>
						<line number="100" hits="1"/>
						<line number="102" hits="1"/>
						<line number="117" hits="1"/>
						<line number="118" hits="1"/>
						<line number="120" hits="1"/>
						<line number="121" hits="1"/>
						<line number="122" hits="1"/>
						<line number="125" hits="1"/>
						<line number="126" hits="1"/>
						<line number="127" hits="0"/>
						<line number="128" hits="1"/>
						<line number="129" hits="0"/>
						<line number="131" hits="1"/>
						<line number="132" hits="1"/>
						<line number="133" hits="1"/>
						<line number="135" hits="1"/>
						<line number="136" hits="1"/>
						<line number="137" hits="1"/>
						<line number="138" hits="1"/>
						<line number="139" hits="1"/>
						<line number="140" hits="1"/>
						<line number="143" hits="0"/>
					</lines>
				</class>
				<class name="utils.py" filename="agents/utils.py" complexity="0" line-rate="0.7206" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="15" hits="1"/>
						<line number="18" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="29" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="1"/>
						<line number="42" hits="1"/>
						<line number="44" hits="1"/>
						<line number="45" hits="1"/>
						<line number="46" hits="1"/>
						<line number="48" hits="1"/>
						<line number="50" hits="1"/>
						<line number="51" hits="1"/>
						<line number="53" hits="1"/>
						<line number="54" hits="1"/>
						<line number="55" hits="1"/>
						<line number="59" hits="1"/>
						<line number="60" hits="1"/>
						<line number="62" hits="1"/>
						<line number="63" hits="1"/>
						<line number="66" hits="1"/>
						<line number="69" hits="1"/>
						<line number="70" hits="1"/>
						<line number="72" hits="0"/>
						<line number="73" hits="0"/>
						<line number="75" hits="0"/>
						<line number="76" hits="0"/>
						<line number="77" hits="0"/>
						<line number="78" hits="0"/>
						<line number="79" hits="0"/>
						<line number="82" hits="0"/>
						<line number="83" hits="0"/>
						<line number="84" hits="0"/>
						<line number="85" hits="0"/>
						<line number="86" hits="0"/>
						<line number="89" hits="1"/>
						<line number="92" hits="1"/>
						<line number="95" hits="1"/>
						<line number="96" hits="1"/>
						<line number="98" hits="1"/>
						<line number="101" hits="1"/>
						<line number="102" hits="1"/>
						<line number="103" hits="1"/>
						<line number="104" hits="0"/>
						<line number="105" hits="0"/>
						<line number="106" hits="1"/>
						<line number="108" hits="1"/>
						<line number="110" hits="1"/>
						<line number="115" hits="0"/>
						<line number="116" hits="0"/>
						<line number="117" hits="0"/>
						<line number="118" hits="0"/>
						<line number="119" hits="0"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="core" line-rate="0.4158" branch-rate="0" complexity="0">
			<classes>
				<class name="__init__.py" filename="core/__init__.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines/>
				</class>
				<class name="black_formatter.py" filename="core/black_formatter.py" complexity="0" line-rate="0.1818" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="0"/>
						<line number="25" hits="0"/>
						<line number="26" hits="0"/>
						<line number="27" hits="0"/>
						<line number="28" hits="0"/>
						<line number="29" hits="0"/>
						<line number="31" hits="0"/>
						<line number="34" hits="0"/>
						<line number="36" hits="0"/>
					</lines>
				</class>
				<class name="capabilities.py" filename="core/capabilities.py" complexity="0" line-rate="0.6053" branch-rate="0">
					<methods/>
					<lines>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="20" hits="1"/>
						<line number="26" hits="1"/>
						<line number="28" hits="1"/>
						<line number="29" hits="1"/>
						<line number="31" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="1"/>
						<line number="40" hits="1"/>
						<line number="50" hits="1"/>
						<line number="53" hits="0"/>
						<line number="55" hits="1"/>
						<line number="56" hits="1"/>
						<line number="57" hits="1"/>
						<line number="58" hits="0"/>
						<line number="59" hits="0"/>
						<line number="61" hits="0"/>
						<line number="62" hits="0"/>
						<line number="63" hits="0"/>
						<line number="64" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="71" hits="0"/>
						<line number="73" hits="1"/>
						<line number="74" hits="1"/>
						<line number="77" hits="1"/>
						<line number="78" hits="0"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="82" hits="0"/>
					</lines>
				</class>
				<class name="clients.py" filename="core/clients.py" complexity="0" line-rate="0.5254" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="15" hits="1"/>
						<line number="18" hits="1"/>
						<line number="24" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="0"/>
						<line number="38" hits="1"/>
						<line number="39" hits="1"/>
						<line number="41" hits="0"/>
						<line number="43" hits="1"/>
						<line number="44" hits="1"/>
						<line number="50" hits="1"/>
						<line number="52" hits="1"/>
						<line number="56" hits="1"/>
						<line number="62" hits="1"/>
						<line number="63" hits="1"/>
						<line number="66" hits="1"/>
						<line number="69" hits="1"/>
						<line number="70" hits="1"/>
						<line number="71" hits="1"/>
						<line number="72" hits="1"/>
						<line number="73" hits="1"/>
						<line number="74" hits="0"/>
						<line number="75" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="85" hits="0"/>
						<line number="89" hits="1"/>
						<line number="95" hits="0"/>
						<line number="100" hits="0"/>
						<line number="101" hits="0"/>
						<line number="104" hits="0"/>
						<line number="107" hits="0"/>
						<line number="108" hits="0"/>
						<line number="109" hits="0"/>
						<line number="110" hits="0"/>
						<line number="111" hits="0"/>
						<line number="112" hits="0"/>
						<line number="113" hits="0"/>
						<line number="114" hits="0"/>
						<line number="115" hits="0"/>
						<line number="116" hits="0"/>
						<line number="120" hits="0"/>
						<line number="121" hits="0"/>
						<line number="122" hits="0"/>
						<line number="126" hits="0"/>
						<line number="129" hits="1"/>
						<line number="135" hits="1"/>
						<line number="136" hits="1"/>
						<line number="141" hits="1"/>
						<line number="144" hits="1"/>
						<line number="150" hits="1"/>
						<line number="152" hits="0"/>
						<line number="157" hits="0"/>
					</lines>
				</class>
				<class name="errors.py" filename="core/errors.py" complexity="0" line-rate="0.75" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="17" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="26" hits="0"/>
						<line number="29" hits="0"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="41" hits="0"/>
						<line number="44" hits="0"/>
						<line number="52" hits="1"/>
					</lines>
				</class>
				<class name="file_handler.py" filename="core/file_handler.py" complexity="0" line-rate="0.4038" branch-rate="0">
					<methods/>
					<lines>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="18" hits="1"/>
						<line number="21" hits="1"/>
						<line number="28" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="0"/>
						<line number="40" hits="1"/>
						<line number="41" hits="1"/>
						<line number="42" hits="1"/>
						<line number="44" hits="1"/>
						<line number="45" hits="1"/>
						<line number="48" hits="1"/>
						<line number="49" hits="1"/>
						<line number="51" hits="1"/>
						<line number="55" hits="0"/>
						<line number="56" hits="0"/>
						<line number="57" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="68" hits="0"/>
						<line number="69" hits="0"/>
						<line number="70" hits="0"/>
						<line number="72" hits="1"/>
						<line number="76" hits="0"/>
						<line number="77" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="83" hits="0"/>
						<line number="84" hits="0"/>
						<line number="89" hits="0"/>
						<line number="91" hits="0"/>
						<line number="92" hits="0"/>
						<line number="94" hits="0"/>
						<line number="95" hits="0"/>
						<line number="99" hits="0"/>
						<line number="100" hits="0"/>
						<line number="102" hits="0"/>
						<line number="103" hits="0"/>
						<line number="108" hits="0"/>
						<line number="109" hits="0"/>
						<line number="110" hits="0"/>
						<line number="111" hits="0"/>
						<line number="112" hits="0"/>
						<line number="115" hits="0"/>
					</lines>
				</class>
				<class name="git_service.py" filename="core/git_service.py" complexity="0" line-rate="0.6818" branch-rate="0">
					<methods/>
					<lines>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="17" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="28" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="1"/>
						<line number="36" hits="1"/>
						<line number="46" hits="1"/>
						<line number="47" hits="1"/>
						<line number="48" hits="1"/>
						<line number="51" hits="1"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="54" hits="0"/>
						<line number="56" hits="1"/>
						<line number="63" hits="1"/>
						<line number="64" hits="1"/>
						<line number="65" hits="0"/>
						<line number="66" hits="1"/>
						<line number="68" hits="1"/>
						<line number="76" hits="1"/>
						<line number="79" hits="1"/>
						<line number="80" hits="1"/>
						<line number="81" hits="0"/>
						<line number="82" hits="0"/>
						<line number="84" hits="1"/>
						<line number="85" hits="1"/>
						<line number="86" hits="0"/>
						<line number="89" hits="0"/>
						<line number="90" hits="0"/>
						<line number="93" hits="0"/>
						<line number="96" hits="1"/>
						<line number="103" hits="1"/>
						<line number="104" hits="1"/>
						<line number="106" hits="1"/>
						<line number="110" hits="0"/>
						<line number="112" hits="1"/>
						<line number="117" hits="0"/>
						<line number="118" hits="0"/>
						<line number="119" hits="0"/>
					</lines>
				</class>
				<class name="intent_alignment.py" filename="core/intent_alignment.py" complexity="0" line-rate="0.2791" branch-rate="0">
					<methods/>
					<lines>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="18" hits="1"/>
						<line number="25" hits="1"/>
						<line number="28" hits="1"/>
						<line number="30" hits="0"/>
						<line number="31" hits="0"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="34" hits="0"/>
						<line number="35" hits="0"/>
						<line number="36" hits="0"/>
						<line number="39" hits="1"/>
						<line number="41" hits="0"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="52" hits="0"/>
						<line number="55" hits="1"/>
						<line number="57" hits="0"/>
						<line number="60" hits="1"/>
						<line number="67" hits="0"/>
						<line number="68" hits="0"/>
						<line number="69" hits="0"/>
						<line number="72" hits="0"/>
						<line number="73" hits="0"/>
						<line number="74" hits="0"/>
						<line number="77" hits="0"/>
						<line number="78" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="82" hits="0"/>
						<line number="83" hits="0"/>
						<line number="84" hits="0"/>
						<line number="85" hits="0"/>
						<line number="87" hits="0"/>
						<line number="88" hits="0"/>
					</lines>
				</class>
				<class name="intent_guard.py" filename="core/intent_guard.py" complexity="0" line-rate="0.2619" branch-rate="0">
					<methods/>
					<lines>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="17" hits="1"/>
						<line number="21" hits="1"/>
						<line number="27" hits="1"/>
						<line number="31" hits="0"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="34" hits="0"/>
						<line number="35" hits="0"/>
						<line number="37" hits="0"/>
						<line number="38" hits="0"/>
						<line number="40" hits="0"/>
						<line number="44" hits="1"/>
						<line number="46" hits="0"/>
						<line number="47" hits="0"/>
						<line number="48" hits="0"/>
						<line number="49" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="53" hits="1"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="59" hits="0"/>
						<line number="60" hits="0"/>
						<line number="61" hits="0"/>
						<line number="62" hits="0"/>
						<line number="64" hits="0"/>
						<line number="67" hits="0"/>
						<line number="68" hits="0"/>
						<line number="69" hits="0"/>
						<line number="73" hits="1"/>
						<line number="78" hits="0"/>
						<line number="81" hits="0"/>
						<line number="84" hits="0"/>
						<line number="87" hits="0"/>
						<line number="89" hits="0"/>
						<line number="93" hits="0"/>
						<line number="102" hits="0"/>
					</lines>
				</class>
				<class name="intent_model.py" filename="core/intent_model.py" complexity="0" line-rate="0.931" branch-rate="0">
					<methods/>
					<lines>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="17" hits="1"/>
						<line number="20" hits="1"/>
						<line number="26" hits="1"/>
						<line number="28" hits="1"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="38" hits="1"/>
						<line number="40" hits="1"/>
						<line number="42" hits="1"/>
						<line number="49" hits="1"/>
						<line number="50" hits="0"/>
						<line number="52" hits="1"/>
						<line number="54" hits="1"/>
						<line number="55" hits="0"/>
						<line number="59" hits="1"/>
						<line number="61" hits="1"/>
						<line number="68" hits="1"/>
						<line number="70" hits="1"/>
						<line number="75" hits="1"/>
						<line number="76" hits="1"/>
						<line number="78" hits="1"/>
						<line number="79" hits="1"/>
						<line number="80" hits="1"/>
						<line number="82" hits="1"/>
						<line number="92" hits="1"/>
						<line number="93" hits="1"/>
						<line number="94" hits="1"/>
					</lines>
				</class>
				<class name="main.py" filename="core/main.py" complexity="0" line-rate="0.7872" branch-rate="0">
					<methods/>
					<lines>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="28" hits="1"/>
						<line number="29" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="1"/>
						<line number="39" hits="1"/>
						<line number="40" hits="1"/>
						<line number="43" hits="1"/>
						<line number="46" hits="1"/>
						<line number="49" hits="1"/>
						<line number="50" hits="1"/>
						<line number="54" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="1"/>
						<line number="58" hits="1"/>
						<line number="60" hits="1"/>
						<line number="61" hits="1"/>
						<line number="62" hits="0"/>
						<line number="66" hits="1"/>
						<line number="68" hits="1"/>
						<line number="69" hits="1"/>
						<line number="70" hits="1"/>
						<line number="71" hits="1"/>
						<line number="72" hits="1"/>
						<line number="73" hits="1"/>
						<line number="75" hits="1"/>
						<line number="76" hits="1"/>
						<line number="77" hits="1"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="82" hits="1"/>
						<line number="83" hits="1"/>
						<line number="84" hits="1"/>
						<line number="87" hits="1"/>
						<line number="88" hits="1"/>
						<line number="91" hits="1"/>
						<line number="94" hits="1"/>
						<line number="97" hits="1"/>
						<line number="100" hits="1"/>
						<line number="101" hits="1"/>
						<line number="104" hits="1"/>
						<line number="105" hits="1"/>
						<line number="107" hits="0"/>
						<line number="108" hits="0"/>
						<line number="111" hits="1"/>
						<line number="112" hits="1"/>
						<line number="114" hits="0"/>
						<line number="115" hits="0"/>
						<line number="116" hits="0"/>
						<line number="117" hits="0"/>
						<line number="118" hits="0"/>
						<line number="119" hits="0"/>
						<line number="120" hits="0"/>
						<line number="121" hits="0"/>
						<line number="122" hits="0"/>
						<line number="127" hits="1"/>
						<line number="128" hits="1"/>
						<line number="130" hits="1"/>
						<line number="131" hits="1"/>
						<line number="133" hits="1"/>
						<line number="135" hits="1"/>
						<line number="139" hits="1"/>
						<line number="142" hits="1"/>
						<line number="147" hits="1"/>
						<line number="154" hits="1"/>
						<line number="156" hits="1"/>
						<line number="157" hits="1"/>
						<line number="158" hits="1"/>
						<line number="163" hits="0"/>
						<line number="164" hits="0"/>
						<line number="166" hits="0"/>
						<line number="167" hits="0"/>
						<line number="168" hits="0"/>
						<line number="171" hits="1"/>
						<line number="172" hits="1"/>
						<line number="174" hits="0"/>
					</lines>
				</class>
				<class name="prompt_pipeline.py" filename="core/prompt_pipeline.py" complexity="0" line-rate="0.2958" branch-rate="0">
					<methods/>
					<lines>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="18" hits="1"/>
						<line number="21" hits="1"/>
						<line number="24" hits="1"/>
						<line number="30" hits="1"/>
						<line number="37" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="1"/>
						<line number="42" hits="1"/>
						<line number="43" hits="1"/>
						<line number="45" hits="1"/>
						<line number="47" hits="0"/>
						<line number="48" hits="0"/>
						<line number="49" hits="0"/>
						<line number="50" hits="0"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="54" hits="0"/>
						<line number="55" hits="0"/>
						<line number="56" hits="0"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="60" hits="1"/>
						<line number="62" hits="0"/>
						<line number="64" hits="1"/>
						<line number="66" hits="1"/>
						<line number="68" hits="0"/>
						<line number="69" hits="0"/>
						<line number="70" hits="0"/>
						<line number="72" hits="0"/>
						<line number="73" hits="0"/>
						<line number="74" hits="0"/>
						<line number="75" hits="0"/>
						<line number="76" hits="0"/>
						<line number="77" hits="0"/>
						<line number="78" hits="0"/>
						<line number="80" hits="1"/>
						<line number="82" hits="0"/>
						<line number="84" hits="1"/>
						<line number="86" hits="0"/>
						<line number="87" hits="0"/>
						<line number="89" hits="0"/>
						<line number="91" hits="1"/>
						<line number="93" hits="0"/>
						<line number="95" hits="1"/>
						<line number="97" hits="0"/>
						<line number="98" hits="0"/>
						<line number="99" hits="0"/>
						<line number="101" hits="0"/>
						<line number="102" hits="0"/>
						<line number="103" hits="0"/>
						<line number="104" hits="0"/>
						<line number="106" hits="0"/>
						<line number="107" hits="0"/>
						<line number="109" hits="0"/>
						<line number="110" hits="0"/>
						<line number="111" hits="0"/>
						<line number="112" hits="0"/>
						<line number="114" hits="0"/>
						<line number="115" hits="0"/>
						<line number="118" hits="0"/>
						<line number="123" hits="0"/>
						<line number="125" hits="1"/>
						<line number="127" hits="0"/>
						<line number="130" hits="1"/>
						<line number="135" hits="0"/>
						<line number="136" hits="0"/>
						<line number="137" hits="0"/>
						<line number="138" hits="0"/>
						<line number="139" hits="0"/>
					</lines>
				</class>
				<class name="ruff_linter.py" filename="core/ruff_linter.py" complexity="0" line-rate="0.2571" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="20" hits="1"/>
						<line number="35" hits="0"/>
						<line number="36" hits="0"/>
						<line number="39" hits="0"/>
						<line number="40" hits="0"/>
						<line number="42" hits="0"/>
						<line number="44" hits="0"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="56" hits="0"/>
						<line number="64" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="76" hits="0"/>
						<line number="78" hits="0"/>
						<line number="79" hits="0"/>
						<line number="81" hits="0"/>
						<line number="87" hits="0"/>
						<line number="88" hits="0"/>
						<line number="89" hits="0"/>
						<line number="90" hits="0"/>
						<line number="91" hits="0"/>
						<line number="92" hits="0"/>
						<line number="93" hits="0"/>
						<line number="95" hits="0"/>
						<line number="96" hits="0"/>
					</lines>
				</class>
				<class name="self_correction_engine.py" filename="core/self_correction_engine.py" complexity="0" line-rate="0" branch-rate="0">
					<methods/>
					<lines>
						<line number="8" hits="0"/>
						<line number="9" hits="0"/>
						<line number="11" hits="0"/>
						<line number="12" hits="0"/>
						<line number="13" hits="0"/>
						<line number="14" hits="0"/>
						<line number="15" hits="0"/>
						<line number="17" hits="0"/>
						<line number="18" hits="0"/>
						<line number="19" hits="0"/>
						<line number="23" hits="0"/>
						<line number="25" hits="0"/>
						<line number="28" hits="0"/>
						<line number="29" hits="0"/>
						<line number="30" hits="0"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="35" hits="0"/>
						<line number="36" hits="0"/>
						<line number="42" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="53" hits="0"/>
						<line number="55" hits="0"/>
						<line number="56" hits="0"/>
						<line number="62" hits="0"/>
						<line number="64" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="73" hits="0"/>
						<line number="76" hits="0"/>
					</lines>
				</class>
				<class name="syntax_checker.py" filename="core/syntax_checker.py" complexity="0" line-rate="0.3077" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="0"/>
						<line number="26" hits="0"/>
						<line number="27" hits="0"/>
						<line number="29" hits="0"/>
						<line number="30" hits="0"/>
						<line number="31" hits="0"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="34" hits="0"/>
					</lines>
				</class>
				<class name="test_runner.py" filename="core/test_runner.py" complexity="0" line-rate="0" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="0"/>
						<line number="8" hits="0"/>
						<line number="9" hits="0"/>
						<line number="10" hits="0"/>
						<line number="11" hits="0"/>
						<line number="12" hits="0"/>
						<line number="14" hits="0"/>
						<line number="15" hits="0"/>
						<line number="17" hits="0"/>
						<line number="21" hits="0"/>
						<line number="23" hits="0"/>
						<line number="24" hits="0"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="34" hits="0"/>
						<line number="36" hits="0"/>
						<line number="37" hits="0"/>
						<line number="38" hits="0"/>
						<line number="39" hits="0"/>
						<line number="40" hits="0"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="55" hits="0"/>
						<line number="56" hits="0"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="60" hits="0"/>
						<line number="61" hits="0"/>
						<line number="62" hits="0"/>
						<line number="63" hits="0"/>
						<line number="64" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="68" hits="0"/>
						<line number="69" hits="0"/>
						<line number="70" hits="0"/>
						<line number="71" hits="0"/>
						<line number="73" hits="0"/>
						<line number="74" hits="0"/>
						<line number="76" hits="0"/>
						<line number="77" hits="0"/>
						<line number="80" hits="0"/>
						<line number="82" hits="0"/>
						<line number="83" hits="0"/>
						<line number="84" hits="0"/>
						<line number="85" hits="0"/>
						<line number="86" hits="0"/>
						<line number="89" hits="0"/>
						<line number="91" hits="0"/>
						<line number="92" hits="0"/>
						<line number="93" hits="0"/>
						<line number="94" hits="0"/>
						<line number="95" hits="0"/>
						<line number="96" hits="0"/>
						<line number="97" hits="0"/>
						<line number="100" hits="0"/>
						<line number="102" hits="0"/>
						<line number="103" hits="0"/>
						<line number="104" hits="0"/>
						<line number="105" hits="0"/>
						<line number="106" hits="0"/>
						<line number="111" hits="0"/>
						<line number="112" hits="0"/>
						<line number="113" hits="0"/>
						<line number="114" hits="0"/>
						<line number="115" hits="0"/>
					</lines>
				</class>
				<class name="validation_pipeline.py" filename="core/validation_pipeline.py" complexity="0" line-rate="0.3577" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="26" hits="1"/>
						<line number="29" hits="1"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="34" hits="0"/>
						<line number="35" hits="0"/>
						<line number="36" hits="0"/>
						<line number="37" hits="0"/>
						<line number="40" hits="1"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="44" hits="0"/>
						<line number="45" hits="0"/>
						<line number="46" hits="0"/>
						<line number="47" hits="0"/>
						<line number="48" hits="0"/>
						<line number="49" hits="0"/>
						<line number="52" hits="1"/>
						<line number="54" hits="0"/>
						<line number="55" hits="0"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="60" hits="0"/>
						<line number="64" hits="0"/>
						<line number="67" hits="0"/>
						<line number="70" hits="0"/>
						<line number="71" hits="0"/>
						<line number="73" hits="0"/>
						<line number="74" hits="0"/>
						<line number="78" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="84" hits="0"/>
						<line number="86" hits="0"/>
						<line number="88" hits="0"/>
						<line number="89" hits="0"/>
						<line number="90" hits="0"/>
						<line number="91" hits="0"/>
						<line number="92" hits="0"/>
						<line number="93" hits="0"/>
						<line number="95" hits="0"/>
						<line number="96" hits="0"/>
						<line number="105" hits="0"/>
						<line number="106" hits="0"/>
						<line number="107" hits="0"/>
						<line number="108" hits="0"/>
						<line number="116" hits="0"/>
						<line number="117" hits="0"/>
						<line number="118" hits="0"/>
						<line number="126" hits="0"/>
						<line number="129" hits="1"/>
						<line number="131" hits="0"/>
						<line number="132" hits="0"/>
						<line number="133" hits="0"/>
						<line number="134" hits="0"/>
						<line number="135" hits="0"/>
						<line number="136" hits="0"/>
						<line number="144" hits="0"/>
						<line number="148" hits="1"/>
						<line number="150" hits="0"/>
						<line number="151" hits="0"/>
						<line number="152" hits="0"/>
						<line number="154" hits="0"/>
						<line number="155" hits="0"/>
						<line number="158" hits="1"/>
						<line number="163" hits="0"/>
						<line number="166" hits="0"/>
						<line number="167" hits="0"/>
						<line number="168" hits="0"/>
						<line number="170" hits="0"/>
						<line number="179" hits="0"/>
						<line number="182" hits="0"/>
						<line number="183" hits="0"/>
						<line number="186" hits="0"/>
						<line number="187" hits="0"/>
						<line number="189" hits="0"/>
						<line number="190" hits="0"/>
						<line number="193" hits="0"/>
						<line number="194" hits="0"/>
						<line number="196" hits="0"/>
						<line number="199" hits="1"/>
						<line number="201" hits="1"/>
						<line number="202" hits="1"/>
						<line number="203" hits="1"/>
						<line number="204" hits="0"/>
						<line number="205" hits="0"/>
						<line number="213" hits="1"/>
						<line number="216" hits="1"/>
						<line number="218" hits="1"/>
						<line number="219" hits="1"/>
						<line number="220" hits="0"/>
						<line number="221" hits="1"/>
						<line number="222" hits="1"/>
						<line number="223" hits="1"/>
						<line number="224" hits="1"/>
						<line number="225" hits="0"/>
						<line number="229" hits="1"/>
						<line number="231" hits="1"/>
						<line number="236" hits="1"/>
						<line number="237" hits="1"/>
						<line number="238" hits="0"/>
						<line number="240" hits="1"/>
						<line number="241" hits="1"/>
						<line number="243" hits="1"/>
						<line number="244" hits="0"/>
						<line number="245" hits="1"/>
						<line number="246" hits="1"/>
						<line number="249" hits="1"/>
						<line number="250" hits="1"/>
						<line number="252" hits="1"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="shared" line-rate="0.8452" branch-rate="0" complexity="0">
			<classes>
				<class name="config.py" filename="shared/config.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="1" hits="1"/>
						<line number="3" hits="1"/>
						<line number="6" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="28" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="36" hits="1"/>
						<line number="39" hits="1"/>
						<line number="46" hits="1"/>
					</lines>
				</class>
				<class name="config_loader.py" filename="shared/config_loader.py" complexity="0" line-rate="0.75" branch-rate="0">
					<methods/>
					<lines>
						<line number="3" hits="1"/>
						<line number="4" hits="1"/>
						<line number="5" hits="1"/>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="28" hits="0"/>
						<line number="31" hits="0"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="42" hits="1"/>
						<line number="43" hits="0"/>
						<line number="44" hits="0"/>
						<line number="46" hits="1"/>
						<line number="47" hits="1"/>
						<line number="48" hits="1"/>
						<line number="49" hits="1"/>
						<line number="50" hits="1"/>
						<line number="52" hits="1"/>
						<line number="53" hits="1"/>
						<line number="54" hits="0"/>
						<line number="55" hits="0"/>
						<line number="56" hits="0"/>
					</lines>
				</class>
				<class name="logger.py" filename="shared/logger.py" complexity="0" line-rate="0.8333" branch-rate="0">
					<methods/>
					<lines>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="21" hits="1"/>
						<line number="24" hits="1"/>
						<line number="26" hits="0"/>
						<line number="35" hits="0"/>
						<line number="38" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="0"/>
						<line number="42" hits="1"/>
						<line number="45" hits="1"/>
						<line number="60" hits="1"/>
						<line number="61" hits="1"/>
						<line number="62" hits="0"/>
						<line number="65" hits="1"/>
						<line number="66" hits="1"/>
						<line number="67" hits="0"/>
						<line number="70" hits="1"/>
						<line number="73" hits="1"/>
						<line number="77" hits="1"/>
						<line number="79" hits="1"/>
						<line number="81" hits="1"/>
						<line number="82" hits="1"/>
						<line number="83" hits="1"/>
						<line number="87" hits="1"/>
					</lines>
				</class>
				<class name="path_utils.py" filename="shared/path_utils.py" complexity="0" line-rate="0.8889" branch-rate="0">
					<methods/>
					<lines>
						<line number="3" hits="1"/>
						<line number="4" hits="1"/>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="26" hits="0"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="shared.schemas" line-rate="0.7308" branch-rate="0" complexity="0">
			<classes>
				<class name="manifest_validator.py" filename="shared/schemas/manifest_validator.py" complexity="0" line-rate="0.7308" branch-rate="0">
					<methods/>
					<lines>
						<line number="3" hits="1"/>
						<line number="4" hits="1"/>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="11" hits="1"/>
						<line number="14" hits="1"/>
						<line number="28" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="0"/>
						<line number="33" hits="1"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="0"/>
						<line number="37" hits="0"/>
						<line number="42" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="1"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="61" hits="1"/>
						<line number="62" hits="1"/>
						<line number="64" hits="1"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="69" hits="1"/>
						<line number="70" hits="1"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="shared.utils" line-rate="0.9038" branch-rate="0" complexity="0">
			<classes>
				<class name="__init__.py" filename="shared/utils/__init__.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines/>
				</class>
				<class name="import_scanner.py" filename="shared/utils/import_scanner.py" complexity="0" line-rate="0.9" branch-rate="0">
					<methods/>
					<lines>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="19" hits="1"/>
						<line number="29" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="1"/>
						<line number="40" hits="1"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="45" hits="1"/>
					</lines>
				</class>
				<class name="manifest_aggregator.py" filename="shared/utils/manifest_aggregator.py" complexity="0" line-rate="0.9062" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="16" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="0"/>
						<line number="37" hits="0"/>
						<line number="39" hits="1"/>
						<line number="41" hits="1"/>
						<line number="42" hits="1"/>
						<line number="44" hits="1"/>
						<line number="45" hits="1"/>
						<line number="46" hits="1"/>
						<line number="47" hits="0"/>
						<line number="49" hits="1"/>
						<line number="50" hits="1"/>
						<line number="51" hits="1"/>
						<line number="52" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="1"/>
						<line number="57" hits="1"/>
						<line number="59" hits="1"/>
						<line number="63" hits="1"/>
						<line number="64" hits="1"/>
						<line number="65" hits="1"/>
						<line number="66" hits="1"/>
						<line number="68" hits="1"/>
						<line number="75" hits="1"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="system.admin" line-rate="0.4312" branch-rate="0" complexity="0">
			<classes>
				<class name="__init__.py" filename="system/admin/__init__.py" complexity="0" line-rate="0.9167" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="24" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="1"/>
						<line number="56" hits="1"/>
						<line number="57" hits="0"/>
						<line number="58" hits="1"/>
						<line number="59" hits="0"/>
						<line number="60" hits="1"/>
						<line number="61" hits="0"/>
						<line number="63" hits="1"/>
						<line number="65" hits="1"/>
						<line number="68" hits="1"/>
						<line number="72" hits="1"/>
						<line number="73" hits="1"/>
						<line number="74" hits="1"/>
						<line number="75" hits="1"/>
						<line number="76" hits="1"/>
						<line number="77" hits="1"/>
						<line number="78" hits="1"/>
						<line number="79" hits="1"/>
						<line number="80" hits="1"/>
						<line number="82" hits="1"/>
					</lines>
				</class>
				<class name="agent.py" filename="system/admin/agent.py" complexity="0" line-rate="0.5538" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="25" hits="1"/>
						<line number="28" hits="1"/>
						<line number="39" hits="1"/>
						<line number="42" hits="1"/>
						<line number="43" hits="1"/>
						<line number="46" hits="1"/>
						<line number="47" hits="1"/>
						<line number="48" hits="1"/>
						<line number="49" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="54" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="0"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="59" hits="0"/>
						<line number="60" hits="0"/>
						<line number="62" hits="0"/>
						<line number="65" hits="1"/>
						<line number="66" hits="1"/>
						<line number="67" hits="1"/>
						<line number="68" hits="1"/>
						<line number="69" hits="1"/>
						<line number="70" hits="1"/>
						<line number="71" hits="1"/>
						<line number="72" hits="0"/>
						<line number="73" hits="0"/>
						<line number="74" hits="0"/>
						<line number="75" hits="0"/>
						<line number="76" hits="0"/>
						<line number="77" hits="0"/>
						<line number="78" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="82" hits="0"/>
						<line number="83" hits="0"/>
						<line number="84" hits="0"/>
						<line number="86" hits="0"/>
						<line number="87" hits="0"/>
						<line number="88" hits="0"/>
						<line number="89" hits="0"/>
						<line number="90" hits="0"/>
						<line number="91" hits="0"/>
						<line number="92" hits="0"/>
						<line number="93" hits="0"/>
						<line number="94" hits="0"/>
						<line number="95" hits="1"/>
						<line number="96" hits="1"/>
						<line number="98" hits="1"/>
						<line number="99" hits="1"/>
						<line number="100" hits="0"/>
						<line number="103" hits="1"/>
						<line number="107" hits="1"/>
						<line number="115" hits="1"/>
						<line number="116" hits="1"/>
						<line number="130" hits="1"/>
						<line number="131" hits="1"/>
						<line number="133" hits="1"/>
						<line number="134" hits="1"/>
						<line number="137" hits="1"/>
						<line number="139" hits="1"/>
						<line number="140" hits="0"/>
						<line number="142" hits="1"/>
						<line number="144" hits="1"/>
						<line number="145" hits="1"/>
						<line number="148" hits="1"/>
						<line number="149" hits="1"/>
						<line number="152" hits="1"/>
						<line number="153" hits="1"/>
						<line number="154" hits="1"/>
						<line number="156" hits="1"/>
						<line number="157" hits="1"/>
						<line number="160" hits="1"/>
						<line number="162" hits="1"/>
						<line number="163" hits="1"/>
						<line number="164" hits="1"/>
						<line number="167" hits="1"/>
						<line number="168" hits="0"/>
						<line number="169" hits="0"/>
						<line number="170" hits="0"/>
						<line number="172" hits="0"/>
						<line number="173" hits="0"/>
						<line number="176" hits="0"/>
						<line number="177" hits="0"/>
						<line number="178" hits="0"/>
						<line number="179" hits="0"/>
						<line number="180" hits="0"/>
						<line number="182" hits="1"/>
						<line number="187" hits="1"/>
						<line number="189" hits="1"/>
						<line number="190" hits="1"/>
						<line number="191" hits="1"/>
						<line number="192" hits="0"/>
						<line number="193" hits="1"/>
						<line number="197" hits="0"/>
						<line number="200" hits="1"/>
						<line number="203" hits="1"/>
						<line number="204" hits="1"/>
						<line number="212" hits="0"/>
						<line number="213" hits="0"/>
						<line number="215" hits="0"/>
						<line number="216" hits="0"/>
						<line number="217" hits="0"/>
						<line number="218" hits="0"/>
						<line number="225" hits="0"/>
						<line number="226" hits="0"/>
						<line number="227" hits="0"/>
						<line number="229" hits="0"/>
						<line number="230" hits="0"/>
						<line number="232" hits="0"/>
						<line number="233" hits="0"/>
						<line number="236" hits="1"/>
						<line number="238" hits="1"/>
					</lines>
				</class>
				<class name="byor.py" filename="system/admin/byor.py" complexity="0" line-rate="0.2444" branch-rate="0">
					<methods/>
					<lines>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="18" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="28" hits="1"/>
						<line number="46" hits="0"/>
						<line number="49" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="54" hits="0"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="59" hits="0"/>
						<line number="62" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="79" hits="0"/>
						<line number="88" hits="0"/>
						<line number="97" hits="0"/>
						<line number="98" hits="0"/>
						<line number="109" hits="0"/>
						<line number="122" hits="0"/>
						<line number="123" hits="0"/>
						<line number="124" hits="0"/>
						<line number="125" hits="0"/>
						<line number="126" hits="0"/>
						<line number="127" hits="0"/>
						<line number="129" hits="0"/>
						<line number="131" hits="0"/>
						<line number="132" hits="0"/>
						<line number="133" hits="0"/>
						<line number="134" hits="0"/>
						<line number="135" hits="0"/>
						<line number="136" hits="0"/>
						<line number="138" hits="0"/>
						<line number="139" hits="0"/>
						<line number="143" hits="0"/>
						<line number="146" hits="1"/>
						<line number="148" hits="1"/>
					</lines>
				</class>
				<class name="fixer.py" filename="system/admin/fixer.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
					</lines>
				</class>
				<class name="guard.py" filename="system/admin/guard.py" complexity="0" line-rate="0.5342" branch-rate="0">
					<methods/>
					<lines>
						<line number="31" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="1"/>
						<line number="42" hits="1"/>
						<line number="45" hits="1"/>
						<line number="51" hits="1"/>
						<line number="52" hits="1"/>
						<line number="53" hits="1"/>
						<line number="63" hits="0"/>
						<line number="64" hits="0"/>
						<line number="65" hits="0"/>
						<line number="80" hits="1"/>
						<line number="82" hits="0"/>
						<line number="89" hits="1"/>
						<line number="91" hits="1"/>
						<line number="104" hits="1"/>
						<line number="105" hits="0"/>
						<line number="106" hits="0"/>
						<line number="107" hits="1"/>
						<line number="110" hits="1"/>
						<line number="112" hits="0"/>
						<line number="113" hits="0"/>
						<line number="115" hits="0"/>
						<line number="116" hits="0"/>
						<line number="117" hits="0"/>
						<line number="118" hits="0"/>
						<line number="120" hits="0"/>
						<line number="121" hits="0"/>
						<line number="123" hits="0"/>
						<line number="124" hits="0"/>
						<line number="128" hits="0"/>
						<line number="129" hits="0"/>
						<line number="130" hits="0"/>
						<line number="131" hits="0"/>
						<line number="133" hits="0"/>
						<line number="134" hits="0"/>
						<line number="135" hits="0"/>
						<line number="136" hits="0"/>
						<line number="137" hits="0"/>
						<line number="138" hits="0"/>
						<line number="141" hits="1"/>
						<line number="143" hits="1"/>
						<line number="151" hits="1"/>
						<line number="153" hits="1"/>
						<line number="154" hits="1"/>
						<line number="201" hits="1"/>
						<line number="202" hits="1"/>
						<line number="203" hits="1"/>
						<line number="205" hits="1"/>
						<line number="206" hits="1"/>
						<line number="208" hits="1"/>
						<line number="209" hits="1"/>
						<line number="210" hits="1"/>
						<line number="211" hits="1"/>
						<line number="216" hits="1"/>
						<line number="217" hits="0"/>
						<line number="219" hits="1"/>
						<line number="221" hits="1"/>
						<line number="222" hits="1"/>
						<line number="224" hits="1"/>
						<line number="225" hits="1"/>
						<line number="255" hits="0"/>
						<line number="256" hits="0"/>
						<line number="262" hits="0"/>
						<line number="266" hits="0"/>
						<line number="267" hits="0"/>
						<line number="268" hits="0"/>
						<line number="272" hits="0"/>
					</lines>
				</class>
				<class name="guard_logic.py" filename="system/admin/guard_logic.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="3" hits="1"/>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="12" hits="1"/>
						<line number="15" hits="1"/>
						<line number="21" hits="1"/>
						<line number="24" hits="1"/>
						<line number="27" hits="1"/>
						<line number="30" hits="1"/>
					</lines>
				</class>
				<class name="keys.py" filename="system/admin/keys.py" complexity="0" line-rate="0.4444" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="34" hits="0"/>
						<line number="36" hits="0"/>
						<line number="37" hits="0"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="45" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="53" hits="0"/>
						<line number="58" hits="0"/>
						<line number="59" hits="0"/>
						<line number="63" hits="0"/>
						<line number="74" hits="0"/>
					</lines>
				</class>
				<class name="migrator.py" filename="system/admin/migrator.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
					</lines>
				</class>
				<class name="new.py" filename="system/admin/new.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="11" hits="1"/>
						<line number="14" hits="1"/>
					</lines>
				</class>
				<class name="proposals.py" filename="system/admin/proposals.py" complexity="0" line-rate="0.1894" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="31" hits="1"/>
						<line number="33" hits="1"/>
						<line number="36" hits="1"/>
						<line number="39" hits="1"/>
						<line number="40" hits="1"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="44" hits="0"/>
						<line number="45" hits="0"/>
						<line number="47" hits="0"/>
						<line number="48" hits="0"/>
						<line number="49" hits="0"/>
						<line number="51" hits="0"/>
						<line number="52" hits="0"/>
						<line number="54" hits="0"/>
						<line number="55" hits="0"/>
						<line number="56" hits="0"/>
						<line number="57" hits="0"/>
						<line number="61" hits="0"/>
						<line number="64" hits="0"/>
						<line number="65" hits="0"/>
						<line number="67" hits="0"/>
						<line number="68" hits="0"/>
						<line number="69" hits="0"/>
						<line number="72" hits="1"/>
						<line number="73" hits="1"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="82" hits="0"/>
						<line number="83" hits="0"/>
						<line number="84" hits="0"/>
						<line number="86" hits="0"/>
						<line number="87" hits="0"/>
						<line number="89" hits="0"/>
						<line number="90" hits="0"/>
						<line number="92" hits="0"/>
						<line number="96" hits="0"/>
						<line number="97" hits="0"/>
						<line number="100" hits="0"/>
						<line number="109" hits="0"/>
						<line number="110" hits="0"/>
						<line number="113" hits="1"/>
						<line number="114" hits="1"/>
						<line number="121" hits="0"/>
						<line number="122" hits="0"/>
						<line number="123" hits="0"/>
						<line number="124" hits="0"/>
						<line number="125" hits="0"/>
						<line number="127" hits="0"/>
						<line number="128" hits="0"/>
						<line number="129" hits="0"/>
						<line number="130" hits="0"/>
						<line number="131" hits="0"/>
						<line number="133" hits="0"/>
						<line number="134" hits="0"/>
						<line number="135" hits="0"/>
						<line number="139" hits="0"/>
						<line number="140" hits="0"/>
						<line number="141" hits="0"/>
						<line number="142" hits="0"/>
						<line number="143" hits="0"/>
						<line number="144" hits="0"/>
						<line number="145" hits="0"/>
						<line number="146" hits="0"/>
						<line number="147" hits="0"/>
						<line number="148" hits="0"/>
						<line number="151" hits="0"/>
						<line number="153" hits="0"/>
						<line number="156" hits="0"/>
						<line number="157" hits="0"/>
						<line number="158" hits="0"/>
						<line number="160" hits="0"/>
						<line number="163" hits="0"/>
						<line number="164" hits="0"/>
						<line number="166" hits="0"/>
						<line number="170" hits="0"/>
						<line number="174" hits="0"/>
						<line number="175" hits="0"/>
						<line number="178" hits="0"/>
						<line number="180" hits="0"/>
						<line number="181" hits="0"/>
						<line number="182" hits="0"/>
						<line number="188" hits="0"/>
						<line number="189" hits="0"/>
						<line number="190" hits="0"/>
						<line number="193" hits="0"/>
						<line number="195" hits="0"/>
						<line number="196" hits="0"/>
						<line number="197" hits="0"/>
						<line number="199" hits="0"/>
						<line number="200" hits="0"/>
						<line number="201" hits="0"/>
						<line number="207" hits="0"/>
						<line number="208" hits="0"/>
						<line number="211" hits="0"/>
						<line number="213" hits="0"/>
						<line number="214" hits="0"/>
						<line number="215" hits="0"/>
						<line number="216" hits="0"/>
						<line number="218" hits="0"/>
						<line number="219" hits="0"/>
						<line number="220" hits="0"/>
						<line number="222" hits="0"/>
						<line number="223" hits="0"/>
						<line number="224" hits="0"/>
						<line number="226" hits="0"/>
						<line number="227" hits="0"/>
						<line number="228" hits="0"/>
						<line number="229" hits="0"/>
						<line number="230" hits="0"/>
						<line number="231" hits="0"/>
						<line number="232" hits="0"/>
						<line number="233" hits="0"/>
						<line number="235" hits="0"/>
						<line number="238" hits="0"/>
						<line number="242" hits="1"/>
						<line number="244" hits="0"/>
					</lines>
				</class>
				<class name="reviewer.py" filename="system/admin/reviewer.py" complexity="0" line-rate="0.2125" branch-rate="0">
					<methods/>
					<lines>
						<line number="8" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="17" hits="1"/>
						<line number="20" hits="1"/>
						<line number="23" hits="1"/>
						<line number="25" hits="0"/>
						<line number="28" hits="1"/>
						<line number="30" hits="0"/>
						<line number="31" hits="0"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="36" hits="0"/>
						<line number="38" hits="0"/>
						<line number="40" hits="0"/>
						<line number="42" hits="0"/>
						<line number="44" hits="0"/>
						<line number="45" hits="0"/>
						<line number="46" hits="0"/>
						<line number="47" hits="0"/>
						<line number="48" hits="0"/>
						<line number="49" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="55" hits="0"/>
						<line number="56" hits="0"/>
						<line number="58" hits="0"/>
						<line number="62" hits="0"/>
						<line number="63" hits="0"/>
						<line number="64" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="68" hits="0"/>
						<line number="69" hits="0"/>
						<line number="70" hits="0"/>
						<line number="71" hits="0"/>
						<line number="73" hits="0"/>
						<line number="76" hits="1"/>
						<line number="89" hits="0"/>
						<line number="90" hits="0"/>
						<line number="91" hits="0"/>
						<line number="92" hits="0"/>
						<line number="93" hits="0"/>
						<line number="96" hits="1"/>
						<line number="112" hits="0"/>
						<line number="113" hits="0"/>
						<line number="117" hits="0"/>
						<line number="119" hits="0"/>
						<line number="120" hits="0"/>
						<line number="121" hits="0"/>
						<line number="122" hits="0"/>
						<line number="123" hits="0"/>
						<line number="125" hits="0"/>
						<line number="126" hits="0"/>
						<line number="128" hits="0"/>
						<line number="130" hits="0"/>
						<line number="131" hits="0"/>
						<line number="134" hits="0"/>
						<line number="135" hits="0"/>
						<line number="136" hits="0"/>
						<line number="137" hits="0"/>
						<line number="139" hits="0"/>
						<line number="143" hits="0"/>
						<line number="144" hits="0"/>
						<line number="148" hits="0"/>
						<line number="149" hits="0"/>
						<line number="151" hits="0"/>
						<line number="152" hits="0"/>
						<line number="153" hits="0"/>
						<line number="154" hits="0"/>
						<line number="157" hits="1"/>
						<line number="159" hits="1"/>
						<line number="160" hits="1"/>
						<line number="161" hits="1"/>
						<line number="162" hits="1"/>
					</lines>
				</class>
				<class name="utils.py" filename="system/admin/utils.py" complexity="0" line-rate="0.4878" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="21" hits="1"/>
						<line number="24" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="0"/>
						<line number="28" hits="1"/>
						<line number="29" hits="0"/>
						<line number="30" hits="1"/>
						<line number="37" hits="1"/>
						<line number="39" hits="0"/>
						<line number="42" hits="1"/>
						<line number="44" hits="0"/>
						<line number="47" hits="1"/>
						<line number="52" hits="0"/>
						<line number="58" hits="0"/>
						<line number="59" hits="0"/>
						<line number="60" hits="0"/>
						<line number="63" hits="1"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="70" hits="0"/>
						<line number="71" hits="0"/>
						<line number="74" hits="1"/>
						<line number="76" hits="0"/>
						<line number="77" hits="0"/>
						<line number="78" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="85" hits="0"/>
						<line number="97" hits="0"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="system.governance" line-rate="0.876" branch-rate="0" complexity="0">
			<classes>
				<class name="__init__.py" filename="system/governance/__init__.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines/>
				</class>
				<class name="constitutional_auditor.py" filename="system/governance/constitutional_auditor.py" complexity="0" line-rate="0.8621" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="25" hits="1"/>
						<line number="29" hits="1"/>
						<line number="32" hits="1"/>
						<line number="35" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="1"/>
						<line number="40" hits="1"/>
						<line number="47" hits="1"/>
						<line number="48" hits="1"/>
						<line number="51" hits="1"/>
						<line number="52" hits="1"/>
						<line number="53" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="0"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="59" hits="0"/>
						<line number="61" hits="1"/>
						<line number="63" hits="1"/>
						<line number="66" hits="1"/>
						<line number="68" hits="1"/>
						<line number="69" hits="1"/>
						<line number="70" hits="1"/>
						<line number="71" hits="1"/>
						<line number="72" hits="1"/>
						<line number="73" hits="1"/>
						<line number="76" hits="1"/>
						<line number="77" hits="1"/>
						<line number="78" hits="1"/>
						<line number="80" hits="1"/>
						<line number="82" hits="1"/>
						<line number="83" hits="1"/>
						<line number="85" hits="1"/>
						<line number="86" hits="1"/>
						<line number="87" hits="1"/>
						<line number="89" hits="1"/>
						<line number="90" hits="1"/>
						<line number="91" hits="1"/>
						<line number="92" hits="1"/>
						<line number="95" hits="1"/>
						<line number="96" hits="1"/>
						<line number="98" hits="1"/>
						<line number="99" hits="1"/>
						<line number="102" hits="1"/>
						<line number="103" hits="1"/>
						<line number="110" hits="1"/>
						<line number="111" hits="1"/>
						<line number="112" hits="1"/>
						<line number="113" hits="1"/>
						<line number="114" hits="1"/>
						<line number="115" hits="0"/>
						<line number="116" hits="0"/>
						<line number="120" hits="1"/>
						<line number="121" hits="1"/>
						<line number="122" hits="1"/>
						<line number="124" hits="1"/>
						<line number="126" hits="1"/>
						<line number="134" hits="1"/>
						<line number="135" hits="1"/>
						<line number="136" hits="1"/>
						<line number="137" hits="1"/>
						<line number="138" hits="1"/>
						<line number="139" hits="1"/>
						<line number="140" hits="1"/>
						<line number="141" hits="1"/>
						<line number="142" hits="1"/>
						<line number="143" hits="1"/>
						<line number="144" hits="0"/>
						<line number="145" hits="1"/>
						<line number="146" hits="1"/>
						<line number="147" hits="1"/>
						<line number="148" hits="1"/>
						<line number="149" hits="0"/>
						<line number="150" hits="0"/>
						<line number="153" hits="0"/>
						<line number="161" hits="1"/>
						<line number="162" hits="1"/>
						<line number="163" hits="1"/>
						<line number="165" hits="1"/>
						<line number="167" hits="1"/>
						<line number="168" hits="1"/>
						<line number="170" hits="1"/>
						<line number="171" hits="1"/>
						<line number="172" hits="1"/>
						<line number="174" hits="0"/>
						<line number="175" hits="0"/>
						<line number="177" hits="1"/>
						<line number="180" hits="1"/>
						<line number="182" hits="1"/>
						<line number="183" hits="1"/>
						<line number="184" hits="1"/>
						<line number="185" hits="1"/>
						<line number="186" hits="1"/>
						<line number="187" hits="1"/>
						<line number="188" hits="0"/>
						<line number="192" hits="0"/>
						<line number="193" hits="1"/>
						<line number="194" hits="0"/>
						<line number="195" hits="0"/>
						<line number="198" hits="1"/>
						<line number="199" hits="1"/>
					</lines>
				</class>
				<class name="models.py" filename="system/governance/models.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="10" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="system.governance.checks" line-rate="0.7517" branch-rate="0" complexity="0">
			<classes>
				<class name="__init__.py" filename="system/governance/checks/__init__.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines/>
				</class>
				<class name="architecture_checks.py" filename="system/governance/checks/architecture_checks.py" complexity="0" line-rate="0.8095" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="12" hits="1"/>
						<line number="18" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="27" hits="1"/>
						<line number="28" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="0"/>
						<line number="37" hits="0"/>
						<line number="38" hits="0"/>
						<line number="42" hits="0"/>
						<line number="46" hits="1"/>
						<line number="47" hits="1"/>
						<line number="55" hits="1"/>
					</lines>
				</class>
				<class name="base.py" filename="system/governance/checks/base.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="9" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
					</lines>
				</class>
				<class name="environment_checks.py" filename="system/governance/checks/environment_checks.py" complexity="0" line-rate="0.75" branch-rate="0">
					<methods/>
					<lines>
						<line number="3" hits="1"/>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="9" hits="1"/>
						<line number="13" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="0"/>
						<line number="30" hits="0"/>
						<line number="32" hits="1"/>
						<line number="33" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="0"/>
						<line number="40" hits="1"/>
						<line number="41" hits="0"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="45" hits="1"/>
						<line number="53" hits="1"/>
					</lines>
				</class>
				<class name="file_checks.py" filename="system/governance/checks/file_checks.py" complexity="0" line-rate="0.8211" branch-rate="0">
					<methods/>
					<lines>
						<line number="4" hits="1"/>
						<line number="5" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="11" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="21" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="26" hits="1"/>
						<line number="28" hits="1"/>
						<line number="29" hits="0"/>
						<line number="36" hits="0"/>
						<line number="38" hits="1"/>
						<line number="39" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="1"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="51" hits="1"/>
						<line number="52" hits="1"/>
						<line number="60" hits="1"/>
						<line number="63" hits="1"/>
						<line number="65" hits="1"/>
						<line number="66" hits="1"/>
						<line number="68" hits="1"/>
						<line number="73" hits="1"/>
						<line number="74" hits="1"/>
						<line number="75" hits="1"/>
						<line number="76" hits="0"/>
						<line number="77" hits="1"/>
						<line number="78" hits="1"/>
						<line number="79" hits="1"/>
						<line number="80" hits="1"/>
						<line number="81" hits="0"/>
						<line number="82" hits="0"/>
						<line number="90" hits="0"/>
						<line number="91" hits="0"/>
						<line number="100" hits="1"/>
						<line number="101" hits="1"/>
						<line number="108" hits="1"/>
						<line number="109" hits="1"/>
						<line number="112" hits="1"/>
						<line number="114" hits="1"/>
						<line number="115" hits="1"/>
						<line number="116" hits="1"/>
						<line number="118" hits="1"/>
						<line number="119" hits="0"/>
						<line number="121" hits="1"/>
						<line number="122" hits="1"/>
						<line number="128" hits="1"/>
						<line number="130" hits="1"/>
						<line number="131" hits="0"/>
						<line number="132" hits="0"/>
						<line number="140" hits="1"/>
						<line number="147" hits="1"/>
						<line number="149" hits="1"/>
						<line number="151" hits="1"/>
						<line number="152" hits="1"/>
						<line number="153" hits="0"/>
						<line number="155" hits="1"/>
						<line number="156" hits="1"/>
						<line number="157" hits="0"/>
						<line number="158" hits="0"/>
						<line number="160" hits="1"/>
						<line number="164" hits="1"/>
						<line number="167" hits="1"/>
						<line number="168" hits="1"/>
						<line number="169" hits="1"/>
						<line number="172" hits="1"/>
						<line number="174" hits="1"/>
						<line number="176" hits="1"/>
						<line number="177" hits="1"/>
						<line number="178" hits="1"/>
						<line number="179" hits="1"/>
						<line number="180" hits="1"/>
						<line number="182" hits="1"/>
						<line number="183" hits="1"/>
						<line number="184" hits="1"/>
						<line number="185" hits="0"/>
						<line number="186" hits="0"/>
						<line number="187" hits="1"/>
						<line number="189" hits="1"/>
						<line number="190" hits="1"/>
						<line number="192" hits="1"/>
						<line number="194" hits="1"/>
						<line number="195" hits="1"/>
						<line number="197" hits="1"/>
						<line number="198" hits="1"/>
						<line number="199" hits="1"/>
						<line number="200" hits="1"/>
						<line number="204" hits="1"/>
					</lines>
				</class>
				<class name="health_checks.py" filename="system/governance/checks/health_checks.py" complexity="0" line-rate="0.9" branch-rate="0">
					<methods/>
					<lines>
						<line number="4" hits="1"/>
						<line number="5" hits="1"/>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="12" hits="1"/>
						<line number="15" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="22" hits="1"/>
						<line number="24" hits="1"/>
						<line number="33" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="39" hits="1"/>
						<line number="42" hits="1"/>
						<line number="46" hits="1"/>
						<line number="47" hits="1"/>
						<line number="50" hits="1"/>
						<line number="51" hits="1"/>
						<line number="52" hits="1"/>
						<line number="53" hits="0"/>
						<line number="55" hits="1"/>
						<line number="56" hits="1"/>
						<line number="57" hits="1"/>
						<line number="58" hits="1"/>
						<line number="59" hits="1"/>
						<line number="62" hits="1"/>
						<line number="63" hits="1"/>
						<line number="64" hits="1"/>
						<line number="65" hits="1"/>
						<line number="66" hits="0"/>
						<line number="70" hits="0"/>
						<line number="73" hits="1"/>
						<line number="74" hits="1"/>
						<line number="77" hits="1"/>
						<line number="78" hits="0"/>
						<line number="80" hits="1"/>
						<line number="81" hits="1"/>
						<line number="82" hits="1"/>
						<line number="83" hits="1"/>
						<line number="85" hits="1"/>
						<line number="86" hits="1"/>
						<line number="87" hits="1"/>
						<line number="88" hits="1"/>
						<line number="93" hits="1"/>
						<line number="98" hits="1"/>
						<line number="99" hits="0"/>
						<line number="107" hits="1"/>
						<line number="108" hits="1"/>
						<line number="110" hits="1"/>
					</lines>
				</class>
				<class name="proposal_checks.py" filename="system/governance/checks/proposal_checks.py" complexity="0" line-rate="0.4043" branch-rate="0">
					<methods/>
					<lines>
						<line number="4" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="28" hits="0"/>
						<line number="29" hits="0"/>
						<line number="33" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="0"/>
						<line number="37" hits="1"/>
						<line number="43" hits="1"/>
						<line number="45" hits="0"/>
						<line number="46" hits="0"/>
						<line number="47" hits="0"/>
						<line number="48" hits="0"/>
						<line number="49" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="54" hits="1"/>
						<line number="55" hits="1"/>
						<line number="60" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="68" hits="0"/>
						<line number="70" hits="1"/>
						<line number="74" hits="0"/>
						<line number="75" hits="0"/>
						<line number="76" hits="0"/>
						<line number="77" hits="0"/>
						<line number="78" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="82" hits="0"/>
						<line number="91" hits="0"/>
						<line number="99" hits="0"/>
						<line number="100" hits="0"/>
						<line number="108" hits="0"/>
						<line number="110" hits="1"/>
						<line number="112" hits="0"/>
						<line number="113" hits="0"/>
						<line number="114" hits="0"/>
						<line number="115" hits="0"/>
						<line number="117" hits="0"/>
						<line number="118" hits="0"/>
						<line number="120" hits="0"/>
						<line number="121" hits="0"/>
						<line number="129" hits="0"/>
						<line number="131" hits="0"/>
						<line number="132" hits="0"/>
						<line number="133" hits="0"/>
						<line number="136" hits="0"/>
						<line number="145" hits="0"/>
						<line number="154" hits="0"/>
						<line number="155" hits="0"/>
						<line number="163" hits="0"/>
						<line number="168" hits="1"/>
						<line number="170" hits="1"/>
						<line number="171" hits="0"/>
						<line number="179" hits="1"/>
						<line number="180" hits="1"/>
						<line number="181" hits="1"/>
						<line number="189" hits="0"/>
						<line number="190" hits="0"/>
						<line number="191" hits="0"/>
						<line number="192" hits="0"/>
						<line number="193" hits="0"/>
						<line number="196" hits="1"/>
						<line number="198" hits="1"/>
						<line number="199" hits="1"/>
						<line number="200" hits="1"/>
						<line number="202" hits="0"/>
						<line number="203" hits="0"/>
						<line number="204" hits="0"/>
						<line number="205" hits="0"/>
						<line number="208" hits="1"/>
						<line number="210" hits="1"/>
						<line number="211" hits="1"/>
						<line number="212" hits="1"/>
						<line number="213" hits="1"/>
						<line number="220" hits="0"/>
						<line number="222" hits="0"/>
					</lines>
				</class>
				<class name="quality_checks.py" filename="system/governance/checks/quality_checks.py" complexity="0" line-rate="0.9459" branch-rate="0">
					<methods/>
					<lines>
						<line number="4" hits="1"/>
						<line number="5" hits="1"/>
						<line number="8" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="29" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="0"/>
						<line number="46" hits="1"/>
						<line number="49" hits="1"/>
						<line number="51" hits="1"/>
						<line number="52" hits="1"/>
						<line number="53" hits="1"/>
						<line number="54" hits="1"/>
						<line number="55" hits="1"/>
						<line number="57" hits="1"/>
						<line number="58" hits="1"/>
						<line number="59" hits="1"/>
						<line number="60" hits="1"/>
						<line number="61" hits="1"/>
						<line number="62" hits="1"/>
						<line number="63" hits="1"/>
						<line number="64" hits="1"/>
						<line number="65" hits="1"/>
						<line number="66" hits="1"/>
						<line number="67" hits="1"/>
						<line number="75" hits="1"/>
						<line number="76" hits="0"/>
						<line number="83" hits="1"/>
					</lines>
				</class>
				<class name="security_checks.py" filename="system/governance/checks/security_checks.py" complexity="0" line-rate="0.8108" branch-rate="0">
					<methods/>
					<lines>
						<line number="3" hits="1"/>
						<line number="4" hits="1"/>
						<line number="6" hits="1"/>
						<line number="9" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="20" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="0"/>
						<line number="41" hits="0"/>
						<line number="43" hits="1"/>
						<line number="44" hits="1"/>
						<line number="45" hits="1"/>
						<line number="47" hits="1"/>
						<line number="51" hits="1"/>
						<line number="52" hits="1"/>
						<line number="54" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="1"/>
						<line number="57" hits="0"/>
						<line number="59" hits="1"/>
						<line number="60" hits="1"/>
						<line number="61" hits="1"/>
						<line number="62" hits="1"/>
						<line number="63" hits="1"/>
						<line number="64" hits="1"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="74" hits="0"/>
						<line number="75" hits="0"/>
						<line number="77" hits="1"/>
						<line number="78" hits="1"/>
						<line number="86" hits="1"/>
					</lines>
				</class>
				<class name="structure_checks.py" filename="system/governance/checks/structure_checks.py" complexity="0" line-rate="0.8372" branch-rate="0">
					<methods/>
					<lines>
						<line number="4" hits="1"/>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="10" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="0"/>
						<line number="23" hits="0"/>
						<line number="30" hits="1"/>
						<line number="31" hits="1"/>
						<line number="38" hits="1"/>
						<line number="41" hits="1"/>
						<line number="43" hits="1"/>
						<line number="44" hits="1"/>
						<line number="45" hits="1"/>
						<line number="48" hits="1"/>
						<line number="53" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="0"/>
						<line number="64" hits="1"/>
						<line number="65" hits="1"/>
						<line number="72" hits="1"/>
						<line number="75" hits="1"/>
						<line number="77" hits="1"/>
						<line number="78" hits="1"/>
						<line number="79" hits="1"/>
						<line number="82" hits="1"/>
						<line number="83" hits="1"/>
						<line number="85" hits="1"/>
						<line number="91" hits="1"/>
						<line number="92" hits="1"/>
						<line number="93" hits="0"/>
						<line number="101" hits="1"/>
						<line number="102" hits="1"/>
						<line number="109" hits="1"/>
						<line number="112" hits="1"/>
						<line number="114" hits="1"/>
						<line number="115" hits="1"/>
						<line number="116" hits="1"/>
						<line number="117" hits="1"/>
						<line number="118" hits="1"/>
						<line number="121" hits="1"/>
						<line number="122" hits="0"/>
						<line number="123" hits="0"/>
						<line number="124" hits="0"/>
						<line number="131" hits="1"/>
						<line number="132" hits="1"/>
						<line number="139" hits="1"/>
						<line number="144" hits="1"/>
						<line number="146" hits="1"/>
						<line number="147" hits="1"/>
						<line number="148" hits="1"/>
						<line number="149" hits="1"/>
						<line number="150" hits="1"/>
						<line number="151" hits="1"/>
						<line number="152" hits="0"/>
						<line number="159" hits="0"/>
						<line number="160" hits="1"/>
						<line number="161" hits="1"/>
						<line number="164" hits="1"/>
						<line number="165" hits="0"/>
						<line number="166" hits="0"/>
						<line number="174" hits="1"/>
						<line number="177" hits="1"/>
						<line number="178" hits="1"/>
						<line number="179" hits="1"/>
						<line number="180" hits="0"/>
						<line number="181" hits="1"/>
						<line number="182" hits="1"/>
						<line number="183" hits="1"/>
						<line number="184" hits="1"/>
						<line number="185" hits="1"/>
						<line number="186" hits="1"/>
						<line number="187" hits="1"/>
						<line number="190" hits="1"/>
						<line number="191" hits="0"/>
						<line number="192" hits="0"/>
						<line number="199" hits="1"/>
						<line number="200" hits="1"/>
						<line number="207" hits="1"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="system.guard" line-rate="0.8214" branch-rate="0" complexity="0">
			<classes>
				<class name="capability_discovery.py" filename="system/guard/capability_discovery.py" complexity="0" line-rate="0.619" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="16" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="0"/>
						<line number="28" hits="0"/>
						<line number="29" hits="0"/>
						<line number="32" hits="0"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="1"/>
						<line number="40" hits="0"/>
						<line number="41" hits="0"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="48" hits="1"/>
						<line number="52" hits="1"/>
					</lines>
				</class>
				<class name="drift_detector.py" filename="system/guard/drift_detector.py" complexity="0" line-rate="0.9" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="15" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="28" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="40" hits="1"/>
						<line number="43" hits="1"/>
						<line number="45" hits="0"/>
						<line number="46" hits="0"/>
					</lines>
				</class>
				<class name="models.py" filename="system/guard/models.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="29" hits="1"/>
						<line number="31" hits="1"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="system.guard.discovery" line-rate="0.5625" branch-rate="0" complexity="0">
			<classes>
				<class name="from_kgb.py" filename="system/guard/discovery/from_kgb.py" complexity="0" line-rate="0.88" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="15" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="24" hits="0"/>
						<line number="27" hits="1"/>
						<line number="29" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="0"/>
						<line number="43" hits="0"/>
					</lines>
				</class>
				<class name="from_manifest.py" filename="system/guard/discovery/from_manifest.py" complexity="0" line-rate="0.8421" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="0"/>
						<line number="14" hits="0"/>
						<line number="16" hits="1"/>
						<line number="19" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="29" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="0"/>
						<line number="34" hits="1"/>
						<line number="37" hits="1"/>
						<line number="43" hits="1"/>
						<line number="44" hits="0"/>
						<line number="46" hits="1"/>
						<line number="47" hits="1"/>
						<line number="49" hits="1"/>
						<line number="50" hits="1"/>
						<line number="51" hits="1"/>
						<line number="52" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="1"/>
						<line number="57" hits="1"/>
						<line number="59" hits="1"/>
						<line number="60" hits="1"/>
						<line number="61" hits="1"/>
						<line number="62" hits="0"/>
						<line number="64" hits="0"/>
						<line number="66" hits="1"/>
					</lines>
				</class>
				<class name="from_source_scan.py" filename="system/guard/discovery/from_source_scan.py" complexity="0" line-rate="0.1837" branch-rate="0">
					<methods/>
					<lines>
						<line number="2" hits="1"/>
						<line number="4" hits="1"/>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="0"/>
						<line number="15" hits="0"/>
						<line number="16" hits="0"/>
						<line number="18" hits="0"/>
						<line number="19" hits="0"/>
						<line number="20" hits="0"/>
						<line number="21" hits="0"/>
						<line number="24" hits="1"/>
						<line number="29" hits="0"/>
						<line number="31" hits="0"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="34" hits="0"/>
						<line number="35" hits="0"/>
						<line number="37" hits="0"/>
						<line number="38" hits="0"/>
						<line number="39" hits="0"/>
						<line number="42" hits="1"/>
						<line number="52" hits="0"/>
						<line number="55" hits="0"/>
						<line number="56" hits="0"/>
						<line number="59" hits="0"/>
						<line number="61" hits="0"/>
						<line number="62" hits="0"/>
						<line number="63" hits="0"/>
						<line number="64" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="69" hits="0"/>
						<line number="70" hits="0"/>
						<line number="71" hits="0"/>
						<line number="74" hits="0"/>
						<line number="75" hits="0"/>
						<line number="76" hits="0"/>
						<line number="77" hits="0"/>
						<line number="78" hits="0"/>
						<line number="79" hits="0"/>
						<line number="81" hits="0"/>
						<line number="84" hits="0"/>
						<line number="85" hits="0"/>
						<line number="86" hits="0"/>
					</lines>
				</class>
			</classes>
		</package>
		<package name="system.tools" line-rate="0.6029" branch-rate="0" complexity="0">
			<classes>
				<class name="__init__.py" filename="system/tools/__init__.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines/>
				</class>
				<class name="ast_utils.py" filename="system/tools/ast_utils.py" complexity="0" line-rate="0.9762" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="16" hits="1"/>
						<line number="22" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="29" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="1"/>
						<line number="36" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="1"/>
						<line number="40" hits="1"/>
						<line number="42" hits="1"/>
						<line number="48" hits="0"/>
						<line number="49" hits="1"/>
						<line number="52" hits="1"/>
						<line number="54" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="1"/>
						<line number="57" hits="1"/>
						<line number="58" hits="1"/>
						<line number="59" hits="1"/>
						<line number="60" hits="1"/>
						<line number="63" hits="1"/>
						<line number="65" hits="1"/>
						<line number="66" hits="1"/>
						<line number="67" hits="1"/>
						<line number="68" hits="1"/>
						<line number="69" hits="1"/>
						<line number="70" hits="1"/>
						<line number="71" hits="1"/>
						<line number="74" hits="1"/>
						<line number="76" hits="1"/>
						<line number="85" hits="1"/>
						<line number="87" hits="1"/>
					</lines>
				</class>
				<class name="ast_visitor.py" filename="system/tools/ast_visitor.py" complexity="0" line-rate="0.5" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="12" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="18" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="27" hits="1"/>
						<line number="30" hits="1"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="34" hits="0"/>
						<line number="35" hits="0"/>
						<line number="37" hits="1"/>
						<line number="39" hits="0"/>
						<line number="41" hits="0"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="44" hits="0"/>
						<line number="46" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="55" hits="0"/>
						<line number="57" hits="1"/>
						<line number="59" hits="0"/>
						<line number="61" hits="1"/>
						<line number="63" hits="0"/>
						<line number="65" hits="1"/>
						<line number="67" hits="0"/>
					</lines>
				</class>
				<class name="codegraph_builder.py" filename="system/tools/codegraph_builder.py" complexity="0" line-rate="0.8621" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="8" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="24" hits="1"/>
						<line number="27" hits="1"/>
						<line number="30" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="1"/>
						<line number="42" hits="1"/>
						<line number="43" hits="1"/>
						<line number="46" hits="1"/>
						<line number="47" hits="1"/>
						<line number="48" hits="1"/>
						<line number="49" hits="1"/>
						<line number="50" hits="1"/>
						<line number="52" hits="1"/>
						<line number="54" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="0"/>
						<line number="57" hits="0"/>
						<line number="58" hits="1"/>
						<line number="59" hits="1"/>
						<line number="61" hits="1"/>
						<line number="63" hits="1"/>
						<line number="65" hits="1"/>
						<line number="66" hits="1"/>
						<line number="69" hits="1"/>
						<line number="70" hits="1"/>
						<line number="71" hits="1"/>
						<line number="73" hits="0"/>
						<line number="75" hits="1"/>
						<line number="81" hits="1"/>
						<line number="84" hits="1"/>
						<line number="89" hits="1"/>
						<line number="91" hits="1"/>
						<line number="92" hits="1"/>
						<line number="94" hits="1"/>
						<line number="95" hits="1"/>
						<line number="96" hits="0"/>
						<line number="97" hits="0"/>
						<line number="101" hits="0"/>
						<line number="104" hits="1"/>
						<line number="105" hits="1"/>
						<line number="108" hits="1"/>
						<line number="109" hits="1"/>
						<line number="111" hits="0"/>
						<line number="112" hits="0"/>
						<line number="115" hits="1"/>
						<line number="116" hits="1"/>
					</lines>
				</class>
				<class name="docstring_adder.py" filename="system/tools/docstring_adder.py" complexity="0" line-rate="0.1724" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="13" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="28" hits="1"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="34" hits="0"/>
						<line number="35" hits="0"/>
						<line number="37" hits="0"/>
						<line number="38" hits="0"/>
						<line number="39" hits="0"/>
						<line number="40" hits="0"/>
						<line number="43" hits="0"/>
						<line number="44" hits="0"/>
						<line number="45" hits="0"/>
						<line number="46" hits="0"/>
						<line number="48" hits="0"/>
						<line number="50" hits="0"/>
						<line number="51" hits="0"/>
						<line number="52" hits="0"/>
						<line number="54" hits="0"/>
						<line number="55" hits="0"/>
						<line number="58" hits="1"/>
						<line number="62" hits="0"/>
						<line number="64" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="69" hits="0"/>
						<line number="71" hits="0"/>
						<line number="72" hits="0"/>
						<line number="74" hits="0"/>
						<line number="75" hits="0"/>
						<line number="76" hits="0"/>
						<line number="77" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="82" hits="0"/>
						<line number="95" hits="0"/>
						<line number="96" hits="0"/>
						<line number="99" hits="0"/>
						<line number="101" hits="0"/>
						<line number="102" hits="0"/>
						<line number="103" hits="0"/>
						<line number="105" hits="0"/>
						<line number="106" hits="0"/>
						<line number="113" hits="0"/>
						<line number="115" hits="0"/>
						<line number="116" hits="0"/>
						<line number="119" hits="0"/>
						<line number="121" hits="0"/>
						<line number="126" hits="0"/>
						<line number="127" hits="0"/>
						<line number="128" hits="0"/>
						<line number="131" hits="0"/>
						<line number="133" hits="0"/>
						<line number="136" hits="0"/>
						<line number="137" hits="0"/>
						<line number="141" hits="0"/>
						<line number="143" hits="0"/>
						<line number="144" hits="0"/>
						<line number="145" hits="0"/>
						<line number="147" hits="0"/>
						<line number="148" hits="0"/>
						<line number="151" hits="1"/>
						<line number="153" hits="0"/>
						<line number="155" hits="0"/>
						<line number="156" hits="0"/>
						<line number="157" hits="0"/>
						<line number="158" hits="0"/>
						<line number="159" hits="0"/>
						<line number="160" hits="0"/>
						<line number="161" hits="0"/>
						<line number="162" hits="0"/>
						<line number="163" hits="0"/>
						<line number="165" hits="0"/>
						<line number="166" hits="0"/>
						<line number="167" hits="0"/>
						<line number="168" hits="0"/>
						<line number="175" hits="0"/>
						<line number="176" hits="0"/>
						<line number="179" hits="0"/>
						<line number="181" hits="0"/>
						<line number="185" hits="0"/>
						<line number="187" hits="0"/>
						<line number="189" hits="0"/>
						<line number="190" hits="0"/>
						<line number="192" hits="0"/>
						<line number="193" hits="0"/>
						<line number="198" hits="0"/>
						<line number="200" hits="0"/>
						<line number="204" hits="1"/>
						<line number="212" hits="0"/>
						<line number="214" hits="0"/>
						<line number="215" hits="0"/>
						<line number="216" hits="0"/>
						<line number="217" hits="0"/>
						<line number="218" hits="0"/>
						<line number="219" hits="0"/>
						<line number="220" hits="0"/>
						<line number="221" hits="0"/>
						<line number="222" hits="0"/>
						<line number="223" hits="0"/>
						<line number="226" hits="1"/>
						<line number="227" hits="0"/>
					</lines>
				</class>
				<class name="domain_mapper.py" filename="system/tools/domain_mapper.py" complexity="0" line-rate="0.7353" branch-rate="0">
					<methods/>
					<lines>
						<line number="2" hits="1"/>
						<line number="3" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="11" hits="1"/>
						<line number="14" hits="1"/>
						<line number="17" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="31" hits="1"/>
						<line number="33" hits="1"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="0"/>
						<line number="38" hits="0"/>
						<line number="39" hits="0"/>
						<line number="41" hits="1"/>
						<line number="42" hits="1"/>
						<line number="43" hits="0"/>
						<line number="44" hits="0"/>
						<line number="46" hits="1"/>
						<line number="47" hits="1"/>
						<line number="48" hits="1"/>
						<line number="49" hits="1"/>
						<line number="50" hits="1"/>
						<line number="53" hits="1"/>
						<line number="56" hits="1"/>
						<line number="58" hits="1"/>
						<line number="61" hits="1"/>
						<line number="65" hits="1"/>
						<line number="66" hits="0"/>
						<line number="68" hits="1"/>
						<line number="69" hits="0"/>
						<line number="70" hits="0"/>
						<line number="71" hits="0"/>
						<line number="72" hits="0"/>
						<line number="75" hits="0"/>
						<line number="78" hits="1"/>
						<line number="81" hits="1"/>
						<line number="85" hits="1"/>
						<line number="87" hits="1"/>
						<line number="90" hits="1"/>
						<line number="92" hits="1"/>
						<line number="93" hits="1"/>
						<line number="94" hits="1"/>
						<line number="97" hits="1"/>
						<line number="98" hits="0"/>
						<line number="99" hits="0"/>
						<line number="100" hits="0"/>
						<line number="101" hits="0"/>
						<line number="104" hits="0"/>
						<line number="106" hits="1"/>
						<line number="107" hits="1"/>
						<line number="109" hits="0"/>
						<line number="112" hits="0"/>
						<line number="114" hits="1"/>
						<line number="116" hits="1"/>
						<line number="117" hits="1"/>
						<line number="123" hits="1"/>
						<line number="124" hits="1"/>
						<line number="125" hits="1"/>
						<line number="126" hits="1"/>
						<line number="127" hits="1"/>
						<line number="128" hits="1"/>
					</lines>
				</class>
				<class name="entry_point_detector.py" filename="system/tools/entry_point_detector.py" complexity="0" line-rate="0.9318" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="12" hits="1"/>
						<line number="14" hits="1"/>
						<line number="17" hits="1"/>
						<line number="20" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="27" hits="1"/>
						<line number="29" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="0"/>
						<line number="32" hits="0"/>
						<line number="33" hits="1"/>
						<line number="34" hits="1"/>
						<line number="36" hits="1"/>
						<line number="38" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="1"/>
						<line number="42" hits="1"/>
						<line number="43" hits="1"/>
						<line number="44" hits="1"/>
						<line number="45" hits="1"/>
						<line number="46" hits="1"/>
						<line number="48" hits="1"/>
						<line number="50" hits="1"/>
						<line number="52" hits="1"/>
						<line number="59" hits="1"/>
						<line number="63" hits="1"/>
						<line number="64" hits="1"/>
						<line number="65" hits="1"/>
						<line number="66" hits="1"/>
						<line number="70" hits="1"/>
						<line number="72" hits="1"/>
						<line number="73" hits="0"/>
						<line number="74" hits="1"/>
						<line number="75" hits="1"/>
						<line number="76" hits="1"/>
					</lines>
				</class>
				<class name="file_scanner.py" filename="system/tools/file_scanner.py" complexity="0" line-rate="0.86" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="22" hits="1"/>
						<line number="25" hits="1"/>
						<line number="28" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="1"/>
						<line number="36" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="1"/>
						<line number="40" hits="1"/>
						<line number="41" hits="1"/>
						<line number="44" hits="1"/>
						<line number="45" hits="1"/>
						<line number="48" hits="1"/>
						<line number="49" hits="1"/>
						<line number="52" hits="1"/>
						<line number="54" hits="1"/>
						<line number="56" hits="0"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="59" hits="0"/>
						<line number="60" hits="0"/>
						<line number="61" hits="0"/>
						<line number="63" hits="1"/>
						<line number="71" hits="1"/>
						<line number="72" hits="0"/>
						<line number="75" hits="1"/>
						<line number="78" hits="1"/>
						<line number="79" hits="1"/>
						<line number="82" hits="1"/>
						<line number="83" hits="1"/>
						<line number="84" hits="1"/>
						<line number="87" hits="1"/>
						<line number="89" hits="1"/>
						<line number="125" hits="1"/>
						<line number="128" hits="1"/>
						<line number="129" hits="1"/>
						<line number="130" hits="1"/>
						<line number="131" hits="1"/>
						<line number="133" hits="1"/>
					</lines>
				</class>
				<class name="graph_serializer.py" filename="system/tools/graph_serializer.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="16" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="25" hits="1"/>
						<line number="33" hits="1"/>
						<line number="34" hits="1"/>
						<line number="36" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="1"/>
						<line number="43" hits="1"/>
						<line number="45" hits="1"/>
						<line number="55" hits="1"/>
						<line number="56" hits="1"/>
						<line number="58" hits="1"/>
						<line number="60" hits="1"/>
						<line number="61" hits="1"/>
						<line number="63" hits="1"/>
						<line number="64" hits="1"/>
					</lines>
				</class>
				<class name="manifest_migrator.py" filename="system/tools/manifest_migrator.py" complexity="0" line-rate="0.2281" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="10" hits="1"/>
						<line number="12" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="24" hits="1"/>
						<line number="34" hits="0"/>
						<line number="37" hits="0"/>
						<line number="38" hits="0"/>
						<line number="46" hits="0"/>
						<line number="49" hits="0"/>
						<line number="51" hits="0"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="57" hits="0"/>
						<line number="58" hits="0"/>
						<line number="65" hits="0"/>
						<line number="66" hits="0"/>
						<line number="67" hits="0"/>
						<line number="68" hits="0"/>
						<line number="69" hits="0"/>
						<line number="70" hits="0"/>
						<line number="77" hits="0"/>
						<line number="78" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="82" hits="0"/>
						<line number="84" hits="0"/>
						<line number="87" hits="0"/>
						<line number="88" hits="0"/>
						<line number="89" hits="0"/>
						<line number="90" hits="0"/>
						<line number="91" hits="0"/>
						<line number="92" hits="0"/>
						<line number="99" hits="0"/>
						<line number="100" hits="0"/>
						<line number="101" hits="0"/>
						<line number="103" hits="0"/>
						<line number="104" hits="0"/>
						<line number="105" hits="0"/>
						<line number="106" hits="0"/>
						<line number="107" hits="0"/>
						<line number="115" hits="0"/>
						<line number="116" hits="0"/>
						<line number="117" hits="0"/>
						<line number="118" hits="0"/>
						<line number="119" hits="0"/>
						<line number="124" hits="0"/>
						<line number="127" hits="1"/>
						<line number="128" hits="0"/>
					</lines>
				</class>
				<class name="models.py" filename="system/tools/models.py" complexity="0" line-rate="1" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="16" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="20" hits="1"/>
						<line number="21" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="1"/>
						<line number="27" hits="1"/>
						<line number="28" hits="1"/>
						<line number="29" hits="1"/>
						<line number="30" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="1"/>
					</lines>
				</class>
				<class name="pattern_matcher.py" filename="system/tools/pattern_matcher.py" complexity="0" line-rate="0.8125" branch-rate="0">
					<methods/>
					<lines>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="16" hits="1"/>
						<line number="19" hits="1"/>
						<line number="27" hits="1"/>
						<line number="28" hits="1"/>
						<line number="30" hits="1"/>
						<line number="37" hits="1"/>
						<line number="41" hits="1"/>
						<line number="42" hits="1"/>
						<line number="43" hits="1"/>
						<line number="45" hits="1"/>
						<line number="46" hits="1"/>
						<line number="49" hits="1"/>
						<line number="50" hits="1"/>
						<line number="51" hits="1"/>
						<line number="54" hits="1"/>
						<line number="56" hits="1"/>
						<line number="64" hits="1"/>
						<line number="65" hits="1"/>
						<line number="66" hits="1"/>
						<line number="68" hits="1"/>
						<line number="71" hits="1"/>
						<line number="73" hits="1"/>
						<line number="74" hits="1"/>
						<line number="75" hits="1"/>
						<line number="76" hits="1"/>
						<line number="77" hits="0"/>
						<line number="78" hits="0"/>
						<line number="81" hits="0"/>
						<line number="83" hits="1"/>
						<line number="84" hits="1"/>
						<line number="85" hits="1"/>
						<line number="86" hits="1"/>
						<line number="87" hits="0"/>
						<line number="88" hits="1"/>
						<line number="89" hits="1"/>
						<line number="91" hits="1"/>
						<line number="92" hits="1"/>
						<line number="93" hits="1"/>
						<line number="94" hits="1"/>
						<line number="95" hits="1"/>
						<line number="98" hits="1"/>
						<line number="99" hits="0"/>
						<line number="100" hits="1"/>
						<line number="101" hits="1"/>
						<line number="102" hits="1"/>
						<line number="103" hits="0"/>
						<line number="104" hits="0"/>
						<line number="107" hits="0"/>
						<line number="109" hits="0"/>
						<line number="111" hits="1"/>
						<line number="112" hits="1"/>
						<line number="113" hits="1"/>
						<line number="115" hits="1"/>
						<line number="116" hits="1"/>
						<line number="118" hits="1"/>
						<line number="119" hits="0"/>
						<line number="120" hits="0"/>
						<line number="121" hits="0"/>
					</lines>
				</class>
				<class name="project_structure.py" filename="system/tools/project_structure.py" complexity="0" line-rate="0.8333" branch-rate="0">
					<methods/>
					<lines>
						<line number="5" hits="1"/>
						<line number="6" hits="1"/>
						<line number="7" hits="1"/>
						<line number="9" hits="1"/>
						<line number="11" hits="1"/>
						<line number="14" hits="1"/>
						<line number="17" hits="1"/>
						<line number="20" hits="1"/>
						<line number="22" hits="1"/>
						<line number="23" hits="1"/>
						<line number="24" hits="1"/>
						<line number="25" hits="1"/>
						<line number="26" hits="0"/>
						<line number="29" hits="1"/>
						<line number="31" hits="1"/>
						<line number="32" hits="1"/>
						<line number="33" hits="0"/>
						<line number="35" hits="1"/>
						<line number="36" hits="1"/>
						<line number="37" hits="1"/>
						<line number="38" hits="1"/>
						<line number="39" hits="0"/>
						<line number="40" hits="0"/>
						<line number="41" hits="0"/>
						<line number="44" hits="1"/>
						<line number="46" hits="1"/>
						<line number="49" hits="1"/>
						<line number="56" hits="1"/>
						<line number="57" hits="1"/>
						<line number="59" hits="1"/>
					</lines>
				</class>
				<class name="scaffolder.py" filename="system/tools/scaffolder.py" complexity="0" line-rate="0.1899" branch-rate="0">
					<methods/>
					<lines>
						<line number="7" hits="1"/>
						<line number="8" hits="1"/>
						<line number="10" hits="1"/>
						<line number="11" hits="1"/>
						<line number="13" hits="1"/>
						<line number="14" hits="1"/>
						<line number="15" hits="1"/>
						<line number="17" hits="1"/>
						<line number="18" hits="1"/>
						<line number="19" hits="1"/>
						<line number="22" hits="1"/>
						<line number="25" hits="1"/>
						<line number="32" hits="0"/>
						<line number="33" hits="0"/>
						<line number="35" hits="0"/>
						<line number="38" hits="0"/>
						<line number="40" hits="0"/>
						<line number="42" hits="0"/>
						<line number="43" hits="0"/>
						<line number="45" hits="0"/>
						<line number="46" hits="0"/>
						<line number="50" hits="1"/>
						<line number="52" hits="0"/>
						<line number="53" hits="0"/>
						<line number="54" hits="0"/>
						<line number="58" hits="0"/>
						<line number="59" hits="0"/>
						<line number="60" hits="0"/>
						<line number="61" hits="0"/>
						<line number="64" hits="0"/>
						<line number="67" hits="0"/>
						<line number="68" hits="0"/>
						<line number="72" hits="0"/>
						<line number="79" hits="0"/>
						<line number="80" hits="0"/>
						<line number="81" hits="0"/>
						<line number="82" hits="0"/>
						<line number="84" hits="0"/>
						<line number="85" hits="0"/>
						<line number="86" hits="0"/>
						<line number="88" hits="0"/>
						<line number="89" hits="0"/>
						<line number="92" hits="0"/>
						<line number="97" hits="0"/>
						<line number="99" hits="0"/>
						<line number="100" hits="0"/>
						<line number="101" hits="0"/>
						<line number="102" hits="0"/>
						<line number="103" hits="0"/>
						<line number="104" hits="0"/>
						<line number="108" hits="0"/>
						<line number="110" hits="1"/>
						<line number="112" hits="0"/>
						<line number="113" hits="0"/>
						<line number="114" hits="0"/>
						<line number="115" hits="0"/>
						<line number="118" hits="1"/>
						<line number="135" hits="0"/>
						<line number="136" hits="0"/>
						<line number="139" hits="0"/>
						<line number="140" hits="0"/>
						<line number="141" hits="0"/>
						<line number="146" hits="0"/>
						<line number="147" hits="0"/>
						<line number="148" hits="0"/>
						<line number="149" hits="0"/>
						<line number="150" hits="0"/>
						<line number="153" hits="0"/>
						<line number="155" hits="0"/>
						<line number="156" hits="0"/>
						<line number="157" hits="0"/>
						<line number="158" hits="0"/>
						<line number="159" hits="0"/>
						<line number="160" hits="0"/>
						<line number="162" hits="0"/>
						<line number="163" hits="0"/>
						<line number="164" hits="0"/>
						<line number="167" hits="0"/>
						<line number="168" hits="0"/>
					</lines>
				</class>
			</classes>
		</package>
	</packages>
</coverage>

--- END OF FILE ./coverage.xml ---

--- START OF FILE ./docs/01_PHILOSOPHY.md ---
# 1. The CORE Philosophy

## Prime Directive

**CORE exists to transform human intent into complete, evolving software systems — without drift, duplication, or degradation.**

It does not merely generate code; it **governs**, **learns**, and **rewrites** itself under the authority of an explicit, machine-readable constitution. It is a system designed to build other systems, safely and transparently.

## The CORE Belief System

Our architecture is founded on a set of core beliefs about the future of software development:

1.  **Intent, Not Instructions:** Software development should be about declaring a desired outcome (`intent`), not writing a list of procedural steps (`instructions`).
2.  **Governance is a Feature:** In a world of autonomous AI agents, safety, alignment, and auditability are not afterthoughts—they are the primary features of a trustworthy system.
3.  **Code is a Liability:** All code must justify its existence. It must be traceable to a declared purpose, validated against constitutional principles, and be as simple as possible. Unnecessary or un-auditable code is a source of risk.
4.  **A System Must Know Itself:** To evolve safely, a system must have a deep and accurate understanding of its own structure, capabilities, and rules. Self-awareness (`introspection`) is the prerequisite for self-improvement.

## The Ten-Phase Loop of Reasoned Action

All autonomous actions in CORE are governed by a ten-phase loop. This structure ensures that every action is deliberate, justified, and validated. It prevents the system from taking impulsive or un-auditable shortcuts.

The phases are:

1.  **GOAL:** A high-level objective is received from a human operator.
    *(e.g., "Add cryptographic signing to the approval process.")*

2.  **WHY:** The system links the goal to a core constitutional principle.
    *(e.g., "This serves the `safe_by_default` principle.")*

3.  **INTENT:** The goal and its justification are formalized into a clear, machine-readable intent.
    *(e.g., Formalize the request into a plan to modify the `core-admin` tool.)*

4.  **AGENT:** The system selects the appropriate agent(s) for the task.
    *(e.g., The `PlannerAgent` is assigned.)*

5.  **MEANS:** The agent consults its capabilities and knowledge to determine *how* it can achieve the intent.
    *(e.g., The agent knows it has `code_generation` and `introspection` capabilities.)*

6.  **PLAN:** The agent produces a detailed, step-by-step, auditable plan.
    *(e.g., 1. Add `cryptography` library. 2. Add `keygen` function. 3. Modify `approve` function...)*

7.  **ACTION:** The system executes the plan, one validated step at a time.
    *(e.g., The `GeneratorAgent` writes new code to files.)*

8.  **FEEDBACK:** The system's "immune system" (`ConstitutionalAuditor`, `pytest`, linters) provides feedback on the action.
    *(e.g., "The new code fails a linting check." or "All tests pass.")*

9.  **ADAPTATION:** The system uses the feedback to self-correct or confirm the change.
    *(e.g., The `SelfCorrectionEngine` fixes the linting error, or `GitService` commits the successful change.)*

10. **EVOLUTION:** The system updates its self-image (`KnowledgeGraph`) to reflect its new state, completing the loop.

This loop ensures that CORE does not simply act, but *reasons*. Every change is a deliberate, auditable, and constitutionally-aligned evolution.
--- END OF FILE ./docs/01_PHILOSOPHY.md ---

--- START OF FILE ./docs/02_ARCHITECTURE.md ---
# 2. The CORE Architecture

## The Mind-Body Problem, Solved

The central architectural pattern in CORE is a strict separation between the system's "Mind" and its "Body."

-   **The Mind (`.intent/`):** A declarative, version-controlled, and human-readable collection of files that represents the system's complete self-knowledge, purpose, and rules. It is the single source of truth for what the system *is* and *should be*.
-   **The Body (`src/`):** An imperative, executable collection of Python modules that acts upon the world. Its sole purpose is to carry out the will of the Mind, and its every action is governed by the rules declared within the Mind.

This separation is not just a convention; it is a constitutional law enforced by the system itself. The `ConstitutionalAuditor` is the bridge between the two, constantly ensuring the Body is in perfect alignment with the Mind.

## The Anatomy of the Mind (`.intent/`)

The `.intent/` directory is structured to provide a complete and transparent view of the system's governance.

| Directory | Purpose | Key Files |
|---|---|---|
| **`/mission`** | **The Constitution's Soul:** High-level, philosophical principles. | `principles.yaml`, `northstar.yaml` |
| **`/policies`** | **The Constitution's Laws:** Specific, machine-readable rules that govern agent behavior. | `safety_policies.yaml`, `intent_guard.yaml` |
| **`/knowledge`** | **The System's Self-Image:** Declarative knowledge about the system's own structure. | `knowledge_graph.json`, `source_structure.yaml` |
| **`/constitution`** | **The Machinery of Governance:** Defines the human operators and processes for changing the constitution. | `approvers.yaml` |
| **`/proposals`** | **The Legislative Floor:** A safe, temporary "sandbox" for drafting and signing proposed constitutional changes. | `cr-*.yaml` |
| **`/config`** | **Environmental Awareness:** Declares the system's dependencies on its runtime environment. | `runtime_requirements.yaml` |
| **`/schemas`** | **The Blueprint:** JSON schemas that define the structure of the knowledge files. | `knowledge_graph_entry.schema.json` |

## The Anatomy of the Body (`src/`)

The `src/` directory is organized into strict architectural **domains**. These domains are defined in `.intent/knowledge/source_structure.yaml`, and cross-domain communication is tightly controlled by rules enforced by the `ConstitutionalAuditor`.

| Directory | Domain | Responsibility |
|---|---|---|
| **`/core`** | `core` | The central nervous system. Handles the main application loop, API, and core services like file handling and Git integration. |
| **`/agents`** | `agents` | The specialized AI actors. Contains the `PlannerAgent` and its related models and utilities. |
| **`/system`** | `system` | The machinery of self-governance. Contains the `ConstitutionalAuditor`, `core-admin` CLI, and introspection tools. |
| **`/shared`** | `shared` | The common library. Provides shared utilities like logging and configuration loading that are accessible by all other domains. |

## The Flow of Knowledge: From Code to Graph

The syst
--- END OF FILE ./docs/02_ARCHITECTURE.md ---

--- START OF FILE ./docs/03_GOVERNANCE.md ---
# Governance & Enforcement Guide

> **Constitution = `.intent/`**. Everything else conforms to it.

## 1. What lives where

* **Index:** `.intent/meta.yaml`
* **Policies:** `.intent/policies/intent_guard.yaml`
* **Domain map:** `.intent/knowledge/source_structure.yaml`
* **Manifest schema:** `.intent/schemas/manifest.schema.json`
* **Domain manifests:** `.intent/manifests/*.manifest.json`
* **Drift evidence:** `reports/drift_report.json` (auto-generated)

## 2. Daily workflow (short)

1. **Scaffold & validate manifests**

   ```bash
   make manifests-scaffold
   make manifests-validate
   make manifests-dups         # set FAIL_ON_CONFLICTS=1 to enforce
   ```

2. **Generate drift evidence & view**

   ```bash
   make drift                   # writes reports/drift_report.json and prints a summary
   ```

3. **Run guard checks (imports/boundaries)**

   ```bash
   make guard-check             # pretty table output
   PYTHONPATH=src poetry run core-admin guard check --format=json --no-fail
   ```

4. **Run tests & quality**

   ```bash
   make fast-check              # lint + tests
   make check                   # lint + tests + audit
   ```

## 3. Adding or changing capabilities

* Pick the right **domain** (see “allowed imports” in `source_structure.yaml`).
* Update its manifest in `.intent/manifests/<domain>.manifest.json`.
* Keep capability names **unique** and **domain‑prefixed** (e.g., `core:task-router`).
* Re-run:

  ```bash
  make migrate                  # scaffold + validate + duplicate check in one go
  make guard-check
  ```
* Implement code **inside that domain only**. For cross-domain calls, go through **core interfaces**.

## 4. Rules you must not break

* **No cycles** between domains.
* **Default deny**: if a domain pair isn’t explicitly allowed, it’s not allowed.
* **Agents → system** is **forbidden** (system may import agents, not the other way).
* **Core → data** is **forbidden** (Dependency Inversion: data implements core ports).
* **Networking:** use `httpx` (not `requests`). The policy forbids `requests`.
* If you hit a rule during a refactor and need temporary relief, add a **waiver** in `intent_guard.yaml` **with an expiry date**. Keep waivers rare and short‑lived.

## 5. Troubleshooting (common failures)

* **“may not import” error**
  You imported across domains without permission. Move code or introduce a core interface.

* **“forbidden-library 'requests'”**
  Replace with `httpx`. Don’t add `requests` to dependencies.

* **Schema validation errors**
  Fix the manifest to match `.intent/schemas/manifest.schema.json`. Ensure `capabilities` is a **non-empty array** with **unique strings**.

* **Duplicate capabilities**
  Rename or consolidate; a capability must belong to **one** domain.

## 6. CI recommendation (optional snippet)

Add a job that fails on drift or guard violations:

```bash
make migrate FAIL_ON_CONFLICTS=1
PYTHONPATH=src poetry run core-admin guard check
```

## 7. Glossary

* **Drift:** schema errors or duplicate capabilities across manifests.
* **Guard:** static checks enforcing domain boundaries & library policy.
* **Constitution:** `.intent/` directory; source of truth for governance.

--- END OF FILE ./docs/03_GOVERNANCE.md ---

--- START OF FILE ./docs/04_ROADMAP.md ---
# 4. The CORE Project Roadmap

## Preamble: From Foundation to Future

The initial development of CORE focused on building a stable, self-aware, and constitutionally governed foundation. That foundational work established our current stable state and is now considered complete. A historical record of that process can be found in `docs/archive/StrategicPlan.md`.

**This document outlines our current and future direction.** With a stable and secure foundation in place, the project is now moving into its next major phase: **enhancing agent intelligence and scaling complexity.**

The following sections outline the key architectural challenges and features on our roadmap. We welcome discussion and contributions on these topics.

---

## Phase 1: Scaling the Constitution

As identified in our external architectural reviews, the current constitutional structure, while sound, faces several scalability challenges. Our next priority is to evolve the `.intent/` directory to support a system that can manage hundreds or thousands of files.

### 1.1: Implement Modular Manifests
-   **Status:** ✅ **Complete.** The `ConstitutionalAuditor` now aggregates capabilities from domain-specific `manifest.yaml` files.

### 1.2: Implement Hierarchical Capabilities
-   **Status:** ⏳ **Not Started**

### 1.3: Implement Hierarchical Domains
-   **Status:** ⏳ **Not Started**

---

## Phase 2: Enhancing Agent Reasoning

The next step is to make the system's AI agents smarter and safer in how they interpret and act upon the constitution.

### 2.1: Implement a Precedence of Principles
-   **Status:** ⏳ **Not Started**

### 2.2: Enforce Auditable Justification Logs
-   **Status:** ⏳ **Not Started**

---

## Phase 3: Autonomous Application Generation

This is the ultimate goal of the CORE project. With a scalable constitution and smarter agents, we will build the capabilities for CORE to generate and manage new software projects from a high-level intent.

-   **Status:** ✅ **MVP Complete.** The `core-admin agent scaffold` command can successfully generate a new, working, and constitutionally-governed application from a high-level goal. Future work will focus on increasing the complexity of generated applications.

---

## Phase 4: Constitutional Self-Improvement

This phase focuses on enabling CORE to reason about and improve its own "Mind". The goal is to build meta-capabilities that allow the system to use external intelligence to enhance its own governance.

### 4.1: Implement Constitutional Peer Review
-   **Status:** ✅ **Complete.** The `review export` and `review peer-review` commands are implemented. The system can successfully use an external LLM to critique its own constitution.

### 4.2: Implement Content Drift Detection
-   **Status:** ⏳ **Not Started**

---

## Phase 5: Achieving Operational Robustness

This phase, based on feedback from the AI Peer Review Board, focuses on adding the critical policies and procedures required for real-world operation and enterprise-grade governance.

### 5.1: Formalize Enforcement Levels
-   **Challenge:** The terms "soft" and "hard" enforcement are ambiguous.
-   **Goal:** Create a new policy file (`.intent/policies/enforcement_model.yaml`) that formally defines the behavior of each enforcement level, ensuring consistent and predictable governance.
-   **Status:** ⏳ **Not Started**

### 5.2: Implement Critical Operational Policies
-   **Challenge:** The constitution lacks policies for critical real-world operations.
-   **Goal:** Formalize policies for **Data Privacy** (minimization, erasure, encryption), **Secrets Management**, **Incident Response**, and **External Dependency Management** (licensing, vulnerabilities).
-   **Status:** 🚧 **In Progress.** Secrets, Incident Response, and Dependency Management policies have been defined. Auditor checks are pending.

### 5.3: Define Human Operator Lifecycle
-   **Status:** ✅ **Complete.** This was addressed by creating `docs/07_PEER_REVIEW.md` and updating `approvers.yaml`.

---

## Phase 6: Improving Architectural Health

This phase addresses technical debt identified by the `ConstitutionalAuditor` to ensure the long-term health and maintainability of the codebase, upholding the `separation_of_concerns` principle.

### 6.1: Refactor `codegraph_builder.py`
-   **Challenge:** The `KnowledgeGraphBuilder` class has grown too large and mixes responsibilities.
-   **Goal:** Decompose `KnowledgeGraphBuilder` into smaller, single-responsibility helper classes or modules.
-   **Status:** ⏳ **Not Started**

### 6.2: Refactor `proposal_checks.py`
-   **Challenge:** The `ProposalChecks` class is becoming a complexity outlier.
-   **Goal:** Refactor the large check methods into smaller, more focused helper functions to improve readability and testability.
-   **Status:** ⏳ **Not Started**

---

## Phase 7: Agentic Self-Improvement

This phase focuses on evolving CORE's agents to be more autonomous and resilient by teaching them to handle common development friction and failures without human intervention.

### 7.1: Implement Autonomous Formatting & Linting Fixes
-   **Challenge:** The system's agents can generate code that violates formatting or linting rules, requiring human intervention to fix. This represents an incomplete `ADAPTATION` loop.
-   **Goal:** Enhance the agent execution loop to automatically apply formatting and auto-fixable linting changes to any generated code before it is written to disk.
-   **Status:** ⏳ **Not Started**

---

## Phase 8: Enterprise Readiness (New)

This phase, based on feedback from the AI Peer Review Board, focuses on adding policies and documentation required for enterprise-grade operation.

### 8.1: Define a Formal Testing Policy
-   **Challenge:** Testing standards are not formally defined in the constitution.
-   **Goal:** Create a new policy file governing test coverage requirements, data management, and mocking strategies.
-   **Status:** ⏳ **Not Started**

### 8.2: Define Formal Documentation Standards
-   **Challenge:** Documentation quality is a principle, but the standards are not explicit.
-   **Goal:** Create a new policy defining minimum documentation requirements for capabilities and a style guide for docstrings.
-   **Status:** ⏳ **Not Started**

### 8.3: Define Error Recovery Procedures
-   **Challenge:** Procedures for recovering from a corrupted state are not documented.
-   **Goal:** Create a formal document outlining procedures for automatic rollback scenarios and constitutional crisis resolution.
-   **Status:** ⏳ **Not Started**
--- END OF FILE ./docs/04_ROADMAP.md ---

--- START OF FILE ./docs/05_BYOR.md ---
# 5. Bring Your Own Repo (BYOR) Quickstart

## The Guiding Principle: Ingestion Isomorphism

CORE is designed to be impartial. It applies the same rigorous constitutional analysis to any repository that it applies to itself. This principle, known as **Ingestion Isomorphism**, means that CORE can analyze, understand, and help govern any project without special treatment.

This guide will walk you through the process of pointing CORE at an existing repository and generating a starter constitution for it.

## The Goal: See Your Project Through CORE's Eyes

The `core-admin byor-init` command is a powerful introspection tool. It does not modify your code. Its purpose is to:

1. **Analyze** your repository's structure and capabilities.
2. **Infer** a set of domains based on your directory layout.
3. **Propose** a minimal, non-intrusive `.intent/` constitution based on its findings.

This gives you an instant health check and a starting point for bringing your project under CORE's governance.

---

## Step 1: The Safe Dry Run (Read-Only Analysis)

By default, the command runs in a safe, read-only **dry run** mode. It will show you what it would do **without changing a single file**.

**Commands**

```bash
# Analyze the current CORE repository
poetry run core-admin byor-init .

# Analyze a different project on your machine
poetry run core-admin byor-init /path/to/your/other/project
```

**Understanding the output**

The command first builds a Knowledge Graph of the target repository. Then, it shows the content of five constitutional files it proposes to create:

* `source_structure.yaml` — A map of your project, with each subdirectory in `src/` treated as a domain.
* `project_manifest.yaml` — An inventory of all the `# CAPABILITY` tags it discovered in your code.
* `capability_tags.yaml` — A dictionary for you to define and describe each of those capabilities.
* `principles.yaml` — A starter set of CORE's philosophical principles.
* `safety_policies.yaml` — A starter set of basic safety rules.

---

## Step 2: Applying the Constitution (Write Mode)

Once you’ve reviewed the dry run output and you’re happy with the proposed constitution, run the command again with the `--write` flag. This will create the `.intent/` directory and all proposed files inside your target repository.

**Command**

```bash
# Apply the starter constitution to the current repository
poetry run core-admin byor-init . --write
```

---

## Step 3: The First Audit

Your target repository is now **CORE-aware**—it has a nascent "Mind." The next step is to ask CORE to perform its first constitutional audit on the project.

From within the CORE project, configure the auditor to point at the new project. (In a future version, CORE will be able to attach to it directly.) The result is a continuous, automated health check on your project's architectural integrity and alignment with its newly declared principles.

This process is the first step in transforming any repository from a simple collection of code into a governed, self-aware system.

--- END OF FILE ./docs/05_BYOR.md ---

--- START OF FILE ./docs/06_STARTER_KITS.md ---
# ./docs/06\_STARTER\_KITS.md

# 6. Starter Kits & The Philosophy of Intent

## The CORE Partnership

CORE is not a vending machine for code. It is an intelligent partner designed to translate a human's intent into a governed, working software system. This partnership requires two things:

1. **The Human's Responsibility:** Provide a clear, high-level intent—the "why" behind the project.
2. **CORE's Responsibility:** Translate that intent into a complete system, asking for clarification and guidance along the way.

If the human provides no intent ("I do not care"), CORE will do nothing. The partnership requires a starting point.

## Starter Kits: Your First Declaration of Intent

To facilitate this partnership, the `core-admin new` command uses **Starter Kits**. A starter kit is not just a collection of template files; it is a **pre-packaged declaration of intent**. It is a way for you to tell CORE about the *kind* of system you want to build from day one.

By choosing a starter kit, you are providing the "minimal viable intent" that CORE needs to begin its work.

### How to Use Starter Kits

When you create a new project, you can specify a `--profile` option. This tells the scaffolder which starter kit to use.

```bash
# Scaffold a new project using the 'default' balanced starter kit
poetry run core-admin new my-new-app --profile default

# Scaffold a project with high-security policies from the start
poetry run core-admin new my-secure-api --profile security
```

If you do not provide a profile, CORE will default to the safest, most balanced option.

## The Life of a Starter Kit

* **Scaffolding:** CORE creates your new project structure and populates the `.intent/` directory with the constitutional files from your chosen starter kit.
* **Ownership:** From that moment on, that constitution is **yours**. It is no longer a template. It is the living "Mind" of your new project.
* **Evolution:** You can (and should) immediately begin to amend and evolve your new constitution to perfectly match your project's unique goals, using the standard proposals workflow.

Starter kits are just the beginning of the conversation, not the end. They are the most effective way to kickstart the CORE partnership and begin the journey of building a truly intent-driven system.

---

# ./README.md

# CORE — The Self-Improving System Architect

> **Where Intelligence Lives.**

[![Status: Architectural Prototype](https://img.shields.io/badge/status-architectural%20prototype-blue.svg)](#-project-status)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

CORE is a self-governing, constitutionally aligned AI development framework that can plan, write, validate, and evolve software systems — autonomously and safely. It is designed for environments where **trust, traceability, and governance matter**.

---

## 🏛️ Project Status: Architectural Prototype

The core self-governance and constitutional amendment loop is complete and stable. The system can audit and modify its own constitution via a human-in-the-loop, cryptographically signed approval process.

The next phase, as outlined in our **[Strategic Plan](docs/StrategicPlan.md)**, is to expand agent capabilities so CORE can generate and manage entirely new applications based on user intent.

We’re making the project public now to invite collaboration on this foundational architecture.

---

## 🧠 What CORE *is*

* 🧾 Evolves itself through **declared intent**, not hidden assumptions.
* 🛡️ Enforces **constitutional rules**, **domain boundaries**, and **safety policies**.
* 🌱 Creates new, governed applications from **[Starter Kits](docs/06_STARTER_KITS.md)** that capture initial intent.
* 🧩 Uses a modular agent architecture with a clear separation of concerns.
* 📚 Ensures every decision is **documented, reversible, and introspectable**.

---

## 🦮 Key Concepts

| Concept                     | Description                                                                              |
| --------------------------- | ---------------------------------------------------------------------------------------- |
| **`.intent/`**              | The “mind” of CORE: constitution, policies, capability maps, and self-knowledge.         |
| **`ConstitutionalAuditor`** | The “immune system,” continuously verifying code aligns with the constitution.           |
| **`PlannerAgent`**          | Decomposes high-level goals into executable plans.                                       |
| **`core-admin` CLI**        | Human-in-the-loop tool for managing the system's lifecycle.                              |
| **Starter Kits**            | Pre-packaged constitutions that serve as the user's first declaration of intent.         |
| **Canary Check**            | Applies proposed changes to an isolated copy and runs a full self-audit before approval. |
| **Knowledge Graph**         | Machine-readable map of symbols, roles, capabilities, and relationships.                 |

---

## 🚀 Getting Started

1. **Install dependencies**

   ```bash
   poetry install
   ```

2. **Set up environment**

   ```bash
   cp .env.example .env
   # Edit .env with your keys/URLs. See .intent/config/runtime_requirements.yaml for required variables.
   ```

3. **Run a full self-audit**

   ```bash
   make audit
   ```

---

## 🧑‍⚖️ Human-in-the-Loop (CLI)

The `core-admin` CLI is your primary tool for guiding the system.

### Creating New Projects

```bash
# Create a new, governed application using a starter kit
core-admin new my-new-app --profile default
```

### Onboarding Existing Projects

```bash
# Analyze an existing repo and propose a starter constitution
core-admin byor-init /path/to/existing-repo
```

### Managing the Constitution

```bash
# List pending constitutional changes
core-admin proposals-list

# Sign a proposal with your key
core-admin proposals-sign cr-example.yaml

# Approve a proposal (runs a canary self-audit)
core-admin proposals-approve cr-example.yaml
```

> If `core-admin` isn’t found, try: `poetry run core-admin ...`

---

## 🌱 Contributing

We welcome contributions from AI engineers, DevOps pros, and governance experts.

* See **CONTRIBUTING.md** to get started.
* Check the **Strategic Plan** for where we're headed.

---

## 📄 License

Licensed under the MIT License. See **LICENSE**.

--- END OF FILE ./docs/06_STARTER_KITS.md ---

--- START OF FILE ./docs/07_PEER_REVIEW.md ---
# 7. Constitutional Peer Review

## The Principle: Proactive Self-Improvement

A core principle of CORE is that a system must not only govern itself but also actively seek to improve its own governance. The Constitutional Peer Review feature is the primary mechanism for this proactive self-improvement.

It answers the question: **"Is our constitution the best it can be?"**

This process allows CORE to leverage powerful, external Large Language Models (LLMs) as expert consultants. It can ask for a "second opinion" on its own principles, policies, and structure, identifying gaps, ambiguities, or potential improvements that it might not be able to see on its own.

## The Workflow: A Safe, Human-in-the-Loop Process

The peer review process is designed to be fundamentally safe, keeping the human operator in full control of any resulting changes. The external LLM can only **suggest**; it can never **act**.

---

### Step 1: Exporting the Constitutional Bundle

The first step is to package the system's entire "Mind" (`.intent/` directory) into a single, portable file that an LLM can analyze.

**Command:**

```bash
poetry run core-admin review export
```

This command reads your `meta.yaml` to discover all constitutional files and bundles them into `reports/constitution_bundle.txt`. This is useful for manual inspection or for sending to different AI models.

---

### Step 2: Requesting the Peer Review

The main command automates the bundling and review request in a single step.

**Command:**

```bash
poetry run core-admin review peer-review
```

**What this command does:**

* Performs the same **export** process internally to create the constitutional bundle.
* Loads a specialized set of instructions from `.intent/prompts/constitutional_review.prompt`.
* Sends the bundle and the instructions to the Orchestrator LLM configured in your `.env` file.
* Saves the AI's detailed feedback as a Markdown file to `reports/constitutional_review.md`.

---

### Step 3: Taking Action on the Feedback

The output report is a set of suggestions for the human operator. It is your responsibility to review this feedback and decide what to act on.

* A strong piece of feedback from the review (e.g., *"The policy for secrets management is incomplete"*) should be transformed into a new item on the **Project Roadmap** or a formal **constitutional amendment proposal** (`.intent/proposals/cr-*.yaml`).
* This loop allows CORE to use external intelligence to evolve its own constitution **without ever sacrificing** the safety and control of its human-in-the-loop governance model.

--- END OF FILE ./docs/07_PEER_REVIEW.md ---

--- START OF FILE ./docs/archive/StrategicPlan.md ---
# Project CORE: A Strategic Plan for Refactoring and Evolution

## 0. Context: The Story of Our Foundation

**This document is a historical record.** It outlines the foundational work that was completed to evolve CORE from an early prototype into the stable, self-governing system it is today. It tells the story of "how we got here."

For our future plans and the challenges we are tackling next, please see the living **[Project Roadmap (`04_ROADMAP.md`)](04_ROADMAP.md)**.

---

## 1. Preamble: From Diagnosis to Vision

This document outlines the strategic plan to evolve the CORE system from its current state to a truly self-governing, resilient, and evolvable architecture.

The initial feeling of "running in circles" was a correct diagnosis of a system struggling with internal inconsistencies. The recent comprehensive audit, while displaying numerous errors and warnings, was not a sign of failure. It was the **first successful act of self-diagnosis** by the system's nascent "brain." The audit provided a clear, actionable roadmap, revealing a fundamental disconnect between the declared `intent` and the `source code` reality.

This plan details the two major phases of our work:
*   **Part A: Foundational Refactoring.** To achieve a stable, constitutionally compliant baseline by fixing the issues diagnosed by the audit.
*   **Part B: Enabling True Self-Governance.** To build the necessary mechanisms for the system to evolve its own code and constitution safely and autonomously.

---

## Part A: Foundational Refactoring (Achieving Stability)

This phase focuses on acting on the audit's results to create a clean, consistent, and understandable codebase. It is the work required to teach the system what a "good" state looks like.

### Step A1: Unify the "Brain"
*   **Goal:** Eliminate data redundancy and create a single source of truth for the system's knowledge of its own code.
*   **Status:** ✅ **COMPLETE**
*   **Outcome:** The dual `codegraph.json` and `function_manifest.json` files have been replaced by a single, comprehensive `.intent/knowledge/knowledge_graph.json`. The `KnowledgeGraphBuilder` tool is now the sole producer of this artifact.

### Step A2: Consolidate Governance
*   **Goal:** Eliminate redundant tools and establish a single, authoritative script for verifying the system's integrity.
*   **Status:** ✅ **COMPLETE**
*   **Outcome:** The `ConstitutionalAuditor` is now the master tool for all static analysis. Older, fragmented tools (`architectural_auditor`, `principle_validator`, etc.) have been merged into it or deleted.

### Step A3: Stabilize the System
*   **Goal:** Ensure the system has a reliable safety net for development.
*   **Status:** ✅ **COMPLETE**
*   **Outcome:** The test suite has been repaired. Obsolete tests were deleted, and configuration issues (`pythonpath`, dependency conflicts) were resolved, resulting in a stable and passing test run.

### Step A4: Achieve Constitutional Compliance
*   **Goal:** Resolve all critical errors reported by the `ConstitutionalAuditor`.
*   **Status:** ✅ **COMPLETE**
*   **Outcome:** All structural errors (domain mismatches, illegal imports) have been fixed. The "mind-body problem" has been solved by manually annotating the existing codebase with `# CAPABILITY:` tags, fully aligning the `project_manifest.yaml` with the implementation. The audit now reports **ZERO critical errors.**

---

## Part B: The Path Forward (Enabling Evolution)

With a stable foundation, we can now build the mechanisms that allow CORE to fulfill its prime directive: to evolve itself safely.

### Step B1: Trust the Brain (Simplification & Cleanup)

*   **Goal:** Eliminate all audit warnings by making the system's "brain" more intelligent.
*   **Guiding Principle:** We did not blindly patch the auditor. Instead, we enhanced the `KnowledgeGraphBuilder` so it could understand more complex, valid code patterns, thus resolving the root cause of the false warnings.
*   **Status:** ✅ **COMPLETE**
*   **Outcome:** The `KnowledgeGraphBuilder` has been upgraded with a context-aware AST visitor and a declarative pattern-matching engine (`.intent/knowledge/entry_point_patterns.yaml`). It now correctly identifies inheritance, framework callbacks, and CLI entry points. All schema violations were corrected, and all code was fully documented. The `ConstitutionalAuditor` now reports **ZERO errors and ZERO warnings.**

### Step B2: Build the Immune System (Governed Creation)

*   **Goal:** Evolve the `PlannerAgent` to ensure that all *newly generated* code is automatically compliant with the constitution, preventing future errors.
*   **Status:** ✅ **COMPLETE**
*   **Outcome:** The `PlannerAgent._execute_task` method now follows a complete `Generate -> Govern -> Validate -> Self-Correct -> Write` loop. It automatically adds capability tags, enforces docstrings, validates the code, and triggers a self-correction cycle on any validation failure, ensuring only constitutionally compliant code is ever written to disk.

### Step B3: The Constitutional Amendment Process

*   **Goal:** Transform `.intent/` from a "notepad" into a true constitution with a formal, safe amendment process. This allows CORE to evolve its own brain.
*   **Status:** ✅ **COMPLETE**
*   **The Mechanism:**
    1.  **The Waiting Room (`.intent/proposals/`):** A dedicated directory for drafting changes to the constitution. Files here are not active.
    2.  **The Proposal Bundle:** A standardized YAML format for change requests (e.g., `cr-....yaml`) containing the `target_path`, `action`, `justification`, and proposed `content`.
    3.  **The Governance Layer:**
        *   **`IntentGuard`:** Enforces a new, critical rule: **No direct writes are allowed into `.intent/` except to the `proposals/` directory.**
        *   **`ConstitutionalAuditor`:** Scans `proposals/`, validates the format of any pending proposals, and reports them in a new "Pending Amendments" section of its report.
    4.  **The Ratification Mechanism (`core-admin` tool):** A human operator uses a simple CLI tool to manage the amendment lifecycle. The key command is `approve`.
    5.  **The "Canary" Pre-flight Check:** The `core-admin approve` command is designed to be fundamentally safe and solves the "how does it know it's broken if it's broken?" paradox.
        *   It spawns a **temporary, isolated "canary" instance** of CORE with the proposed change applied *in memory*.
        *   It commands this canary instance to run a full self-audit.
        *   If the canary audit succeeds, the change is permanently applied to the real `.intent/` directory.
        *   If the canary audit fails, the change is rejected, and the real `.intent/` directory is never touched, preventing system failure.

## 2. Conclusion

Upon completion of this plan, CORE will have evolved from a promising but inconsistent prototype into a robust, self-aware system. It will possess a stable foundation, a clear understanding of its own structure, and—most importantly—a safe, governed process for both creating code and evolving its own foundational intent.

This plan transforms CORE from a system that is merely *audited* to one that is truly *governed*.

--- END OF FILE ./docs/archive/StrategicPlan.md ---

--- START OF FILE ./docs/NORTH_STAR.md ---
# CORE North Star

**Goal:** Turn high-level goals into governed, safe, running software — autonomously.

**Invariant:** Safety never degrades. Every change is proven aligned to intent before it lands (policies, tests, canary audit).

## The Autonomy Ladder

- **A0 · Observe** — Attach to any repo, build knowledge graph, run auditor (read-only).
- **A1 · Propose** — Open PRs with small, safe fixes (docstrings, capability tags, logging). Human merges.
- **A2 · Governed Writes** — Canary-validated changes self-apply with policy (human optional by risk tier).
- **A3 · BYOR** — CORE-fy arbitrary repos (including CORE), propose structure, capabilities, tests.
- **A4 · Birth New Apps** — From a goal, scaffold feature-first app (Mind/Body), tests, CI/CD.
- **A5 · Continuous Intent Dev** — Roadmap evolves in natural language; CORE plans, implements, releases within risk budgets.

## Guardrails
- **Risk tiers:** auto-merge only for low-risk scopes at first.
- **Evidence gates:** static policies + tests + canary must pass.
- **Idempotence:** ingestion produces stable results; re-runs don’t thrash.
- **Kill switch:** rollback on anomaly/error-budget breach.

## KPIs
- Auditor errors/warnings → sustained zero
- % code with capability tags
- Proposal acceptance rate / MTTR
- % PRs generated by CORE
- % no-human releases by risk tier
- Post-merge incident rate

## Current Status
- Project status: **Architectural Prototype**.
- Targeting **M1 (A0→A1)**: CORE overlay, CI audit, auto-PRs for docstrings/tags/logging.

--- END OF FILE ./docs/NORTH_STAR.md ---

--- START OF FILE ./docs/releases/v0.2.0.md ---
# v0.2.0 — MVP: Autonomous Application Generation (2025-08-15)

**Highlights**
- First MVP that can autonomously scaffold a governed application from a high-level goal.
- Nightly constitutional audit + capability drift checks in CI.
- Starter Kit: default profile shipped.
- CLI: `core-admin new`, `core-admin agent scaffold`, `core-admin proposals ...`.

**Upgrade notes**
- Use Python 3.11+.
- Install with `poetry install`.
- Run checks locally: `black --check . && ruff check . && pytest`.

--- END OF FILE ./docs/releases/v0.2.0.md ---

--- START OF FILE ./docs/TheDocument.md ---
# The CORE Constitution

## Section 1: Prime Directive

**CORE exists to transform human intent into complete, evolving software systems — without drift, duplication, or degradation.**

It does not merely generate code; it **governs**, **learns**, and **rewrites** itself under the authority of an explicit, machine-readable constitution. It is a system designed to build other systems, safely and transparently.

---

## Section 2: Purpose of This Document

This document defines the **philosophy** and **operating principles** of the CORE system. It serves as the primary, human-facing contract that justifies and explains all of the system's automated governance mechanisms.

All rules enforced by the system's "Mind" (`.intent/`) are derived from the principles explained here. For a deeper dive into the specific mechanics, please refer to the foundational documents:

1.  **[The CORE Philosophy (`01_PHILOSOPHY.md`)](01_PHILOSOPHY.md)** — The *why* behind the project and the Ten-Phase Loop of Reasoned Action.
2.  **[The System Architecture (`02_ARCHITECTURE.md`)](02_ARCHITECTURE.md)** — The *how* of the Mind/Body separation.
3.  **[The Governance Model (`03_GOVERNANCE.md`)](03_GOVERNANCE.md)** — The formal process for safe, constitutional change.

---

## Section 3: The Ten-Phase Loop of Reasoned Action

All autonomous actions in CORE are governed by a ten-phase loop. This structure ensures that every action is deliberate, justified, validated, and traceable to a core principle. It prevents the system from taking impulsive or un-auditable shortcuts.

**GOAL** → **WHY** → **INTENT** → **AGENT** → **MEANS** → **PLAN** → **ACTION** → **FEEDBACK** → **ADAPTATION** → **EVOLUTION**

This loop ensures that CORE does not simply act, but *reasons*. Every change is a deliberate, auditable, and constitutionally-aligned evolution of the system.

--- END OF FILE ./docs/TheDocument.md ---

--- START OF FILE ./drift.txt ---
🧭 Generating drift evidence (schema + duplicates)...
PYTHONPATH=src python3 -m poetry run python3 src/system/tools/manifest_migrator.py all  || true
INFO No scaffolding needed. All manifests exist.
INFO OK: .intent/manifests/shared.manifest.json
INFO OK: .intent/manifests/core.manifest.json
INFO OK: .intent/manifests/agents.manifest.json
INFO OK: .intent/manifests/system.manifest.json
INFO OK: .intent/manifests/data.manifest.json
    Duplicate Capabilities    
                              
  Capability   Domains        
 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 
  —            No duplicates  
                              
INFO Wrote drift report: reports/drift_report.json
📄 Displaying guard drift view (short)...
PYTHONPATH=src python3 -m poetry run core-admin guard drift --format short
{
  "mismatched_mappings": [
    {
      "capability": "add_missing_docstrings",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "tooling",
        "owner": null
      }
    },
    {
      "capability": "alignment_checking",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.capability_coverage",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.capability_definitions",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.codebase_health",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.dead_code",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.docstrings",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.domain_integrity",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.duplication",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.environment",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.knowledge_graph_schema",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.orphaned_intent_files",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.project_manifest",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.proposals_drift",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.proposals_list",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.proposals_schema",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.required_files",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.secrets",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.syntax",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "change_safety_enforcement",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "code_generation",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "agents",
        "owner": null
      }
    },
    {
      "capability": "code_quality_analysis",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "intent_guarding",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "introspection",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "llm_orchestration",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "agents",
        "owner": null
      }
    },
    {
      "capability": "prompt_interpretation",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "scaffold_project",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "self_correction",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "semantic_validation",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "syntax_validation",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "system_logging",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "shared",
        "owner": null
      }
    }
  ],
  "missing_in_code": [
    "manifest_updating",
    "test_execution"
  ],
  "undeclared_in_manifest": []
}

--- END OF FILE ./drift.txt ---

--- START OF FILE ./.env.example ---
# CORE Environment Variables
# ---------------------------
# Copy this file to .env and fill in the values below.
# The .env file is ignored by git, so your secrets will not be committed.

# --- Path Configuration ---
# These are typically not needed to be changed if running from the repo root.
MIND=".intent"
BODY="src"
REPO_PATH="."
CORE_ACTION_LOG_PATH="logs/action_log.jsonl"
# Logging/flags (added)
CORE_LOG_JSON=false
CORE_LOG_LEVEL=INFO
LLM_ENABLED=false

# --- Orchestrator LLM Configuration (For high-level planning) ---
# Example for a local model server: http://localhost:11434/v1
ORCHESTRATOR_API_URL=""
ORCHESTRATOR_API_KEY="your_api_key_here"
ORCHESTRATOR_MODEL_NAME="deepseek-chat"

# --- Generator LLM Configuration (For code generation) ---
# Example for a local model server: http://localhost:11434/v1
GENERATOR_API_URL=""
GENERATOR_API_KEY="your_api_key_here"
GENERATOR_MODEL_NAME="deepseek-coder"
--- END OF FILE ./.env.example ---

--- START OF FILE ./GH_STATUS.md ---
# GitHub Status Report — DariuszNewecki/CORE

Generated: 2025-08-18 09:07:35Z

## Repository
{
  "name": "CORE",
  "visibility": "public",
  "default_branch": "main",
  "open_issues_count": 15,
  "description": "The last developer you’ll ever need."
}

## Milestones
{
  "number": 10,
  "title": "v0.6: Operational Hardening & Governance Maturity",
  "state": "open",
  "due_on": null,
  "open_issues": 0,
  "closed_issues": 3,
  "description": "**Goal:** Implement the high-priority \"Tier 2\" recommendations from the AI Peer Review Board.\n\nThis phase focuses on transforming the CORE constitution from a set of rules for code into a complete governance framework for a real-world, operational project.\n\n**Key Objectives:**\n1.  Formalize and define all policy enforcement levels.\n2.  Implement critical new policies for Data Privacy, Secrets Management, and Incident Response.\n3.  Formally document the lifecycle and procedures for human operators."
}
{
  "number": 1,
  "title": "v0.2: Proposal Format & Drift",
  "state": "open",
  "due_on": "2025-09-15T07:00:00Z",
  "open_issues": 1,
  "closed_issues": 4,
  "description": "Purpose:\nNormalize amendment proposals, enforce schema, and surface proposal drift clearly in audits.\nGoal: Normalize proposal formats and eliminate documentation drift around .intent/proposals/* so governance is predictable.\nScope:\n\nLock proposal format with proposal.schema.json (required fields, signatures, rollback hints).\n\nAdd robust auditor checks for proposal schema + “pending summary.”\n\nAdd drift checks (proposal token ≠ current content; missing critical metadata).\n\nUpdate docs & examples; provide a golden sample.\nDone when:\n\nAuditor shows ✅ or actionable ❌ for every proposal, with file paths.\n\nA sample proposal passes end-to-end (sign → canary → approve).\n\nCONTRIBUTING.md and GOVERNANCE docs reference the new format."
}
{
  "number": 7,
  "title": "v0.3: Modular Manifests",
  "state": "open",
  "due_on": "2025-10-25T07:00:00Z",
  "open_issues": 1,
  "closed_issues": 3,
  "description": "Goal: Scale .intent by splitting project_manifest.yaml into per-domain manifests (e.g., src/agents/manifest.yaml, src/system/manifest.yaml).\nScope:\n\nAggregator to produce an in-memory global view.\n\nAuditor reads aggregated view only.\n\nBackward compatible (monolith accepted but discouraged).\nDone when:\n\nAll current data moved to per-domain files.\n\nAuditor passes with only aggregated view.\n\nDocs describe the pattern + migration notes."
}
{
  "number": 8,
  "title": "v0.4: BYOR (Bring Your Own Repo) Ingestion Isomorphism",
  "state": "open",
  "due_on": "2025-11-30T08:00:00Z",
  "open_issues": 1,
  "closed_issues": 3,
  "description": "Goal: Treat any external repo as a first-class citizen. CORE should infer structure, generate a starter .intent, and run a safe audit without modifying the target by default.\nScope:\n\n“BYOR init” command (dry-run and write modes).\n\nKnowledgeGraphBuilder: resilient scanning of unknown layouts.\n\nStarter .intent scaffold + guard rails.\nDone when:\n\nPointing CORE at a repo produces a minimal, valid .intent/ and a readable audit report.\n\nCORE can bootstrap itself via the same path."
}
{
  "number": 9,
  "title": "v0.5: CORE-fication Pipeline & Starter Kits",
  "state": "open",
  "due_on": "2026-01-31T08:00:00Z",
  "open_issues": 1,
  "closed_issues": 2,
  "description": "Goal: Provide first-class scaffolding and pipelines to create new “Mind/Body” apps or CORE-fy existing ones, with optional multi-repo mode.\nScope:\n\ncore-admin new --app <name> scaffolds two modes: single-repo and dual-repo (mind/body separated repos).\n\nPolicy bundles per risk level (low/med/high).\n\nOpinionated logging and tests baked in.\nDone when:\n\nOne command yields a runnable app with docs + CI that passes an initial auditor run."
}

## Open Issues
{
  "number": 30,
  "title": "❌ Nightly Constitutional Audit failed",
  "milestone": null,
  "labels": [
    "ci",
    "audit"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/30",
  "createdAt": "2025-08-16T03:20:13Z"
}
{
  "number": 27,
  "title": "[Refactor] Consolidate Duplicated Logic to Uphold 'dry_by_design' Principle",
  "milestone": null,
  "labels": [
    "priority:high",
    "area:refactoring"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/27",
  "createdAt": "2025-08-12T15:14:17Z"
}
{
  "number": 22,
  "title": "Templates: logging + tests + health endpoints",
  "milestone": "v0.5: CORE-fication Pipeline & Starter Kits",
  "labels": [
    "size:M",
    "priority:medium",
    "type:tooling",
    "area:devx"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/22",
  "createdAt": "2025-08-09T10:55:17Z"
}
{
  "number": 17,
  "title": "KnowledgeGraphBuilder: unknown-layout heuristics",
  "milestone": "v0.4: BYOR (Bring Your Own Repo) Ingestion Isomorphism",
  "labels": [
    "priority:high",
    "size:M",
    "type:tooling",
    "area:introspection"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/17",
  "createdAt": "2025-08-09T10:53:05Z"
}
{
  "number": 14,
  "title": "Docs: Modular manifests guide + examples",
  "milestone": "v0.3: Modular Manifests",
  "labels": [
    "size:S",
    "type:docs",
    "priority:medium"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/14",
  "createdAt": "2025-08-09T10:51:08Z"
}
{
  "number": 9,
  "title": "CLI: core-admin proposals-sample to scaffold golden proposal",
  "milestone": "v0.2: Proposal Format & Drift",
  "labels": [
    "area:governance",
    "size:S",
    "priority:medium",
    "type:cli"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/9",
  "createdAt": "2025-08-09T10:49:31Z"
}
{
  "number": 7,
  "title": "Pilot domain package (proposals)",
  "milestone": null,
  "labels": [
    "roadmap",
    "organizational"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/7",
  "createdAt": "2025-08-09T06:00:34Z"
}
{
  "number": 2,
  "title": "Add JSON logging & request IDs",
  "milestone": null,
  "labels": [
    "roadmap",
    "organizational",
    "type:ci"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/2",
  "createdAt": "2025-08-09T06:00:28Z"
}

## Recently Closed Issues
{
  "number": 29,
  "title": "❌ Nightly Constitutional Audit failed",
  "milestone": null,
  "labels": [
    "ci",
    "audit"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/29",
  "closedAt": "2025-08-15T12:23:04Z"
}
{
  "number": 28,
  "title": "❌ Nightly Constitutional Audit failed",
  "milestone": null,
  "labels": [
    "ci",
    "audit"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/28",
  "closedAt": "2025-08-14T18:32:12Z"
}
{
  "number": 26,
  "title": "[v0.6] EPIC: Implement Critical Operational Policies",
  "milestone": "v0.6: Operational Hardening & Governance Maturity",
  "labels": [
    "type:epic",
    "area:governance"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/26",
  "closedAt": "2025-08-14T18:30:31Z"
}
{
  "number": 25,
  "title": "[v0.6] Formalize Policy Enforcement Levels",
  "milestone": "v0.6: Operational Hardening & Governance Maturity",
  "labels": [
    "area:governance",
    "area:auditor"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/25",
  "closedAt": "2025-08-14T18:28:56Z"
}
{
  "number": 24,
  "title": "[v0.6] Formally Document the Human Operator Lifecycle",
  "milestone": "v0.6: Operational Hardening & Governance Maturity",
  "labels": [
    "area:governance",
    "type:docs"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/24",
  "closedAt": "2025-08-14T18:27:58Z"
}
{
  "number": 23,
  "title": "❌ Nightly Constitutional Audit failed",
  "milestone": null,
  "labels": [
    "ci",
    "audit"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/23",
  "closedAt": "2025-08-12T15:11:18Z"
}
{
  "number": 21,
  "title": "Starter kits: low/med/high risk policy bundles",
  "milestone": "v0.5: CORE-fication Pipeline & Starter Kits",
  "labels": [
    "size:M",
    "priority:medium",
    "type:tooling",
    "area:intent"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/21",
  "closedAt": "2025-08-11T12:19:50Z"
}
{
  "number": 20,
  "title": "CLI: core-admin new (single-repo & dual-repo)",
  "milestone": "v0.5: CORE-fication Pipeline & Starter Kits",
  "labels": [
    "priority:medium",
    "type:cli",
    "size:L",
    "area:scaffolding"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/20",
  "closedAt": "2025-08-11T12:20:36Z"
}
{
  "number": 19,
  "title": "Docs: BYOR quickstart",
  "milestone": "v0.4: BYOR (Bring Your Own Repo) Ingestion Isomorphism",
  "labels": [
    "size:S",
    "type:docs",
    "priority:medium"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/19",
  "closedAt": "2025-08-11T11:29:45Z"
}
{
  "number": 18,
  "title": "Seed .intent template pack",
  "milestone": "v0.4: BYOR (Bring Your Own Repo) Ingestion Isomorphism",
  "labels": [
    "size:M",
    "priority:medium",
    "type:tooling",
    "area:intent"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/18",
  "closedAt": "2025-08-11T11:20:57Z"
}
{
  "number": 16,
  "title": "CLI: core-admin byor-init <path> [--write] [--dry-run]",
  "milestone": "v0.4: BYOR (Bring Your Own Repo) Ingestion Isomorphism",
  "labels": [
    "priority:high",
    "type:cli",
    "size:L",
    "area:ingestion"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/16",
  "closedAt": "2025-08-11T10:12:37Z"
}
{
  "number": 15,
  "title": "Migration tool: split monolithic manifest",
  "milestone": "v0.3: Modular Manifests",
  "labels": [
    "size:M",
    "priority:medium",
    "type:cli",
    "area:intent"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/15",
  "closedAt": "2025-08-10T14:42:03Z"
}
{
  "number": 13,
  "title": "Auditor: consume only aggregated manifest",
  "milestone": "v0.3: Modular Manifests",
  "labels": [
    "area:auditor",
    "priority:high",
    "size:M",
    "type:tooling"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/13",
  "closedAt": "2025-08-11T08:46:12Z"
}
{
  "number": 12,
  "title": "Implement manifest aggregator (per-domain → global)",
  "milestone": "v0.3: Modular Manifests",
  "labels": [
    "priority:high",
    "type:tooling",
    "area:intent",
    "size:L"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/12",
  "closedAt": "2025-08-11T08:45:13Z"
}
{
  "number": 11,
  "title": "CI: validate proposals on PRs",
  "milestone": "v0.2: Proposal Format & Drift",
  "labels": [
    "type:epic",
    "priority:high",
    "size:S"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/11",
  "closedAt": "2025-08-10T14:19:56Z"
}
{
  "number": 10,
  "title": "Docs: add \"Proposals & Canary\" quickstart + sample",
  "milestone": "v0.2: Proposal Format & Drift",
  "labels": [
    "size:S",
    "type:docs",
    "priority:medium"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/10",
  "closedAt": "2025-08-10T14:21:11Z"
}
{
  "number": 8,
  "title": "Auditor: add ProposalChecks (schema + pending summary + drift)",
  "milestone": "v0.2: Proposal Format & Drift",
  "labels": [
    "area:auditor",
    "priority:high",
    "size:M",
    "type:tooling"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/8",
  "closedAt": "2025-08-10T14:14:36Z"
}
{
  "number": 6,
  "title": "Modular manifests (aggregator + fallback)",
  "milestone": null,
  "labels": [
    "roadmap",
    "organizational",
    "type:epic"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/6",
  "closedAt": "2025-08-10T14:42:47Z"
}
{
  "number": 5,
  "title": "Governance: proposal.schema.json + proposal_checks",
  "milestone": "v0.2: Proposal Format & Drift",
  "labels": [
    "roadmap",
    "organizational",
    "area:governance",
    "area:auditor",
    "priority:high",
    "size:M",
    "type:tooling"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/5",
  "closedAt": "2025-08-10T14:21:56Z"
}
{
  "number": 4,
  "title": "Docs: CONVENTIONS.md & DEPENDENCIES.md",
  "milestone": null,
  "labels": [
    "roadmap",
    "organizational",
    "type:docs"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/4",
  "closedAt": "2025-08-15T12:29:24Z"
}
{
  "number": 3,
  "title": "Pre-commit hooks (Black, Ruff)",
  "milestone": null,
  "labels": [
    "roadmap",
    "organizational",
    "type:ci"
  ],
  "url": "https://github.com/DariuszNewecki/CORE/issues/3",
  "closedAt": "2025-08-15T12:28:40Z"
}
{
  "number": 1,
  "title": "[Enhancement] Refactor validation_pipeline to use a unified violations list",
  "milestone": null,
  "labels": [],
  "url": "https://github.com/DariuszNewecki/CORE/issues/1",
  "closedAt": "2025-08-08T14:15:39Z"
}

## Labels
{
  "name": "organizational",
  "color": "a2eeef",
  "description": "Project organization"
}
{
  "name": "roadmap",
  "color": "0366d6",
  "description": "Roadmap item"
}
{
  "name": "type:ci",
  "color": "5319e7",
  "description": "CI/CD"
}
{
  "name": "type:epic",
  "color": "0366d6",
  "description": "Multi-issue epic"
}
{
  "name": "type:vision",
  "color": "6f42c1",
  "description": "Vision/North Star"
}
{
  "name": "type:task",
  "color": "a2eeef",
  "description": "Executable task"
}
{
  "name": "ws:byor",
  "color": "fbca04",
  "description": ""
}
{
  "name": "ws:governance",
  "color": "d73a4a",
  "description": ""
}
{
  "name": "ws:planner",
  "color": "0e8a16",
  "description": ""
}
{
  "name": "ws:validation",
  "color": "c2e0c6",
  "description": ""
}
{
  "name": "area:governance",
  "color": "0366d6",
  "description": ""
}
{
  "name": "area:auditor",
  "color": "0e8a16",
  "description": ""
}
{
  "name": "priority:high",
  "color": "d73a4a",
  "description": ""
}
{
  "name": "size:M",
  "color": "bfdadc",
  "description": ""
}
{
  "name": "size:S",
  "color": "c2e0c6",
  "description": ""
}
{
  "name": "type:docs",
  "color": "0075ca",
  "description": ""
}
{
  "name": "priority:medium",
  "color": "fbca04",
  "description": ""
}
{
  "name": "type:cli",
  "color": "e4e669",
  "description": ""
}
{
  "name": "type:tooling",
  "color": "ededed",
  "description": ""
}
{
  "name": "area:intent",
  "color": "006b75",
  "description": ""
}
{
  "name": "size:L",
  "color": "a2eeef",
  "description": ""
}
{
  "name": "area:ingestion",
  "color": "1f883d",
  "description": ""
}
{
  "name": "area:introspection",
  "color": "5319e7",
  "description": ""
}
{
  "name": "area:scaffolding",
  "color": "8250df",
  "description": ""
}
{
  "name": "area:devx",
  "color": "0969da",
  "description": ""
}
{
  "name": "audit",
  "color": "ededed",
  "description": ""
}
{
  "name": "ci",
  "color": "ededed",
  "description": ""
}
{
  "name": "area:refactoring",
  "color": "305763",
  "description": ""
}
{
  "name": "dependencies",
  "color": "0366d6",
  "description": "Pull requests that update a dependency file"
}
{
  "name": "github_actions",
  "color": "000000",
  "description": "Pull requests that update GitHub Actions code"
}

## Projects (Projects v2)
6	CORE Roadmap	open	PVT_kwHOAxIPlc4BACPl

## Releases
v0.2.0	Latest	v0.2.0	2025-08-15T20:50:16Z


--- END OF FILE ./GH_STATUS.md ---

--- START OF FILE ./.github/dependabot.yml ---
# .github/dependabot.yml
#
# CORE's automated dependency management configuration.
# This file enables Dependabot to automatically create pull requests to keep
# our dependencies up-to-date, enhancing our supply-chain security.

version: 2
updates:
  # Maintain dependencies for GitHub Actions
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"
    commit-message:
      prefix: "chore(actions)"
      include: "scope"

  # Maintain dependencies for Python (Poetry)
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "weekly"
    commit-message:
      prefix: "chore(deps)"
      include: "scope"

--- END OF FILE ./.github/dependabot.yml ---

--- START OF FILE ./.github/ISSUE_TEMPLATE/bug_report.yml ---
---
name: Bug report
about: Create a report to help us improve
labels: [type:bug, priority:triage]
---

## Describe the bug
A clear and concise description of what the bug is.

## To Reproduce
Steps to reproduce the behavior:
1. Go to '...'
2. Run '...'
3. See error '...'

## Expected behavior
What you expected to happen.

## Screenshots / Logs
If applicable, add screenshots or logs.

## Environment
- OS: [e.g. Ubuntu 22.04]
- Python: [e.g. 3.12]
- CORE commit/branch: [e.g. v0.2.0]

## Additional context
Add any other context about the problem here.
---


--- END OF FILE ./.github/ISSUE_TEMPLATE/bug_report.yml ---

--- START OF FILE ./.github/ISSUE_TEMPLATE/config.yml ---
blank_issues_enabled: false
contact_links:
  - name: Questions & Ideas
    url: https://github.com/DariuszNewecki/CORE/discussions
    about: Use Discussions for Q&A and brainstorming before filing issues.
--- END OF FILE ./.github/ISSUE_TEMPLATE/config.yml ---

--- START OF FILE ./.github/ISSUE_TEMPLATE/deature_request.yml ---
---
name: Feature request
about: Suggest an idea for CORE
labels: [type:feature, size:small]
---

## Problem / Motivation
What problem are you trying to solve? Why is this important?

## Proposal
Describe the solution you'd like (high level).

## Alternatives
Describe alternatives you've considered.

## Acceptance criteria
- [ ] Criterion 1
- [ ] Criterion 2

## Additional context
Links, designs, or references.
--- END OF FILE ./.github/ISSUE_TEMPLATE/deature_request.yml ---

--- START OF FILE ./.github/ISSUE_TEMPLATE/governance_proposal.yml ---
name: Constitutional Proposal
description: Propose a change to the Mind (.intent) via a signed proposal file.
labels: ["audit","roadmap"]
body:
  - type: textarea
    id: justification
    attributes:
      label: Justification
      description: Why is this change needed?
    validations:
      required: true
  - type: input
    id: target
    attributes:
      label: Target path
      placeholder: .intent/policies/safety_policies.yaml
    validations:
      required: true
  - type: textarea
    id: content
    attributes:
      label: Proposed content
      description: Paste or attach content that will replace the file.
  - type: markdown
    attributes:
      value: |
        **How to submit:**
        1. Create `.intent/proposals/cr-*.yaml` using the schema.
        2. (Optional) Sign with `core-admin proposals-sign`.
        3. Ask a maintainer to run `proposals-approve`.

--- END OF FILE ./.github/ISSUE_TEMPLATE/governance_proposal.yml ---

--- START OF FILE ./.github/labeler.yml ---
# Auto-apply labels based on changed paths
feat:
  - "src/**"
fix:
  - "src/**"
docs:
  - "README.md"
  - "docs/**"
test:
  - "tests/**"
governance:
  - ".intent/**"
  - "src/system/**"
drift:
  - "reports/drift_report.json"
ci:
  - ".github/workflows/**"
deps:
  - "pyproject.toml"
  - "poetry.lock"

--- END OF FILE ./.github/labeler.yml ---

--- START OF FILE ./.github/PULL_REQUEST_TEMPLATE.md: ---
## Summary
<!-- What does this change do? Keep it crisp. One or two sentences. -->

## Type
- [ ] feat
- [ ] fix
- [ ] refactor
- [ ] docs
- [ ] chore
- [ ] test

## Why
<!-- Link related issue(s). Explain intent. Reference `.intent/` where relevant. -->

## How
<!-- High-level approach; call out risks or trade-offs. -->

## Scope / Domains
<!-- Tick all impacted domains -->
- [ ] shared
- [ ] core
- [ ] agents
- [ ] system
- [ ] data (disabled by default)

---

## Governance Checklist (constitution-first)
**Run locally before opening this PR. Keep commands & outputs attached where requested.**

- [ ] **Manifests** are valid and de-duplicated  
      `make migrate FAIL_ON_CONFLICTS=1`
- [ ] **Guard** passes (no domain boundary violations / forbidden libs)  
      `make guard-check`  *(or)*  `poetry run core-admin guard check`
- [ ] **Drift evidence** reviewed (0 validation errors, 0 duplicates)  
      `make drift` → attach `reports/drift_report.json` (see Evidence)
- [ ] Any **waivers** added in `.intent/policies/intent_guard.yaml` are **time-boxed** with rationale & expiry
- [ ] Capability names are **unique** and **domain-prefixed** (e.g., `core:task-router`)
- [ ] No addition of `requests` (use **httpx**)

## Quality Checklist
- [ ] `black --check .`
- [ ] `ruff check .`
- [ ] `pytest` (add `-q` for quick run)
- [ ] Docs updated if behavior/rules changed (README or `docs/`)
- [ ] Security: no secrets or credentials added

---

## Evidence Bundle (attach/log)
- **Diff summary:** (inline or link)
- **Guard output:** (paste table or JSON)
- **Drift report:** attach `reports/drift_report.json`
- **Tests:** pass/fail summary (and coverage, if relevant)

## Risk
- Category: **Functional** / **Non-functional**
- Impacted area(s): …
- Rollback plan: Revert this PR

## Notes for Reviewers (optional)
<!-- Anything that helps reviewers navigate the change quickly. -->

--- END OF FILE ./.github/PULL_REQUEST_TEMPLATE.md: ---

--- START OF FILE ./.github/realase.yml ---
changelog:
  categories:
    - title: ✨ Features
      labels: [feat, type:feature]
    - title: 🐛 Fixes
      labels: [fix, type:bug]
    - title: 🧹 Chores
      labels: [chore]
    - title: 🧠 Docs
      labels: [docs, type:docs]
    - title: 🔧 Refactors
      labels: [refactor]
    - title: ✅ Tests
      labels: [test]


--- END OF FILE ./.github/realase.yml ---

--- START OF FILE ./.github/workflows/core-ci.yml ---
name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

permissions:
  contents: read

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # --- Install Poetry and ensure it's on PATH ---
      - name: Install Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install pipx
          python -m pipx ensurepath
          echo "$HOME/.local/bin" >> $GITHUB_PATH
          pipx install poetry
      - name: Poetry version
        run: poetry --version

      # (Optional but handy) Put the venv inside the repo so we can cache it
      - name: Configure Poetry
        run: poetry config virtualenvs.in-project true

      # Cache the virtualenv based on poetry.lock
      - name: Cache .venv
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}

      - name: Install dependencies
        run: poetry install --no-interaction --no-ansi

      - name: Static checks
        run: |
          poetry run ruff check .
          poetry run black --check .

      - name: Run tests
        run: poetry run pytest -q --cov=src --cov-report=xml

      - name: Export knowledge graph
        run: |
          mkdir -p reports
          poetry run core-admin guard kg-export

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: core-reports
          path: |
            coverage.xml
            reports/knowledge_graph.json

--- END OF FILE ./.github/workflows/core-ci.yml ---

--- START OF FILE ./.github/workflows/guard-and-drift.yml ---
name: Guard & Drift

on:
  push:
    branches: [ "**" ]
  pull_request:

jobs:
  guard-and-drift:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Poetry
        run: pipx install poetry

      - name: Install deps
        run: poetry install --no-interaction --no-ansi

      - name: Generate drift evidence (schema + duplicates)
        env:
          PYTHONPATH: src
        run: poetry run python src/system/tools/manifest_migrator.py all --fail-on-conflicts

      - name: Run import/dependency guard
        env:
          PYTHONPATH: src
        run: poetry run core-admin guard check

      - name: Lint + Tests (fast check)
        run: make fast-check

      - name: Upload drift_report.json
        if: ${{ hashFiles('reports/drift_report.json') != '' }}
        uses: actions/upload-artifact@v4
        with:
          name: drift_report
          path: reports/drift_report.json

--- END OF FILE ./.github/workflows/guard-and-drift.yml ---

--- START OF FILE ./.github/workflows/nightly-audit.yml ---
name: Nightly Constitutional Auditor (rolling issue)

on:
  workflow_dispatch:
  schedule:
    - cron: "30 2 * * *" # nightly @ 02:30 UTC

permissions:
  contents: read
  issues: write
  actions: read

concurrency:
  group: nightly-auditor
  cancel-in-progress: false

jobs:
  audit:
    name: Run auditor + drift and update rolling issue
    runs-on: ubuntu-latest
    timeout-minutes: 20
    continue-on-error: true
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: "1.8.3"
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Install dependencies
        run: poetry install --no-interaction --no-ansi

      - name: Run Knowledge Graph export + Auditor + Drift
        id: run_checks
        shell: bash
        run: |
          set +e
          mkdir -p reports

          # --- Knowledge Graph export (module form only; avoid admin/__init__ heavy imports) ---
          # Try with explicit --output first; if not supported, capture stdout to file.
          if PYTHONPATH=src poetry run python -m src.system.admin.guard kg-export --output reports/knowledge_graph.json 2> reports/kg_stderr.txt; then
            echo "KG export (with --output) OK" >> reports/kg_stderr.txt
          else
            PYTHONPATH=src poetry run python -m src.system.admin.guard kg-export > reports/knowledge_graph.json 2>> reports/kg_stderr.txt || true
            echo "KG export (stdout capture) attempted" >> reports/kg_stderr.txt
          fi

          # --- Constitutional Auditor ---
          PYTHONPATH=src poetry run python -m src.core.capabilities \
            > reports/capabilities_stdout.txt \
            2> reports/auditor_stderr.txt
          AUDIT_EXIT=$?

          # --- Drift (JSON report to reports/) ---
          PYTHONPATH=src poetry run python -m src.system.admin.guard drift \
            --strict-intent --fail-on any --format json --output reports/drift_report.json \
            > reports/drift_stdout.txt 2> reports/drift_stderr.txt
          DRIFT_EXIT=$?

          echo "audit_exit=${AUDIT_EXIT}" >> "$GITHUB_OUTPUT"
          echo "drift_exit=${DRIFT_EXIT}" >> "$GITHUB_OUTPUT"

          # Summary
          if [ "$AUDIT_EXIT" -eq 0 ]; then echo "✅ Constitutional Auditor passed" >> $GITHUB_STEP_SUMMARY; else echo "❌ Constitutional Auditor failed (exit $AUDIT_EXIT)" >> $GITHUB_STEP_SUMMARY; fi
          if [ "$DRIFT_EXIT" -eq 0 ]; then echo "✅ Drift clean" >> $GITHUB_STEP_SUMMARY; else echo "❌ Drift violations found (exit $DRIFT_EXIT)" >> $GITHUB_STEP_SUMMARY; fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: nightly-auditor-artifacts
          path: |
            reports/**
          retention-days: 5

      - name: Create or update the rolling issue
        uses: actions/github-script@v7
        env:
          AUDIT_EXIT: ${{ steps.run_checks.outputs.audit_exit }}
          DRIFT_EXIT: ${{ steps.run_checks.outputs.drift_exit }}
        with:
          script: |
            const { AUDIT_EXIT, DRIFT_EXIT } = process.env;
            const owner = context.repo.owner;
            const repo  = context.repo.repo;
            const title = "Nightly Constitutional Audit status";

            const ok = (x) => String(x) === "0";
            const head = (ok(AUDIT_EXIT) && ok(DRIFT_EXIT)) ? "✅" : "❌";
            const now  = new Date().toISOString();

            const summary = [
              `**Run:** ${context.runId} (workflow: ${context.workflow})`,
              `**Time (UTC):** ${now}`,
              `**Auditor:** ${ok(AUDIT_EXIT) ? "✅" : "❌"} exit=${AUDIT_EXIT}`,
              `**Drift:** ${ok(DRIFT_EXIT) ? "✅" : "❌"} exit=${DRIFT_EXIT}`,
              ``,
              `**Artifacts:** See job artifacts: "nightly-auditor-artifacts"`,
              ``,
              `<details><summary>Raw exits</summary>`,
              ``,
              `- AUDIT_EXIT=${AUDIT_EXIT}`,
              `- DRIFT_EXIT=${DRIFT_EXIT}`,
              ``,
              `</details>`
            ].join("\n");

            const header = [
              "# Nightly Constitutional Audit",
              "",
              "This is a rolling issue. Each nightly run updates this thread.",
              "",
              "Policy gates:",
              "- Ruff / Black / Tests are required on PRs",
              "- Constitutional Auditor should pass (or show only actionable warnings)",
              "- Drift must be clean",
              ""
            ].join("\n");

            const openIssues = await github.paginate(
              github.rest.issues.listForRepo,
              { owner, repo, state: "open", per_page: 100 }
            );
            const existing = openIssues.find(i => i.title.endsWith(title));

            if (!existing) {
              const created = await github.rest.issues.create({
                owner, repo,
                title: `${head} ${title}`,
                body: `${header}\n\n---\n\n## Latest Run\n\n${summary}`,
                labels: ["ci","audit"]
              });
              core.info(`Created issue #${created.data.number}`);
            } else {
              await github.rest.issues.createComment({
                owner, repo, issue_number: existing.number,
                body: `## Latest Run\n\n${summary}`
              });
              if (!existing.title.startsWith(head)) {
                await github.rest.issues.update({
                  owner, repo, issue_number: existing.number,
                  title: `${head} ${title}`
                });
              }
            }

--- END OF FILE ./.github/workflows/nightly-audit.yml ---

--- START OF FILE ./.gitignore ---
# --- Python bytecode & caches ---
__pycache__/
*.py[cod]
*.so
*.pyd
*.pyo
*.pyd
.pytest_cache/
.mypy_cache/
.pyre/
.pytype/
.ruff_cache/

# --- Virtual environments ---
.venv/
venv/
env/
ENV/

# --- Build & packaging ---
build/
dist/
*.egg-info/
.eggs/
pip-wheel-metadata/

# --- Coverage / testing artifacts ---
.coverage
.coverage.*
htmlcov/
.tox/

# --- Logs ---
logs/
*.log

# --- OS / Editor junk ---
.DS_Store
Thumbs.db
.idea/
.vscode/
*.swp
*.swo

# --- Environment files (keep an example if you want) ---
.env
.env.*
!.env.example

# --- Node (if present for UI) ---
node_modules/
npm-debug.log*
yarn.lock
pnpm-lock.yaml

# --- Reports: ignore everything except drift evidence & keep-file ---
/reports/*
!/reports/drift_report.json
!/reports/.gitkeep

--- END OF FILE ./.gitignore ---

--- START OF FILE ./.intent/config/local_mode.yaml ---
# .intent/config/local_mode.yaml

mode: local_fallback
apis:
  llm:
    enabled: false
    fallback: local_validator
  git:
    ignore_validation: false
--- END OF FILE ./.intent/config/local_mode.yaml ---

--- START OF FILE ./.intent/config/runtime_requirements.yaml ---
# .intent/config/runtime_requirements.yaml
#
# PURPOSE: This file makes the system aware of its dependencies on the external environment.
# It allows the ConstitutionalAuditor to verify that the system is correctly configured
# at startup, without ever reading secret values.

required_environment_variables:
  - name: "MIND"
    description: "The relative path to the system's declarative 'mind' (.intent directory)."
    type: "config"
    required: true

  - name: "BODY"
    description: "The relative path to the system's executable 'body' (src directory)."
    type: "config"
    required: true

  - name: "REPO_PATH"
    description: "The absolute path to the root of the repository."
    type: "config"
    required: true

  - name: "ORCHESTRATOR_API_URL"
    description: "The API endpoint for the high-level planning LLM."
    type: "config"
    required: false
    required_when: "apis.llm.enabled == true"

  - name: "ORCHESTRATOR_API_KEY"
    description: "The API key for the high-level planning LLM."
    type: "secret"
    required: false
    required_when: "apis.llm.enabled == true"

  - name: "ORCHESTRATOR_MODEL_NAME"
    description: "The name of the model to use for orchestration."
    type: "config"
    required: false
    required_when: "apis.llm.enabled == true"

  - name: "GENERATOR_API_URL"
    description: "The API endpoint for the code generation LLM."
    type: "config"
    required: false
    required_when: "apis.llm.enabled == true"

  - name: "GENERATOR_API_KEY"
    description: "The API key for the code generation LLM."
    type: "secret"
    required: false
    required_when: "apis.llm.enabled == true"

  - name: "CORE_ACTION_LOG_PATH"
    description: "Path to .intent/change_log.json used by safety policies."
    type: "config"
    required: true
--- END OF FILE ./.intent/config/runtime_requirements.yaml ---

--- START OF FILE ./.intent/constitution/approvers.yaml ---
# .intent/constitution/approvers.yaml
#
# This file defines the human operators authorized to approve constitutional
# changes and the rules governing that process.
#
# The formal procedures for onboarding, offboarding, and key management are
# constitutionally defined in the following document:
#
# ./operator_lifecycle.md

approvers:
  - identity: "core-team@core-system.ai"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEA3dK7Jt4jJh6+QvZvY6XcGx3q8R0e7m5JwqYk8qFtU9U=
      -----END PUBLIC KEY-----
    created_at: "2025-08-05T15:50:53.995534+00:00"
    role: "maintainer"
    description: "Primary CORE development team"

  - identity: "security-audit@core-system.ai"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEApJ+8mNvL7wY2XfDcR9q3Q5t4yZx7v6hB8gKj0sF3T5U=
      -----END PUBLIC KEY-----
    created_at: "2025-08-05T15:50:53.995534+00:00"
    role: "security"
    description: "Security audit team for constitutional changes"

  - identity: "d.newecki@gmail.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VwAyEA+V4iUN4DElKdqXmU4ivNthnG8VgPb7QqZgzdJuh4igs=
      -----END PUBLIC KEY-----
    created_at: "2025-08-12T10:36:49.000000Z"
    role: "maintainer"
    description: "Mentor"

# Minimum number of signatures required for constitutional amendments
quorum:
  # Defines quorum levels for different operational stages.
  development:
    standard: 1
    critical: 1
  production:
    standard: 2
    critical: 3
  
  # The currently active mode. Must be one of the valid_modes.
  current_mode: development
  
  # --- NEW: As suggested by peer review ---
  # Defines the valid operational modes for the system.
  valid_modes: [development, production]
  # NOTE: Changing the current_mode is a critical change and requires the critical quorum.

# Critical policy paths that require higher quorum
critical_paths:
  - ".intent/policies/intent_guard.yaml"
  - ".intent/constitution/approvers.yaml"
  - ".intent/meta.yaml"

--- END OF FILE ./.intent/constitution/approvers.yaml ---

--- START OF FILE ./.intent/constitution/approvers.yaml.example ---
# .intent/constitution/approvers.yaml
#
# PURPOSE: This file enables cryptographic verification of constitutional approvals,
# preventing unauthorized changes. It contains the public keys of all authorized
# constitutional approvers for this instance of CORE.
#
# TO ADD A NEW APPROVER:
# 1. Run the command: `core-admin keygen "your.email@example.com"`
# 2. The command will output a JSON/YAML block.
# 3. Paste that block into the 'approvers' list below.

approvers:
  - identity: "your.name@example.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
      -----END PUBLIC KEY-----
    created_at: "YYYY-MM-DDTHH:MM:SSZ"
    role: "maintainer"
    description: "Primary maintainer of this CORE instance"

  - identity: "another.approver@example.com"
    public_key: |
      -----BEGIN PUBLIC KEY-----
      MCowBQYDK2VuAyEBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB=
      -----END PUBLIC KEY-----
    created_at: "YYYY-MM-DDTHH:MM:SSZ"
    role: "contributor"
    description: "Authorized contributor"

# Minimum number of valid signatures required to approve a constitutional amendment.
quorum:
  # Regular amendments (e.g., changing a policy) require 1 signature.
  standard: 1
  # Critical changes (e.g., altering this file) require 2 signatures.
  critical: 2

# A list of file paths that are considered "critical". Any proposal targeting
# these files will require the 'critical' quorum to be met.
critical_paths:
  - ".intent/policies/intent_guard.yaml"
  - ".intent/constitution/approvers.yaml"
  - ".intent/meta.yaml"

--- END OF FILE ./.intent/constitution/approvers.yaml.example ---

--- START OF FILE ./.intent/constitution/operator_lifecycle.md ---
# Human Operator Lifecycle Procedures

This document defines the formal, constitutionally-mandated procedures for managing human operators who have the authority to approve changes to the CORE constitution. Adherence to these procedures is mandatory and enforced by peer review during any change to the `approvers.yaml` file.

## Onboarding a New Approver

1.  **Key Generation:** The candidate operator MUST generate a new, secure Ed25519 key pair using the following command from the CORE repository root:
    ```bash
    poetry run core-admin keygen "candidate.email@example.com"
    ```
2.  **Proposal Creation:** A currently active, authorized approver MUST create a formal constitutional amendment proposal.
    - The `target_path` of the proposal MUST be `.intent/constitution/approvers.yaml`.
    - The `justification` MUST clearly state the reason for adding the new approver, including their role and identity.
    - The `content` of the proposal MUST be the complete `approvers.yaml` file with the new approver's YAML block appended.
3.  **Ratification:** The proposal must be signed and approved, meeting the required quorum as defined in `approvers.yaml`. Upon successful canary validation and approval, the new operator is considered active.

## Standard Revocation of an Approver

1.  **Proposal Creation:** An authorized approver MUST create a formal proposal to remove the target operator's block from the `approvers` list in `approvers.yaml`.
2.  **Justification:** The `justification` MUST clearly state the non-emergency reason for revocation (e.g., operator has left the project).
3.  **Ratification:** The proposal must be signed and approved, meeting the required quorum.

## Emergency Revocation of a Compromised Key

1.  **Proposal Creation:** In the event of a suspected or confirmed private key compromise, any active approver MUST immediately create an emergency revocation proposal targeting `approvers.yaml`.
2.  **Quorum:** This proposal requires the **critical** quorum to be met for approval.
3.  **Immediate Invalidation:** The moment a revocation proposal is created for an identity, that identity's signature is considered invalid for all quorum calculations on that proposal and any subsequent proposals until the matter is resolved.
--- END OF FILE ./.intent/constitution/operator_lifecycle.md ---

--- START OF FILE ./.intent/evaluation/audit_checklist.yaml ---
audit_checklist:
  - id: declared_intent
    item: "Was the intent declared before the change?"
    required: true
  - id: explanation
    item: "Was the change explained or justified?"
    required: true
  - id: manifest_sync
    item: "Did the change include a manifest update?"
    required: true
  - id: checkpoint
    item: "Was a rollback plan or checkpoint created?"
    required: false
  - id: quality_verified
    item: "Was code quality verified post-write?"
    required: true

--- END OF FILE ./.intent/evaluation/audit_checklist.yaml ---

--- START OF FILE ./.intent/evaluation/score_policy.yaml ---
score_policy:
  strategy: weighted_criteria

  criteria:
    - id: intent_alignment
      description: "Does this change serve a declared intent?"
      weight: 0.4

    - id: structural_compliance
      description: "Does it follow folder conventions and manifest structure?"
      weight: 0.2

    - id: safety
      description: "Was the change gated by a test or checkpoint?"
      weight: 0.2

    - id: code_quality
      description: "Does it pass formatting, linting, and basic semantic checks?"
      weight: 0.2

  thresholds:
    pass: 0.7
    warn: 0.5
    fail: 0.4

--- END OF FILE ./.intent/evaluation/score_policy.yaml ---

--- START OF FILE ./.intent/knowledge/agent_roles.yaml ---
# .intent/knowledge/agent_roles.yaml

roles:
  planner:
    description: "Responsible for breaking down intents, sequencing tasks, and preparing bundles."
    allowed_tags:
      - introspection
      - llm_orchestration
      - prompt_interpretation

  builder:
    description: "Executes generation and modification tasks according to a validated plan."
    allowed_tags:
      - code_generation
      - semantic_validation
      - test_execution

  reviewer:
    description: "Evaluates changes for safety, structure, and declared alignment."
    allowed_tags:
      - semantic_validation
      - alignment_checking
      - test_execution

  orchestrator:
    description: "Coordinates flows, executes bundles, and manages lifecycle rules."
    allowed_tags:
      - llm_orchestration
      - intent_guarding
      - alignment_checking

  guardian:
    description: "Handles enforcement of rules and monitors intent integrity."
    allowed_tags:
      - intent_guarding
      - alignment_checking
--- END OF FILE ./.intent/knowledge/agent_roles.yaml ---

--- START OF FILE ./.intent/knowledge/capability_tags.yaml ---
# .intent/knowledge/capability_tags.yaml
#
# This is the canonical dictionary of all valid capability tags in the CORE system.
# The ConstitutionalAuditor verifies that any # CAPABILITY tag used in the source code
# is defined in this file.

tags:
  # --- System & Governance ---
  - name: introspection
    description: "Enables self-analysis of the system's own structure, code, or intent."
  - name: alignment_checking
    description: "Verifies that system components or actions align with constitutional principles."
  - name: manifest_updating
    description: "Modifies or generates knowledge artifacts like the knowledge_graph.json."
  - name: self_review
    description: "Enables the system to analyze its own code for quality, correctness, or improvements."
  - name: intent_guarding
    description: "Enforces constitutional rules at runtime, preventing forbidden actions."
  - name: change_safety_enforcement
    description: "Implements safety checks or operations related to modifying files or state (e.g., Git)."
  - name: system_logging
    description: "Provides system-wide logging capabilities."

  # --- Code Validation & Quality ---
  - name: semantic_validation
    description: "Performs semantic analysis on code, beyond simple syntax checks."
  - name: syntax_validation
    description: "Performs syntax validation on code or configuration files."
  - name: code_quality_analysis
    description: "Runs a pipeline of quality checks (e.g., formatting, linting)."
  - name: test_execution
    description: "Executes automated tests (e.g., pytest) and reports results."

  # --- LLM & Agent Orchestration ---
  - name: llm_orchestration
    description: "Manages the flow of requests and plans to one or more LLMs."
  - name: prompt_interpretation
    description: "Processes and enriches prompts with context before sending them to an LLM."
  - name: code_generation
    description: "Specifically handles the generation of new source code."
  - name: self_correction
    description: "Attempts to automatically fix errors based on validation or test feedback."

  # --- Constitutional Auditor Checks (discoverable micro-capabilities) ---
  - name: audit.check.required_files
    description: "Auditor check: Verifies the existence of critical .intent files."
  - name: audit.check.syntax
    description: "Auditor check: Validates the syntax of all .intent YAML/JSON files."
  - name: audit.check.project_manifest
    description: "Auditor check: Validates the integrity of project_manifest.yaml."
  - name: audit.check.capability_coverage
    description: "Auditor check: Ensures all required capabilities are implemented."
  - name: audit.check.capability_definitions
    description: "Auditor check: Ensures all implemented capabilities are defined in this file."
  - name: audit.check.knowledge_graph_schema
    description: "Auditor check: Validates all knowledge graph symbols against the schema."
  - name: audit.check.domain_integrity
    description: "Auditor check: Checks for domain mismatches and illegal imports."
  - name: audit.check.docstrings
    description: "Auditor check: Finds symbols missing docstrings or having generic intents."
  - name: audit.check.dead_code
    description: "Auditor check: Detects unreferenced public symbols."
  - name: audit.check.orphaned_intent_files
    description: "Auditor check: Finds .intent files that are not referenced in meta.yaml."
  - name: audit.check.environment
    description: "Auditor check: Verifies that required environment variables are set."
  - name: audit.check.proposals_schema
    description: "Auditor check: Validates each proposal against its JSON schema."
  - name: audit.check.proposals_drift
    description: "Auditor check: Detects if a proposal's content has changed after being signed."
  - name: audit.check.proposals_list
    description: "Auditor check: Lists all pending proposals for visibility during an audit."
  - name: audit.check.duplication
    description: "Auditor check: Finds structurally identical code, violating the 'dry_by_design' principle."
  - name: audit.check.content_drift
    description: "Auditor check: Detects duplicated or inconsistent content across designated data files."
  - name: audit.check.codebase_health
    description: "Auditor check: Measures code complexity and atomicity against defined policies."
  - name: audit.check.secrets
    description: "Auditor check: Scans for hardcoded secrets like API keys or passwords."    
  - name: scaffold_project
    description: "Generates a complete, multi-file project structure from a high-level goal, including a starter constitution."

  # --- Planned or Placeholder Capabilities ---
  - name: add_missing_docstrings
    description: "A planned capability to automatically add docstrings to undocumented code."
  - name: refactor_to_shared_function
    description: "A planned capability to consolidate duplicated logic into a single shared function."

  # --- Constitutional Peer Review ---
  - name: export_constitution
    description: "Packages the full .intent/ directory into a single bundle for external analysis."
  - name: constitutional_peer_review
    description: "Orchestrates sending the constitutional bundle to an external LLM for critique and suggestions."
--- END OF FILE ./.intent/knowledge/capability_tags.yaml ---

--- START OF FILE ./.intent/knowledge/entry_point_patterns.yaml ---
# .intent/knowledge/entry_point_patterns.yaml
#
# A declarative set of rules for the KnowledgeGraphBuilder to identify valid
# system entry points that are not discoverable through simple call-graph analysis.
# This prevents the auditor from incorrectly flagging valid code as "dead."

patterns:
  - name: "python_magic_method"
    description: "Standard Python __dunder__ methods are entry points called by the interpreter."
    match:
      type: "function"
      name_regex: "^__.+__$"
    entry_point_type: "magic_method"

  - name: "ast_visitor_method"
    description: "Methods in ast.NodeVisitor subclasses starting with 'visit_' are entry points for the visitor pattern."
    match:
      type: "function"
      name_regex: "^visit_"
      base_class_includes: "NodeVisitor"
    entry_point_type: "visitor_method"

  - name: "capability_implementation"
    description: "Any symbol tagged with a # CAPABILITY is a primary entry point for the CORE system's reasoning loop."
    match:
      has_capability_tag: true
    entry_point_type: "capability"
    
  # --- THIS IS THE NEW RULE ---
  - name: "typer_cli_command"
    description: "Functions registered as Typer CLI commands are valid entry points called by the user."
    match:
      # This is a heuristic. A more robust check might look for specific decorators,
      # but for our project, checking the module path is very effective.
      module_path_contains: "src/system/admin"
      is_public_function: true # i.e., does not start with an underscore
    entry_point_type: "cli_command"
  # --- END OF NEW RULE ---

  - name: "framework_base_class"
    description: "Classes that other components inherit from are valid entry points."
    match:
      type: "class"
      is_base_class: true
    entry_point_type: "base_class"

  - name: "pydantic_model"
    description: "Pydantic models are data structures, not callable code, and are valid entry points."
    match:
      type: "class"
      base_class_includes: "BaseModel"
    entry_point_type: "data_model"

  - name: "enum_definition"
    description: "Enum classes are data structures, not callable code, and are valid entry points."
    match:
      type: "class"
      base_class_includes: "Enum"
    entry_point_type: "enum"

  - name: "dataclass_definition"
    description: "Dataclasses are data structures, not callable code, and are valid entry points."
    match:
      type: "class"
      has_decorator: "dataclass"
    entry_point_type: "data_model"
--- END OF FILE ./.intent/knowledge/entry_point_patterns.yaml ---

--- START OF FILE ./.intent/knowledge/file_handlers.yaml ---
handlers:
  - type: python
    extensions: [".py"]
    parse_as: ast
    editable: true
    description: Python source code with manifest-enforced governance

  - type: markdown
    extensions: [".md"]
    parse_as: text
    editable: true
    description: Human-readable docs. Require manual review in sensitive areas.

  - type: yaml
    extensions: [".yaml", ".yml"]
    parse_as: structured
    editable: true
    description: Configuration, policies, intent declarations

  - type: json
    extensions: [".json"]
    parse_as: structured
    editable: true
    description: Machine-readable manifests and graphs

  - type: binary
    extensions: [".png", ".jpg", ".pdf"]
    parse_as: none
    editable: false
    description: Visual artifacts — viewable only

--- END OF FILE ./.intent/knowledge/file_handlers.yaml ---

--- START OF FILE ./.intent/knowledge/knowledge_graph.json ---
{
  "schema_version": "2.0.0",
  "metadata": {
    "files_scanned": 2,
    "total_symbols": 0,
    "timestamp_utc": "2025-08-18T18:14:37.954435+00:00"
  },
  "symbols": {}
}
--- END OF FILE ./.intent/knowledge/knowledge_graph.json ---

--- START OF FILE ./.intent/knowledge/knowledge_graph.json.lock ---
[EMPTY FILE]

--- END OF FILE ./.intent/knowledge/knowledge_graph.json.lock ---

--- START OF FILE ./.intent/knowledge/source_structure.yaml ---
# .intent/knowledge/source_structure.yaml
# Purpose: Single source of truth for domains, their folders, and import allow-lists.
# This file is consumed by guards and tooling. It aligns with File 2 (intent_guard.yaml).

version: 1
about: "Domain map + allowed imports. Deny-by-default outside what’s listed here."

# ---- DRY anchors (not parsed as domains; just reusable lists) ----
anchors:
  stdlib_common: &stdlib_common
    - typing
    - types
    - pathlib
    - logging
    - re
    - dataclasses
    - json
    - os
    - sys
    - uuid
    - itertools
    - functools
    - datetime

  third_party_core: &third_party_core
    - fastapi
    - pydantic
    - httpx
    - uvicorn
    - yaml
    - jsonschema
    - rich

  third_party_cli: &third_party_cli
    - typer
    - black

# ---- Domains ----
domains:
  - domain: shared
    path: src/shared
    enabled: true
    description: "Cross-cutting types, constants, helpers. NO runtime side-effects."
    allowed_imports:
      - shared
      - *stdlib_common
      - pydantic        # shared models/helpers can use pydantic
    forbidden_imports:
      - core
      - agents
      - system
      - data

  - domain: core
    path: src/core
    enabled: true
    description: "Business logic and services. Depends on shared only (DIP)."
    allowed_imports:
      - core
      - shared
      - *stdlib_common
      - *third_party_core
    forbidden_imports:
      - agents
      - system
      - data            # DIP: core does NOT import data; data implements core interfaces

  - domain: agents
    path: src/agents
    enabled: true
    description: "Agent behaviors. May call core; never system or data."
    allowed_imports:
      - agents
      - core
      - shared
      - *stdlib_common
      - *third_party_core
      - subprocess      # gated by policy; only for scaffolding capability
    forbidden_imports:
      - system
      - data

  - domain: system
    path: src/system
    enabled: true
    description: "Orchestration/CLI/infra wiring. One-way bridge to agents."
    allowed_imports:
      - system
      - core
      - agents          # explicit bridge (system → agents)
      - shared
      - *stdlib_common
      - *third_party_core
      - *third_party_cli
    forbidden_imports:
      - data            # wiring to data happens via core abstractions

  - domain: data
    path: src/data
    enabled: false      # toggle to true when the package exists and is included
    description: "Implementations of core ports/adapters (DB, external systems)."
    allowed_imports:
      - data
      - shared
      - core            # implements interfaces defined in core (DIP)
      - *stdlib_common
      # Add DB libs here when you actually use them (example):
      # - sqlalchemy
      # - asyncpg
    forbidden_imports:
      - agents
      - system

# Optional: global notes for tools (non-enforced hints)
notes:
  - "Default stance is deny; keep lists minimal. Add imports only when needed."
  - "Requests is intentionally NOT allowed; use httpx."
  - "If you enable 'data', ensure your packaging includes src/data or flip enabled:true later."

--- END OF FILE ./.intent/knowledge/source_structure.yaml ---

--- START OF FILE ./.intent/manifests/agents.manifest.json ---
{
  "domain": "agents",
  "version": "0.1.0",
  "description": "Agent behaviors. May call core; never system or data.",
  "owners": [],
  "capabilities": [
    "agents:scaffolding",
    "agents:docstring-fixer",
    "agents:proposal-review"
  ],
  "imports": ["shared", "core"],
  "notes": "Agents invoke core services; data access must go via core."
}

--- END OF FILE ./.intent/manifests/agents.manifest.json ---

--- START OF FILE ./.intent/manifests/core.manifest.json ---
{
  "domain": "core",
  "version": "0.1.0",
  "description": "Business logic and services. Depends on shared only (DIP).",
  "owners": [],
  "capabilities": [
    "core:business-services",
    "core:intent-guard-engine",
    "core:capability-mapping"
  ],
  "imports": ["shared"],
  "notes": "Defines interfaces/ports; data implementations live elsewhere."
}

--- END OF FILE ./.intent/manifests/core.manifest.json ---

--- START OF FILE ./.intent/manifests/data.manifest.json ---
{
  "domain": "data",
  "version": "0.1.0",
  "description": "Adapters for DB/external systems that implement core ports.",
  "owners": [],
  "capabilities": [
    "data:adapters",
    "data:repositories",
    "data:external-integrations"
  ],
  "imports": ["shared", "core"],
  "notes": "Even if domain is disabled in source_structure.yaml, a manifest is fine and keeps validation green."
}

--- END OF FILE ./.intent/manifests/data.manifest.json ---

--- START OF FILE ./.intent/manifests/shared.manifest.json ---
{
  "domain": "shared",
  "version": "0.1.0",
  "description": "Cross-cutting types, constants, helpers. NO runtime side-effects.",
  "owners": [],
  "capabilities": [
    "shared:primitives",
    "shared:validation-utils",
    "shared:config-helpers"
  ],
  "imports": [],
  "notes": "Foundation utilities used by all other domains."
}

--- END OF FILE ./.intent/manifests/shared.manifest.json ---

--- START OF FILE ./.intent/manifests/system.manifest.json ---
{
  "domain": "system",
  "version": "0.1.0",
  "description": "Orchestration/CLI/infra wiring. One-way bridge to agents.",
  "owners": [],
  "capabilities": [
    "system:cli-orchestration",
    "system:service-wiring",
    "system:guard-execution"
  ],
  "imports": ["shared", "core", "agents"],
  "notes": "System may import agents (bridge); agents must NOT import system."
}

--- END OF FILE ./.intent/manifests/system.manifest.json ---

--- START OF FILE ./.intent/meta.yaml ---
version: "0.1.0"

# PURPOSE: canonical index of constitutional & governance files (.intent is the constitution)
constitution:
  approvers: "constitution/approvers.yaml"
  operator_lifecycle: "constitution/operator_lifecycle.md"

mission:
  northstar: "mission/northstar.yaml"
  manifesto: "mission/manifesto.md"
  principles: "mission/principles.yaml"

policies:
  intent_guard: "policies/intent_guard.yaml"
  safety_policies: "policies/safety_policies.yaml"
  security_intents: "policies/security_intents.yaml"
  code_health_policy: "policies/code_health_policy.yaml"
  enforcement_model: "policies/enforcement_model.yaml"
  secrets_management: "policies/secrets_management.yaml"
  dependency_management: "policies/dependency_management.yaml"
  incident_response: "policies/incident_response.yaml"

prompts:
  constitutional_review: "prompts/constitutional_review.prompt"

knowledge:
  # Source-of-truth domain map (File 3 updated this to `domains:` format)
  source_structure: "knowledge/source_structure.yaml"
  # Optional knowledge graph artifact used by some tools
  codegraph: "knowledge/knowledge_graph.json"
  capability_tags: "knowledge/capability_tags.yaml"
  agent_roles: "knowledge/agent_roles.yaml"
  entry_point_patterns: "knowledge/entry_point_patterns.yaml"
  file_handlers: "knowledge/file_handlers.yaml"

evaluation:
  score_policy: "evaluation/score_policy.yaml"
  audit_checklist: "evaluation/audit_checklist.yaml"

config:
  local_mode: "config/local_mode.yaml"
  runtime_requirements: "config/runtime_requirements.yaml"

schemas:
  # NEW: manifest schema used by manifest_migrator.py (File 1)
  manifest: "schemas/manifest.schema.json"
  # Existing
  config: "schemas/config_schema.yaml"
  knowledge_graph_entry: "schemas/knowledge_graph_entry.schema.json"

# NEW: explicit manifests directory for domain manifests (used by manifest_migrator.py)
manifests:
  dir: "manifests"

# NEW: top-level reports path for tools that write drift evidence (File 4 uses this)
reports:
  drift: "reports/drift_report.json"

# Operator UX defaults for guard commands (keep as-is; CLI reads these)
operator_experience:
  guard:
    drift:
      default_format: pretty
      default_fail_on: any
      evidence_path: reports/drift_report.json
      labels:
        none: "NONE"
        success: "✅ No capability drift"
        failure: "🚨 Drift detected"

--- END OF FILE ./.intent/meta.yaml ---

--- START OF FILE ./.intent/mission/manifesto.md ---
# CORE Manifesto

CORE is not a framework. It is not a plugin system.  
CORE is a system that evolves itself — governed by explicit declarations of purpose, constraints, and structure.

---

### Beliefs

- Software is never done, but it must always be explainable.
- Changes must serve a declared intent — or challenge it explicitly.
- LLMs are powerful, but must be contained by purpose, not left to guess.
- Governance is not bureaucracy; it’s alignment between vision and action.

---

### CORE exists to:

- Understand its own capabilities
- Apply changes safely and transparently
- Validate every change against declared rules
- Learn from its outcomes and improve
- Inspire new systems to govern themselves

---

CORE does not pretend to be perfect.  
But it refuses to be mindless.

--- END OF FILE ./.intent/mission/manifesto.md ---

--- START OF FILE ./.intent/mission/northstar.yaml ---
# .intent/mission/northstar.yaml

# PURPOSE: This fulfills evolvable_structure and defines CORE's NorthStar.
name: CORE
version: v0.1.0
purpose: >
  CORE is a self-improving, intent-aware development system.
  Its purpose is to orchestrate safe, meaningful, and governed changes
  to its own codebase and related artifacts through intent bundles and introspective loops.

scope:
  - Planning and decomposition of tasks
  - Code generation via LLMs
  - Change validation and governance enforcement
  - Self-introspection and structural analysis
  - Knowledge management via manifests and graphs
  - Continuous self-evaluation and auditability

values:
  - Clarity over cleverness
  - Safety before speed
  - Traceability of every action
  - Alignment with declared purpose
  - Capability-driven reasoning

notes:
  - CORE evolves iteratively, but never silently.
  - All changes must fulfill a declared intent or generate a proposal to revise that intent.
--- END OF FILE ./.intent/mission/northstar.yaml ---

--- START OF FILE ./.intent/mission/principles.yaml ---
# .intent/mission/principles.yaml
#
# CORE's Constitution: clear, enforceable, and readable by humans and LLMs.
# Any agent (including future LLMs) must understand and obey these rules.
# This file contains high-level, aspirational values. Specific, machine-enforceable
# rules are defined in the relevant policy files.

principles:

  - id: clarity_first
    description: >
      Prioritize clear, understandable code and documentation that effectively
      communicates its intent to both humans and machines. If something is
      ambiguous, it must be simplified.

  - id: safe_by_default
    description: >
      Every change must assume rollback or rejection unless explicitly validated.
      No file write, code execution, or intent update may proceed without confirmation.
      Rollback must be possible at every stage.

  - id: reason_with_purpose
    description: >
      Every autonomous planning step must be traceable to a core constitutional
      principle or a declared high-level goal, ensuring all actions are deliberate
      and auditable.

  - id: evolvable_structure
    description: >
      The system's structure and constitution must be designed to evolve safely.
      Self-modification must be governed by a formal, secure, and auditable
      amendment process.

  - id: no_orphaned_logic
    description: >
      No function, file, or rule may exist without being discoverable and traceable
      through the system's knowledge artifacts (e.g., knowledge_graph.json).
      All logic must serve a declared purpose.

  - id: use_intent_bundle
    description: >
      All significant autonomous actions must be executed via a structured
      IntentBundle that reflects the Ten-Phase Loop of Reasoned Action. No phase
      may be skipped.

  - id: minimalism_over_completeness
    description: >
      Prefer small, focused changes. Do not generate stubs, placeholders, or
      unused functions. Unused or untestable logic is a liability and must be removed.

  - id: dry_by_design
    description: >
      "Don't Repeat Yourself." No logic or configuration may be duplicated. If a
      function, pattern, or rule exists in one place, it must be reused or
      referenced, not rewritten.

  - id: single_source_of_truth
    description: >
      `project_manifest.yaml` is the single source of truth for capabilities, structure, and intent.
      Derived artifacts (e.g., `knowledge_graph.json`) must be generated from it; manual edits are rejected.

  - id: separation_of_concerns
    description: >
      Each architectural domain must have a single, clearly defined responsibility.
      Inter-domain communication must be explicitly declared and governed by the
      constitution.

  - id: predictable_side_effects
    description: >
      Any action that modifies the system's state (e.g., a file write) must be
      explicit, logged, and reversible. Silent or unlogged changes are forbidden.

  - id: policy_change_requires_human_review
    description: >
      Any change to a policy file within the `.intent/policies/` directory must be
      ratified through the formal constitutional amendment process, requiring
      human review and approval.
--- END OF FILE ./.intent/mission/principles.yaml ---

--- START OF FILE ./.intent/policies/code_health_policy.yaml ---
# .intent/policies/code_health_policy.yaml
#
# This policy defines the machine-enforceable rules for maintaining the
# health, clarity, and atomicity of the codebase ("The Body").
# These rules are enforced by the 'audit.check.codebase_health' capability.

rules:
  # Defines the maximum acceptable Cognitive Complexity for any single function.
  # Functions exceeding this limit are considered too difficult for a human to
  # reliably understand and maintain.
  # Recommended starting value: 15
  max_cognitive_complexity: 15

  # Defines the maximum acceptable nesting depth (e.g., if inside for inside if).
  # Deeply nested logic is a primary source of bugs and confusion.
  # This rule formalizes the soft rule that was previously in intent_guard.yaml.
  # Recommended starting value: 4
  max_nesting_depth: 4

  # --- THIS IS THE NEW RULE ---
  # Defines the recommended maximum line length for code clarity.
  # This is a "soft" guideline. While 'black' will attempt to format to 88
  # characters, this rule allows the auditor to flag lines that remain
  # excessively long, suggesting they may need manual refactoring.
  max_line_length:
    # Set slightly higher than black's default to only catch egregious cases.
    limit: 100
    # This is a 'soft' rule; it will produce a WARNING, not an ERROR,
    # encouraging refactoring without blocking commits.
    enforcement: soft
  # --- END OF NEW RULE ---

  # Defines the statistical threshold for identifying file size outliers.
  # A file is flagged as an outlier if its Logical Lines of Code (LLOC)
  # is greater than (average_lloc + standard_deviations * lloc_std_dev).
  # Recommended starting value: 2.0
  outlier_standard_deviations: 2.0
--- END OF FILE ./.intent/policies/code_health_policy.yaml ---

--- START OF FILE ./.intent/policies/dependency_management.yaml ---
# .intent/policies/dependency_management.yaml
#
# PURPOSE: This policy governs the addition and auditing of third-party
# dependencies (e.g., Python packages from PyPI). It includes rules
# for license compatibility, vulnerability scanning, and supply chain security.
#
# STATUS: This policy is defined but not yet enforced by an auditor check.

rules:
  - id: license_compatibility
    description: All dependencies must have OSI-approved licenses compatible with our MIT license.
    enforcement: hard

  - id: vulnerability_scanning
    description: All dependencies must be scanned for known vulnerabilities before addition.
    enforcement: hard

  - id: pinned_versions
    description: Production dependencies must have exact version pins.
    enforcement: hard
--- END OF FILE ./.intent/policies/dependency_management.yaml ---

--- START OF FILE ./.intent/policies/enforcement_model.yaml ---
# .intent/policies/enforcement_model.yaml
#
# PURPOSE: This file fulfills the 'clarity_first' principle by providing a
# single, canonical source of truth for the definition of all enforcement
# levels used in CORE's governance policies.

enforcement_levels:
  - level: hard
    description: >
      A non-negotiable rule. If a proposed action violates a 'hard' policy,
      the action MUST be automatically rejected by the system's IntentGuard.
      No override is possible without a formal, approved constitutional amendment
      to the policy itself. This is the highest level of enforcement.

  - level: soft
    description: >
      A strong guideline. If a proposed action violates a 'soft' policy, the
      system's IntentGuard MUST log a formal WARNING. However, the action is
      allowed to proceed. This is used for rules that are important but may have
      valid exceptions, or for new rules being phased in.

  - level: manual_review
    description: >
      A gate that requires human intervention. If a proposed action violates a
      'manual_review' policy, the system MUST pause the execution and require
      formal, cryptographic proof of human intent before proceeding. This is
      typically handled via the 'core-admin proposals-approve' workflow.
--- END OF FILE ./.intent/policies/enforcement_model.yaml ---

--- START OF FILE ./.intent/policies/incident_response.yaml ---
# .intent/policies/incident_response.yaml
#
# PURPOSE: This document defines the formal procedure for responding to
# security incidents, constitutional violations, or critical operational failures.
# It includes steps for containment, analysis, and remediation.
#
# STATUS: This policy is defined but not yet enforced by an auditor check.

procedures:
  - id: containment
    description: "The immediate steps to limit the impact of an incident."
    steps:
      - "Isolate affected components from the network."
      - "Freeze all pending constitutional change proposals."
      - "Switch system to safe/local_fallback mode to prevent external actions."

  - id: analysis
    description: "The process for understanding the root cause of an incident."
    steps:
      - "Create a forensic copy of the .intent/ directory and all change logs."
      - "Trace the sequence of actions leading to the incident using audit logs."

  - id: remediation
    description: "The actions taken to resolve the incident and prevent recurrence."
    steps:
      - "Develop and ratify a constitutional amendment to address the root cause."
      - "Rotate all cryptographic keys and secrets if a compromise is suspected."
--- END OF FILE ./.intent/policies/incident_response.yaml ---

--- START OF FILE ./.intent/policies/intent_guard.yaml ---
# .intent/policies/intent_guard.yaml
# Purpose: Enforce clean domain boundaries, avoid cycles, and document explicit bridges.
version: 2
about: Import/dependency guard for domain boundaries. .intent/ is the constitution; this file is the law.

rules:
  imports:
    # HARD RULE: never allow dependency cycles between domains.
    disallow_cycles: true

    # Treat any pair NOT listed as DENY.
    default_policy: deny

    # Respect the domain switch in knowledge/source_structure.yaml.
    # If a domain is marked enabled: false, ignore it in checks (no violations emitted).
    respect_source_structure_enabled: true

    # Declare which domains may import which other domains (one-way unless bridged).
    domains:
      shared:
        may_import: []            # lowest layer; no internal deps
      core:
        may_import: [shared]      # business logic depends only on shared
      agents:
        may_import: [shared, core]  # agents call core services; no system/data dependency
      system:
        may_import: [shared, core, agents]  # one-way bridge: system → agents (CLI/orchestrators)
      data:
        may_import: [shared, core] # data implements core ports; core does NOT import data

    # Explicit bridges (documented exceptions that are STILL acyclic).
    allowed_bridges:
      - from: system
        to: agents
        rationale: "CLIs/services in system orchestrate agents; keep direction one-way to avoid cycles."

    # Hard forbids to make intent loud & clear (even if someone edits the domain map).
    forbidden:
      - from: agents
        to: system
        rationale: "Prevents system↔agents cycle. Agents must not depend on orchestration layer."
      - from: agents
        to: data
        rationale: "Data access is centralized in core; agents go via core services."
      - from: core
        to: data
        rationale: "Dependency inversion principle: core defines interfaces; data implements them."

    # Libraries guard (package-level).
    libraries:
      # Default stance for 3rd-party is allow-list; keep this list short.
      policy: allow
      allowed:
        - fastapi
        - httpx
        - pydantic
        - rich
        - uvicorn
        - yaml
        - jsonschema
        - python-dotenv
        - typer
        - black
      # Make intent explicit: prefer httpx; forbid requests.
      forbidden:
        - requests
      # Always ignore these stdlib imports (noise reduction).
      ignore_stdlib_prefixes:
        - typing
        - pathlib
        - logging
        - re
        - dataclasses
        - json
        - os
        - sys
        - uuid
        - itertools
        - functools
        - datetime

  # Guard potentially dangerous operations; we’ll wire this in tooling.
  subprocess:
    allowed_callers:
      - domain: agents
        capability: scaffolding
    require_justification: true
    log_all_invocations: true

enforcement:
  # Default is "fail". You can temporarily switch to "warn" during refactors.
  mode: fail

  # Paths to skip (regex). Constitution is never the target.
  ignored_paths:
    - "^tests/"         # test scaffolds may import fixtures freely
    - "^scripts/"       # one-off scripts; keep out of shipping packages
    - "^\\.intent/"     # the constitution itself

  # Extra file-level exemptions (optional, regex). Keep this list short and time-boxed.
  waivers:
    # Example (remove when clean):
    # - path: "^src/system/legacy/.*\\.py$"
    #   reason: "Temporary waiver while refactoring legacy bootstrap"
    #   expires: "2025-12-31"

reporting:
  # How violations are reported (tooling reads this to format output)
  show_allowed_bridges: true
  show_forbidden_reasons: true
  group_by: ["domain_from", "domain_to"]

--- END OF FILE ./.intent/policies/intent_guard.yaml ---

--- START OF FILE ./.intent/policies/safety_policies.yaml ---
# .intent/policies/safety_policies.yaml
meta:
  version: "0.5.0"
  last_updated: "2025-08-15T10:00:00Z"
  author: "CORE Constitution"
  description: >
    The single source of truth for all security and safety policies governing
    code generation, execution, and self-modification.

rules:
  # ===================================================================
  # RULE: Govern Self-Modification (Immutable Constitution)
  # ===================================================================
  - id: immutable_constitution
    description: >
      The core mission files are immutable and can only be changed via the full,
      human-in-the-loop constitutional amendment process.
    enforcement: manual_review
    applies_to:
      paths:
        - ".intent/mission/principles.yaml"
        - ".intent/mission/manifesto.md"
        - ".intent/mission/northstar.yaml"

  # ===================================================================
  # RULE: No self-modification of core loop
  # ===================================================================
  - id: deny_core_loop_edit
    description: >
      CORE cannot modify its own core orchestration and governance engine
      without explicit human review via the formal amendment process.
    enforcement: manual_review
    applies_to:
      paths:
        - "src/core/main.py"
        - "src/core/intent_guard.py"
        - ".intent/policies/intent_guard.yaml"
        - ".intent/policies/safety_policies.yaml" # The policy cannot edit itself.
    action: require_human_approval
    feedback: |
      🔒 Core logic modification detected. Human review required before application.

  # ===================================================================
  # RULE: Block dangerous execution primitives (HARDENED)
  # ===================================================================
  - id: no_dangerous_execution
    description: >
      Generated or modified code must not contain calls to dangerous functions
      that enable arbitrary code execution, shell access, or unsafe deserialization.
    enforcement: hard
    scope:
      domains: [core, agents, features]
      exclude:
        - path: "tests/**"
          rationale: "Test files require direct execution for validation"
        - path: "src/core/git_service.py"
          rationale: >
            This file is exempt as it safely uses subprocess.run() without shell=True.
    detection:
      type: regex
      patterns:
        - "eval\\("
        - "exec\\("
        - "compile\\("
        - "os\\.system\\("
        - "os\\.popen\\("
        - "subprocess\\.(run|Popen|call)\\([^)]*shell\\s*=\\s*True"
        - "shutil\\.rmtree\\("
        - "os\\.remove\\("
        - "os\\.rmdir\\("
    action: reject
    feedback: |
      ❌ Dangerous execution detected: '{{pattern}}'. Use safe wrappers or avoid shell=True.

  # ===================================================================
  # RULE: Restrict network access (NEW)
  # ===================================================================
  - id: restrict_network_access
    description: >
      Only explicitly allowed domains may be contacted. All outbound network
      calls must be through approved integration points.
    enforcement: hard
    allowed_domains:
      - "api.openai.com"
      - "github.com"
      - "api.deepseek.com" # Added from your .env.example
    action: reject
    feedback: |
      ❌ Attempt to contact unauthorized domain: {{domain}}. Update safety_policies.yaml to allow if needed.

  # ===================================================================
  # RULE: All changes must be logged
  # ===================================================================
  - id: change_must_be_logged
    description: >
      Every file change must be preceded by a log entry in .intent/change_log.json
      with IntentBundle ID and description.
    enforcement: hard
    triggers:
      - before_write
    validator: change_log_checker
    action: reject_if_unlogged
    feedback: |
      ❌ No prior log entry found for this change. Use CHANGE_LOG_PATH to register intent first.
--- END OF FILE ./.intent/policies/safety_policies.yaml ---

--- START OF FILE ./.intent/policies/secrets_management.yaml ---
# .intent/policies/secrets_management.yaml
#
# PURPOSE: This policy governs the handling of secrets, API keys, and
# other sensitive credentials within the CORE system and any applications
# it generates.

rules:
  - id: no_hardcoded_secrets
    description: >
      Source code MUST NOT contain hardcoded secrets (e.g., API keys, passwords).
      Secrets must be loaded from environment variables or a secure vault,
      as declared in runtime_requirements.yaml.
    enforcement: hard
    detection:
      type: regex
      patterns:
        - |
          (?i)api_key\s*=\s*["'](?!your_api_key_here)[a-zA-Z0-9_]{16,}
        - |
          (?i)secret\s*=\s*["'][a-zA-Z0-9_]{16,}
        - |
          (?i)password\s*=\s*["'][a-zA-Z0-9_]{8,}
      exclude:
        # Allow examples in documentation and config templates
        - "*.md"
        - ".env.example"
--- END OF FILE ./.intent/policies/secrets_management.yaml ---

--- START OF FILE ./.intent/policies/security_intents.yaml ---
# .intent/policies/security_intents.yaml
# Purpose: Central source of truth for security stances that the system + contributors must follow.
# Status: Policy-first (advisory). Enforcement will be added gradually in guard/tools.

version: 1
about: "Network, subprocess, secrets, and file I/O security intents. Constitution-first, deny by default."

# ──────────────────────────────────────────────────────────────────────────────
networking:
  # Overall stance for *outbound* connections made by code.
  # Options: deny | allow | monitor   (monitor = allow but require logging/justification)
  default_policy: monitor

  # Approved client libraries (must match dependency & guard policies).
  clients:
    allowed:
      - httpx
    forbidden:
      - requests

  # Transport requirements.
  tls:
    required: true          # no plaintext HTTP for external endpoints
    min_version: "1.2"
    allow_self_signed: false
    # If you must allow a specific self-signed endpoint, pin its certificate SHA256 here:
    allowed_fingerprints: []  # e.g., ["a1:b2:..."]

  # Domain rules (advisory today; useful for audits and future enforcement).
  domains:
    allowlist: []           # empty = no hard allowlist; prefer adding approved domains here over time
    blocklist:
      - "169.254.169.254"   # block instance metadata by default
      - "metadata.google.internal"
      - "latest.meta.internal"

  # Request hygiene (applies when code performs HTTP calls).
  hygiene:
    redact_headers:
      - Authorization
      - X-API-Key
      - X-Auth-Token
      - Api-Key
      - Cookie
    log:
      mode: minimal         # minimal | full (never log bodies by default)
      include:
        - method
        - scheme
        - host
        - path
        - status_code
        - duration_ms
      exclude_query_keys:
        - token
        - key
        - signature
        - password
    timeouts:
      connect_seconds: 5
      read_seconds: 30
      total_seconds: 60
    retries:
      policy: exponential_backoff
      max_retries: 3
      jitter_ms: 200

# ──────────────────────────────────────────────────────────────────────────────
subprocess:
  # Default stance: never spawn subprocesses unless explicitly allowed.
  default_policy: deny

  # Who may call subprocess, for what capability.
  allowed_callers:
    - domain: agents
      capability: scaffolding
      rationale: "Code generation / project scaffolding may need controlled shells."

  # Restrict which binaries may be executed (exact names or safe regex).
  # Keep this list tight; expand only with justification.
  allowed_binaries: []  # e.g., ["python", "bash", "sh", "git"]

  # Environmental controls for allowed calls.
  controls:
    require_justification: true               # human-readable reason in logs
    log_all_invocations: true                 # command + args + duration; NEVER log secrets
    max_runtime_seconds: 120                  # hard cap to prevent runaway processes
    working_dir_policy: repo_root_only        # repo_root_only | anywhere
    env_passlist:
      - PATH
      - PYTHONPATH
    env_blocklist:
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - GCP_SERVICE_ACCOUNT_KEY
      - AZURE_CLIENT_SECRET
      - OPENAI_API_KEY
      - ANTHROPIC_API_KEY

# ──────────────────────────────────────────────────────────────────────────────
secrets:
  # Never commit or log secrets. Keep this list small and focused.
  detection:
    # Heuristics/patterns to treat as sensitive if seen in config/logs.
    patterns:
      - "(?i)api[_-]?key"
      - "(?i)secret"
      - "(?i)token"
      - "(?i)password"
      - "(?i)private[_-]?key"
  handling:
    log_redaction: "****"      # replacement when redacting
    allow_plaintext_secrets_in_repo: false
    allow_secrets_in_logs: false
    env_access_policy: allowlist
    env_allowlist:
      - PYTHONPATH
      - CORE_*
    env_blocklist:
      - "*KEY*"
      - "*TOKEN*"
      - "*SECRET*"
      - "*PASSWORD*"

# ──────────────────────────────────────────────────────────────────────────────
filesystem:
  # Where tools/agents may write by default (explicitly).
  write_paths_allowlist:
    - "reports/**"
    - "sandbox/**"
    - "pending_writes/**"
  write_paths_blocklist:
    - ".intent/**"     # constitution must be edited via PRs, not runtime
    - "src/**"         # no code writes at runtime
  # Reading is generally allowed inside the repo. External paths are monitor-only for now.
  read_policy: monitor

# ──────────────────────────────────────────────────────────────────────────────
telemetry:
  # Outbound telemetry to third parties (analytics, error reporting).
  default_policy: deny
  allowlist: []        # e.g., ["sentry.io"] if explicitly approved later
  pii_in_telemetry: false

# ──────────────────────────────────────────────────────────────────────────────
enforcement:
  mode: advise         # advise | warn | fail
  # Tooling will read these for future checks; non-breaking today.
  reporting:
    show_rationales: true
    group_by: ["domain", "capability", "rule"]
  waivers: []

--- END OF FILE ./.intent/policies/security_intents.yaml ---

--- START OF FILE ./.intent/project_manifest.yaml ---
# .intent/project_manifest.yaml
#
# This is the single, canonical source of truth for the CORE project's
# high-level intent and top-level configuration.
#
# Project-wide capabilities are now defined in a decentralized way, located in
# manifest.yaml files within each source domain (e.g., src/core/manifest.yaml).
# The ConstitutionalAuditor automatically discovers and aggregates them.

name: "CORE"
version: "0.6.0" # Using the more recent version from the .json file
intent: "Build a self-improving AI coding assistant that can evolve itself through prompts, code, and validation."

# Defines the primary agents responsible for executing CORE's logic.
active_agents:
  - planner_agent
  - test_runner
  - validator_agent # Added for clarity

# Defines high-level roles for key directories.
folder_roles:
  "src/core": "Core logic and FastAPI services"
  "src/system": "Governance, introspection, and lifecycle tools"
  "src/agents": "Specialized AI actors (planners, reviewers, suggesters)"
  "tests": "Pytest-based validation for core behaviors"
  ".intent": "The 'brain' of the system: declarations, policies, and knowledge"

# Top-level configuration flags for system behavior.
configuration:
  allow_self_rewrites: true
  execution_mode: "auto" # 'auto' or 'manual_review'

# Metadata about the manifest file itself.
meta:
  created_by: "CORE v0.1 bootstrap"
  created_at: "2025-07-23T00:00:00Z"
  last_updated: "2025-08-15T12:00:00Z" # Using a placeholder for today

# --- Operator UX policy -------------------------------------------------------
operator_experience:
  guard:
    drift:
      # Default presentation when the operator runs the command with no flags:
      default_format: pretty        # options: pretty | table | json
      default_fail_on: any          # options: any | missing | undeclared
      strict_default: true          # require KG/artifact by default
      evidence_json: true           # always write JSON evidence to disk
      evidence_path: reports/drift_report.json
      labels:
        none: "NONE"                # how to label empty sections
        success: "✅ No capability drift"
        failure: "🚨 Drift detected"
--- END OF FILE ./.intent/project_manifest.yaml ---

--- START OF FILE ./.intent/prompts/constitutional_review.prompt ---
You are an expert AI system architect and a specialist in writing clear, machine-readable governance documents.

You will be provided with a "constitutional bundle" from a self-governing software system named CORE. This bundle contains the entire ".intent/" directory, which is the system's "Mind". It defines all of the system's principles, policies, capabilities, and self-knowledge.

Your task is to perform a critical peer review of this constitution. Your goal is to provide actionable suggestions to improve its clarity, completeness, and internal consistency.

Analyze the entire bundle and provide your feedback in the following format:

**1. Overall Assessment:**
A brief, high-level summary of the constitution's strengths and weaknesses.

**2. Specific Suggestions for Improvement:**
Provide a numbered list of specific, actionable suggestions. For each suggestion, you MUST include:
- **File:** The full path to the file that should be changed (e.g., `.intent/mission/principles.yaml`).
- **Justification:** A clear, concise reason explaining WHY this change is an improvement and which core principle it serves (e.g., "This serves the `clarity_first` principle by making the rule less ambiguous.").
- **Proposed Change:** A concrete example of the new content. Use a git-style diff format if possible (lines starting with '-' for removal, '+' for addition).

**3. Gaps and Missing Concepts:**
Identify any potential gaps in the constitution. Are there missing policies, undefined principles, or areas that seem incomplete? For example, is there a policy for data privacy? Is the process for adding new human operators clearly defined?

**Review Criteria:**
- **Clarity:** Is every rule and principle easy to understand for both a human and an LLM? Is there any ambiguity?
- **Completeness:** Does the constitution cover all critical aspects of the system's governance?
- **Consistency:** Are there any conflicting rules or principles?
- **Actionability:** Are the rules specific enough to be automatically enforced?

Begin your review now. The constitutional bundle is provided below.

--- END OF FILE ./.intent/prompts/constitutional_review.prompt ---

--- START OF FILE ./.intent/proposals/README.md ---
# Proposals

Create proposals here with filename pattern `cr-*.yaml`.

## Format
- `target_path`: repo-relative path (e.g., `.intent/policies/safety_policies.yaml`)
- `action`: currently only `replace_file`
- `justification`: why this is needed
- `content`: full new file contents (string)
- `rollback_plan` (optional): notes to revert
- `signatures`: added by `core-admin proposals-sign`

See `cr-example.yaml` for a starter.

--- END OF FILE ./.intent/proposals/README.md ---

--- START OF FILE ./.intent/remediation_catalog.yaml ---
# .intent/remediation_catalog.yaml
# Intent: Map governance findings to safe, reversible actions CORE may take autonomously.
# Scope: Non-functional fixes only (docs/tags/manifests). All changes are gated by tests & auditor.
version: 1

allowed_actions:
  - add_docstring           # add/update docstrings or "Intent:" header blocks
  - add_capability_tag      # insert '# CAPABILITY: <name>' above the implementing symbol
  - update_manifest         # add/remove capability entries in a domain's manifest.yaml

scopes:
  # Where each action is allowed. Keep narrow; expand as confidence grows.
  core:    [add_docstring, add_capability_tag, update_manifest]
  shared:  [add_docstring, add_capability_tag, update_manifest]
  agents:  [add_docstring, add_capability_tag]        # manifest changes usually live in domain folders
  system:  [add_docstring, add_capability_tag, update_manifest]
  tooling: [add_docstring, add_capability_tag, update_manifest]

policy:
  acceptance_gates:
    tests: must_pass              # pytest must be green
    auditor: must_not_worsen      # constitutional auditor must be no worse than before
    drift: must_be_clean          # capability drift must be clean after the change
  evidence_bundle:
    - diff                        # git patch/diff
    - auditor_before              # auditor stderr/output BEFORE fixes
    - auditor_after               # auditor stderr/output AFTER fixes
    - drift_report                # reports/drift_report.json
    - tests_summary               # pytest summary / coverage.xml

# Map concrete finding keys to actions. Keys mirror the auditor/drift messages.
remediations:
  missing_docstring:
    action: add_docstring

  generic_intent:
    action: add_docstring
    params:
      intent_mode: true           # nudge the generator to write an "Intent:"-style summary

  missing_capability_implementation:
    action: add_capability_tag

  undeclared_capability:
    action: update_manifest
    params:
      mode: add                   # add capability to the correct domain manifest

  declared_but_missing_in_code:
    action: update_manifest
    params:
      mode: remove                # remove capability from manifest if truly gone

# Findings we do NOT auto-fix (diagnose only). CORE should open a "diagnostic PR" with plan.
diagnostics_only:
  - potential_dead_code
  - complexity_outlier

--- END OF FILE ./.intent/remediation_catalog.yaml ---

--- START OF FILE ./.intent/schemas/config_schema.yaml ---
# .intent/schemas/config_schema.yaml
git:
  ignore_validation:
    type: boolean
    default: false
    description: >
      If true, skips Git pre-write checks. MUST be false in production or fallback modes
      to maintain rollback safety. Only for emergency recovery.
--- END OF FILE ./.intent/schemas/config_schema.yaml ---

--- START OF FILE ./.intent/schemas/knowledge_graph_entry.schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://core.system/schema/knowledge_graph_entry.json",
  "title": "Knowledge Graph Symbol Entry",
  "description": "Schema for a single symbol (function or class) in the knowledge_graph.json file.",
  "type": "object",
  "required": [
    "key",
    "name",
    "type",
    "file",
    "domain",
    "agent",
    "capability",
    "intent",
    "last_updated",
    "calls",
    "line_number",
    "is_async",
    "parameters",
    "is_class",
    "structural_hash"
  ],
  "properties": {
    "key": { "type": "string", "description": "The unique identifier for the symbol (e.g., 'path/to/file.py::MyClass')." },
    "name": { "type": "string", "description": "The name of the function or class." },
    "type": { "type": "string", "enum": ["FunctionDef", "ClassDef", "AsyncFunctionDef"] },
    "file": { "type": "string", "description": "The relative path to the source file." },
    "domain": { "type": "string", "description": "The logical domain from source_structure.yaml." },
    "agent": { "type": "string", "description": "The inferred agent responsible for this symbol's domain." },
    "capability": { "type": "string", "description": "The high-level capability this symbol provides, or 'unassigned'." },
    "intent": { "type": "string", "description": "A clear, concise statement of the symbol's purpose." },
    "docstring": { "type": ["string", "null"], "description": "The raw docstring from source code." },
    "calls": { "type": "array", "items": { "type": "string" }, "description": "List of other functions called by this one." },
    "line_number": { "type": "integer", "minimum": 0 },
    "is_async": { "type": "boolean" },
    "parameters": { "type": "array", "items": { "type": "string" } },
    "entry_point_type": { "type": ["string", "null"], "description": "Type of entry point if applicable (e.g., 'fastapi_route_post')." },
    "last_updated": { "type": "string", "format": "date-time" },
    "is_class": { "type": "boolean", "description": "True if the symbol is a class definition." },
    "base_classes": {
      "type": "array",
      "items": { "type": "string" },
      "description": "A list of base classes this symbol inherits from (if it is a class)."
    },
    "entry_point_justification": {
      "type": ["string", "null"],
      "description": "The name of the pattern that identified this symbol as an entry point."
    },
    "parent_class_key": {
      "type": ["string", "null"],
      "description": "The key of the parent class, if this symbol is a method."
    },
    "structural_hash": {
      "type": "string",
      "description": "A SHA256 hash of the symbol's structure, ignoring comments and docstrings."
    }
  },
  "additionalProperties": false
}

--- END OF FILE ./.intent/schemas/knowledge_graph_entry.schema.json ---

--- START OF FILE ./.intent/schemas/manifest.schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Domain Manifest",
  "type": "object",
  "additionalProperties": true,
  "required": ["domain", "description", "capabilities"],
  "properties": {
    "domain": {
      "type": "string",
      "minLength": 1,
      "pattern": "^[a-z][a-z0-9_]*$",
      "description": "Short, code-safe domain name (e.g., core, system, agents)."
    },
    "description": {
      "type": "string",
      "minLength": 1,
      "description": "Human-readable purpose of this domain."
    },
    "version": {
      "type": "string",
      "description": "Optional semantic version for the manifest itself (e.g., '1.0.0')."
    },
    "owners": {
      "type": "array",
      "items": { "type": "string" },
      "description": "Optional list of accountable owners or roles."
    },
    "capabilities": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "string",
        "minLength": 1
      },
      "uniqueItems": true,
      "description": "Capabilities this domain provides (unique, human-readable labels)."
    },
    "imports": {
      "type": "array",
      "items": {
        "type": "string",
        "minLength": 1
      },
      "uniqueItems": true,
      "description": "Optional list of other domains this domain depends on."
    },
    "notes": {
      "type": "string",
      "description": "Any additional free-form notes."
    }
  }
}

--- END OF FILE ./.intent/schemas/manifest.schema.json ---

--- START OF FILE ./.intent/schemas/proposal.schema.json ---
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://core.local/schemas/proposal.schema.json",
  "title": "CORE Proposal (v1)",
  "type": "object",
  "additionalProperties": false,
  "required": ["target_path", "action", "justification", "content"],
  "properties": {
    "target_path": {
      "type": "string",
      "description": "Repo-relative path to the Mind file to be replaced. Must live under .intent/, but not under .intent/proposals/.",
      "pattern": "^\\.intent\\/(?!proposals\\/)[\\w\\-\\.\\/]+$",
      "$comment": "Forward slashes only; prevents writing directly into .intent/proposals/."
    },
    "action": {
      "type": "string",
      "enum": ["replace_file"],
      "description": "Currently only full file replacement is supported."
    },
    "justification": {
      "type": "string",
      "minLength": 10,
      "description": "Human-readable rationale for the change.",
      "pattern": "\\S",
      "$comment": "Reject all-whitespace strings."
    },
    "content": {
      "type": "string",
      "minLength": 1,
      "description": "The full proposed content of the target file."
    },
    "rollback_plan": {
      "type": "object",
      "additionalProperties": false,
      "required": ["description"],
      "properties": {
        "description": { "type": "string", "minLength": 3 },
        "previous_version_hint": { "type": "string" }
      }
    },
    "metadata": {
      "type": "object",
      "additionalProperties": false,
      "description": "Optional authoring metadata (not used for security decisions).",
      "properties": {
        "created_by": { "type": "string", "format": "email" },
        "created_at": { "type": "string", "format": "date-time" },
        "content_mime": {
          "type": "string",
          "enum": ["application/yaml", "application/json", "text/markdown", "text/plain"]
        }
      }
    },
    "content_sha256": {
      "$ref": "#/$defs/sha256",
      "description": "Optional helper: precomputed SHA-256 of `content` for audit tooling."
    },
    "labels": {
      "type": "array",
      "items": { "type": "string", "minLength": 1, "maxLength": 64 },
      "maxItems": 16,
      "uniqueItems": true,
      "description": "Optional tags for filtering/search (e.g., ['governance','policy'])."
    },
    "signatures": {
      "type": "array",
      "description": "Optional array of signature objects. Quorum is enforced outside the schema.",
      "items": { "$ref": "#/$defs/signature" }
    }
  },
  "$defs": {
    "sha256": {
      "type": "string",
      "pattern": "^[a-f0-9]{64}$",
      "$comment": "Lowercase hex digest."
    },
    "base64": {
      "type": "string",
      "pattern": "^(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?$",
      "$comment": "RFC 4648 base64 without newlines."
    },
    "signature": {
      "type": "object",
      "additionalProperties": false,
      "required": ["identity", "signature_b64", "token", "timestamp"],
      "properties": {
        "identity": {
          "type": "string",
          "description": "Human identity for the signer (email preferred)."
        },
        "signature_b64": {
          "$ref": "#/$defs/base64",
          "description": "Ed25519 signature, base64-encoded."
        },
        "token": {
          "type": "string",
          "pattern": "^core-proposal-v1:[a-f0-9]{64}$",
          "description": "Approval token over proposal content. Must match admin CLI format."
        },
        "timestamp": {
          "type": "string",
          "format": "date-time",
          "description": "RFC 3339 timestamp when the signature was created."
        }
      }
    }
  },
  "examples": [
    {
      "target_path": ".intent/policies/safety_policies.yaml",
      "action": "replace_file",
      "justification": "Align policy wording with clarity_first; no behavior change.",
      "content": "# new YAML here",
      "metadata": {
        "created_by": "alice@example.com",
        "created_at": "2025-08-08T12:34:56Z",
        "content_mime": "application/yaml"
      },
      "labels": ["governance", "policy"]
    }
  ]
}

--- END OF FILE ./.intent/schemas/proposal.schema.json ---

--- START OF FILE ./LICENSE ---
MIT License

Copyright (c) 2024 Dariusz Newecki

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
--- END OF FILE ./LICENSE ---

--- START OF FILE ./Makefile ---
# Makefile for CORE – Cognitive Orchestration Runtime Engine

SHELL := /bin/bash
.SHELLFLAGS := -eu -o pipefail -c
.DEFAULT_GOAL := help

# ---- Configurable knobs -----------------------------------------------------
POETRY  ?= python3 -m poetry
PYTHON  ?= python3
APP     ?= src.core.main:app
HOST    ?= 0.0.0.0
PORT    ?= 8000
RELOAD  ?= --reload
ENV_FILE ?= .env
PATHS   ?= src tests

# Manifest migrator (File 4)
MIGRATOR := src/system/tools/manifest_migrator.py
# FAIL_ON_CONFLICTS=1 will make duplicate capabilities fail the run
FAIL_ON_CONFLICTS ?= 0
ifeq ($(FAIL_ON_CONFLICTS),1)
	MIGRATOR_CONFLICT_FLAG := --fail-on-conflicts
else
	MIGRATOR_CONFLICT_FLAG :=
endif

# Drift output format for guard UI: short | pretty
FORMAT ?= short

.PHONY: help install lock run stop audit lint format test coverage check fast-check clean distclean nuke context \
        drift migrate manifests-validate manifests-scaffold manifests-dups guard-check fix-docstrings

help:
	@echo "CORE Development Makefile"
	@echo "-------------------------"
	@echo "make install                 - Install deps with Poetry"
	@echo "make lock                    - Update poetry.lock"
	@echo "make run                     - Start uvicorn ($(APP)) on $(HOST):$(PORT)"
	@echo "make stop                    - Stop dev server reliably by killing process on port $(PORT)"
	@echo "make audit                   - Run the full self-audit (KnowledgeGraph + Auditor)"
	@echo "make lint                    - Check formatting and code quality with Black and Ruff."
	@echo "make format                  - Auto-format code with Black and Ruff."
	@echo "make test [ARGS=]            - Pytest (pass ARGS='-k expr -vv')"
	@echo "make coverage                - Pytest with coverage"
	@echo "make fast-check              - Run fast checks (lint, test)."
	@echo "make check                   - Run all checks (lint, test, audit)."
	@echo "make migrate [FAIL_ON_CONFLICTS=0|1]"
	@echo "                             - Run manifest migrator (scaffold+validate+dups)."
	@echo "make manifests-validate      - Validate all manifests against schema"
	@echo "make manifests-scaffold      - Create missing manifests with placeholders"
	@echo "make manifests-dups          - Show duplicate capabilities across domains"
	@echo "make drift [FORMAT=short]    - Generate drift evidence + show guard drift view"
	@echo "make guard-check             - Run import/dependency guard checks (if available)"
	@echo "make align GOAL=...          - Check goal ↔ NorthStar alignment via API"
	@echo "make context                 - Build the project context file for AI collaboration"
	@echo "make clean                   - Remove caches, pending_writes, sandbox"
	@echo "make distclean               - Clean + venv/build leftovers"
	@echo "make nuke                    - git clean -fdx (danger)"
	@echo
	@echo "Variables:"
	@echo "  FAIL_ON_CONFLICTS=0|1   (default 0)  Fail if duplicate capabilities exist"
	@echo "  FORMAT=short|pretty     (default short) Guard drift output style"
	@echo "  HOST, PORT, RELOAD, ENV_FILE  (server run options)"

install:
	@echo "📦 Installing dependencies..."
	$(POETRY) install

lock:
	@echo "🔒 Resolving and locking dependencies..."
	$(POETRY) lock

run:
	@echo "🚀 Starting FastAPI server at http://$(HOST):$(PORT)"
	PYTHONPATH=src $(POETRY) run uvicorn $(APP) --host $(HOST) --port $(PORT) $(RELOAD) --env-file $(ENV_FILE)

stop:
	@echo "🛑 Stopping any process on port $(PORT)..."
	@if command -v lsof >/dev/null 2>&1; then \
		PID=$$(lsof -t -i:$(PORT) || true); \
		if [ -n "$$PID" ]; then \
			echo "  -> Found process with PID: $$PID. Terminating..."; \
			kill $$PID || true; \
		else \
			echo "  -> No process found on port $(PORT)."; \
		fi; \
	else \
		echo "  -> 'lsof' not found. Trying 'pkill'. You might want to install 'lsof' for better reliability."; \
		pkill -f "uvicorn.*$(APP)" || true; \
	fi

audit:
	@echo "🧠 Running constitutional self-audit..."
	PYTHONPATH=src $(POETRY) run python -m core.capabilities

lint:
	@echo "🎨 Checking code style with Black and Ruff..."
	$(POETRY) run black --check $(PATHS)
	$(POETRY) run ruff check $(PATHS)

format:
	@echo "✨ Formatting code with Black and Ruff..."
	$(POETRY) run black $(PATHS)
	$(POETRY) run ruff check --fix $(PATHS)

test:
	@echo "🧪 Running tests with pytest..."
	$(POETRY) run pytest $(ARGS)

coverage:
	@echo "🧮 Running tests with coverage..."
	$(POETRY) run pytest --cov=src --cov-report=term-missing:skip-covered $(ARGS)

fast-check: lint test

check: fast-check audit

# ---- Governance helpers -----------------------------------------------------

# Generate drift evidence first (manifest validation + duplicate scan),
# then show the guard's drift view (reads reports/drift_report.json).
drift:
	@echo "🧭 Generating drift evidence (schema + duplicates)..."
	PYTHONPATH=src $(POETRY) run $(PYTHON) $(MIGRATOR) all $(MIGRATOR_CONFLICT_FLAG) || true
	@echo "📄 Displaying guard drift view ($(FORMAT))..."
	PYTHONPATH=src $(POETRY) run core-admin guard drift --format $(FORMAT)

# Run full migrator pipeline explicitly (scaffold + validate + dup check)
migrate:
	@echo "🛠  Running manifest migrator pipeline..."
	PYTHONPATH=src $(POETRY) run $(PYTHON) $(MIGRATOR) all $(MIGRATOR_CONFLICT_FLAG)

manifests-validate:
	@echo "✅ Validating manifests against schema..."
	PYTHONPATH=src $(POETRY) run $(PYTHON) $(MIGRATOR) validate

manifests-scaffold:
	@echo "🧱 Scaffolding missing manifests..."
	PYTHONPATH=src $(POETRY) run $(PYTHON) $(MIGRATOR) scaffold

manifests-dups:
	@echo "🔍 Checking for duplicate capabilities across domains..."
	PYTHONPATH=src $(POETRY) run $(PYTHON) $(MIGRATOR) check-duplicates $(MIGRATOR_CONFLICT_FLAG)

guard-check:
	@echo "🔒 Running import/dependency guard checks (if available)..."
	# If your core-admin has 'guard check', this will run it; otherwise, remove this target.
	PYTHONPATH=src $(POETRY) run core-admin guard check --all || { echo 'guard-check: command unavailable' ; exit 0; }

fix-docstrings:
	@echo "✍️  Using agent to fix missing docstrings..."
	@echo "   (This will use the Generator LLM and may take a moment)"
	PYTHONPATH=src $(POETRY) run core-admin fix docstrings --write

# Usage: make align GOAL='scaffold a governed starter from intent'
align:
	@test -n "$(GOAL)" || (echo 'GOAL is required. Example: make align GOAL="build a governed starter kit"'; exit 2)
	@echo "🔎 Checking goal↔NorthStar alignment via API..."
	@echo "   (ensure the API is running: make run)"
	@curl -s -X POST http://$(HOST):$(PORT)/guard/align \
	  -H 'Content-Type: application/json' \
	  -d '{"goal":"$(GOAL)"}' | (command -v jq >/dev/null 2>&1 && jq . || cat)

# ---- Clean targets ---------------------------------------------------------

clean:
	@echo "🧹 Cleaning up temporary files and caches..."
	find . -type f -name '*.pyc' -delete
	find . -type d -name '__pycache__' -prune -exec rm -rf {} +
	rm -rf .pytest_cache .ruff_cache .mypy_cache .cache
	rm -f .coverage
	rm -rf htmlcov
	rm -rf build dist *.egg-info
	rm -rf pending_writes sandbox
	@echo "✅ Clean complete."

distclean: clean
	@echo "🧨 Distclean: removing virtual environments and build leftovers..."
	rm -rf .venv
	@echo "✅ Distclean complete."

nuke:
	@echo "☢️  Running 'git clean -fdx' in 3s (CTRL+C to cancel)..."
	@sleep 3
	git clean -fdx
	@echo "✅ Repo nuked (untracked files/dirs removed)."

# ---- Developer Tooling ------------------------------------------------------
context:
	@echo "📦 Building project context for AI collaboration..."
	@scripts/concat_project.sh

--- END OF FILE ./Makefile ---

--- START OF FILE ./pyproject.toml ---
# pyproject.toml

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "core"
version = "0.1.0"
description = "CORE: A self-governing, intent-driven software development system."
authors = ["Dariusz Newecki <d.newecki@gmail.com>"]
license = "MIT"
readme = "README.md"
packages = [
    { include = "core", from = "src" },
    { include = "agents", from = "src" },
    { include = "system", from = "src" },
    { include = "shared", from = "src" }
    # { include = "data", from = "src" },  # enable only when src/data exists
]

# --- Main Dependencies ---
[tool.poetry.dependencies]
python = ">=3.9"

# API stack (compatible with Pydantic v2)
fastapi = ">=0.103.0"
uvicorn = { extras = ["standard"], version = ">=0.24.0" }

# Core libraries
pyyaml = ">=6.0.1"
httpx = ">=0.27.0"
python-dotenv = ">=1.0.0"
pydantic = ">=2.5.0"
pydantic-settings = "^2.0.0"
cryptography = ">=42.0.0"
jsonschema = "^4"
typer = "^0.9.0"
radon = ">=5.1.0"
filelock = "^3.13.0"

# NOTE:
# - We intentionally DO NOT depend on 'requests' (policy forbids it; use httpx).
# - 'rich' is optional; our tools degrade gracefully if it's not installed.

# --- Development Dependencies ---
[tool.poetry.group.dev.dependencies]
pytest = ">=7.0,<8.0"
pytest-asyncio = "==0.21.0"
black = "^24.8.0"
ruff = ">=0.0.254"
pytest-mock = "^3.12.0"
pytest-cov = "^4.0"

# --- Command-Line Scripts ---
[tool.poetry.scripts]
core-admin = "system.admin_cli:app"

# --- Configuration for development tools ---
[tool.black]
line-length = 88

[tool.ruff]
line-length = 88

[tool.ruff.lint]
select = ["E", "W", "F", "I"]
# This makes 'black' the single source of truth for formatting.
ignore = ["E501"]

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
pythonpath = ["src"]
addopts = ["-c", "pyproject.toml"]

log_cli = true
log_cli_level = "DEBUG"

--- END OF FILE ./pyproject.toml ---

--- START OF FILE ./README.md ---
# CORE — The Self-Improving System Architect

<!-- Governance Status -->

[![Guard & Drift](https://github.com/DariuszNewecki/CORE/actions/workflows/guard-and-drift.yml/badge.svg)](https://github.com/DariuszNewecki/CORE/actions/workflows/guard-and-drift.yml)

> **Where Intelligence Lives.**

[![Latest release](https://img.shields.io/github/v/release/DariuszNewecki/CORE?sort=semver)](https://github.com/DariuszNewecki/CORE/releases)
![Status: MVP Achieved](https://img.shields.io/badge/status-MVP%20achieved-brightgreen.svg)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## 🟢 Project Status: **MVP v0.2.0** (released 2025-08-15)

CORE can **enforce a constitution-backed workflow**: validate domain manifests, check domain boundaries via an AST-based guard, surface drift evidence, and run these checks in CI.

* **Releases:** [https://github.com/DariuszNewecki/CORE/releases](https://github.com/DariuszNewecki/CORE/releases)
* CI runs governance checks (guard & drift) plus lint and tests on every push/PR.

---

## Governance Status

This project is governed by the **constitution** in `.intent/`. Every push/PR runs:

* **Manifest migration & validation** (schema + duplicate capability check)
* **Intent Guard** (domain boundary & library policy)
* **Fast checks** (lint + tests)

### Quick commands

```bash
# Create missing manifests, validate, and check duplicates
make migrate

# Generate drift evidence and view summary
make drift            # writes reports/drift_report.json

# Enforce import boundaries
make guard-check
```

### 🎥 90-second demo

The GIF shows a typical loop: `make migrate` → `make drift` → `core-admin guard check`.

---

## What is CORE?

CORE is a self-governing, constitution-aligned AI development framework. It plans, validates, and evolves software with traceability and guardrails.

### Mind/Body model

```mermaid
graph TD
  subgraph Mind[".intent/ — Constitution & Knowledge"]
    A[Principles & Policies]
    B[NorthStar / Mission]
    C[Capabilities & Manifests]
    D[Knowledge Graph]
  end
  subgraph Body["src/ — Executable System"]
    E[Agents]
    F[Auditor & Drift Checks]
    G[Tools & Pipelines]
    H["CLI (core-admin)"]
  end
  A -- governs --> E
  C -- maps to --> G
  D -- validates --> F
  H -- operates --> E
```

---

## Quickstart (≈90 seconds)

Requires **Python 3.9+** (3.11 recommended), Linux/macOS. Uses **Poetry**.

```bash
# 1) Clone & install
git clone https://github.com/DariuszNewecki/CORE.git
cd CORE
poetry install

# 2) Sanity checks (format, lint, tests)
poetry run black --check .
poetry run ruff check .
poetry run pytest -q

# 3) Governance: scaffold/validate manifests and check boundaries
make migrate
make drift
poetry run core-admin guard check
```

If you prefer **pip**, you can export dependencies:

```bash
poetry export -f requirements.txt --output requirements.txt --without-hashes
pip install -r requirements.txt
```

---

## Key Capabilities (MVP scope)

* Constitution-first governance (`.intent/` is the source of truth)
* Manifest validation (JSON Schema) and duplicate capability detection
* AST-based import guard enforcing domain boundaries and library policy
* Drift evidence generation to `reports/drift_report.json` and CLI surfacing
* CI enforcement via GitHub Actions (Guard & Drift workflow)

---

## CI / CD

* **Governance:** Guard & Drift workflow on push/PR (`.github/workflows/guard-and-drift.yml`)
* **Format & Lint:** Black, Ruff
* **Tests:** pytest

See `.github/workflows/` for details.

---

## Roadmap

* **v0.3:** richer starter kits, extended guard rules, coverage gating
* **v0.4:** policy-as-code expansions, contributor UX polish
* **v0.5:** deeper introspection & automated refactor loops

Track progress in **Projects** and **Issues**.

---

## Contributing

We welcome focused, high-quality contributions:

* Read `CONTRIBUTING.md`
* Use conventional commits (`feat:`, `fix:`, `chore:`, …)
* Open an issue before major changes
* Run the checks above before pushing

---

## Security

Please report vulnerabilities privately. See `SECURITY.md` for the disclosure process.

---

## License

MIT — see `LICENSE`.

--- END OF FILE ./README.md ---

--- START OF FILE ./reports/drift_report.json ---
{
  "mismatched_mappings": [
    {
      "capability": "add_missing_docstrings",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "tooling",
        "owner": null
      }
    },
    {
      "capability": "alignment_checking",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.capability_coverage",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.capability_definitions",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.codebase_health",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.dead_code",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.docstrings",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.domain_integrity",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.duplication",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.environment",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.knowledge_graph_schema",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.orphaned_intent_files",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.project_manifest",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.proposals_drift",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.proposals_list",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.proposals_schema",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.required_files",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.secrets",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "audit.check.syntax",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "change_safety_enforcement",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "code_generation",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "agents",
        "owner": null
      }
    },
    {
      "capability": "code_quality_analysis",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "intent_guarding",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "introspection",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "llm_orchestration",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "agents",
        "owner": null
      }
    },
    {
      "capability": "prompt_interpretation",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "scaffold_project",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "system",
        "owner": null
      }
    },
    {
      "capability": "self_correction",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "semantic_validation",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "syntax_validation",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "core",
        "owner": null
      }
    },
    {
      "capability": "system_logging",
      "code": {
        "domain": null,
        "owner": null
      },
      "manifest": {
        "domain": "shared",
        "owner": null
      }
    }
  ],
  "missing_in_code": [
    "manifest_updating",
    "test_execution"
  ],
  "undeclared_in_manifest": [
    "agents:docstring-fixer"
  ]
}

--- END OF FILE ./reports/drift_report.json ---

--- START OF FILE ./reports/.gitkeep ---
[EMPTY FILE]

--- END OF FILE ./reports/.gitkeep ---

--- START OF FILE ./reports/knowledge_graph.json ---
{
  "nodes": [
    {
      "capability": "add_missing_docstrings",
      "domain": "tooling",
      "owner": "tooling_agent"
    },
    {
      "capability": "alignment_checking",
      "domain": "system",
      "owner": "validator_agent"
    },
    {
      "capability": "audit.check.capability_coverage",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.capability_definitions",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.codebase_health",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.dead_code",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.docstrings",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.domain_integrity",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.duplication",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.environment",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.knowledge_graph_schema",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.orphaned_intent_files",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.project_manifest",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.proposals_drift",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.proposals_list",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.proposals_schema",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.required_files",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.secrets",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "audit.check.syntax",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "change_safety_enforcement",
      "domain": "core",
      "owner": "core_agent"
    },
    {
      "capability": "code_generation",
      "domain": "agents",
      "owner": "generic_agent"
    },
    {
      "capability": "code_quality_analysis",
      "domain": "core",
      "owner": "core_agent"
    },
    {
      "capability": "intent_guarding",
      "domain": "core",
      "owner": "core_agent"
    },
    {
      "capability": "introspection",
      "domain": "core",
      "owner": "core_agent"
    },
    {
      "capability": "llm_orchestration",
      "domain": "agents",
      "owner": "planner_agent"
    },
    {
      "capability": "manifest_updating",
      "domain": "tooling",
      "owner": "tooling_agent"
    },
    {
      "capability": "prompt_interpretation",
      "domain": "core",
      "owner": "core_agent"
    },
    {
      "capability": "scaffold_project",
      "domain": "system",
      "owner": "generic_agent"
    },
    {
      "capability": "self_correction",
      "domain": "core",
      "owner": "core_agent"
    },
    {
      "capability": "semantic_validation",
      "domain": "core",
      "owner": "core_agent"
    },
    {
      "capability": "syntax_validation",
      "domain": "core",
      "owner": "core_agent"
    },
    {
      "capability": "system_logging",
      "domain": "shared",
      "owner": "generic_agent"
    },
    {
      "capability": "test_execution",
      "domain": "core",
      "owner": "core_agent"
    }
  ]
}

--- END OF FILE ./reports/knowledge_graph.json ---

--- START OF FILE ./scripts/bootstrap_issues.sh ---
#!/usr/bin/env bash
# Create a visible, pragmatic roadmap as GitHub issues.
# Prereq: gh auth login  (or GH_TOKEN env)

set -euo pipefail

REPO="${1:-}" # optional: org/repo, otherwise uses current
LABELS_COMMON="roadmap,organizational"
MILESTONE="${2:-Organizational Pass}"

create_issue () {
  local title="$1"
  local body="$2"
  local labels="$3"
  if [[ -n "$REPO" ]]; then
    gh issue create --repo "$REPO" --title "$title" --body "$body" --label "$labels"
  else
    gh issue create --title "$title" --body "$body" --label "$labels"
  fi
}

# Ensure labels exist (idempotent)
ensure_label () {
  local name="$1"; local color="$2"; local desc="$3"
  gh label create "$name" --color "$color" --description "$desc" 2>/dev/null || true
}
ensure_label "roadmap" "0366d6" "Roadmap item"
ensure_label "organizational" "a2eeef" "Project organization"
ensure_label "ci" "7057ff" "CI/CD"
ensure_label "audit" "d73a4a" "Constitutional audit & governance"
ensure_label "docs" "0e8a16" "Documentation"

create_issue "Add JSON logging & request IDs" $'**Goal**: Switch logger to support LOG_FORMAT=json and add request id middleware in FastAPI.\n\n**Acceptance**\n- LOG_FORMAT=json writes structured logs\n- x-request-id is set/propagated\n- Docs updated in docs/CONVENTIONS.md' "$LABELS_COMMON,ci"

create_issue "Pre-commit hooks (Black, Ruff)" $'**Goal**: Add .pre-commit-config.yaml and wire to Make.\n\n**Acceptance**\n- pre-commit runs Black/Ruff locally\n- CI stays green' "$LABELS_COMMON,ci"

create_issue "Docs: CONVENTIONS.md & DEPENDENCIES.md" $'**Goal**: Codify folder map, import rules, capability tags, dependency policy.\n\n**Acceptance**\n- New contributors can place files w/o asking\n- Import discipline matrix documented' "$LABELS_COMMON,docs"

create_issue "Governance: proposal.schema.json + proposal_checks" $'**Goal**: Enforce schema & drift checks for .intent/proposals.\n\n**Acceptance**\n- Auditor shows schema pass/fail\n- Drift (token mismatch) → warning\n- Example proposal present' "$LABELS_COMMON,audit"

create_issue "Modular manifests (aggregator + fallback)" $'**Goal**: Support src/*/manifest.yaml aggregated into .intent/knowledge/project_manifest_aggregated.yaml.\n\n**Acceptance**\n- Auditor prefers aggregated manifest\n- Backward-compatible with monolith' "$LABELS_COMMON"

create_issue "Pilot domain package (proposals)" $'**Goal**: Create src/domain/proposals/{models,services,schemas}.py and migrate only proposal-related code.\n\n**Acceptance**\n- No new audit failures\n- Clear import boundaries documented' "$LABELS_COMMON"

--- END OF FILE ./scripts/bootstrap_issues.sh ---

--- START OF FILE ./scripts/gh_dump_repo_settings.sh ---
#!/usr/bin/env bash
set -euo pipefail

# Simple, readable GitHub status report.
# Usage:
#   OWNER=YourUser REPO=YourRepo scripts/gh_status_report.sh
# Defaults:
OWNER="${OWNER:-DariuszNewecki}"
REPO="${REPO:-CORE}"

has_jq() { command -v jq >/dev/null 2>&1; }
require_gh() { gh auth status >/dev/null 2>&1 || { echo "❌ gh not authenticated. Run: gh auth login"; exit 1; }; }

require_gh
tmpdir="$(mktemp -d)"
trap 'rm -rf "$tmpdir"' EXIT

out="GH_STATUS.md"
{
  echo "# GitHub Status Report — ${OWNER}/${REPO}"
  echo
  echo "Generated: $(date -u +"%Y-%m-%d %H:%M:%SZ")"
  echo

  echo "## Repository"
  gh api "repos/${OWNER}/${REPO}" > "${tmpdir}/repo.json"
  if has_jq; then
    jq '{name,visibility,default_branch,open_issues_count,description}' "${tmpdir}/repo.json"
  else
    cat "${tmpdir}/repo.json"
  fi
  echo

  echo "## Milestones"
  gh api "repos/${OWNER}/${REPO}/milestones?per_page=100&state=all" --paginate > "${tmpdir}/miles.json" || echo "[]" > "${tmpdir}/miles.json"
  if has_jq; then
    jq '.[] | {number,title,state,due_on,open_issues,closed_issues,description}' "${tmpdir}/miles.json"
  else
    cat "${tmpdir}/miles.json"
  fi
  echo

  # --- Issues ---
  echo "## Open Issues"
  gh issue list --repo "${OWNER}/${REPO}" --state open --limit 200 \
    --json number,title,labels,milestone,url,createdAt > "${tmpdir}/issues_open.json"
  if has_jq; then
    jq '.[] | {number,title,milestone: (.milestone.title // null),labels: (.labels | map(.name)),url,createdAt}' "${tmpdir}/issues_open.json"
  else
    cat "${tmpdir}/issues_open.json"
  fi
  echo

  echo "## Recently Closed Issues"
  gh issue list --repo "${OWNER}/${REPO}" --state closed --limit 30 \
    --json number,title,labels,milestone,url,closedAt > "${tmpdir}/issues_closed.json"
  if has_jq; then
    jq '.[] | {number,title,milestone: (.milestone.title // null),labels: (.labels | map(.name)),url,closedAt}' "${tmpdir}/issues_closed.json"
  else
    cat "${tmpdir}/issues_closed.json"
  fi
  echo

  # --- PRs (useful in practice) ---
  echo "## Open Pull Requests"
  gh pr list --repo "${OWNER}/${REPO}" --state open --limit 50 \
    --json number,title,labels,milestone,url,createdAt,updatedAt > "${tmpdir}/prs_open.json" || echo "[]" > "${tmpdir}/prs_open.json"
  if has_jq; then
    jq '.[] | {number,title,milestone: (.milestone.title // null),labels: (.labels | map(.name)),url,createdAt,updatedAt}' "${tmpdir}/prs_open.json"
  else
    cat "${tmpdir}/prs_open.json"
  fi
  echo

  echo "## Labels"
  gh label list --repo "${OWNER}/${REPO}" --json name,color,description > "${tmpdir}/labels.json" || echo "[]" > "${tmpdir}/labels.json"
  if has_jq; then
    jq '.[] | {name,color,description}' "${tmpdir}/labels.json"
  else
    cat "${tmpdir}/labels.json"
  fi
  echo

  echo "## Projects (Projects v2)"
  gh project list --owner "${OWNER}" > "${tmpdir}/projects.txt" || true
  cat "${tmpdir}/projects.txt"
  echo
  if grep -Eo '#[0-9]+' "${tmpdir}/projects.txt" >/dev/null 2>&1; then
    while read -r num; do
      pnum="${num//#/}"
      echo "### Project ${pnum}"
      gh project view "${pnum}" --owner "${OWNER}" --format json || true
      echo
    done < <(grep -Eo '#[0-9]+' "${tmpdir}/projects.txt" | sort -u)
  fi

  echo "## Releases"
  gh release list --repo "${OWNER}/${REPO}" || true
  echo
} > "${out}"

echo "✅ Report written to ${out}"

--- END OF FILE ./scripts/gh_dump_repo_settings.sh ---

--- START OF FILE ./scripts/gh_status_report.sh ---
#!/usr/bin/env bash
set -euo pipefail
OWNER="${OWNER:-DariuszNewecki}"
REPO="${REPO:-CORE}"

has_jq() { command -v jq >/dev/null 2>&1; }

out="GH_STATUS.md"
echo "# GitHub Status Report — $OWNER/$REPO" > "$out"
echo "" >> "$out"
echo "Generated: $(date -u +"%Y-%m-%d %H:%M:%SZ")" >> "$out"
echo "" >> "$out"

echo "## Repository" >> "$out"
gh api repos/$OWNER/$REPO > /tmp/repo.json
if has_jq; then
  jq '{name,visibility,default_branch,open_issues_count,description}' /tmp/repo.json >> "$out"
else
  cat /tmp/repo.json >> "$out"
fi
echo "" >> "$out"

echo "## Milestones" >> "$out"
gh api repos/$OWNER/$REPO/milestones --paginate > /tmp/miles.json || echo "[]">/tmp/miles.json
if has_jq; then
  jq '.[] | {number,title,state,due_on,open_issues,closed_issues,description}' /tmp/miles.json >> "$out"
else
  cat /tmp/miles.json >> "$out"
fi
echo "" >> "$out"

# --- THIS IS THE MODIFIED SECTION ---

echo "## Open Issues" >> "$out"
gh issue list --repo $OWNER/$REPO --state open --limit 200 \
  --json number,title,labels,milestone,url,createdAt > /tmp/issues_open.json
if has_jq; then
  jq '.[] | {number,title,milestone: .milestone.title,labels: [.labels[].name],url,createdAt}' /tmp/issues_open.json >> "$out"
else
  cat /tmp/issues_open.json >> "$out"
fi
echo "" >> "$out"

echo "## Recently Closed Issues" >> "$out"
gh issue list --repo $OWNER/$REPO --state closed --limit 30 \
  --json number,title,labels,milestone,url,closedAt > /tmp/issues_closed.json
if has_jq; then
  jq '.[] | {number,title,milestone: .milestone.title,labels: [.labels[].name],url,closedAt}' /tmp/issues_closed.json >> "$out"
else
  cat /tmp/issues_closed.json >> "$out"
fi
echo "" >> "$out"

# --- END OF MODIFIED SECTION ---

echo "## Labels" >> "$out"
gh label list --repo $OWNER/$REPO --json name,color,description > /tmp/labels.json
if has_jq; then
  jq '.[] | {name,color,description}' /tmp/labels.json >> "$out"
else
  cat /tmp/labels.json >> "$out"
fi
echo "" >> "$out"

echo "## Projects (Projects v2)" >> "$out"
gh project list --owner $OWNER > /tmp/projects.txt || true
cat /tmp/projects.txt >> "$out"
echo "" >> "$out"
if grep -Eo '#[0-9]+' /tmp/projects.txt >/dev/null 2>&1; then
  while read -r num; do
    pnum="${num//#/}"
    echo "### Project $pnum" >> "$out"
    gh project view "$pnum" --owner $OWNER --format json >> "$out" || true
    echo "" >> "$out"
  done < <(grep -Eo '#[0-9]+' /tmp/projects.txt | sort -u)
fi

echo "## Releases" >> "$out"
gh release list --repo $OWNER/$REPO >> "$out" || true
echo "" >> "$out"

echo "Report written to $out"
--- END OF FILE ./scripts/gh_status_report.sh ---

--- START OF FILE ./SECURITY.md ---
# Security Policy

## Supported Versions

Security fixes are provided for the latest minor version release.

| Version | Supported          |
| ------- | ------------------ |
| 0.2.x   | :white_check_mark: |
| < 0.2.0 | :x:                |

## Reporting a Vulnerability

We take all security vulnerabilities seriously. Please do not report security vulnerabilities through public GitHub issues.

Instead, please report them by using GitHub's private vulnerability reporting feature. Go to the "Security" tab of the repository and click "Report a vulnerability".

You should receive an acknowledgment within **3 business days**.

## Disclosure Process

We follow a coordinated disclosure process. Once a vulnerability is reported, we will work to release a patch as quickly as possible. We will publish a security advisory on GitHub to notify users upon release.
--- END OF FILE ./SECURITY.md ---

--- START OF FILE ./src/agents/execution_agent.py ---
# src/agents/execution_agent.py
"""
The ExecutionAgent is responsible for taking a concrete, validated execution
plan from the PlannerAgent and carrying it out. Its concerns are purely
about the "doing": generating code and running the execution tasks.
"""
import textwrap
from typing import List

from agents.models import ExecutionTask
from agents.plan_executor import PlanExecutionError, PlanExecutor
from agents.utils import PlanExecutionContext
from core.clients import GeneratorClient
from core.prompt_pipeline import PromptPipeline
from shared.logger import getLogger

log = getLogger(__name__)


class ExecutionAgent:
    """Orchestrates the execution of a plan, including code generation and validation."""

    def __init__(
        self,
        generator_client: GeneratorClient,
        prompt_pipeline: PromptPipeline,
        plan_executor: PlanExecutor,
    ):
        """Initializes the ExecutionAgent with its required tools."""
        self.generator = generator_client
        self.prompt_pipeline = prompt_pipeline
        self.executor = plan_executor

    # CAPABILITY: code_generation
    async def _generate_code_for_task(self, task: ExecutionTask, goal: str) -> str:
        """Generates the code content for a single task using a generator LLM."""
        log.info(f"✍️  Generating code for task: '{task.step}'...")
        if task.action not in ["create_file", "edit_function"]:
            return ""

        prompt_template = textwrap.dedent(
            """
            You are an expert Python programmer. Generate a single block of Python code to fulfill the task.
            **Overall Goal:** {goal}
            **Current Task:** {step}
            **Target File:** {file_path}
            **Target Symbol (if editing):** {symbol_name}
            **Instructions:** Your output MUST be ONLY the raw Python code. Do not wrap it in markdown blocks.
        """
        ).strip()

        final_prompt = prompt_template.format(
            goal=goal,
            step=task.step,
            file_path=task.params.file_path,
            symbol_name=task.params.symbol_name or "",
        )
        enriched_prompt = self.prompt_pipeline.process(final_prompt)
        return self.generator.make_request(
            enriched_prompt, user_id="execution_agent_coder"
        )

    async def execute_plan(
        self, high_level_goal: str, plan: List[ExecutionTask]
    ) -> tuple[bool, str]:
        """
        Takes a plan, generates code for each step, and then executes the
        fully-populated plan.
        """
        if not plan:
            return False, "Plan is empty or invalid."

        log.info("--- Starting Code Generation Phase ---")
        for task in plan:
            task.params.code = await self._generate_code_for_task(task, high_level_goal)
            if task.action in ["create_file", "edit_function"] and not task.params.code:
                return False, f"Code generation failed for step: '{task.step}'"

        log.info("--- Handing off to Executor ---")
        with PlanExecutionContext(self):
            try:
                await self.executor.execute_plan(plan)
                return True, "✅ Plan executed successfully."
            except PlanExecutionError as e:
                error_detail = str(e)
                log.error(f"Execution failed: {error_detail}", exc_info=True)
                if e.violations:
                    log.error("Violations found:")
                    for v in e.violations:
                        log.error(
                            f"  - [{v.get('rule')}] L{v.get('line')}: {v.get('message')}"
                        )
                return False, f"Plan execution failed: {error_detail}"
            except Exception as e:
                log.error(
                    "An unexpected error occurred during execution.", exc_info=True
                )
                return False, f"An unexpected error occurred: {str(e)}"

--- END OF FILE ./src/agents/execution_agent.py ---

--- START OF FILE ./src/agents/__init__.py ---
# src/agents/__init__.py
# Package marker for src/agents — contains CORE's agent implementations.

--- END OF FILE ./src/agents/__init__.py ---

--- START OF FILE ./src/agents/manifest.yaml ---
capabilities:
- code_generation
- llm_orchestration
description: Specialized AI actors (planners, reviewers, suggesters)
domain: agents

--- END OF FILE ./src/agents/manifest.yaml ---

--- START OF FILE ./src/agents/models.py ---
# src/agents/models.py
"""
Data models for the PlannerAgent and execution tasks.
Defines the structure of plans, tasks, and configurations.
"""
from dataclasses import dataclass
from enum import Enum
from typing import Literal, Optional

from pydantic import BaseModel


class TaskStatus(Enum):
    """Enumeration of possible states for an ExecutionTask."""

    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class ExecutionProgress:
    """Represents the progress of a plan's execution."""

    total_tasks: int
    completed_tasks: int
    current_task: Optional[str] = None
    status: TaskStatus = TaskStatus.PENDING

    @property
    def completion_percentage(self) -> float:
        """
        Calculates the completion percentage of the plan as a float,
        returning 0 if there are no tasks.
        """
        """Calculates the completion percentage of the plan."""
        return (
            (self.completed_tasks / self.total_tasks) * 100
            if self.total_tasks > 0
            else 0
        )


@dataclass
class PlannerConfig:
    """Configuration settings for the PlannerAgent's behavior."""

    max_retries: int = 3
    validation_enabled: bool = True
    auto_commit: bool = True
    rollback_on_failure: bool = True
    task_timeout: int = 300  # seconds


# --- THIS IS THE CORRECT, FLEXIBLE VERSION ---
class TaskParams(BaseModel):
    """Data model for the parameters of a single task in an execution plan."""

    file_path: str
    symbol_name: Optional[str] = None
    tag: Optional[str] = None
    code: Optional[str] = None


class ExecutionTask(BaseModel):
    """Data model for a single, executable step in a plan."""

    step: str
    action: Literal["add_capability_tag", "create_file", "edit_function"]
    params: TaskParams

--- END OF FILE ./src/agents/models.py ---

--- START OF FILE ./src/agents/plan_executor.py ---
# src/agents/plan_executor.py
"""
Intent: Provides a dedicated, atomic service for executing a pre-defined plan.

This module separates the execution logic from the planning and generation logic
of the PlannerAgent, adhering to the 'separation_of_concerns' principle.
"""
import asyncio
from typing import List

from agents.models import ExecutionTask, PlannerConfig, TaskParams
from agents.utils import CodeEditor, SymbolLocator
from core.file_handler import FileHandler
from core.git_service import GitService
from core.validation_pipeline import validate_code
from shared.logger import getLogger

log = getLogger(__name__)


class PlanExecutionError(Exception):
    """Custom exception for failures during plan execution."""

    def __init__(self, message, violations=None):
        super().__init__(message)
        self.violations = violations or []


class PlanExecutor:
    """A service that takes a list of ExecutionTasks and executes them sequentially."""

    def __init__(
        self, file_handler: FileHandler, git_service: GitService, config: PlannerConfig
    ):
        """Initializes the executor with necessary dependencies."""
        self.file_handler = file_handler
        self.git_service = git_service
        self.config = config
        self.repo_path = self.file_handler.repo_path
        self.symbol_locator = SymbolLocator()
        self.code_editor = CodeEditor()
        self._executor = asyncio.get_event_loop().run_in_executor

    async def execute_plan(self, plan: List[ExecutionTask]):
        """Executes the entire plan, one task at a time."""
        for i, task in enumerate(plan, 1):
            log.info(f"--- Executing Step {i}/{len(plan)}: {task.step} ---")
            await self._execute_task_with_timeout(task)

    async def _execute_task_with_timeout(self, task: ExecutionTask):
        """Execute task with timeout protection."""
        timeout = self.config.task_timeout
        try:
            await asyncio.wait_for(self._execute_task(task), timeout=timeout)
        except asyncio.TimeoutError:
            raise PlanExecutionError(f"Task '{task.step}' timed out after {timeout}s")

    async def _execute_task(self, task: ExecutionTask):
        """Dispatcher that executes a single task from a plan based on its action type."""
        action_map = {
            "add_capability_tag": self._execute_add_tag,
            "create_file": self._execute_create_file,
            "edit_function": self._execute_edit_function,
        }
        if task.action in action_map:
            await action_map[task.action](task.params)
        else:
            log.warning(f"Skipping task: Unknown action '{task.action}'.")

    async def _execute_add_tag(self, params: TaskParams):
        """Executes the surgical 'add_capability_tag' action."""
        file_path, symbol_name, tag = params.file_path, params.symbol_name, params.tag
        line_number = await self._executor(
            None,
            self.symbol_locator.find_symbol_line,
            self.repo_path / file_path,
            symbol_name,
        )
        if not line_number:
            raise PlanExecutionError(
                f"Could not find symbol '{symbol_name}' in '{file_path}'."
            )

        full_path = self.repo_path / file_path
        if not full_path.exists():
            raise PlanExecutionError(f"File '{file_path}' does not exist.")

        lines = full_path.read_text(encoding="utf-8").splitlines()

        insertion_index = line_number - 1
        indentation = len(lines[insertion_index]) - len(
            lines[insertion_index].lstrip(" ")
        )
        lines.insert(insertion_index, f"{' ' * indentation}# CAPABILITY: {tag}")
        modified_code = "\n".join(lines)

        validation_result = validate_code(file_path, modified_code)
        if validation_result["status"] == "dirty":
            raise PlanExecutionError(
                f"Surgical modification for '{file_path}' failed validation.",
                violations=validation_result["violations"],
            )

        pending_id = self.file_handler.add_pending_write(
            prompt=f"Goal: add tag to {symbol_name}",
            suggested_path=file_path,
            code=validation_result["code"],
        )
        self.file_handler.confirm_write(pending_id)

        if self.config.auto_commit and self.git_service.is_git_repo():
            self.git_service.add(file_path)
            self.git_service.commit(
                f"refactor(capability): Add '{tag}' tag to {symbol_name}"
            )

    async def _execute_create_file(self, params: TaskParams):
        """Executes the 'create_file' action."""
        file_path, code = params.file_path, params.code
        full_path = self.repo_path / file_path
        if full_path.exists():
            raise FileExistsError(
                f"File '{file_path}' already exists. Use 'edit_function' instead."
            )

        validation_result = validate_code(file_path, code)
        if validation_result["status"] == "dirty":
            raise PlanExecutionError(
                f"Generated code for '{file_path}' failed validation.",
                violations=validation_result["violations"],
            )

        pending_id = self.file_handler.add_pending_write(
            prompt=f"Goal: create file {file_path}",
            suggested_path=file_path,
            code=validation_result["code"],
        )
        self.file_handler.confirm_write(pending_id)

        if self.config.auto_commit and self.git_service.is_git_repo():
            self.git_service.add(file_path)
            self.git_service.commit(f"feat: Create new file {file_path}")

    async def _execute_edit_function(self, params: TaskParams):
        """Executes the 'edit_function' action using the CodeEditor."""
        file_path, symbol_name, new_code = (
            params.file_path,
            params.symbol_name,
            params.code,
        )
        full_path = self.repo_path / file_path

        if not full_path.exists():
            raise FileNotFoundError(
                f"Cannot edit function, file not found: '{file_path}'"
            )

        original_code = await self._executor(None, full_path.read_text, "utf-8")

        validation_result = validate_code(file_path, new_code)
        if validation_result["status"] == "dirty":
            raise PlanExecutionError(
                f"Generated code for '{symbol_name}' failed validation.",
                violations=validation_result["violations"],
            )

        validated_code_snippet = validation_result["code"]
        try:
            final_code = self.code_editor.replace_symbol_in_code(
                original_code, symbol_name, validated_code_snippet
            )
        except ValueError as e:
            raise PlanExecutionError(f"Failed to edit code in '{file_path}': {e}")

        pending_id = self.file_handler.add_pending_write(
            prompt=f"Goal: edit function {symbol_name} in {file_path}",
            suggested_path=file_path,
            code=final_code,
        )
        self.file_handler.confirm_write(pending_id)

        if self.config.auto_commit and self.git_service.is_git_repo():
            self.git_service.add(file_path)
            self.git_service.commit(
                f"feat: Modify function {symbol_name} in {file_path}"
            )

--- END OF FILE ./src/agents/plan_executor.py ---

--- START OF FILE ./src/agents/planner_agent.py ---
# src/agents/planner_agent.py
"""
The primary agent responsible for decomposing high-level goals into executable plans.
"""
import contextvars
import json
import re
import textwrap
from datetime import datetime, timezone
from typing import Dict, List, Optional

from pydantic import ValidationError

from agents.models import ExecutionTask, PlannerConfig
from agents.plan_executor import PlanExecutionError
from core.clients import OrchestratorClient
from core.prompt_pipeline import PromptPipeline
from shared.logger import getLogger

log = getLogger(__name__)
execution_context = contextvars.ContextVar("execution_context")


class PlannerAgent:
    """Decomposes goals into plans but does not execute them."""

    def __init__(
        self,
        orchestrator_client: OrchestratorClient,
        prompt_pipeline: PromptPipeline,
        config: Optional[PlannerConfig] = None,
    ):
        """Initializes the PlannerAgent with its dependencies."""
        self.orchestrator = orchestrator_client
        self.prompt_pipeline = prompt_pipeline
        self.config = config or PlannerConfig()

    def _setup_logging_context(self, goal: str, plan_id: str):
        """Sets up a structured logging context for a planning cycle."""
        execution_context.set(
            {
                "goal": goal,
                "plan_id": plan_id,
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
        )

    def _extract_json_from_response(self, text: str) -> Optional[Dict]:
        """Extracts a JSON object or array from a raw text response, handling markdown blocks."""
        match = re.search(
            r"```json\s*(\{[\s\S]*?\}|\[[\s\S]*?\])\s*```", text, re.DOTALL
        )
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                log.warning(
                    "Found a JSON markdown block, but it contained invalid JSON."
                )
        try:
            start_index = text.find("{")
            if start_index == -1:
                start_index = text.find("[")
            if start_index == -1:
                return None
            decoder = json.JSONDecoder()
            obj, _ = decoder.raw_decode(text[start_index:])
            return obj
        except (json.JSONDecodeError, ValueError):
            log.warning("Could not find a valid JSON object using boundary detection.")
        log.error("Failed to extract any valid JSON from the LLM response.")
        return None

    def _log_plan_summary(self, plan: List[ExecutionTask]) -> None:
        """Logs a human-readable summary of the generated execution plan."""
        log.info(f"📋 Execution Plan Summary ({len(plan)} tasks):")
        for i, task in enumerate(plan, 1):
            log.info(f"  {i}. [{task.action}] {task.step}")

    def _validate_task_params(self, task: ExecutionTask):
        """Validates that a task has all required parameters for its specified action."""
        params = task.params
        required = []
        if task.action == "add_capability_tag":
            required = ["file_path", "symbol_name", "tag"]
        elif task.action == "create_file":
            required = ["file_path"]
        elif task.action == "edit_function":
            required = ["file_path", "symbol_name"]
        if not all(getattr(params, p, None) for p in required):
            raise PlanExecutionError(
                f"Task '{task.step}' is missing required parameters for action '{task.action}'."
            )

    # CAPABILITY: llm_orchestration
    def create_execution_plan(self, high_level_goal: str) -> List[ExecutionTask]:
        """Decomposes a high-level goal into a structured, code-free execution plan using an LLM."""
        plan_id = f"plan_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
        self._setup_logging_context(high_level_goal, plan_id)
        log.info("🧠 Decomposing goal into a high-level plan...")

        prompt_template = textwrap.dedent(
            """
            You are a hyper-competent, meticulous system architect AI. Your task is to decompose a high-level goal into a JSON execution plan.
            Your entire output MUST be a single, valid JSON array of objects.

            **High-Level Goal:** "{goal}"

            **Available Actions & Required Parameters:**
            - Action: `create_file` -> Params: `{{ "file_path": "<path>" }}`
            - Action: `edit_function` -> Params: `{{ "file_path": "<path>", "symbol_name": "<func_name>" }}`
            - Action: `add_capability_tag` -> Params: `{{ "file_path": "<path>", "symbol_name": "<func_name>", "tag": "<tag_name>" }}`

            **CRITICAL RULE:** Do NOT include a `"code"` parameter in this step. Generate the code-free JSON plan now.
            """
        ).strip()
        final_prompt = prompt_template.format(goal=high_level_goal)
        enriched_prompt = self.prompt_pipeline.process(final_prompt)

        for attempt in range(self.config.max_retries):
            try:
                response_text = self.orchestrator.make_request(
                    enriched_prompt, user_id="planner_agent_architect"
                )
                parsed_json = self._extract_json_from_response(response_text)
                if not parsed_json:
                    raise ValueError("No valid JSON found in response")
                if isinstance(parsed_json, dict):
                    parsed_json = [parsed_json]

                validated_plan = [ExecutionTask(**task) for task in parsed_json]
                for task in validated_plan:
                    self._validate_task_params(task)

                self._log_plan_summary(validated_plan)
                return validated_plan
            except (ValueError, json.JSONDecodeError, ValidationError) as e:
                log.warning(f"Plan creation attempt {attempt + 1} failed: {e}")
                if attempt == self.config.max_retries - 1:
                    raise PlanExecutionError(
                        "Failed to create a valid high-level plan after max retries."
                    )
        return []

--- END OF FILE ./src/agents/planner_agent.py ---

--- START OF FILE ./src/agents/utils.py ---
# src/agents/utils.py
"""
Utility classes and functions for CORE agents.
"""
import ast
import textwrap
from pathlib import Path
from typing import Optional, Tuple

from shared.logger import getLogger

log = getLogger(__name__)


class CodeEditor:
    """Provides capabilities to surgically edit code files."""

    def _get_symbol_start_end_lines(
        self, tree: ast.AST, symbol_name: str
    ) -> Optional[Tuple[int, int]]:
        """Finds the 1-based start and end line numbers of a symbol."""
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                if node.name == symbol_name:
                    if hasattr(node, "end_lineno") and node.end_lineno is not None:
                        return node.lineno, node.end_lineno
        return None

    # CAPABILITY: agents:docstring-fixer
    def replace_symbol_in_code(
        self, original_code: str, symbol_name: str, new_code_str: str
    ) -> str:
        """
        Replaces a function/method in code with a new version using a line-based strategy.
        """
        try:
            original_tree = ast.parse(original_code)
        except SyntaxError as e:
            raise ValueError(f"Could not parse original code due to syntax error: {e}")

        symbol_location = self._get_symbol_start_end_lines(original_tree, symbol_name)
        if not symbol_location:
            raise ValueError(f"Symbol '{symbol_name}' not found in the original code.")

        start_line, end_line = symbol_location
        start_index = start_line - 1
        end_index = end_line

        lines = original_code.splitlines()

        original_line = lines[start_index]
        indentation = len(original_line) - len(original_line.lstrip(" "))

        clean_new_code = textwrap.dedent(new_code_str).strip()
        new_code_lines = clean_new_code.splitlines()
        indented_new_code_lines = [
            f"{' ' * indentation}{line}" for line in new_code_lines
        ]

        code_before = lines[:start_index]
        code_after = lines[end_index:]

        final_lines = code_before + indented_new_code_lines + code_after
        return "\n".join(final_lines)


class SymbolLocator:
    """Dedicated class for finding symbols in code files."""

    @staticmethod
    def find_symbol_line(file_path: Path, symbol_name: str) -> Optional[int]:
        """Finds the line number of a function or class definition in a file."""
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        try:
            code = file_path.read_text(encoding="utf-8")
            tree = ast.parse(code)
            for node in ast.walk(tree):
                if isinstance(
                    node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                ):
                    if node.name == symbol_name:
                        return node.lineno
        except (SyntaxError, UnicodeDecodeError) as e:
            raise RuntimeError(f"Failed to parse {file_path}: {e}")
        return None


class PlanExecutionContext:
    """Context manager for safe plan execution with rollback."""

    def __init__(self, execution_agent):
        """Initializes the context with a reference to the ExecutionAgent."""
        # The context now correctly references the executor, which holds the git_service and config.
        self.executor = execution_agent.executor
        self.initial_commit = None

    def __enter__(self):
        """Sets up the execution context, capturing the initial git commit hash."""
        # It now correctly accesses git_service and config through the executor.
        if self.executor.git_service.is_git_repo():
            try:
                self.initial_commit = self.executor.git_service.get_current_commit()
            except Exception as e:
                log.warning(f"Could not get current commit for rollback: {e}")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Cleans up and handles rollback on failure."""
        if (
            exc_type
            and self.initial_commit
            and self.executor.config.rollback_on_failure
        ):
            log.warning("Rolling back to initial state due to failure")
            try:
                self.executor.git_service.reset_to_commit(self.initial_commit)
            except Exception as e:
                log.error(f"Failed to rollback: {e}")

--- END OF FILE ./src/agents/utils.py ---

--- START OF FILE ./src/core/black_formatter.py ---
# src/core/black_formatter.py
"""
Formats Python code using Black before it's written to disk.
"""
import black


# --- MODIFICATION: The function now returns only the formatted code on success ---
# --- and raises a specific exception on failure, simplifying its contract. ---
def format_code_with_black(code: str) -> str:
    """Formats the given Python code using Black, raising `black.InvalidInput` for syntax errors or `Exception` for other formatting issues."""
    """
    Attempts to format the given Python code using Black.

    Args:
        code: The Python source code to format.

    Returns:
        The formatted code as a string.

    Raises:
        black.InvalidInput: If the code contains a syntax error that Black cannot handle.
        Exception: For other unexpected Black formatting errors.
    """
    try:
        mode = black.FileMode()
        formatted_code = black.format_str(code, mode=mode)
        return formatted_code
    except black.InvalidInput as e:
        # Re-raise with a clear message for the pipeline to catch.
        raise black.InvalidInput(
            f"Black could not format the code due to a syntax error: {e}"
        )
    except Exception as e:
        # Catch any other unexpected errors from Black.
        raise Exception(f"An unexpected error occurred during Black formatting: {e}")

--- END OF FILE ./src/core/black_formatter.py ---

--- START OF FILE ./src/core/capabilities.py ---
# src/core/capabilities.py
"""
CORE Capability Registry
This file is the high-level entry point for the system's self-awareness loop.
It defines the `introspection` capability, which orchestrates the system's tools
to perform a full self-analysis.
"""
import subprocess
import sys
from pathlib import Path

from dotenv import load_dotenv

from shared.logger import getLogger

log = getLogger(__name__)


# CAPABILITY: introspection
def introspection():
    """
    Runs a full self-analysis cycle to inspect system structure and health.
    This orchestrates the execution of the system's own introspection tools
    as separate, governed processes.
    """
    log.info("🔍 Starting introspection cycle...")

    project_root = Path(__file__).resolve().parents[2]
    python_executable = sys.executable

    tools_to_run = [
        ("Knowledge Graph Builder", "system.tools.codegraph_builder"),
        ("Constitutional Auditor", "system.governance.constitutional_auditor"),
    ]

    all_passed = True
    for name, module in tools_to_run:
        log.info(f"Running {name}...")
        try:
            result = subprocess.run(
                [python_executable, "-m", module],
                cwd=project_root,
                capture_output=True,
                text=True,
                check=True,
            )
            # --- THIS IS THE FIX ---
            # If the process was successful, print its standard output.
            # This gives us the detailed report from the auditor.
            if result.stdout:
                # We use print() directly here so the rich formatting from the
                # auditor's console is preserved perfectly.
                print(result.stdout)

            if result.stderr:
                log.warning(f"{name} stderr:\n{result.stderr}")
            log.info(f"✅ {name} completed successfully.")
        except subprocess.CalledProcessError as e:
            log.error(f"❌ {name} failed with exit code {e.returncode}.")
            # Print the output on failure so we can see the full error report.
            if e.stdout:
                print(e.stdout)
            if e.stderr:
                print(e.stderr)
            all_passed = False
        except Exception as e:
            log.error(
                f"💥 An unexpected error occurred while running {name}: {e}",
                exc_info=True,
            )
            all_passed = False

    log.info("🧠 Introspection cycle completed.")
    return all_passed


if __name__ == "__main__":
    load_dotenv()
    # Allows running the full introspection cycle directly from the CLI.
    if not introspection():
        sys.exit(1)
    sys.exit(0)

--- END OF FILE ./src/core/capabilities.py ---

--- START OF FILE ./src/core/clients.py ---
# src/core/clients.py
"""
Clients for communicating with the different LLMs in the CORE ecosystem.
This version is updated to use the modern "Chat Completions" API format,
and uses the 'httpx' library for robust, asynchronous network requests.
"""
import json

import httpx
import requests

from shared.config import settings
from shared.logger import getLogger

log = getLogger(__name__)


class BaseLLMClient:
    """
    Base class for LLM clients, handling common request logic for Chat APIs.
    Provides shared initialization and error handling for all LLM clients.
    """

    def __init__(self, api_url: str, api_key: str, model_name: str):
        """
        Initialize the LLM client with API credentials and endpoint.

        Args:
            api_url (str): Base URL for the LLM's chat completions API.
            api_key (str): Authentication token for the API.
            model_name (str): Name of the model to use (e.g., 'gpt-4', 'deepseek-coder').
        """
        if not api_url or not api_key:
            raise ValueError(
                f"{self.__class__.__name__} requires both API_URL and API_KEY."
            )

        if not api_url.endswith("/v1/chat/completions"):
            self.api_url = api_url.rstrip("/") + "/v1/chat/completions"
        else:
            self.api_url = api_url

        self.model_name = model_name
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        # --- THIS IS THE UPGRADE (Part 1 of 2) ---
        # We add an async client for concurrent operations, while keeping the sync client for now.
        self.async_client = httpx.AsyncClient(timeout=180.0)

    def make_request(self, prompt: str, user_id: str = "core_system") -> str:
        """
        Sends a prompt to the configured Chat Completions API. (Synchronous)
        """
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "user": user_id,
        }

        try:
            log.debug(
                f"Sending request to {self.api_url} for model {self.model_name}..."
            )
            response = requests.post(
                self.api_url, headers=self.headers, json=payload, timeout=180
            )
            response.raise_for_status()
            response_data = response.json()
            content = response_data["choices"][0]["message"]["content"]
            log.debug("Successfully received and parsed LLM response.")
            return content if content is not None else ""
        except requests.exceptions.RequestException as e:
            log.error(
                f"Network error during LLM request to {self.api_url}: {e}",
                exc_info=True,
            )
            return f"Error: Could not connect to LLM endpoint. Details: {e}"
        except (KeyError, IndexError):
            log.error(
                f"Error parsing LLM response. Full response: {response.text}",
                exc_info=True,
            )
            return "Error: Could not parse response from API."

    # --- THIS IS THE UPGRADE (Part 2 of 2) ---
    # A new, async version of the make_request method.
    async def make_request_async(
        self, prompt: str, user_id: str = "core_system"
    ) -> str:
        """
        Sends a prompt asynchronously to the configured Chat Completions API.
        """
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "user": user_id,
        }
        try:
            log.debug(
                f"Sending async request to {self.api_url} for model {self.model_name}..."
            )
            response = await self.async_client.post(
                self.api_url, headers=self.headers, json=payload
            )
            response.raise_for_status()
            response_data = response.json()
            content = response_data["choices"][0]["message"]["content"]
            log.debug("Successfully received and parsed async LLM response.")
            return content if content is not None else ""
        except httpx.ReadTimeout:
            log.error(f"Network timeout during async LLM request to {self.api_url}.")
            return "Error: Request timed out."
        except httpx.RequestError as e:
            log.error(
                f"Network error during async LLM request to {self.api_url}: {e}",
                exc_info=True,
            )
            return f"Error: Could not connect to LLM endpoint. Details: {e}"
        except (KeyError, IndexError, json.JSONDecodeError):
            log.error(
                f"Error parsing async LLM response. Full response: {response.text}",
                exc_info=True,
            )
            return "Error: Could not parse response from API."


class OrchestratorClient(BaseLLMClient):
    """
    Client for the Orchestrator LLM (e.g., GPT-4, Claude 3).
    Responsible for high-level planning and intent interpretation.
    """

    def __init__(self):
        super().__init__(
            api_url=settings.ORCHESTRATOR_API_URL,
            api_key=settings.ORCHESTRATOR_API_KEY,
            model_name=settings.ORCHESTRATOR_MODEL_NAME,
        )
        log.info(f"OrchestratorClient initialized for model '{self.model_name}'.")


class GeneratorClient(BaseLLMClient):
    """
    Client for the Generator LLM (e.g., a specialized coding model).
    Responsible for code generation and detailed implementation.
    """

    def __init__(self):
        """Initialize the LLM client with API URL, key, and model name, setting up headers and async client."""
        super().__init__(
            api_url=settings.GENERATOR_API_URL,
            api_key=settings.GENERATOR_API_KEY,
            model_name=settings.GENERATOR_MODEL_NAME,
        )
        log.info(f"GeneratorClient initialized for model '{self.model_name}'.")

--- END OF FILE ./src/core/clients.py ---

--- START OF FILE ./src/core/errors.py ---
# src/core/errors.py
"""
Centralized HTTP exception handlers for the CORE FastAPI application.
This module ensures that no unhandled exceptions leak sensitive stack trace
information to the client, aligning with the 'safe_by_default' principle.
"""
from fastapi import Request
from fastapi.responses import JSONResponse
from starlette import status
from starlette.exceptions import HTTPException as StarletteHTTPException

from shared.logger import getLogger

log = getLogger("core_api.errors")


def register_exception_handlers(app):
    """Registers custom exception handlers with the FastAPI application."""

    @app.exception_handler(StarletteHTTPException)
    async def http_exception_handler(request: Request, exc: StarletteHTTPException):
        """
        Handles FastAPI's built-in HTTP exceptions to ensure consistent
        JSON error responses.
        """
        log.warning(
            f"HTTP Exception: {exc.status_code} {exc.detail} for request: {request.method} {request.url.path}"
        )
        return JSONResponse(
            status_code=exc.status_code,
            content={"error": "request_error", "detail": exc.detail},
        )

    @app.exception_handler(Exception)
    async def unhandled_exception_handler(request: Request, exc: Exception):
        """
        Catches any unhandled exception, logs the full traceback internally,
        and returns a generic 500 Internal Server Error to the client.
        This is a critical security measure to prevent leaking stack traces.
        """
        log.exception(
            f"Unhandled exception for request: {request.method} {request.url.path}"
        )
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "error": "internal_server_error",
                "detail": "An unexpected internal error occurred.",
            },
        )

    log.info("Registered global exception handlers.")

--- END OF FILE ./src/core/errors.py ---

--- START OF FILE ./src/core/file_handler.py ---
# src/core/file_handler.py
"""
Backend File Handling Module (Refactored)

Handles staging and writing file changes. It supports traceable, auditable
operations. All writes go through a pending stage to enable review and rollback.
"""

import json
import threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict
from uuid import uuid4

from shared.logger import getLogger

log = getLogger(__name__)


class FileHandler:
    """
    Central class for safe, auditable file operations in CORE.
    All writes are staged first and require confirmation. Validation is handled
    by the calling agent via the validation_pipeline.
    """

    def __init__(self, repo_path: str):
        """
        Initialize FileHandler with repository root.
        """
        self.repo_path = Path(repo_path).resolve()
        if not self.repo_path.is_dir():
            raise ValueError(f"Invalid repository path provided: {repo_path}")

        # --- THIS IS THE FIX ---
        # All operational directories are now relative to the repo_path
        # that the handler was initialized with. This makes the handler
        # safe to use in different contexts (like our integration test).
        self.log_dir = self.repo_path / "logs"
        self.pending_dir = self.repo_path / "pending_writes"
        self.undo_log = self.log_dir / "undo_log.jsonl"

        self.log_dir.mkdir(exist_ok=True)
        self.pending_dir.mkdir(exist_ok=True)
        # --- END OF FIX ---

        self.pending_writes: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.Lock()

    def add_pending_write(self, prompt: str, suggested_path: str, code: str) -> str:
        """
        Stages a pending write operation for later confirmation.
        """
        pending_id = str(uuid4())
        rel_path = Path(suggested_path).as_posix()
        entry = {
            "id": pending_id,
            "prompt": prompt,
            "path": rel_path,
            "code": code,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

        with self._lock:
            self.pending_writes[pending_id] = entry

        pending_file = self.pending_dir / f"{pending_id}.json"
        pending_file.write_text(json.dumps(entry, indent=2), encoding="utf-8")
        return pending_id

    def confirm_write(self, pending_id: str) -> Dict[str, str]:
        """
        Confirms and applies a pending write to disk. Assumes content has been validated.
        """
        with self._lock:
            pending_op = self.pending_writes.pop(pending_id, None)

        pending_file = self.pending_dir / f"{pending_id}.json"
        if pending_file.exists():
            pending_file.unlink(missing_ok=True)

        if not pending_op:
            return {
                "status": "error",
                "message": f"Pending write ID '{pending_id}' not found or already processed.",
            }

        file_rel_path = pending_op["path"]

        try:
            abs_file_path = self.repo_path / file_rel_path

            if not abs_file_path.resolve().is_relative_to(self.repo_path.resolve()):
                raise ValueError(
                    f"Attempted to write outside of repository boundary: {file_rel_path}"
                )

            abs_file_path.parent.mkdir(parents=True, exist_ok=True)
            abs_file_path.write_text(pending_op["code"], encoding="utf-8")

            log.info(f"Wrote to {file_rel_path}")
            return {
                "status": "success",
                "message": f"Wrote to {file_rel_path}",
                "file_path": file_rel_path,
            }
        except Exception as e:
            if pending_op:
                with self._lock:
                    self.pending_writes[pending_id] = pending_op
                pending_file.write_text(
                    json.dumps(pending_op, indent=2), encoding="utf-8"
                )
            return {"status": "error", "message": f"Failed to write file: {str(e)}"}

--- END OF FILE ./src/core/file_handler.py ---

--- START OF FILE ./src/core/git_service.py ---
# src/core/git_service.py
"""
GitService — CORE's Git Integration Layer

Provides safe, auditable Git operations:
- add, commit, rollback
- status checks
- branch management

Ensures all changes are tracked and reversible.
Used by main.py and self-correction engine.
"""

import subprocess
from pathlib import Path

from shared.logger import getLogger

log = getLogger(__name__)


class GitService:
    """
    Encapsulates Git operations for the CORE system.
    Ensures all file changes are committed with traceable messages.
    """

    def __init__(self, repo_path: str):
        """Initialize GitService with the resolved absolute path to the Git repository; raises ValueError if path is not a valid Git repo."""
        self.repo_path = Path(repo_path).resolve()
        if not self.is_git_repo():
            raise ValueError(f"Invalid Git repository: {repo_path}")
        log.info(f"GitService initialized for repo at {self.repo_path}")

    # CAPABILITY: change_safety_enforcement
    def _run_command(self, command: list) -> str:
        """
        Run a Git command and return stdout.

        Args:
            command (list): Git command as a list (e.g., ['git', 'status']).

        Returns:
            str: Command output, or raises RuntimeError on failure.
        """
        try:
            log.debug(f"Running git command: {' '.join(command)}")
            result = subprocess.run(
                command, cwd=self.repo_path, capture_output=True, text=True, check=True
            )
            return result.stdout.strip()
        except subprocess.CalledProcessError as e:
            log.error(f"Git command failed: {e.stderr}")
            raise RuntimeError(f"Git command failed: {e.stderr}") from e

    def add(self, file_path: str = "."):
        """
        Stage a file or directory for commit.

        Args:
            file_path (str): Path to stage. Defaults to '.' (all changes).
        """
        abs_path = (self.repo_path / file_path).resolve()
        if self.repo_path not in abs_path.parents and abs_path != self.repo_path:
            raise ValueError(f"Cannot stage file outside repo: {file_path}")
        self._run_command(["git", "add", file_path])

    def commit(self, message: str):
        """
        Commit staged changes with a message.
        If there are no changes to commit, this operation is a no-op and will not raise an error.

        Args:
            message (str): Commit message explaining the change.
        """
        try:
            # --- THIS IS THE FIX ---
            # First, check if there are any staged changes.
            status_output = self._run_command(["git", "status", "--porcelain"])
            if not status_output:
                log.info("No changes to commit.")
                return

            self._run_command(["git", "commit", "-m", message])
            log.info(f"Committed changes with message: {message}")
        except RuntimeError as e:
            # It's possible for a race condition, or for the status check to be insufficient.
            # We specifically check for the "nothing to commit" message from Git.
            if "nothing to commit" in str(e).lower():
                log.info("No changes to commit.")
            else:
                # Re-raise any other unexpected error.
                raise e
            # --- END OF FIX ---

    def is_git_repo(self) -> bool:
        """
        Check if the configured path is a valid Git repository.

        Returns:
            bool: True if it's a Git repo, False otherwise.
        """
        git_dir = self.repo_path / ".git"
        return git_dir.is_dir()

    def get_current_commit(self) -> str:
        """
        Gets the full SHA hash of the current commit (HEAD).
        """
        return self._run_command(["git", "rev-parse", "HEAD"])

    def reset_to_commit(self, commit_hash: str):
        """
        Performs a hard reset to a specific commit hash.
        This will discard all current changes.
        """
        log.warning(f"Performing hard reset to commit {commit_hash}...")
        self._run_command(["git", "reset", "--hard", commit_hash])
        log.info(f"Repository reset to {commit_hash}.")

--- END OF FILE ./src/core/git_service.py ---

--- START OF FILE ./src/core/__init__.py ---
# src/core/__init__.py
# Package marker for src/core — central module for CORE's intent-driven engine.

--- END OF FILE ./src/core/__init__.py ---

--- START OF FILE ./src/core/intent_alignment.py ---
# src/core/intent_alignment.py
"""
Lightweight guard to ensure a requested goal aligns with CORE's mission/scope.

- Loads NorthStar/mission text from .intent (best-effort; no hard failures).
- Optional blocklist: .intent/policies/blocked_topics.txt (one term per line).
- Returns (ok: bool, details: dict) with short reason codes only.
"""
from __future__ import annotations

import logging
import re
from pathlib import Path
from typing import Dict, List, Tuple

log = logging.getLogger(__name__)

_INTENT_PATH_CANDIDATES: List[Path] = [
    Path(".intent/mission/northstar.md"),
    Path(".intent/mission/mission.md"),
    Path(".intent/mission/northstar.txt"),
    Path(".intent/NorthStar.md"),
]

_BLOCKLIST_PATH = Path(".intent/policies/blocked_topics.txt")


def _read_text_first(paths: List[Path]) -> str:
    """Finds and reads the first existing file from a list of candidate paths."""
    for p in paths:
        try:
            if p.exists():
                return p.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            log.debug("Failed reading %s", p, exc_info=True)
    return ""


def _read_blocklist() -> List[str]:
    """Reads the blocklist file, returning a list of lowercased, stripped terms."""
    if _BLOCKLIST_PATH.exists():
        try:
            return [
                ln.strip().lower()
                for ln in _BLOCKLIST_PATH.read_text(
                    encoding="utf-8", errors="ignore"
                ).splitlines()
                if ln.strip() and not ln.strip().startswith("#")
            ]
        except Exception:
            log.debug("Failed reading blocklist at %s", _BLOCKLIST_PATH, exc_info=True)
    return []


def _tokenize(text: str) -> List[str]:
    """Converts a string into a list of lowercase alphanumeric tokens."""
    return re.findall(r"[a-zA-Z0-9]+", text.lower())


def check_goal_alignment(
    goal: str, project_root: Path = Path(".")
) -> Tuple[bool, Dict]:
    """
    Returns (ok, details). details = { 'coverage': float|None, 'violations': [codes...] }
    Violations codes: 'blocked_topic', 'low_mission_overlap'
    """
    violations: List[str] = []
    mission = _read_text_first(_INTENT_PATH_CANDIDATES)
    blocked = _read_blocklist()

    # Blocklist
    goal_l = goal.lower()
    if blocked and any(term in goal_l for term in blocked):
        violations.append("blocked_topic")

    # Mission overlap (very simple lexical overlap)
    coverage = None
    if mission:
        g_tokens = set(_tokenize(goal))
        m_tokens = set(_tokenize(mission))
        if g_tokens:
            overlap = len(g_tokens & m_tokens)
            coverage = round(overlap / max(1, len(g_tokens)), 3)
            if coverage < 0.10:  # conservative default; tune later
                violations.append("low_mission_overlap")

    ok = not violations
    return ok, {"coverage": coverage, "violations": violations}

--- END OF FILE ./src/core/intent_alignment.py ---

--- START OF FILE ./src/core/intent_guard.py ---
# src/core/intent_guard.py
"""
IntentGuard — CORE's Constitutional Enforcement Module

Enforces safety, structure, and intent alignment for all file changes.
Loads governance rules from .intent/policies/*.yaml and prevents unauthorized
self-modifications of the CORE constitution.
"""

import json
from pathlib import Path
from typing import Dict, List, Tuple

from shared.config_loader import load_config
from shared.logger import getLogger

log = getLogger(__name__)


# CAPABILITY: intent_guarding
class IntentGuard:
    """
    Central enforcement engine for CORE's safety and governance policies.
    Ensures all proposed file changes comply with declared rules and classifications.
    """

    def __init__(self, repo_path: Path):
        """
        Initialize IntentGuard with repository path and load all policies.
        """
        self.repo_path = Path(repo_path).resolve()
        self.intent_path = self.repo_path / ".intent"
        self.proposals_path = self.intent_path / "proposals"
        self.policies_path = self.intent_path / "policies"
        self.rules: List[Dict] = []

        self._load_policies()
        self.source_code_manifest = self._load_source_manifest()

        log.info(
            f"IntentGuard initialized. {len(self.rules)} rules loaded. Watching {len(self.source_code_manifest)} source files."
        )

    def _load_policies(self):
        """Load rules from all YAML files in the `.intent/policies/` directory."""
        if not self.policies_path.is_dir():
            return
        for policy_file in self.policies_path.glob("*.yaml"):
            content = load_config(policy_file, "yaml")
            if content and "rules" in content and isinstance(content["rules"], list):
                self.rules.extend(content["rules"])

    def _load_source_manifest(self) -> List[str]:
        """
        Load the list of all known source files from the knowledge graph.
        """
        manifest_file = self.intent_path / "knowledge" / "knowledge_graph.json"
        if not manifest_file.exists():
            return []
        try:
            manifest_data = json.loads(manifest_file.read_text(encoding="utf-8"))
            symbols = manifest_data.get("symbols", {})
            # Use a set to get unique file paths, then convert to a sorted list.
            unique_files = {
                entry.get("file") for entry in symbols.values() if entry.get("file")
            }
            return sorted(list(unique_files))
        except (json.JSONDecodeError, TypeError):
            return []

    # --- THIS IS THE FIX ---
    # The method now correctly resolves paths relative to the repository root.
    def check_transaction(self, proposed_paths: List[str]) -> Tuple[bool, List[str]]:
        """
        Check if a proposed set of file changes complies with all active rules.
        This is the primary enforcement point for constitutional integrity.
        """
        violations = []

        # Rule: Prevent direct writes to the .intent directory, except for proposals.
        for path_str in proposed_paths:
            # Resolve the path relative to the repository root, not the current working directory.
            # This makes the check robust regardless of where the script is executed from.
            path = (self.repo_path / path_str).resolve()

            # Check if the path is within the .intent directory
            if self.intent_path.resolve() in path.parents:
                # If it is, check if it's also within the allowed proposals directory
                if (
                    self.proposals_path.resolve() not in path.parents
                    and path.parent != self.proposals_path.resolve()
                ):
                    violations.append(
                        f"Rule Violation (immutable_intent): Direct write to '{path_str}' is forbidden. "
                        "All changes to the constitution must go through '.intent/proposals/'."
                    )

        # Placeholder for future, more sophisticated rule checks
        # for rule in self.rules:
        #    ...

        return not violations, violations

--- END OF FILE ./src/core/intent_guard.py ---

--- START OF FILE ./src/core/intent_model.py ---
# src/core/intent_model.py
"""IntentModel: domain structure loader and helpers.

This module reads `.intent/knowledge/source_structure.yaml` and exposes helpers to:
- Inspect the normalized domain structure.
- Resolve a file path to its domain.
- Read domain-to-domain allowed import bridges.

The loader is robust to either top-level key: `structure:` (current) or `domains:` (alternate).
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml

from shared.logger import getLogger
from shared.path_utils import get_repo_root

log = getLogger(__name__)


class IntentModel:
    """Load and normalize CORE's source/domain structure."""

    def __init__(self, project_root: Optional[Path] = None) -> None:
        """Initialize the intent model.

        Args:
            project_root: Optional project root to anchor lookups. If not provided,
                the repository root is discovered via ``get_repo_root()``.
        """
        self.repo_root: Path = (project_root or get_repo_root()).resolve()
        self.structure: Dict[str, Dict[str, Any]] = self._load_structure()

    # -----------------------------
    # Public API
    # -----------------------------
    def resolve_domain_for_path(self, file_path: Path) -> Optional[str]:
        """Resolve a source file path to its owning domain.

        Chooses the *deepest* matching domain path if overlaps ever occur.

        Args:
            file_path: Absolute or relative path to a source file.

        Returns:
            The domain name (e.g., ``"core"``) or ``None`` when no domain matches.
        """
        path = Path(file_path).resolve()

        best_match: Optional[str] = None
        best_len = -1

        for domain, info in self.structure.items():
            domain_rel = info.get("path")
            if not domain_rel:
                continue

            domain_abs = (self.repo_root / domain_rel).resolve()
            try:
                path.relative_to(domain_abs)  # within domain folder?
            except ValueError:
                continue
            else:
                plen = len(domain_abs.as_posix())
                if plen > best_len:
                    best_len = plen
                    best_match = domain

        return best_match

    def get_domain_permissions(self, domain: str) -> List[str]:
        """Return domain-level allowed import bridges for a given domain.

        The underlying YAML may include lists of standard/third-party libraries.
        This method filters those out and returns **only other domain names**.

        Args:
            domain: Domain to inspect (e.g., ``"core"``).

        Returns:
            A list of domain names that ``domain`` is allowed to import.
        """
        info = self.structure.get(domain, {})
        allowed = info.get("allowed_imports") or []

        flat = list(self._flatten(allowed))
        domains: List[str] = []
        known = set(self.structure.keys())

        for item in flat:
            if isinstance(item, str) and item in known and item not in domains:
                domains.append(item)

        return domains

    # -----------------------------
    # Internal helpers
    # -----------------------------
    def _flatten(self, xs):
        """Yield a flattened stream of items from (possibly nested) lists/tuples.

        Args:
            xs: Iterable that may contain nested lists/tuples.

        Yields:
            Individual items with all list/tuple nesting removed.
        """
        for x in xs:
            if isinstance(x, (list, tuple)):
                yield from self._flatten(x)
            else:
                yield x

    def _load_structure(self) -> Dict[str, Dict[str, Any]]:
        """Load and normalize the source structure from YAML.

        Supports:
            - ``{"structure": [ ...entries... ]}``
            - ``{"domains":   [ ...entries... ]}``
            - A bare list ``[ ...entries... ]``
            - A map form ``{"domains": {"core": {...}, ...}}``

        Returns:
            A normalized mapping: ``{domain: {"path": str, "allowed_imports": list}}``.
        """
        cfg_path = self.repo_root / ".intent" / "knowledge" / "source_structure.yaml"
        if not cfg_path.exists():
            log.warning(
                "Source structure file not found at %s; proceeding with empty structure.",
                cfg_path,
            )
            return {}

        try:
            data = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
        except Exception as exc:  # pragma: no cover (defensive)
            log.error("Failed to load %s: %s", cfg_path, exc)
            return {}

        # Accept both shapes:
        # - {"structure": [ ...entries... ]}
        # - {"domains":   [ ...entries... ]}
        # - or even just a YAML list [ ...entries... ]
        entries: List[Dict[str, Any]] = []
        if isinstance(data, dict):
            if "structure" in data and isinstance(data["structure"], list):
                entries = data["structure"]
            elif "domains" in data and isinstance(data["domains"], list):
                entries = data["domains"]
            elif isinstance(data.get("domains"), dict):
                # tolerate map form: {"domains": {"core": {...}, ...}}
                for dname, dinfo in (data.get("domains") or {}).items():
                    if isinstance(dinfo, dict):
                        dinfo = {"domain": dname, **dinfo}
                        entries.append(dinfo)
        elif isinstance(data, list):
            entries = data  # bare list

        normalized: Dict[str, Dict[str, Any]] = {}
        for item in entries:
            if not isinstance(item, dict):
                continue
            name = item.get("domain")
            path = item.get("path")
            if not name or not path:
                continue

            allowed = item.get("allowed_imports") or []
            # Normalize allowed_imports to a list[str]
            if isinstance(allowed, str):
                allowed = [allowed]
            elif not isinstance(allowed, list):
                allowed = []

            normalized[name] = {
                "path": path,
                "allowed_imports": allowed,
            }

        if not normalized:
            log.warning(
                "No valid domain entries parsed from %s; got %r", cfg_path, data
            )

        return normalized

--- END OF FILE ./src/core/intent_model.py ---

--- START OF FILE ./src/core/main.py ---
# src/core/main.py
"""
main.py — CORE's API Gateway and Execution Engine

Implements the FastAPI server that handles:
- Goal submission
- Write confirmation
- Test execution
- System status

Integrates all core capabilities into a unified interface.
"""
import os
import time
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Annotated

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi import status as http_status
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, StringConstraints

from agents.execution_agent import ExecutionAgent
from agents.plan_executor import PlanExecutor
from agents.planner_agent import PlannerAgent
from core.capabilities import introspection
from core.clients import GeneratorClient, OrchestratorClient
from core.errors import register_exception_handlers
from core.file_handler import FileHandler
from core.git_service import GitService
from core.intent_alignment import check_goal_alignment
from core.intent_guard import IntentGuard
from core.prompt_pipeline import PromptPipeline
from shared.config import settings
from shared.logger import configure_logging, getLogger

log = getLogger(__name__)
load_dotenv()

# --- Pydantic v2 string constraints (trim + require non-empty) ---
GoalText = Annotated[str, StringConstraints(min_length=1, strip_whitespace=True)]

# simple process-start timestamp for /healthz diagnostics
PROCESS_START_TS = time.time()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan handler — runs startup and shutdown logic."""

    # Configure runtime logging once per app start.
    level = os.getenv("CORE_LOG_LEVEL", "INFO")
    json_mode = os.getenv("CORE_LOG_JSON", "false").lower() == "true"
    configure_logging(level=level, json_mode=json_mode)  # logs to sys.__stderr__

    log.info("🚀 Starting CORE system...")

    log.info("🧠 Performing startup introspection...")
    if not introspection():
        log.warning(
            "⚠️ Introspection cycle completed with errors. System may be unstable."
        )
    else:
        log.info("✅ Introspection complete. System state is constitutionally valid.")

    log.info("🛠️  Initializing shared services...")
    repo_path = Path(".")
    app.state.file_handler = FileHandler(str(repo_path))
    app.state.git_service = GitService(str(repo_path))
    app.state.intent_guard = IntentGuard(repo_path)
    app.state.prompt_pipeline = PromptPipeline(repo_path)

    if settings.LLM_ENABLED:
        app.state.orchestrator_client = OrchestratorClient()
        app.state.generator_client = GeneratorClient()
    else:
        app.state.orchestrator_client = None
        app.state.generator_client = None

    log.info("✅ CORE system is online and ready.")
    yield
    log.info("🛑 CORE system shutting down.")


app = FastAPI(lifespan=lifespan)
register_exception_handlers(app)


class GoalRequest(BaseModel):
    """Defines the request body for the /execute_goal endpoint."""

    goal: GoalText


class AlignmentRequest(BaseModel):
    """Request schema for /guard/align."""

    goal: GoalText
    min_coverage: float | None = Field(default=None, ge=0.0, le=1.0)


@app.get("/healthz")
async def healthz():
    """Simple liveness/readiness probe."""
    uptime_s = int(time.time() - PROCESS_START_TS)
    return {"status": "ok", "uptime_seconds": uptime_s}


@app.post("/guard/align")
async def guard_align(payload: AlignmentRequest):
    """Evaluate a goal against the NorthStar and optional blocklist."""
    ok, details = check_goal_alignment(payload.goal, Path("."))
    if payload.min_coverage is not None:
        cov = details.get("coverage")
        if cov is None or cov < payload.min_coverage:
            ok = False
            if "low_mission_overlap" not in details["violations"]:
                details["violations"].append("low_mission_overlap")
    status = "ok" if ok else "rejected"
    return JSONResponse(
        {"status": status, "details": details}, status_code=http_status.HTTP_200_OK
    )


@app.post("/execute_goal")
async def execute_goal(request_data: GoalRequest, request: Request):
    """Execute a high-level goal by planning and generating code."""
    goal = request_data.goal
    log.info("🎯 Received new goal: %r", goal[:200])

    try:
        # 1. Instantiate the PlannerAgent to create the plan
        planner = PlannerAgent(
            orchestrator_client=request.app.state.orchestrator_client,
            prompt_pipeline=request.app.state.prompt_pipeline,
        )
        plan = planner.create_execution_plan(goal)

        # 2. Instantiate the ExecutionAgent to carry out the plan
        plan_executor = PlanExecutor(
            file_handler=request.app.state.file_handler,
            git_service=request.app.state.git_service,
            config=planner.config,  # Use the same config
        )
        execution_agent = ExecutionAgent(
            generator_client=request.app.state.generator_client,
            prompt_pipeline=request.app.state.prompt_pipeline,
            plan_executor=plan_executor,
        )

        # 3. Execute and get the result
        success, message = await execution_agent.execute_plan(goal, plan)

        if success:
            log.info("✅ Goal executed successfully: %s", message)
            return JSONResponse(
                content={"status": "success", "message": message},
                status_code=http_status.HTTP_200_OK,
            )
        else:
            log.error("❌ Goal execution failed: %s", message)
            raise HTTPException(status_code=500, detail=message)

    except Exception as e:
        log.exception("💥 Unexpected error during goal execution")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/")
async def root():
    """Root endpoint — returns system status."""
    return {"message": "CORE system is online and self-governing."}

--- END OF FILE ./src/core/main.py ---

--- START OF FILE ./src/core/manifest.yaml ---
capabilities:
- prompt_interpretation
- syntax_validation
- test_execution
- introspection
- self_correction
- change_safety_enforcement
- intent_guarding
- semantic_validation
- code_quality_analysis
description: Core logic for orchestration, routing, and CLI
domain: core

--- END OF FILE ./src/core/manifest.yaml ---

--- START OF FILE ./src/core/prompt_pipeline.py ---
# src/core/prompt_pipeline.py
"""
PromptPipeline — CORE's Unified Directive Processor

A single pipeline that processes all [[directive:...]] blocks in a user prompt.
Responsible for:
- Injecting context (e.g., file contents)
- Expanding includes
- Adding analysis from introspection tools
- Enriching with manifest data

This is the central "pre-processor" for all LLM interactions.
"""

import re
from pathlib import Path

import yaml

# --- FIX: Define a constant for a reasonable file size limit (1MB) ---
MAX_FILE_SIZE_BYTES = 1 * 1024 * 1024


class PromptPipeline:
    """
    Processes and enriches user prompts by resolving directives like [[include:...]] and [[analysis:...]].
    Ensures the LLM receives full context before generating code.
    """

    def __init__(self, repo_path: Path):
        """
        Initialize PromptPipeline with repository root.

        Args:
            repo_path (Path): Root path of the repository.
        """
        self.repo_path = Path(repo_path).resolve()

        # Regex patterns for directive matching
        self.context_pattern = re.compile(r"\[\[context:(.+?)\]\]")
        self.include_pattern = re.compile(r"\[\[include:(.+?)\]\]")
        self.analysis_pattern = re.compile(r"\[\[analysis:(.+?)\]\]")
        self.manifest_pattern = re.compile(r"\[\[manifest:(.+?)\]\]")

    def _replace_context_match(self, match: re.Match) -> str:
        """Dynamically replaces a [[context:...]] regex match with file content or an error message if the file is missing, unreadable, or exceeds size limits."""
        """Dynamically replaces a [[context:...]] regex match with file content or an error message."""
        file_path = match.group(1).strip()
        abs_path = self.repo_path / file_path
        if abs_path.exists() and abs_path.is_file():
            # --- FIX: Add file size check to prevent memory bloat ---
            if abs_path.stat().st_size > MAX_FILE_SIZE_BYTES:
                return f"\n❌ Could not include {file_path}: File size exceeds 1MB limit.\n"
            try:
                return f"\n--- CONTEXT: {file_path} ---\n{abs_path.read_text(encoding='utf-8')}\n--- END CONTEXT ---\n"
            except Exception as e:
                return f"\n❌ Could not read {file_path}: {str(e)}\n"
        return f"\n❌ File not found: {file_path}\n"

    def _inject_context(self, prompt: str) -> str:
        """Replaces [[context:file.py]] directives with actual file content."""
        return self.context_pattern.sub(self._replace_context_match, prompt)

    """Dynamically replaces an [[include:...]] regex match with the corresponding file's content or an error message if the file is missing, unreadable, or exceeds size limits."""

    def _replace_include_match(self, match: re.Match) -> str:
        """Dynamically replaces an [[include:...]] regex match with file content or an error message."""
        file_path = match.group(1).strip()
        abs_path = self.repo_path / file_path
        if abs_path.exists() and abs_path.is_file():
            # --- FIX: Add file size check to prevent memory bloat ---
            if abs_path.stat().st_size > MAX_FILE_SIZE_BYTES:
                return f"\n❌ Could not include {file_path}: File size exceeds 1MB limit.\n"
            try:
                return f"\n--- INCLUDED: {file_path} ---\n{abs_path.read_text(encoding='utf-8')}\n--- END INCLUDE ---\n"
            except Exception as e:
                return f"\n❌ Could not read {file_path}: {str(e)}\n"
        return f"\n❌ File not found: {file_path}\n"

    def _inject_includes(self, prompt: str) -> str:
        """Replaces [[include:file.py]] directives with file content."""
        return self.include_pattern.sub(self._replace_include_match, prompt)

    def _replace_analysis_match(self, match: re.Match) -> str:
        """Dynamically replaces an [[analysis:...]] regex match with a placeholder analysis message for the given file path."""
        """Dynamically replaces an [[analysis:...]] regex match with a placeholder analysis message."""
        file_path = match.group(1).strip()
        # This functionality is a placeholder.
        return f"\n--- ANALYSIS FOR {file_path} (DEFERRED) ---\n"

    def _inject_analysis(self, prompt: str) -> str:
        """Replaces [[analysis:file.py]] directives with code analysis."""
        return self.analysis_pattern.sub(self._replace_analysis_match, prompt)

    def _replace_manifest_match(self, match: re.Match) -> str:
        """Dynamically replaces a [[manifest:...]] regex match with manifest data or an error."""
        manifest_path = self.repo_path / ".intent" / "project_manifest.yaml"
        if not manifest_path.exists():
            return f"\n❌ Manifest file not found at {manifest_path}\n"

        try:
            manifest = yaml.safe_load(manifest_path.read_text(encoding="utf-8"))
        except Exception:
            return f"\n❌ Could not parse manifest file at {manifest_path}\n"

        field = match.group(1).strip()
        value = manifest
        # Improved logic for nested key access
        for key in field.split("."):
            value = value.get(key) if isinstance(value, dict) else None
            if value is None:
                break

        if value is None:
            return f"\n❌ Manifest field not found: {field}\n"

        # Pretty print for better context
        value_str = (
            yaml.dump(value, indent=2)
            if isinstance(value, (dict, list))
            else str(value)
        )
        return f"\n--- MANIFEST: {field} ---\n{value_str}\n--- END MANIFEST ---\n"

    def _inject_manifest(self, prompt: str) -> str:
        """Replaces [[manifest:field]] directives with data from project_manifest.yaml."""
        return self.manifest_pattern.sub(self._replace_manifest_match, prompt)

    # CAPABILITY: prompt_interpretation
    def process(self, prompt: str) -> str:
        """
        Processes the full prompt by sequentially resolving all directives.
        This is the main entry point for prompt enrichment.
        """
        prompt = self._inject_context(prompt)
        prompt = self._inject_includes(prompt)
        prompt = self._inject_analysis(prompt)
        prompt = self._inject_manifest(prompt)
        return prompt

--- END OF FILE ./src/core/prompt_pipeline.py ---

--- START OF FILE ./src/core/ruff_linter.py ---
# src/core/ruff_linter.py
"""
Runs Ruff lint checks on generated Python code before it's staged.
Returns a success flag and an optional linting message.
"""
import json
import os
import subprocess
import tempfile
from typing import Any, Dict, List, Tuple

from shared.logger import getLogger

log = getLogger(__name__)
Violation = Dict[str, Any]


# --- MODIFICATION: Complete refactor to use Ruff's JSON output. ---
# --- The function now returns the fixed code and a list of structured violations. ---
def fix_and_lint_code_with_ruff(
    code: str, display_filename: str = "<code>"
) -> Tuple[str, List[Violation]]:
    """
    Fix and lint the provided Python code using Ruff's JSON output format.

    Args:
        code (str): Source code to fix and lint.
        display_filename (str): Optional display name for readable error messages.

    Returns:
        A tuple containing:
        - The potentially fixed code as a string.
        - A list of structured violation dictionaries for any remaining issues.
    """
    violations = []
    with tempfile.NamedTemporaryFile(
        suffix=".py", mode="w+", delete=False, encoding="utf-8"
    ) as tmp_file:
        tmp_file.write(code)
        tmp_file_path = tmp_file.name

    try:
        # Step 1: Run Ruff with --fix to apply safe fixes. This modifies the temp file.
        subprocess.run(
            ["ruff", "check", tmp_file_path, "--fix", "--exit-zero", "--quiet"],
            capture_output=True,
            text=True,
            check=False,
        )

        # Step 2: Read the potentially modified code back from the file.
        with open(tmp_file_path, "r", encoding="utf-8") as f:
            fixed_code = f.read()

        # Step 3: Run Ruff again without fix, but with JSON output to get remaining violations.
        result = subprocess.run(
            ["ruff", "check", tmp_file_path, "--format", "json", "--exit-zero"],
            capture_output=True,
            text=True,
            check=False,
        )

        # Parse the JSON output for any remaining violations.
        if result.stdout:
            ruff_violations = json.loads(result.stdout)
            for v in ruff_violations:
                violations.append(
                    {
                        "rule": v.get("code", "RUFF-UNKNOWN"),
                        "message": v.get("message", "Unknown Ruff error"),
                        "line": v.get("location", {}).get("row", 0),
                        "severity": "warning",  # Assume all ruff issues are warnings for now
                    }
                )

        return fixed_code, violations

    except FileNotFoundError:
        log.error("Ruff is not installed or not in your PATH. Please install it.")
        # Return a critical violation if the tool itself is missing.
        tool_missing_violation = {
            "rule": "tooling.missing",
            "message": "Ruff is not installed or not in your PATH.",
            "line": 0,
            "severity": "error",
        }
        return code, [tool_missing_violation]
    except json.JSONDecodeError:
        log.error("Failed to parse Ruff's JSON output.")
        return code, []  # Return empty if we can't parse, to avoid crashing.
    except Exception as e:
        log.error(f"An unexpected error occurred during Ruff execution: {e}")
        return code, []
    finally:
        if os.path.exists(tmp_file_path):
            os.remove(tmp_file_path)

--- END OF FILE ./src/core/ruff_linter.py ---

--- START OF FILE ./src/core/self_correction_engine.py ---
# src/core/self_correction_engine.py
"""
Self-Correction Engine
This module takes failure context (from validation or test failure)
and attempts to repair the issue using a structured LLM prompt,
then stages the corrected version via the file handler.
"""
import json
from pathlib import Path

from core.clients import GeneratorClient
from core.file_handler import FileHandler
from core.prompt_pipeline import PromptPipeline
from core.validation_pipeline import validate_code
from shared.utils.parsing import parse_write_blocks

REPO_PATH = Path(".").resolve()
pipeline = PromptPipeline(repo_path=REPO_PATH)
file_handler = FileHandler(repo_path=REPO_PATH)


# CAPABILITY: self_correction
def attempt_correction(failure_context: dict) -> dict:
    """Attempts to fix a failed validation or test result by generating corrected code via an LLM prompt based on the provided failure context."""
    """
    Attempts to fix a failed validation or test result using an enriched LLM prompt.
    """
    generator = GeneratorClient()
    file_path = failure_context.get("file_path")
    code = failure_context.get("code")
    # --- MODIFICATION: The key is now "violations", not "error_type" or "details" ---
    violations = failure_context.get("violations", [])
    failure_context.get("original_prompt", "")

    if not file_path or not code or not violations:
        return {
            "status": "error",
            "message": "Missing required failure context fields.",
        }

    # --- MODIFICATION: The prompt is updated to send structured violation data to the LLM ---
    correction_prompt = (
        f"You are CORE's self-correction agent.\n\nA recent code generation attempt failed validation.\n"
        f"Please analyze the violations and fix the code below.\n\nFile: {file_path}\n\n"
        f"[[violations]]\n{json.dumps(violations, indent=2)}\n[[/violations]]\n\n"
        f"[[code]]\n{code.strip()}\n[[/code]]\n\n"
        f"Respond with the full, corrected code in a single write block:\n[[write:{file_path}]]\n<corrected code here>\n[[/write]]"
    )

    final_prompt = pipeline.process(correction_prompt)
    llm_output = generator.make_request(final_prompt, user_id="auto_repair")

    write_blocks = parse_write_blocks(llm_output)

    if not write_blocks:
        return {
            "status": "error",
            "message": "LLM did not produce a valid correction in a write block.",
        }

    # Assuming one write block for self-correction
    path, fixed_code = list(write_blocks.items())[0]

    validation_result = validate_code(path, fixed_code)
    # --- MODIFICATION: Check for 'error' severity in the new violations list ---
    if validation_result["status"] == "dirty":
        return {
            "status": "correction_failed_validation",
            "message": "The corrected code still fails validation.",
            "violations": validation_result["violations"],
        }

    pending_id = file_handler.add_pending_write(
        prompt=final_prompt, suggested_path=path, code=validation_result["code"]
    )
    return {
        "status": "retry_staged",
        "pending_id": pending_id,
        "file_path": path,
        "message": "Corrected code staged for approval.",
    }

--- END OF FILE ./src/core/self_correction_engine.py ---

--- START OF FILE ./src/core/syntax_checker.py ---
# src/core/syntax_checker.py
"""
A simple syntax checker utility.
Validates the syntax of Python code before it's staged for write/commit.
"""
import ast
from typing import Any, Dict, List

Violation = Dict[str, Any]


# --- MODIFICATION: The function now returns a list of structured violation dictionaries. ---
# CAPABILITY: syntax_validation
def check_syntax(file_path: str, code: str) -> List[Violation]:
    """Checks the given Python code for syntax errors and returns a list of violations, if any."""
    """
    Checks whether the given code has valid Python syntax.

    Args:
        file_path (str): File name (used to detect .py files).
        code (str): Source code string.

    Returns:
        A list of violation dictionaries. An empty list means the syntax is valid.
    """
    if not file_path.endswith(".py"):
        return []

    try:
        ast.parse(code)
        return []
    except SyntaxError as e:
        error_line = e.text.strip() if e.text else "<source unavailable>"
        return [
            {
                "rule": "E999",  # Ruff's code for syntax errors
                "message": f"Invalid Python syntax: {e.msg} near '{error_line}'",
                "line": e.lineno,
                "severity": "error",
            }
        ]

--- END OF FILE ./src/core/syntax_checker.py ---

--- START OF FILE ./src/core/test_runner.py ---
# src/core/test_runner.py
"""
Core test execution capability for running pytest and interpreting results.
"""
import logging
import subprocess

logger = logging.getLogger(__name__)


def run_tests():
    """
    Run pytest and return a structured result indicating success or failure.
    Returns a dict with 'exit_code', 'summary', and 'stderr' keys.
    """
    logger.info("🧪 Running tests with pytest...")
    try:
        result = subprocess.run(
            ["pytest"],
            capture_output=True,
            text=True,
            check=False,
        )
        exit_code = str(result.returncode)
        stderr = result.stderr

        if result.returncode == 0:
            summary = "✅ Tests passed"
        else:
            summary = "❌ Tests failed"

        return {
            "exit_code": exit_code,
            "summary": summary,
            "stderr": stderr,
        }
    except FileNotFoundError:
        logger.error("Pytest not found: pytest is not installed")
        return {
            "exit_code": "1",
            "summary": "❌ pytest not found",
            "stderr": "pytest is not installed",
        }

--- END OF FILE ./src/core/test_runner.py ---

--- START OF FILE ./src/core/validation_pipeline.py ---
# src/core/validation_pipeline.py
"""
A context-aware validation pipeline that applies different validation steps
based on the type of file being processed. This is the single source of truth
for all code and configuration validation.
"""
import ast
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import black
import yaml

from core.black_formatter import format_code_with_black
from core.ruff_linter import fix_and_lint_code_with_ruff
from core.syntax_checker import check_syntax
from shared.config_loader import load_config
from shared.logger import getLogger
from shared.path_utils import get_repo_root

log = getLogger(__name__)
Violation = Dict[str, Any]

# --- Policy-Aware Validation ---

_safety_policies_cache: Optional[List[Dict]] = None


def _load_safety_policies() -> List[Dict]:
    """Loads and caches the safety policies from the .intent directory."""
    global _safety_policies_cache
    if _safety_policies_cache is None:
        repo_root = get_repo_root()
        policies_path = repo_root / ".intent" / "policies" / "safety_policies.yaml"
        policy_data = load_config(policies_path, "yaml")
        _safety_policies_cache = policy_data.get("rules", [])
    return _safety_policies_cache


def _get_full_attribute_name(node: ast.Attribute) -> str:
    """Recursively builds the full name of an attribute call (e.g., 'os.path.join')."""
    parts = []
    current = node
    while isinstance(current, ast.Attribute):
        parts.insert(0, current.attr)
        current = current.value
    if isinstance(current, ast.Name):
        parts.insert(0, current.id)
    return ".".join(parts)


def _find_dangerous_patterns(tree: ast.AST, file_path: str) -> List[Violation]:
    """Scans the AST for calls and imports forbidden by safety policies."""
    violations: List[Violation] = []
    rules = _load_safety_policies()

    forbidden_calls = set()
    forbidden_imports = set()

    for rule in rules:
        # --- THIS IS THE FIX ---
        # The original code did not check if the items in the 'exclude' list were strings.
        # We now ensure we only try to match string patterns, gracefully ignoring other types.
        exclude_patterns = [
            p for p in rule.get("scope", {}).get("exclude", []) if isinstance(p, str)
        ]
        is_excluded = any(Path(file_path).match(p) for p in exclude_patterns)
        # --- END OF FIX ---

        if is_excluded:
            continue

        if rule.get("id") == "no_dangerous_execution":
            patterns = {
                p.replace("(", "")
                for p in rule.get("detection", {}).get("patterns", [])
            }
            forbidden_calls.update(patterns)
        elif rule.get("id") == "no_unsafe_imports":
            patterns = {
                imp.split(" ")[-1]
                for imp in rule.get("detection", {}).get("forbidden", [])
            }
            forbidden_imports.update(patterns)

    for node in ast.walk(tree):
        # Check for dangerous function calls
        if isinstance(node, ast.Call):
            full_call_name = ""
            if isinstance(node.func, ast.Name):
                full_call_name = node.func.id
            elif isinstance(node.func, ast.Attribute):
                full_call_name = _get_full_attribute_name(node.func)

            if full_call_name in forbidden_calls:
                violations.append(
                    {
                        "rule": "safety.dangerous_call",
                        "message": f"Use of forbidden call: '{full_call_name}'",
                        "line": node.lineno,
                        "severity": "error",
                    }
                )
        # Check for forbidden imports
        elif isinstance(node, ast.Import):
            for alias in node.names:
                if alias.name.split(".")[0] in forbidden_imports:
                    violations.append(
                        {
                            "rule": "safety.forbidden_import",
                            "message": f"Import of forbidden module: '{alias.name}'",
                            "line": node.lineno,
                            "severity": "error",
                        }
                    )
        elif isinstance(node, ast.ImportFrom):
            if node.module and node.module.split(".")[0] in forbidden_imports:
                violations.append(
                    {
                        "rule": "safety.forbidden_import",
                        "message": f"Import from forbidden module: '{node.module}'",
                        "line": node.lineno,
                        "severity": "error",
                    }
                )
    return violations


def _check_for_todo_comments(code: str) -> List[Violation]:
    """Scans source code for TODO/FIXME comments and returns them as violations."""
    violations: List[Violation] = []
    for i, line in enumerate(code.splitlines(), 1):
        if "#" in line:
            comment = line.split("#", 1)[1]
            if "TODO" in comment or "FIXME" in comment:
                violations.append(
                    {
                        "rule": "clarity.no_todo_comments",
                        "message": f"Unresolved '{comment.strip()}' on line {i}",
                        "line": i,
                        "severity": "warning",
                    }
                )
    return violations


# CAPABILITY: semantic_validation
def _check_semantics(code: str, file_path: str) -> List[Violation]:
    """Runs all policy-aware semantic checks on a string of Python code."""
    try:
        tree = ast.parse(code)
    except SyntaxError:
        # Syntax errors are caught by check_syntax, so we can ignore them here.
        return []
    return _find_dangerous_patterns(tree, file_path)


def _validate_python_code(path_hint: str, code: str) -> Tuple[str, List[Violation]]:
    """
    Internal pipeline for Python code validation.
    Returns the final code and a list of all found violations.
    """
    all_violations: List[Violation] = []

    # 1. Format with Black. This can fail on major syntax errors.
    try:
        formatted_code = format_code_with_black(code)
    except (black.InvalidInput, Exception) as e:
        # If Black fails, the code is fundamentally broken.
        all_violations.append(
            {
                "rule": "tooling.black_failure",
                "message": str(e),
                "line": 0,
                "severity": "error",
            }
        )
        # Return the original code since formatting failed.
        return code, all_violations

    # 2. Lint with Ruff (which also fixes).
    fixed_code, ruff_violations = fix_and_lint_code_with_ruff(formatted_code, path_hint)
    all_violations.extend(ruff_violations)

    # 3. Check syntax on the post-Ruff code.
    syntax_violations = check_syntax(path_hint, fixed_code)
    all_violations.extend(syntax_violations)
    # If there's a syntax error, no further checks are reliable.
    if any(v["severity"] == "error" for v in syntax_violations):
        return fixed_code, all_violations

    # 4. Perform semantic and clarity checks on the valid code.
    all_violations.extend(_check_semantics(fixed_code, path_hint))
    all_violations.extend(_check_for_todo_comments(fixed_code))

    return fixed_code, all_violations


def _validate_yaml(code: str) -> Tuple[str, List[Violation]]:
    """Internal pipeline for YAML validation."""
    violations = []
    try:
        yaml.safe_load(code)
    except yaml.YAMLError as e:
        violations.append(
            {
                "rule": "syntax.yaml",
                "message": f"Invalid YAML format: {e}",
                "line": e.problem_mark.line + 1 if e.problem_mark else 0,
                "severity": "error",
            }
        )
    return code, violations


def _get_file_classification(file_path: str) -> str:
    """Determines the file type based on its extension."""
    suffix = Path(file_path).suffix.lower()
    if suffix == ".py":
        return "python"
    if suffix in [".yaml", ".yml"]:
        return "yaml"
    if suffix in [".md", ".txt", ".json"]:
        return "text"
    return "unknown"


# CAPABILITY: code_quality_analysis
def validate_code(file_path: str, code: str, quiet: bool = False) -> Dict[str, Any]:
    """Validate a file's code by routing it to the appropriate validation pipeline based on its file type, returning a standardized dictionary with status, violations, and processed code."""
    """
    The main entry point for validation. It determines the file type
    and routes it to the appropriate validation pipeline, returning a
    standardized dictionary.
    """
    classification = _get_file_classification(file_path)
    if not quiet:
        log.debug(f"Validation: Classifying '{file_path}' as '{classification}'.")

    final_code = code
    violations: List[Violation] = []

    if classification == "python":
        final_code, violations = _validate_python_code(file_path, code)
    elif classification == "yaml":
        final_code, violations = _validate_yaml(code)

    # Determine final status. "dirty" if there are any 'error' severity violations.
    is_dirty = any(v.get("severity") == "error" for v in violations)
    status = "dirty" if is_dirty else "clean"

    return {"status": status, "violations": violations, "code": final_code}

--- END OF FILE ./src/core/validation_pipeline.py ---

--- START OF FILE ./src/shared/config_loader.py ---
# src/shared/config_loader.py

import json
from pathlib import Path
from typing import Any, Dict

import yaml

from shared.logger import getLogger

log = getLogger(__name__)


def load_config(file_path: Path, file_type: str = "auto") -> Dict[str, Any]:
    """Loads a JSON or YAML file into a dictionary, handling missing files, invalid formats, and parsing errors by returning an empty dict."""
    """
    Loads a JSON or YAML file into a dictionary with consistent error handling.

    Args:
        file_path (Path): Path to the file to load.
        file_type (str): 'json', 'yaml', or 'auto' to infer from extension.

    Returns:
        Dict[str, Any]: Parsed file content or empty dict if file is missing/invalid.
    """
    file_path = Path(file_path)
    if not file_path.exists():
        log.warning(
            f"Configuration file not found at {file_path}, returning empty dict."
        )
        return {}

    # Determine file type if 'auto'
    if file_type == "auto":
        suffix = file_path.suffix.lower()
        file_type = (
            "json"
            if suffix == ".json"
            else "yaml" if suffix in (".yaml", ".yml") else None
        )

    if file_type not in ("json", "yaml"):
        log.error(f"Unsupported file type for {file_path}, cannot load.")
        return {}

    try:
        with file_path.open(encoding="utf-8") as f:
            if file_type == "json":
                data = json.load(f)
                return data if isinstance(data, dict) else {}
            else:  # yaml
                data = yaml.safe_load(f)
                return data if isinstance(data, dict) else {}
    except (json.JSONDecodeError, yaml.YAMLError) as e:
        log.error(f"Error parsing {file_path}: {e}", exc_info=True)
        return {}

--- END OF FILE ./src/shared/config_loader.py ---

--- START OF FILE ./src/shared/config.py ---
# src/shared/config.py
"""
Intent: Centralize configuration with safe defaults and backward-compatible
env mapping. Accept both *_MODEL_NAME and *_MODEL, plus provide the fields
tests expect (LLM_ENABLED, ORCHESTRATOR_API_URL, etc.).
"""

from __future__ import annotations

from pydantic import Field, model_validator
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    # Load from .env if present; ignore unknown keys (forward-compatible).
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
    )

    # --- Paths (your repo’s schema) -----------------------------------------
    MIND: str = Field(default=".intent", description="Path to intent files")
    BODY: str = Field(default="src", description="Path to code body")
    REPO_PATH: str = Field(default=".", description="Repo root")
    CORE_ACTION_LOG_PATH: str = Field(
        default="logs/action_log.jsonl", description="Append-only action log path"
    )

    # --- Logging / Flags -----------------------------------------------------
    CORE_LOG_JSON: bool = Field(default=False, description="JSON logs when true")
    CORE_LOG_LEVEL: str = Field(default="INFO", description="Root log level")
    LLM_ENABLED: bool = Field(
        default=False,
        description="Disable external LLM calls by default in tests/CI",
    )

    # --- Orchestrator (accept both NAME and non-NAME) ------------------------
    # Canonical attribute we use internally:
    ORCHESTRATOR_MODEL: str = Field(
        default="deepseek-chat",
        description="Primary model for orchestration",
        validation_alias="ORCHESTRATOR_MODEL_NAME",  # also read *_MODEL_NAME
    )
    ORCHESTRATOR_API_URL: str = Field(
        default="",
        description="HTTP endpoint for OrchestratorClient (can stay empty in tests)",
    )
    ORCHESTRATOR_API_KEY: str = Field(default="", description="API key if needed")

    # --- Generator (accept both NAME and non-NAME) ---------------------------
    GENERATOR_MODEL: str = Field(
        default="deepseek-coder",
        description="Model for code generation",
        validation_alias="GENERATOR_MODEL_NAME",  # also read *_MODEL_NAME
    )
    GENERATOR_API_URL: str = Field(
        default="",
        description="HTTP endpoint for GeneratorClient (can stay empty in tests)",
    )
    GENERATOR_API_KEY: str = Field(default="", description="API key if needed")

    # Back-compat property names so existing code/tests can use either form ----
    @property
    def ORCHESTRATOR_MODEL_NAME(self) -> str:  # noqa: N802
        return self.ORCHESTRATOR_MODEL

    @property
    def GENERATOR_MODEL_NAME(self) -> str:  # noqa: N802
        return self.GENERATOR_MODEL

    # Helpful snake_case conveniences (optional)
    @property
    def orchestrator_model(self) -> str:
        return self.ORCHESTRATOR_MODEL

    @property
    def generator_model(self) -> str:
        return self.GENERATOR_MODEL

    # Normalize empty URLs if you want a local default for dev; leave as-is for CI.
    @model_validator(mode="after")
    def _fill_reasonable_local_defaults(self) -> "Settings":
        def _maybe_fill(url: str) -> str:
            return url or ""  # keep empty by default; tests monkeypatch HTTP anyway

        object.__setattr__(
            self, "ORCHESTRATOR_API_URL", _maybe_fill(self.ORCHESTRATOR_API_URL)
        )
        object.__setattr__(
            self, "GENERATOR_API_URL", _maybe_fill(self.GENERATOR_API_URL)
        )
        return self


# Singleton instance
settings = Settings()

--- END OF FILE ./src/shared/config.py ---

--- START OF FILE ./src/shared/constants.py ---
"""
Centralized location for system-wide constant values.
"""

# Maximum allowed file size for system operations (1MB)
MAX_FILE_SIZE_BYTES = 1 * 1024 * 1024

--- END OF FILE ./src/shared/constants.py ---

--- START OF FILE ./src/shared/logger.py ---
# src/shared/logger.py
"""
CORE's Unified Logging System.

Use `configure_logging()` once at process start to set level/format/stream.
Then get contextual loggers with `getLogger(name)` everywhere else.

Intent: Provide consistent, constitutionally governed logging across CLI and API,
with optional JSON mode for production/CI, while keeping human-readable logs by default.
"""
from __future__ import annotations

import json
import logging
import os
import sys
from datetime import datetime
from typing import Optional, TextIO


class _JsonFormatter(logging.Formatter):
    """Minimal JSON formatter for structured logs."""

    def format(self, record: logging.LogRecord) -> str:
        """Intent: Render a LogRecord as a compact JSON object suitable for ingestion."""
        payload = {
            "time": datetime.utcfromtimestamp(record.created).isoformat() + "Z",
            "level": record.levelname,
            "name": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "filename": record.filename,
            "lineno": record.lineno,
        }
        return json.dumps(payload, ensure_ascii=False)


def _coerce_level(level: str | int) -> int:
    """Intent: Normalize a user-supplied level (str/int) to a logging.* constant."""
    if isinstance(level, int):
        return level
    return getattr(logging, str(level).upper(), logging.INFO)


def configure_logging(
    *,
    level: str | int = "INFO",
    stream: Optional[TextIO] = None,
    json_mode: Optional[bool] = None,
) -> None:
    """
    Configure root logging.

    Args:
        level: "DEBUG"|"INFO"|"WARNING"|"ERROR" or int.
        stream: Target stream; defaults to real stderr (sys.__stderr__).
        json_mode: True → JSON logs; False → human logs. If None, uses env CORE_LOG_JSON.
    """
    # Decide stream & mode
    stream = stream or sys.__stderr__
    if json_mode is None:
        json_mode = os.getenv("CORE_LOG_JSON", "false").lower() == "true"

    # Build handler
    handler = logging.StreamHandler(stream)
    if json_mode:
        handler.setFormatter(_JsonFormatter())
    else:
        # Keep the classic, readable format seen in tests
        handler.setFormatter(logging.Formatter("%(levelname)s:%(name)s: %(message)s"))

    # Apply to root (force clears previous handlers)
    logging.basicConfig(level=_coerce_level(level), handlers=[handler], force=True)


# CAPABILITY: system_logging
def getLogger(name: str) -> logging.Logger:
    """Return a named logger. Call `configure_logging()` once at startup."""
    logger = logging.getLogger(name)
    # When under pytest, be verbose by default unless overridden by configure_logging
    if "pytest" in sys.modules:
        logger.setLevel(logging.DEBUG)
    return logger


# Optional root-level logger
log = getLogger("core_root")

--- END OF FILE ./src/shared/logger.py ---

--- START OF FILE ./src/shared/manifest.yaml ---
capabilities:
- system_logging
description: Shared models, helpers, and config interfaces
domain: shared

--- END OF FILE ./src/shared/manifest.yaml ---

--- START OF FILE ./src/shared/path_utils.py ---
# src/shared/path_utils.py

from pathlib import Path
from typing import Optional


def get_repo_root(start_path: Optional[Path] = None) -> Path:
    """Find and return the repository root by locating the .git directory, starting from the current directory or provided path."""
    """
    Find and return the repository root by locating the .git directory.
    Starts from current directory or provided path.

    Returns:
        Path: Absolute path to repo root.

    Raises:
        RuntimeError: If no .git directory is found.
    """
    current = Path(start_path or Path.cwd()).resolve()

    # Traverse upward until .git is found
    for parent in [current, *current.parents]:
        if (parent / ".git").exists():
            return parent

    raise RuntimeError("Not a git repository: could not find .git directory")

--- END OF FILE ./src/shared/path_utils.py ---

--- START OF FILE ./src/shared/schemas/manifest_validator.py ---
# src/shared/schemas/manifest_validator.py
"""Shared utilities for validating manifest files against schemas."""
import json
from typing import Any, Dict, List, Tuple

import jsonschema

from shared.path_utils import get_repo_root

# The single source of truth for the location of constitutional schemas.
SCHEMA_DIR = get_repo_root() / ".intent" / "schemas"


def load_schema(schema_name: str) -> Dict[str, Any]:
    """
    Load a JSON schema from the .intent/schemas/ directory.

    Args:
        schema_name (str): The filename of the schema (e.g., 'knowledge_graph_entry.schema.json').

    Returns:
        Dict[str, Any]: The loaded JSON schema.

    Raises:
        FileNotFoundError: If the schema file is not found.
        json.JSONDecodeError: If the schema file is not valid JSON.
    """
    schema_path = SCHEMA_DIR / schema_name

    if not schema_path.exists():
        raise FileNotFoundError(f"Schema file not found: {schema_path}")

    try:
        with open(schema_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        raise json.JSONDecodeError(
            f"Invalid JSON in schema file {schema_path}: {e.msg}", e.doc, e.pos
        )


def validate_manifest_entry(
    entry: Dict[str, Any], schema_name: str = "knowledge_graph_entry.schema.json"
) -> Tuple[bool, List[str]]:
    """
    Validate a single manifest entry against a schema.

    Args:
        entry: The dictionary representing a single function/class entry.
        schema_name: The filename of the schema to validate against.

    Returns:
        A tuple of (is_valid: bool, list_of_error_messages: List[str]).
    """
    try:
        schema = load_schema(schema_name)
    except Exception as e:
        return False, [f"Failed to load schema '{schema_name}': {e}"]

    # Use Draft7Validator for compatibility with our schema definition.
    validator = jsonschema.Draft7Validator(schema)
    errors = []

    for error in validator.iter_errors(entry):
        # Create a user-friendly error message
        path = ".".join(str(p) for p in error.absolute_path) or "<root>"
        errors.append(f"Validation error at '{path}': {error.message}")

    is_valid = not errors
    return is_valid, errors

--- END OF FILE ./src/shared/schemas/manifest_validator.py ---

--- START OF FILE ./src/shared/utils/import_scanner.py ---
# src/shared/utils/import_scanner.py

"""
Import Scanner Utility
======================

Scans a Python file for top-level import statements.
"""

import ast
from pathlib import Path
from typing import List

from shared.logger import getLogger

log = getLogger(__name__)


def scan_imports_for_file(file_path: Path) -> List[str]:
    """
    Parse a Python file and extract all imported module paths.

    Args:
        file_path (Path): Path to the file.

    Returns:
        List[str]: List of imported module paths.
    """
    imports = []
    try:
        source = file_path.read_text(encoding="utf-8")
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)

    except Exception as e:
        log.warning(f"Failed to scan imports for {file_path}: {e}", exc_info=True)

    return imports

--- END OF FILE ./src/shared/utils/import_scanner.py ---

--- START OF FILE ./src/shared/utils/__init__.py ---
[EMPTY FILE]

--- END OF FILE ./src/shared/utils/__init__.py ---

--- START OF FILE ./src/shared/utils/manifest_aggregator.py ---
# src/shared/utils/manifest_aggregator.py
"""
A utility to discover and aggregate domain-specific manifests into a single,
unified view of the system's constitution.
"""
from pathlib import Path
from typing import Any, Dict

import yaml

from shared.logger import getLogger

log = getLogger("manifest_aggregator")


def aggregate_manifests(repo_root: Path) -> Dict[str, Any]:
    """
    Finds all domain-specific manifest.yaml files and merges them.

    This function is the heart of the modular manifest system. It reads the
    source structure to find all domains, then searches for a manifest in each
    domain's directory, aggregating their contents.

    Args:
        repo_root (Path): The absolute path to the repository root.

    Returns:
        A dictionary representing the aggregated manifest, primarily focused
        on compiling a unified list of 'required_capabilities'.
    """
    log.info("🔍 Starting manifest aggregation...")
    source_structure_path = (
        repo_root / ".intent" / "knowledge" / "source_structure.yaml"
    )
    if not source_structure_path.exists():
        log.error("❌ Cannot aggregate manifests: source_structure.yaml not found.")
        return {}

    source_structure = yaml.safe_load(source_structure_path.read_text())

    all_capabilities = []
    domains_found = 0

    for domain_entry in source_structure.get("structure", []):
        domain_path_str = domain_entry.get("path")
        if not domain_path_str:
            continue

        manifest_path = repo_root / domain_path_str / "manifest.yaml"
        if manifest_path.exists():
            domains_found += 1
            log.debug(
                f"   -> Found manifest for domain '{domain_entry.get('domain')}' at {manifest_path}"
            )
            domain_manifest = yaml.safe_load(manifest_path.read_text())
            if domain_manifest and "capabilities" in domain_manifest:
                all_capabilities.extend(domain_manifest["capabilities"])

    log.info(f"   -> Aggregated capabilities from {domains_found} domain manifests.")

    # We also keep some top-level info from the original monolithic manifest for now
    # to ensure a smooth transition.
    monolith_path = repo_root / ".intent" / "project_manifest.yaml"
    monolith_data = {}
    if monolith_path.exists():
        monolith_data = yaml.safe_load(monolith_path.read_text())

    aggregated_manifest = {
        "name": monolith_data.get("name", "CORE"),
        "intent": monolith_data.get("intent", "No intent provided."),
        "active_agents": monolith_data.get("active_agents", []),
        "required_capabilities": sorted(list(set(all_capabilities))),
    }

    return aggregated_manifest

--- END OF FILE ./src/shared/utils/manifest_aggregator.py ---

--- START OF FILE ./src/shared/utils/parsing.py ---
# src/shared/utils/parsing.py
"""
Parsing utility functions for the CORE system.
"""
import re
from typing import Dict


def parse_write_blocks(llm_output: str) -> Dict[str, str]:
    """
    Extracts all [[write:...]] blocks from LLM output.

    This function is robust and handles both [[end]] and [[/write]] as valid terminators
    to accommodate different LLM habits.

    Args:
        llm_output (str): The raw text output from a language model.

    Returns:
        A dictionary mapping file paths to their corresponding code content.
    """

    pattern = r"\[\[write:\s*(.+?)\]\](.*?)(?:\[\[end\]\]|\[\[/write\]\])"
    matches = re.findall(pattern, llm_output, re.DOTALL)
    return {path.strip(): code.strip() for path, code in matches}


# def extract_json_from_response(text: str) -> str:
#    """
#    Extracts a JSON object or array from a raw text response.
#    Handles both markdown ```json code blocks and raw JSON strings.#
#        Args:
#        text (str): The raw text output from a language model.#
#    Returns:
#        A string containing the extracted JSON, or an empty string if not found.
#    """
#    match = re.search(r"```json\n([\s\S]*?)\n```", text, re.DOTALL)
#    if match:
#        return match.group(1).strip()
#    match = re.search(r'\[\s*\{[\s\S]*?\}\s*\]', text)
#    if match:
#        return match.group(0).strip()
#    return ""

--- END OF FILE ./src/shared/utils/parsing.py ---

--- START OF FILE ./src/system/admin/agent.py ---
# src/system/admin/agent.py
"""
Intent: Exposes PlannerAgent capabilities directly to the human operator via the CLI.
"""
from __future__ import annotations

import json
import re
import subprocess
import textwrap
from typing import Any

import typer

from core.clients import OrchestratorClient
from core.file_handler import FileHandler
from core.git_service import GitService
from shared.logger import getLogger
from shared.path_utils import get_repo_root
from system.tools.scaffolder import Scaffolder

log = getLogger("core_admin.agent")
CORE_ROOT = get_repo_root()

agent_app = typer.Typer(help="Directly invoke autonomous agent capabilities.")


def _extract_json_from_response(text: str) -> Any:
    """
    Robustly extract the first valid JSON value from a model response.

    Strategy (least → most permissive):
      1) Direct parse if the whole text is JSON.
      2) Trim common wrappers (code fences), try parse.
      3) Find the first balanced JSON object/array in the text and parse that.

    Raises JSONDecodeError if no valid JSON is found.
    """
    s = text.strip()

    # 1) Direct parse
    if s.startswith("{") or s.startswith("["):
        return json.loads(s)

    # 2) Strip common code-fence wrappers like ```json ... ``` or ``` ... ```
    fence = re.compile(r"^```(?:json|JSON)?\s*(.*?)\s*```$", re.DOTALL)
    m = fence.match(s)
    if m:
        inner = m.group(1).strip()
        if inner.startswith("{") or inner.startswith("["):
            return json.loads(inner)

    # Also handle inline fenced blocks appearing anywhere; prefer ```json blocks first
    for pattern in (r"```(?:json|JSON)\s*(.*?)\s*```", r"```\s*(.*?)\s*```"):
        for mm in re.finditer(pattern, s, flags=re.DOTALL):
            candidate = mm.group(1).strip()
            if candidate.startswith("{") or candidate.startswith("["):
                try:
                    return json.loads(candidate)
                except json.JSONDecodeError:
                    # try the next candidate
                    pass

    # 3) Balanced-brace/bracket scan to locate the first JSON value
    def _scan_balanced(src: str) -> str | None:
        """Scan a string for a balanced bracket-enclosed expression (supports {}, [], and nested strings with escapes) and return it if found, else None."""
        openers = {"{": "}", "[": "]"}
        i = 0
        n = len(src)
        while i < n:
            ch = src[i]
            if ch in openers:
                stack = [openers[ch]]
                j = i + 1
                in_str = False
                escape = False
                while j < n:
                    c = src[j]
                    if in_str:
                        if escape:
                            escape = False
                        elif c == "\\":
                            escape = True
                        elif c == '"':
                            in_str = False
                    else:
                        if c == '"':
                            in_str = True
                        elif c in openers:
                            stack.append(openers[c])
                        elif stack and c == stack[-1]:
                            stack.pop()
                            if not stack:
                                return src[i : j + 1]
                    j += 1
            i += 1
        return None

    segment = _scan_balanced(s)
    if segment is not None:
        return json.loads(segment)

    # Give a clear, actionable error
    raise json.JSONDecodeError("No valid JSON found in response", s, 0)


# CAPABILITY: scaffold_project
def scaffold_new_application(
    project_name: str,
    goal: str,
    orchestrator: OrchestratorClient,
    file_handler: FileHandler,
    initialize_git: bool = False,
) -> tuple[bool, str]:
    """Uses an LLM to plan and generate a new, multi-file application."""
    log.info(f"🌱 Starting to scaffold new application '{project_name}'...")
    prompt_template = textwrap.dedent(
        """
        You are a senior software architect. Your task is to design the file structure and content for a new Python application based on a high-level goal.

        **Goal:** "{goal}"

        **Instructions:**
        1.  Think step-by-step about the necessary files for a minimal, working version.
        2.  Your output MUST be a single, valid JSON object with file paths as keys and content as values.
        3.  Include a `pyproject.toml` and a simple `src/main.py`.
        4.  Keep the code simple, clean, and functional.
        """
    ).strip()

    final_prompt = prompt_template.format(goal=goal)
    response_text: str | None = None  # for better error diagnostics

    try:
        response_text = orchestrator.make_request(
            final_prompt, user_id="scaffolding_agent"
        )
        file_structure = _extract_json_from_response(response_text)

        if not isinstance(file_structure, dict):
            raise ValueError("LLM did not return a valid JSON object of files.")

        log.info(f"   -> LLM planned a structure with {len(file_structure)} files.")

        scaffolder = Scaffolder(project_name=project_name)
        scaffolder.scaffold_base_structure()

        # Write the LLM-generated files
        for rel_path, content in file_structure.items():
            scaffolder.write_file(rel_path, content)

        # Add the templated test and CI files
        log.info("   -> Adding starter test and CI workflow...")
        test_template_path = scaffolder.starter_kit_path / "test_main.py.template"
        ci_template_path = scaffolder.starter_kit_path / "ci.yml.template"

        if test_template_path.exists():
            test_content = test_template_path.read_text(encoding="utf-8").format(
                project_name=project_name
            )
            scaffolder.write_file("tests/test_main.py", test_content)

        if ci_template_path.exists():
            ci_content = ci_template_path.read_text(encoding="utf-8")
            scaffolder.write_file(".github/workflows/ci.yml", ci_content)

        # Optionally initialize a Git repository and make an initial commit.
        if initialize_git:
            log.info("   -> Initializing Git repository...")
            git = GitService(scaffolder.project_root)
            try:
                # Create a repo if not already a git repo
                if not git.is_git_repo():
                    subprocess.run(
                        ["git", "init"], cwd=scaffolder.project_root, check=True
                    )
                git.add(".")
                git.commit(f"feat(scaffold): Initialize '{project_name}'")
                log.info("   -> ✅ Initial commit created.")
            except Exception as e:
                log.warning(f"   -> ⚠️ Git initialization skipped: {e}")

        return (
            True,
            f"✅ Successfully scaffolded '{project_name}' in '{scaffolder.workspace.relative_to(file_handler.repo_path)}'.",
        )

    except Exception as e:
        # Extra diagnostic preview when the LLM returns non-JSON and parsing fails
        if isinstance(e, json.JSONDecodeError) and response_text:
            preview = response_text.strip().replace("\n", " ")
            if len(preview) > 200:
                preview = preview[:200] + "…"
            log.error("LLM response was not valid JSON. Preview: %r", preview)
        else:
            log.error(f"❌ Scaffolding failed: {e}", exc_info=True)

        # Preserve the original operator-facing message to keep tests and UX stable
        return False, f"Scaffolding failed: {str(e)}"


@agent_app.command("scaffold")
def agent_scaffold(
    name: str = typer.Argument(..., help="The directory name for the new application."),
    goal: str = typer.Argument(..., help="A high-level goal for the application."),
    git_init: bool = typer.Option(
        True, "--git/--no-git", help="Initialize a Git repository."
    ),
):
    """Uses an LLM agent to autonomously scaffold a new application."""
    log.info(f"🤖 Invoking Agent to scaffold application '{name}'...")
    log.info(f"   -> Goal: '{goal}'")

    try:
        orchestrator = OrchestratorClient()
        file_handler = FileHandler(str(CORE_ROOT))
        success, message = scaffold_new_application(
            project_name=name,
            goal=goal,
            orchestrator=orchestrator,
            file_handler=file_handler,
            initialize_git=git_init,
        )
    except Exception as e:
        log.error(f"❌ Failed to initialize agent tools: {e}", exc_info=True)
        raise typer.Exit(code=1)

    if success:
        typer.secho(f"\n{message}", fg=typer.colors.GREEN)
    else:
        typer.secho(f"\n{message}", fg=typer.colors.RED)
        raise typer.Exit(code=1)


def register(app: typer.Typer):
    """Register the 'agent' command group with the main CLI app."""
    app.add_typer(agent_app, name="agent")

--- END OF FILE ./src/system/admin/agent.py ---

--- START OF FILE ./src/system/admin/byor.py ---
# src/system/admin/byor.py
"""
Intent: Implements the 'byor-init' command for the CORE Admin CLI.

This command is the entry point for the "Bring Your Own Repo" capability.
It analyzes an external repository and proposes a minimal `.intent/`
scaffold to begin governing it with CORE.
"""

from pathlib import Path

import typer
import yaml

from shared.logger import getLogger
from system.tools.codegraph_builder import KnowledgeGraphBuilder

log = getLogger("core_admin.byor")

# --- THIS IS THE FIX ---
# We now point to the 'starter_kits/default' directory as the single
# source of truth for all project scaffolding templates.
CORE_ROOT = Path(__file__).resolve().parents[2]
TEMPLATES_DIR = CORE_ROOT / "system" / "starter_kits" / "default"
# --- END OF FIX ---


def initialize_repository(
    path: Path = typer.Argument(
        ...,
        help="The path to the external repository to analyze.",
        exists=True,
        file_okay=False,
        dir_okay=True,
        resolve_path=True,
    ),
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--write",
        help="Show the proposed .intent/ scaffold without writing files. Use --write to apply.",
    ),
):
    """
    Analyzes an external repository and scaffolds a minimal `.intent/` constitution.
    """
    log.info(f"🚀 Starting analysis of repository at: {path}")

    # Step 1: Build the Knowledge Graph.
    log.info("   -> Step 1: Building Knowledge Graph of the target repository...")
    try:
        builder = KnowledgeGraphBuilder(root_path=path)
        graph = builder.build()
        total_symbols = len(graph.get("symbols", {}))
        log.info(
            f"   -> ✅ Knowledge Graph built successfully. Found {total_symbols} symbols."
        )
    except Exception as e:
        log.error(f"   -> ❌ Failed to build Knowledge Graph: {e}", exc_info=True)
        raise typer.Exit(code=1)

    # Step 2: Generate the content for the new constitutional files.
    log.info("   -> Step 2: Generating starter constitution from analysis...")

    # File 1: source_structure.yaml
    domains = builder.domain_map
    source_structure_content = {
        "structure": [
            {
                "domain": name,
                "path": path_str,
                "description": f"Domain for '{name}' inferred by CORE.",
                "allowed_imports": [name, "shared"],
            }
            for path_str, name in domains.items()
        ]
    }

    # File 2: project_manifest.yaml
    discovered_capabilities = sorted(
        list(
            set(
                s["capability"]
                for s in graph.get("symbols", {}).values()
                if s.get("capability") != "unassigned"
            )
        )
    )
    project_manifest_content = {
        "name": path.name,
        "version": "0.1.0-core-scaffold",
        "intent": "A high-level description of what this project is intended to do.",
        "required_capabilities": discovered_capabilities,
    }

    # File 3: capability_tags.yaml (dynamically populated)
    # The content is read from the now-consolidated template file.
    (TEMPLATES_DIR / "capability_tags.yaml.template").read_text()
    capability_tags_content = {
        "tags": [
            {
                "name": cap,
                "description": "A clear explanation of what this capability does.",
            }
            for cap in discovered_capabilities
        ]
    }

    # The files we will create and their content.
    files_to_generate = {
        ".intent/knowledge/source_structure.yaml": source_structure_content,
        ".intent/project_manifest.yaml": project_manifest_content,
        ".intent/knowledge/capability_tags.yaml": capability_tags_content,
        ".intent/mission/principles.yaml": (
            TEMPLATES_DIR / "principles.yaml"
        ).read_text(),
        ".intent/policies/safety_policies.yaml": (
            TEMPLATES_DIR / "safety_policies.yaml"
        ).read_text(),
    }

    # Step 3: Write the files or display the dry run.
    if dry_run:
        log.info("\n💧 Dry Run Mode: No files will be written.")
        for rel_path, content in files_to_generate.items():
            typer.secho(f"\n📄 Proposed `{rel_path}`:", fg=typer.colors.YELLOW)
            if isinstance(content, dict):
                typer.echo(yaml.dump(content, indent=2))
            else:
                typer.echo(content)
    else:
        log.info("\n💾 **Write Mode:** Applying changes to disk.")
        for rel_path, content in files_to_generate.items():
            target_path = path / rel_path
            target_path.parent.mkdir(parents=True, exist_ok=True)
            if isinstance(content, dict):
                target_path.write_text(yaml.dump(content, indent=2))
            else:
                target_path.write_text(content)
            typer.secho(
                f"   -> ✅ Wrote starter file to {target_path}", fg=typer.colors.GREEN
            )

    log.info("\n🎉 BYOR initialization complete.")


def register(app: typer.Typer) -> None:
    """Register BYOR commands (e.g., `byor-init`) under the admin CLI."""
    app.command("byor-init")(initialize_repository)

--- END OF FILE ./src/system/admin/byor.py ---

--- START OF FILE ./src/system/admin_cli.py ---
# src/system/admin_cli.py
"""
Intent: Stable public entrypoint for the CORE Admin CLI.
Re-exports the Typer application without exposing internal wiring.
"""
from system.admin import app

__all__ = ["app"]

--- END OF FILE ./src/system/admin_cli.py ---

--- START OF FILE ./src/system/admin/DEL-migrator.py ---
# src/system/admin/migrator.py
"""
Intent: Registers the manifest migration tool with the CORE Admin CLI.
"""

import typer

from system.tools.manifest_migrator import migrate_manifest


def register(app: typer.Typer) -> None:
    """Register migration commands (manifest-migrator) under the admin CLI."""
    """Intent: Register migration commands under the admin CLI."""
    app.command("manifest-migrator")(migrate_manifest)

--- END OF FILE ./src/system/admin/DEL-migrator.py ---

--- START OF FILE ./src/system/admin/fixer.py ---
# src/system/admin/fixer.py
"""
Intent: Registers self-healing and code-fixing tools with the CORE Admin CLI.
"""

import typer

from system.tools.docstring_adder import fix_missing_docstrings


def register(app: typer.Typer) -> None:
    """Intent: Register fixer commands under the admin CLI."""
    fixer_app = typer.Typer(help="Self-healing and code quality tools.")
    app.add_typer(fixer_app, name="fix")

    fixer_app.command("docstrings")(fix_missing_docstrings)

--- END OF FILE ./src/system/admin/fixer.py ---

--- START OF FILE ./src/system/admin/guard_logic.py ---
# src/system/admin/guard_logic.py

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict

from system.guard.capability_discovery import (
    collect_code_capabilities,
    load_manifest_capabilities,
)
from system.guard.drift_detector import detect_capability_drift


def run_drift(root: Path, strict_intent: bool = False) -> Dict[str, Any]:
    """
    Pure function: computes capability drift and returns a report dictionary.
    This function MUST NOT print, write files, or exit.
    """
    # 1. Discover capabilities from the code using the appropriate discovery tools.
    code_caps = collect_code_capabilities(root, require_kgb=strict_intent)

    # 2. Load the declared capabilities from all domain manifests.
    manifest_caps = load_manifest_capabilities(root)

    # 3. Compare the two sets to find any drift.
    report = detect_capability_drift(manifest_caps, code_caps)

    # 4. Return the result as a JSON-serializable dictionary.
    return report.to_dict()

--- END OF FILE ./src/system/admin/guard_logic.py ---

--- START OF FILE ./src/system/admin/guard.py ---
# src/system/admin/guard.py
"""
Intent: Governance/validation guard commands exposed to the operator.

This module wires operator-friendly CLI commands on top of the pure logic in
`system.admin.guard_logic`. It *only* deals with inputs/outputs (CLI args,
printing, and writing evidence files) and intentionally keeps business logic
elsewhere so behavior stays testable and stable.

Commands
--------
- guard drift
    Compare capabilities declared in domain manifests with capabilities
    discovered in source code. Produces a JSON evidence file and optionally a
    human-readable summary.

    Examples:
      core-admin guard drift --format pretty
      core-admin guard drift --format json --output reports/drift.json
      core-admin guard drift --fail-on any --strict-intent
      core-admin guard drift --dry-run --format json

- guard kg-export
    Emit a minimal knowledge-graph artifact of discovered capabilities for
    downstream tooling.

Note: Defaults (format, fail policy, evidence path, labels) are read from
`.intent/meta.yaml` under:
  operator_experience.guard.drift
"""
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, List, Optional

import typer
import yaml

from system.admin.guard_logic import run_drift
from system.admin.utils import should_fail
from system.guard.capability_discovery import collect_code_capabilities


def _load_ux_defaults(root: Path) -> Dict[str, Any]:
    """Extract UX-related defaults for the drift command from `.intent/meta.yaml`.

    Falls back to sensible built-in defaults if the file or keys are missing.
    This influences *only* presentation/paths, not detection behavior.
    """
    meta_path = root / ".intent" / "meta.yaml"
    if not meta_path.exists():
        return {
            "default_format": "pretty",
            "default_fail_on": "any",
            "evidence_path": "reports/drift_report.json",
            "labels": {
                "none": "NONE",
                "success": "✅ No capability drift",
                "failure": "🚨 Drift detected",
            },
        }
    raw = yaml.safe_load(meta_path.read_text(encoding="utf-8")) or {}
    ux = raw.get("operator_experience", {}).get("guard", {}).get("drift", {})
    return {
        "default_format": ux.get("default_format", "pretty"),
        "default_fail_on": ux.get("default_fail_on", "any"),
        "evidence_path": ux.get("evidence_path", "reports/drift_report.json"),
        "labels": ux.get(
            "labels",
            {
                "none": "NONE",
                "success": "✅ No capability drift",
                "failure": "🚨 Drift detected",
            },
        ),
    }


def _is_clean(report: dict) -> bool:
    """Return True if the drift report contains no missing/undeclared/mismatched items."""
    return not (
        report.get("missing_in_code")
        or report.get("undeclared_in_manifest")
        or report.get("mismatched_mappings")
    )


def _normalize_report(report: dict) -> dict:
    """Return a copy of the report with deterministic ordering for stable diffs."""
    norm = {
        "missing_in_code": sorted(set(report.get("missing_in_code", []))),
        "undeclared_in_manifest": sorted(set(report.get("undeclared_in_manifest", []))),
        "mismatched_mappings": sorted(
            report.get("mismatched_mappings", []),
            key=lambda m: (
                m.get("capability") or "",
                str(m.get("manifest")),
                str(m.get("code")),
            ),
        ),
    }
    # Preserve any extra keys deterministically
    for k in sorted(set(report.keys()) - set(norm.keys())):
        v = report[k]
        norm[k] = v
    return norm


def _print_pretty(report_dict: dict, labels: Dict[str, str]) -> None:
    """Print a compact, human-friendly summary using basic TTY styling."""
    status = labels["success"] if _is_clean(report_dict) else labels["failure"]
    typer.secho(f"\n--- {status} ---", bold=True)

    def print_section(title: str, items: List[str]):
        """Prints a titled section with a list of items, highlighting the title in bold and items in yellow, or a 'none' label in green if the list is empty."""
        typer.secho(f"\n{title}:", bold=True)
        if not items:
            typer.secho(f"  {labels['none']}", fg=typer.colors.GREEN)
        else:
            for item in items:
                typer.secho(f"  - {item}", fg=typer.colors.YELLOW)

    print_section("Missing in code", report_dict.get("missing_in_code", []))
    print_section(
        "Undeclared in manifest", report_dict.get("undeclared_in_manifest", [])
    )

    mismatches = report_dict.get("mismatched_mappings", [])
    typer.secho("\nMismatched mappings:", bold=True)
    if not mismatches:
        typer.secho(f"  {labels['none']}", fg=typer.colors.GREEN)
    else:
        for m in mismatches:
            cap = m.get("capability")
            typer.secho(f"  - {cap}:", fg=typer.colors.YELLOW)
            typer.secho(f"    Manifest: {m.get('manifest')}", fg=typer.colors.YELLOW)
            typer.secho(f"    Code:     {m.get('code')}", fg=typer.colors.YELLOW)
    typer.echo("")


def register(app: typer.Typer) -> None:
    """Register the 'guard' command group with the main Admin CLI."""
    guard = typer.Typer(
        help=(
            "Governance & validation guards.\n\n"
            "Use 'guard drift' to detect capability drift between domain manifests "
            "and source code. Use 'guard kg-export' to emit a minimal capability graph."
        ),
        no_args_is_help=True,
    )
    app.add_typer(guard, name="guard")

    @guard.command("drift")
    def drift(
        root: Path = typer.Option(
            Path("."),
            help="Path to the repository root (where .intent/ lives).",
        ),
        output: Optional[Path] = typer.Option(
            None,
            help=(
                "Write the JSON evidence report here. "
                "Default is read from .intent/meta.yaml "
                "(operator_experience.guard.drift.evidence_path)."
            ),
        ),
        format: Optional[str] = typer.Option(
            None,
            help=(
                "Output mode: 'json' for machine-readable or 'pretty' for human-readable. "
                "Defaults to operator_experience.guard.drift.default_format."
            ),
        ),
        fail_on: Optional[str] = typer.Option(
            None,
            help=(
                "Exit with code 2 when drift of this type is present. "
                "Accepted values: 'any', 'missing', 'undeclared'. "
                "Defaults to operator_experience.guard.drift.default_fail_on."
            ),
        ),
        strict_intent: bool = typer.Option(
            False,
            "--strict-intent",
            help=(
                "Discovery mode that relies only on the constitutionally approved "
                "KnowledgeGraphBuilder (safer, potentially slower). Recommended for CI."
            ),
        ),
        dry_run: bool = typer.Option(
            False,
            "--dry-run",
            help="Do not write the evidence file; print the report only.",
        ),
    ):
        """Compare manifest declarations vs. code to detect capability drift.

        Writes a JSON evidence file (unless --dry-run) and prints either a pretty
        summary or raw JSON. Exit codes: 0 = OK, 2 = drift detected per --fail-on policy.
        """
        ux = _load_ux_defaults(root)
        fmt = (format or ux["default_format"]).lower()
        fail_policy = (fail_on or ux["default_fail_on"]).lower()

        raw_report = run_drift(root, strict_intent)
        report = _normalize_report(raw_report)

        if not dry_run:
            final_output_path = output or (root / ux["evidence_path"])
            final_output_path.parent.mkdir(parents=True, exist_ok=True)
            final_output_path.write_text(
                json.dumps(report, indent=2, sort_keys=True) + "\n",
                encoding="utf-8",
            )

        if fmt == "pretty":
            _print_pretty(report, ux["labels"])
        else:
            typer.echo(json.dumps(report, indent=2, sort_keys=True))

        if should_fail(report, fail_policy):
            raise typer.Exit(code=2)

    @guard.command("kg-export")
    def kg_export(
        root: Path = typer.Option(
            Path("."),
            help="Path to the repository root (where .intent/ lives).",
        ),
        output: Optional[Path] = typer.Option(
            None,
            help=(
                "Target file for the knowledge-graph artifact. "
                "Defaults to 'reports/knowledge_graph.json' under --root."
            ),
        ),
        include: Optional[List[str]] = typer.Option(
            None,
            help="Optional glob(s) to include (e.g., 'src/**.py'). If omitted, scan *.py.",
        ),
        exclude: Optional[List[str]] = typer.Option(
            None,
            help="Optional glob(s) to exclude from scanning.",
        ),
        prefer: str = typer.Option(
            "auto",
            case_sensitive=False,
            help=(
                "Source of truth for capability discovery: "
                "'auto' (prefer KGB when available), 'kgb' (require KGB), or 'grep' (fallback scan)."
            ),
        ),
    ):
        """Emit a minimal capability knowledge-graph artifact for downstream tools."""
        require_kgb = prefer.lower() == "kgb"
        caps = collect_code_capabilities(
            root,
            include_globs=include,
            exclude_globs=exclude,
            require_kgb=require_kgb,
        )
        nodes = [
            {"capability": k, "domain": v.domain, "owner": v.owner}
            for k, v in sorted(caps.items())
        ]
        out_path = output or (root / "reports" / "knowledge_graph.json")
        out_path.parent.mkdir(parents=True, exist_ok=True)
        out_path.write_text(
            json.dumps({"nodes": nodes}, indent=2, sort_keys=True) + "\n",
            encoding="utf-8",
        )
        typer.echo(
            f"✅ Wrote knowledge-graph artifact with {len(nodes)} capability nodes -> {out_path}"
        )

--- END OF FILE ./src/system/admin/guard.py ---

--- START OF FILE ./src/system/admin/__init__.py ---
# src/system/admin/__init__.py
"""
Intent: Modular CORE Admin CLI root. Wires subcommand groups (keys, proposals, guard)
without changing the public console script target (system.admin_cli:app).
"""
from __future__ import annotations

import os
import sys

import typer

from shared.logger import configure_logging
from system.admin import agent as _agent
from system.admin import byor as _byor
from system.admin import fixer as _fixer
from system.admin import guard as _guard
from system.admin import keys as _keys

# from system.admin import migrator as _migrator
from system.admin import new as _new
from system.admin import proposals
from system.admin import reviewer as _reviewer

app = typer.Typer(
    help="""
    🏛️  CORE Admin CLI

    The command-line interface for the CORE Human Operator.
    Provides safe, governed commands for managing the system's constitution.
    """,
    no_args_is_help=True,
)


# Configure logging once per CLI invocation.
# - Defaults to WARNING for quietness.
# - -v → INFO, -vv → DEBUG
# - --quiet/-q → ERROR
# - JSON logs opt-in via env CORE_LOG_JSON=true (stderr only)
@app.callback()
def _configure_logging(
    verbose: int = typer.Option(
        0,
        "--verbose",
        "-v",
        count=True,
        help="Increase verbosity (-v=INFO, -vv=DEBUG).",
    ),
    quiet: bool = typer.Option(
        False,
        "--quiet",
        "-q",
        help="Silence non-errors (ERROR level only).",
    ),
) -> None:
    if quiet:
        level = "ERROR"
    elif verbose >= 2:
        level = "DEBUG"
    elif verbose == 1:
        level = "INFO"
    else:
        level = "WARNING"

    json_mode = os.getenv("CORE_LOG_JSON", "false").lower() == "true"

    # IMPORTANT: log to the original stderr so Click/Typer's stdout capture is untouched.
    configure_logging(level=level, stream=sys.__stderr__, json_mode=json_mode)


# Register command groups
_agent.register(app)
_keys.register(app)
app.add_typer(proposals.proposals_app, name="proposals")
_guard.register(app)
# _migrator.register(app)
_fixer.register(app)
_byor.register(app)
_reviewer.register(app)
_new.register(app)

__all__ = ["app"]

--- END OF FILE ./src/system/admin/__init__.py ---

--- START OF FILE ./src/system/admin/keys.py ---
# src/system/admin/keys.py
"""
Intent: Key management commands for the CORE Admin CLI.
Provides Ed25519 key generation and helper output for approver configuration.
"""

from __future__ import annotations

import os

import typer
import yaml
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ed25519

from shared.config import settings
from shared.logger import getLogger

log = getLogger("core_admin")


def register(app: typer.Typer) -> None:
    """Intent: Register key management commands under the admin CLI."""

    @app.command("keygen")
    def keygen(
        identity: str = typer.Argument(
            help="Identity for the key pair (e.g., 'your.name@example.com')."
        ),
    ) -> None:
        """Intent: Generate a new Ed25519 key pair and print an approver YAML block."""
        log.info(f"🔑 Generating new key pair for identity: {identity}")
        settings.KEY_STORAGE_DIR.mkdir(parents=True, exist_ok=True)
        private_key_path = settings.KEY_STORAGE_DIR / "private.key"

        if private_key_path.exists():
            typer.confirm(
                "⚠️ A private key already exists. Overwriting it will invalidate your old identity. Continue?",
                abort=True,
            )

        private_key = ed25519.Ed25519PrivateKey.generate()
        public_key = private_key.public_key()

        pem_private = private_key.private_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PrivateFormat.PKCS8,
            encryption_algorithm=serialization.NoEncryption(),
        )
        private_key_path.write_bytes(pem_private)
        os.chmod(private_key_path, 0o600)

        pem_public = public_key.public_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PublicFormat.SubjectPublicKeyInfo,
        )

        log.info(f"\n✅ Private key saved securely to: {private_key_path}")
        log.info(
            "\n📋 Add the following YAML block to '.intent/constitution/approvers.yaml' under 'approvers':\n"
        )

        approver_yaml = yaml.dump(
            [
                {
                    "identity": identity,
                    "public_key": pem_public.decode("utf-8"),
                    "role": "maintainer",
                    "description": "Primary maintainer",
                }
            ],
            indent=2,
        )
        print(approver_yaml)

--- END OF FILE ./src/system/admin/keys.py ---

--- START OF FILE ./src/system/admin/new.py ---
# src/system/admin/new.py
"""
Intent: Defines the 'core-admin new' command, a user-facing wrapper
around the Scaffolder tool.
"""
import typer

from system.tools.scaffolder import new_project


def register(app: typer.Typer) -> None:
    """Register the 'new' command with the main CLI app."""
    # Directly register the imported new_project function under the name 'new'
    app.command("new")(new_project)

--- END OF FILE ./src/system/admin/new.py ---

--- START OF FILE ./src/system/admin/proposals.py ---
# src/system/admin/proposals.py
"""
Intent: Proposal lifecycle commands (list, sign, approve) for constitution-governed changes.
Integrates with the ConstitutionalAuditor via a canary workspace before applying changes.
"""

from __future__ import annotations

import base64
import shutil
import subprocess
import sys
import tempfile
from datetime import datetime
from pathlib import Path

import typer
from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ed25519

from shared.config import settings
from shared.logger import getLogger
from system.admin.utils import (
    archive_rollback_plan,
    generate_approval_token,
    load_private_key,
    load_yaml_file,
    save_yaml_file,
)
from system.governance.constitutional_auditor import ConstitutionalAuditor

log = getLogger("core_admin")

# Create a Typer app for the "proposals" subcommand group
proposals_app = typer.Typer(help="Work with constitutional proposals")


@proposals_app.command("list")
def proposals_list() -> None:
    """List pending constitutional proposals and display their justification, target path, and signature/quorum status."""
    log.info("🔍 Finding pending constitutional proposals...")
    proposals_dir = settings.MIND / "proposals"
    proposals_dir.mkdir(exist_ok=True)
    proposals = sorted(proposals_dir.glob("cr-*.yaml"))

    if not proposals:
        log.info("✅ No pending proposals found.")
        return

    log.info(f"Found {len(proposals)} pending proposal(s):")
    approvers_config = load_yaml_file(settings.MIND / "constitution" / "approvers.yaml")

    for prop_path in proposals:
        config = load_yaml_file(prop_path)
        justification = config.get("justification", "No justification provided.")
        is_critical = any(
            config.get("target_path", "").endswith(p)
            for p in approvers_config.get("critical_paths", [])
        )
        required = approvers_config.get("quorum", {}).get(
            "critical" if is_critical else "standard", 1
        )
        current = len(config.get("signatures", []))
        status = "✅ Ready" if current >= required else f"⏳ {current}/{required} sigs"

        log.info(f"\n  - **{prop_path.name}**: {justification.strip()}")
        log.info(f"    Target: {config.get('target_path')}")
        log.info(f"    Status: {status} ({'Critical' if is_critical else 'Standard'})")


@proposals_app.command("sign")
def proposals_sign(
    proposal_name: str = typer.Argument(
        ...,  # The '...' makes this argument required
        help="Filename of the proposal to sign (e.g., 'cr-new-policy.yaml').",
    ),
) -> None:
    """Sign a proposal with the operator's private key (content-bound token)."""
    log.info(f"✍️ Signing proposal: {proposal_name}")
    proposal_path = settings.MIND / "proposals" / proposal_name
    if not proposal_path.exists():
        log.error(f"❌ Proposal '{proposal_name}' not found.")
        raise typer.Exit(code=1)

    proposal = load_yaml_file(proposal_path)
    private_key = load_private_key()

    token = generate_approval_token(proposal)
    signature = private_key.sign(token.encode("utf-8"))

    identity = typer.prompt(
        "Enter your identity (e.g., name@domain.com) to associate with this signature"
    )

    proposal.setdefault("signatures", [])
    proposal["signatures"] = [
        s for s in proposal["signatures"] if s.get("identity") != identity
    ]
    proposal["signatures"].append(
        {
            "identity": identity,
            "signature_b64": base64.b64encode(signature).decode("utf-8"),
            "token": token,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
    )

    save_yaml_file(proposal_path, proposal)
    log.info("✅ Signature added to proposal file.")


@proposals_app.command("approve")
def proposals_approve(
    proposal_name: str = typer.Argument(
        ...,  # The '...' makes this argument required
        help="Filename of the proposal to approve.",
    ),
) -> None:
    """Verify signatures/quorum, run a canary constitutional audit, then apply the proposal if valid."""
    log.info(f"🚀 Attempting to approve proposal: {proposal_name}")
    proposal_path = settings.MIND / "proposals" / proposal_name
    if not proposal_path.exists():
        log.error(f"❌ Proposal '{proposal_name}' not found.")
        raise typer.Exit(code=1)

    proposal = load_yaml_file(proposal_path)
    target_rel_path = proposal.get("target_path")
    if not target_rel_path:
        log.error("❌ Proposal is invalid: missing 'target_path'.")
        raise typer.Exit(code=1)

    log.info("🔐 Verifying cryptographic signatures...")
    approvers_config = load_yaml_file(settings.MIND / "constitution" / "approvers.yaml")
    approver_keys = {
        a["identity"]: a["public_key"] for a in approvers_config.get("approvers", [])
    }

    valid_signatures = 0
    for sig in proposal.get("signatures", []):
        identity = sig.get("identity")
        pem = approver_keys.get(identity)
        if not pem:
            continue
        try:
            pub_key = serialization.load_pem_public_key(pem.encode("utf-8"))
            if not isinstance(pub_key, ed25519.Ed25519PublicKey):
                log.warning(
                    f"   ⚠️ Key for '{identity}' is not a valid Ed25519 signing key. Skipping."
                )
                continue

            pub_key.verify(
                base64.b64decode(sig["signature_b64"]), sig["token"].encode("utf-8")
            )
            if sig["token"] == generate_approval_token(proposal):
                log.info(f"   ✅ Valid signature from '{identity}'.")
                valid_signatures += 1
            else:
                log.warning(
                    f"   ⚠️ Signature from '{identity}' is for outdated content."
                )
        except (InvalidSignature, ValueError, TypeError):
            log.warning(f"   ⚠️ Invalid signature for '{identity}'.")

    is_critical = any(
        str(target_rel_path).endswith(p)
        for p in approvers_config.get("critical_paths", [])
    )
    required = approvers_config.get("quorum", {}).get(
        "critical" if is_critical else "standard", 1
    )

    if valid_signatures < required:
        log.error(
            f"❌ Approval failed: Quorum not met. Have {valid_signatures}/{required} valid signatures."
        )
        raise typer.Exit(code=1)

    log.info("\n🧠 Generating fresh Knowledge Graph before canary validation...")
    try:
        subprocess.run(
            [sys.executable, "-m", "src.system.tools.codegraph_builder"],
            cwd=settings.REPO_PATH,
            check=True,
            capture_output=True,
        )
        log.info("   -> Knowledge Graph regenerated successfully.")
    except subprocess.CalledProcessError as e:
        log.error(
            f"❌ Failed to regenerate Knowledge Graph. Aborting. Stderr: {e.stderr.decode()}"
        )
        raise typer.Exit(code=1)

    log.info("\n🐦 Spinning up canary environment for validation...")
    with tempfile.TemporaryDirectory() as tmp:
        tmp_path = Path(tmp)

        log.info(f"   -> Creating a clean clone of the repository at {tmp_path}...")
        try:
            subprocess.run(
                ["git", "clone", str(settings.REPO_PATH), "."],
                cwd=tmp_path,
                check=True,
                capture_output=True,
            )
        except subprocess.CalledProcessError as e:
            log.error(
                f"❌ Failed to create clean git clone for canary. Aborting. Stderr: {e.stderr.decode()}"
            )
            raise typer.Exit(code=1)

        env_file = settings.REPO_PATH / ".env"
        if env_file.exists():
            shutil.copy(env_file, tmp_path / ".env")
            log.info("   -> Copied environment configuration to canary.")

        canary_target_path = tmp_path / target_rel_path
        canary_target_path.parent.mkdir(parents=True, exist_ok=True)
        canary_target_path.write_text(proposal.get("content", ""), encoding="utf-8")

        log.info("🔬 Commanding canary to perform a self-audit...")
        auditor = ConstitutionalAuditor(repo_root_override=tmp_path)
        success = auditor.run_full_audit()

        if success:
            log.info("✅ Canary audit PASSED. Change is constitutionally valid.")
            archive_rollback_plan(proposal_name, proposal)
            live_target_path = settings.REPO_PATH / target_rel_path
            live_target_path.parent.mkdir(parents=True, exist_ok=True)
            live_target_path.write_text(proposal.get("content", ""), encoding="utf-8")
            proposal_path.unlink()
            log.info(f"✅ Successfully approved and applied '{proposal_name}'.")
        else:
            log.error(
                "❌ Canary audit FAILED. Proposal rejected; live system untouched."
            )
            raise typer.Exit(code=1)


# This function will now be simplified to just register the app
def register(app: typer.Typer) -> None:
    """Register proposal lifecycle commands under the admin CLI."""
    app.add_typer(proposals_app, name="proposals")

--- END OF FILE ./src/system/admin/proposals.py ---

--- START OF FILE ./src/system/admin.py ---
# src/system/admin.py
"""
CORE Admin CLI (Poetry script: core-admin)

Commands
--------
- core-admin guard check [--format pretty|json] [--no-fail]
    Runs the Intent Guard (AST-based import checks) using .intent policies.
    Uses the same logic as src/system/tools/intent_guard_runner.py (File 12).

- core-admin guard drift [--format short|pretty|json]
    Displays the drift evidence written by manifest_migrator (File 4),
    reading the evidence path from .intent/meta.yaml (reports.drift).

- core-admin fix docstrings [--write]
    Placeholder (safe no-op). Kept so Makefile target doesn't break.

Notes
-----
- This CLI is intentionally light and constitution-aware. Paths are resolved
  from .intent/meta.yaml whenever possible.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path

import typer
import yaml

app = typer.Typer(help="CORE Admin CLI")
guard_app = typer.Typer(help="Constitutional guard utilities")
fix_app = typer.Typer(help="Developer productivity helpers")

app.add_typer(guard_app, name="guard")
app.add_typer(fix_app, name="fix")


# ---------- Utilities ---------------------------------------------------------

REPO = (
    Path(__file__).resolve().parents[2]
)  # .../src/system/admin.py -> repo root candidate
INTENT = (REPO / ".intent") if (REPO / ".intent").exists() else Path(".intent")


def _load_yaml(p: Path) -> dict:
    if not p.exists():
        typer.secho(f"ERROR: Missing file: {p}", fg=typer.colors.RED, err=True)
        raise typer.Exit(2)
    try:
        return yaml.safe_load(p.read_text()) or {}
    except Exception as e:
        typer.secho(f"ERROR: YAML error in {p}: {e}", fg=typer.colors.RED, err=True)
        raise typer.Exit(2)


def _meta_paths() -> tuple[Path, Path]:
    meta = _load_yaml(INTENT / "meta.yaml")
    pol = meta.get("policies", {})
    knowledge = meta.get("knowledge", {})
    reports = meta.get("reports", {})

    policy_path = INTENT / pol.get("intent_guard", "policies/intent_guard.yaml")
    source_map = INTENT / knowledge.get(
        "source_structure", "knowledge/source_structure.yaml"
    )
    drift_path = (Path(reports.get("drift", "reports/drift_report.json"))).resolve()
    return policy_path, source_map, drift_path


# ---------- guard check -------------------------------------------------------


@guard_app.command("check")
def guard_check(
    fmt: str = typer.Option("pretty", "--format", help="Output format: pretty|json"),
    no_fail: bool = typer.Option(
        False, "--no-fail", help="Always exit 0 (override enforcement.mode)"
    ),
):
    """
    Run Intent Guard (import/dependency checks).
    """
    # Import the runner and call its main() with args.
    try:
        from system.tools.intent_guard_runner import (
            main as guard_runner_main,  # type: ignore
        )
    except Exception as e:  # pragma: no cover
        typer.secho(
            f"ERROR: guard runner unavailable: {e}", fg=typer.colors.RED, err=True
        )
        raise typer.Exit(2)

    argv = ["check", f"--format={fmt}"] + (["--no-fail"] if no_fail else [])
    try:
        guard_runner_main(argv)  # will sys.exit() internally
    except SystemExit as exc:
        # Propagate exit code so Makefile can fail if needed.
        raise typer.Exit(exc.code)


# ---------- guard drift -------------------------------------------------------


@dataclass
class DriftSummary:
    validation_errors: int
    duplicate_caps: int


def _summarize_drift(payload: dict) -> DriftSummary:
    val_errs = payload.get("validation_errors") or []
    dups = payload.get("duplicates") or {}
    return DriftSummary(validation_errors=len(val_errs), duplicate_caps=len(dups))


@guard_app.command("drift")
def guard_drift(
    fmt: str = typer.Option(
        "short", "--format", help="Output format: short|pretty|json"
    ),
):
    """
    Display drift evidence produced by manifest_migrator (schema errors + capability duplicates).
    """
    _, _, drift_path = _meta_paths()

    if not drift_path.exists():
        typer.secho(
            f"⚠️  No drift evidence found at {drift_path}. "
            f"Hint: run `make drift` or `make migrate` first.",
            fg=typer.colors.YELLOW,
        )
        raise typer.Exit(0)

    try:
        payload = json.loads(drift_path.read_text())
    except Exception as e:
        typer.secho(
            f"ERROR: Invalid JSON in {drift_path}: {e}", fg=typer.colors.RED, err=True
        )
        raise typer.Exit(2)

    if fmt == "json":
        typer.echo(json.dumps(payload, indent=2))
        raise typer.Exit(0)

    summary = _summarize_drift(payload)

    if fmt == "short":
        msg = f"validation_errors={summary.validation_errors} duplicate_capabilities={summary.duplicate_caps}"
        color = (
            typer.colors.GREEN
            if (summary.validation_errors == 0 and summary.duplicate_caps == 0)
            else typer.colors.RED
        )
        typer.secho(msg, fg=color)
        raise typer.Exit(0)

    # pretty
    try:
        from rich import box
        from rich.console import Console
        from rich.table import Table

        console = Console()
        table = Table(title="Capability Drift", box=box.SIMPLE_HEAVY)
        table.add_column("Metric")
        table.add_column("Count")
        table.add_row("Schema validation errors", str(summary.validation_errors))
        table.add_row("Duplicate capabilities", str(summary.duplicate_caps))
        console.print(table)

        # Show details if any
        if payload.get("validation_errors"):
            console.print("\n[bold]Validation Errors[/]")
            for line in payload["validation_errors"]:
                console.print(f" - {line}")

        if payload.get("duplicates"):
            console.print("\n[bold]Duplicates[/]")
            for cap, doms in sorted(payload["duplicates"].items()):
                console.print(f" - {cap}: {', '.join(sorted(doms))}")

    except Exception:
        # Fallback to plain text
        typer.echo("Capability Drift")
        typer.echo(f"- Schema validation errors: {summary.validation_errors}")
        typer.echo(f"- Duplicate capabilities: {summary.duplicate_caps}")

    raise typer.Exit(0)


# ---------- fix docstrings (placeholder) -------------------------------------


@fix_app.command("docstrings")
def fix_docstrings(
    write: bool = typer.Option(
        False, "--write", help="(Placeholder) If set, would apply fixes in place."
    ),
):
    """
    Placeholder command so 'make fix-docstrings' doesn't break.
    """
    typer.secho(
        "ℹ️  Docstring fixer not wired yet. This is a safe placeholder.",
        fg=typer.colors.YELLOW,
    )
    if write:
        typer.secho(
            "Pretending to write fixes... (no changes made)", fg=typer.colors.BLUE
        )
    raise typer.Exit(0)


# ---------- Main --------------------------------------------------------------

if __name__ == "__main__":
    app()

--- END OF FILE ./src/system/admin.py ---

--- START OF FILE ./src/system/admin/reviewer.py ---
# src/system/admin/reviewer.py
"""
Intent: Implements commands related to constitutional review and improvement.
This includes exporting the constitution for external analysis and orchestrating
an AI-powered peer review.
"""

from pathlib import Path

import typer
import yaml

from core.clients import OrchestratorClient
from shared.config import settings
from shared.logger import getLogger

log = getLogger("core_admin.review")

# A set of files/patterns within .intent/ to ignore during export.
IGNORE_PATTERNS = {"proposals", "knowledge_graph.json", ".bak", ".example"}


def _is_ignored(path_str: str) -> bool:
    """Checks if a given file path should be ignored based on IGNORE_PATTERNS."""
    return any(pattern in path_str for pattern in IGNORE_PATTERNS)


def _get_bundle_content() -> str:
    """Gathers and bundles the content of all constitutional files."""
    intent_dir = settings.MIND
    meta_path = intent_dir / "meta.yaml"
    if not meta_path.exists():
        log.error(
            f"❌ Critical file not found: {meta_path}. Cannot export constitution."
        )
        raise typer.Exit(code=1)

    meta_content = yaml.safe_load(meta_path.read_text())

    bundle_parts = []

    def find_paths_in_meta(data):
        """Recursively extracts all strings containing '/' from nested dictionaries, lists, or strings in `data`."""
        paths = []
        if isinstance(data, dict):
            for value in data.values():
                paths.extend(find_paths_in_meta(value))
        elif isinstance(data, list):
            for item in data:
                paths.extend(find_paths_in_meta(item))
        elif isinstance(data, str) and "/" in data:
            paths.append(data)
        return paths

    discovered_paths = find_paths_in_meta(meta_content)
    discovered_paths.append("meta.yaml")

    log.info(
        f"   -> Found {len(list(set(discovered_paths)))} constitutional files declared in meta.yaml."
    )

    for rel_path_str in sorted(list(set(discovered_paths))):
        if _is_ignored(rel_path_str):
            continue

        file_path = intent_dir / rel_path_str
        if file_path.exists() and file_path.is_file():
            content = file_path.read_text(encoding="utf-8")
            bundle_parts.append(f"--- START OF FILE .intent/{rel_path_str} ---\n")
            bundle_parts.append(content)
            bundle_parts.append(f"\n--- END OF FILE .intent/{rel_path_str} ---\n\n")

    return "".join(bundle_parts)


def export_constitution(
    output: Path = typer.Option(
        Path("reports/constitution_bundle.txt"),
        "--output",
        "-o",
        help="The path to save the exported constitutional bundle.",
    ),
):
    """
    Packages the full .intent/ directory into a single bundle for external analysis.
    This command reads the meta.yaml file to discover all constitutional files
    and concatenates them into a single, LLM-friendly text file.
    """
    log.info("🏛️  Exporting constitutional bundle...")
    final_bundle = _get_bundle_content()
    output.parent.mkdir(parents=True, exist_ok=True)
    output.write_text(final_bundle, encoding="utf-8")
    log.info(f"✅ Successfully exported constitutional bundle to: {output}")


def peer_review(
    output: Path = typer.Option(
        Path("reports/constitutional_review.md"),
        "--output",
        "-o",
        help="The path to save the LLM's review.",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        help="Prepare the full prompt and print it to the console without sending to the LLM.",
    ),
):
    """
    Orchestrates sending the constitutional bundle to an external LLM for critique.
    """
    if dry_run:
        log.info(
            "🤖 Preparing constitutional prompt for manual review (dry-run mode)..."
        )
    else:
        log.info("🤖 Orchestrating Constitutional Peer Review...")

    prompt_path = settings.MIND / "prompts" / "constitutional_review.prompt"
    if not prompt_path.exists():
        log.error(f"❌ Review prompt not found at {prompt_path}. Cannot proceed.")
        raise typer.Exit(code=1)
    review_prompt_template = prompt_path.read_text(encoding="utf-8")

    log.info("   -> Bundling the constitution for review...")
    bundle = _get_bundle_content()

    final_prompt = f"{review_prompt_template}\n\n{bundle}"

    if dry_run:
        typer.secho(
            "\n--- Final Prompt to be Sent ---", fg=typer.colors.YELLOW, bold=True
        )
        typer.echo(final_prompt)
        typer.secho("--- End of Prompt ---", fg=typer.colors.YELLOW, bold=True)
        log.info("✅ Dry-run complete. No request was sent.")
        raise typer.Exit()

    log.info(
        "   -> Sending bundle to external LLM for analysis. This may take a moment..."
    )

    orchestrator = OrchestratorClient()
    review_feedback = orchestrator.make_request(
        final_prompt, user_id="constitutional_reviewer"
    )

    output.parent.mkdir(parents=True, exist_ok=True)
    output.write_text(review_feedback, encoding="utf-8")

    log.info("✅ Successfully received feedback from peer review.")
    log.info(f"   -> Full review saved to: {output}")
    typer.secho("\n--- Review Summary ---", bold=True)
    typer.echo(review_feedback)


def register(app: typer.Typer):
    """Registers the 'review' command group and its subcommands."""
    review_app = typer.Typer(help="Tools for constitutional review and improvement.")
    app.add_typer(review_app, name="review")
    review_app.command("export")(export_constitution)
    review_app.command("peer-review")(peer_review)

--- END OF FILE ./src/system/admin/reviewer.py ---

--- START OF FILE ./src/system/admin/utils.py ---
# src/system/admin/utils.py
"""
Intent: Shared admin utilities used by CLI commands. These helpers are small,
documented, and domain-aligned so they can be safely referenced by the auditor.
"""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict

import yaml
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import ed25519

from shared.config import settings
from shared.logger import getLogger

log = getLogger("core_admin")


def should_fail(report: dict, fail_on: str) -> bool:
    """Determines if the CLI should exit with an error code based on the drift report and the specified fail condition (missing, undeclared, or any drift)."""
    if fail_on == "missing":
        return bool(report.get("missing_in_code"))
    if fail_on == "undeclared":
        return bool(report.get("undeclared_in_manifest"))
    return bool(
        report.get("missing_in_code")
        or report.get("undeclared_in_manifest")
        or report.get("mismatched_mappings")
    )


def load_yaml_file(path: Path) -> Dict[str, Any]:
    """Intent: Load YAML for governance operations. Returns {} for empty documents."""
    return yaml.safe_load(path.read_text(encoding="utf-8")) or {}


def save_yaml_file(path: Path, data: Dict[str, Any]) -> None:
    """Intent: Persist YAML with stable ordering disabled to preserve human readability."""
    path.write_text(yaml.safe_dump(data, sort_keys=False), encoding="utf-8")


def generate_approval_token(proposal: Dict[str, Any]) -> str:
    """
    Intent: Produce a deterministic token for approvals bound to the *full proposal intent*,
    not just raw content. This prevents replay against a different target.
    """
    payload = {
        "version": "v2",
        "target_path": proposal.get("target_path"),
        "action": proposal.get("action"),
        "content": proposal.get("content", ""),
    }
    digest = hashes.Hash(hashes.SHA256())
    digest.update(json.dumps(payload, sort_keys=True).encode("utf-8"))
    return f"core-proposal-v2:{digest.finalize().hex()}"


def load_private_key() -> ed25519.Ed25519PrivateKey:
    """Intent: Load the operator's Ed25519 private key from the protected key store."""
    key_path = settings.KEY_STORAGE_DIR / "private.key"
    if not key_path.exists():
        log.error(
            "❌ Private key not found. Please run 'core-admin keygen' to create one."
        )
        raise SystemExit(1)
    return serialization.load_pem_private_key(key_path.read_bytes(), password=None)


def archive_rollback_plan(proposal_name: str, proposal: Dict[str, Any]) -> None:
    """Intent: Persist a rollback plan snapshot for approved proposals under .intent/constitution/rollbacks/."""
    rollback_plan = proposal.get("rollback_plan")
    if not rollback_plan:
        return
    rollbacks_dir = settings.MIND / "constitution" / "rollbacks"
    rollbacks_dir.mkdir(parents=True, exist_ok=True)
    archive_path = (
        rollbacks_dir
        / f"{datetime.utcnow().strftime('%Y%m%d%H%M%S')}-{proposal_name}.json"
    )
    archive_path.write_text(
        json.dumps(
            {
                "proposal_name": proposal_name,
                "target_path": proposal.get("target_path"),
                "justification": proposal.get("justification"),
                "rollback_plan": rollback_plan,
            },
            indent=2,
        ),
        encoding="utf-8",
    )
    log.info(f"📖 Rollback plan archived to {archive_path}")

--- END OF FILE ./src/system/admin/utils.py ---

--- START OF FILE ./src/system/governance/checks/architecture_checks.py ---
# src/system/governance/checks/architecture_checks.py
"""
Auditor checks for higher-level architectural principles and smells,
forming the basis of CORE's "architectural conscience."
"""
from collections import defaultdict

from system.governance.checks.base import BaseAuditCheck
from system.governance.models import AuditFinding, AuditSeverity


class ArchitectureChecks(BaseAuditCheck):
    """Container for architectural integrity checks."""

    # The __init__ method has been removed.

    # CAPABILITY: audit.check.duplication
    def check_for_structural_duplication(self) -> list[AuditFinding]:
        """
        Finds symbols with identical structural hashes, violating `dry_by_design`.
        This check uses the content-addressed nature of the knowledge graph to
        detect code duplication with perfect accuracy.
        """
        findings = []
        check_name = "Architectural Integrity: Code Duplication"

        hashes = defaultdict(list)
        for symbol in self.context.symbols_list:
            # We only care about functions and classes, not their methods for now.
            if symbol.get("structural_hash") and not symbol.get("parent_class_key"):
                hashes[symbol["structural_hash"]].append(symbol["key"])

        duplicates_found = False
        for structural_hash, keys in hashes.items():
            if len(keys) > 1:
                duplicates_found = True
                locations = ", ".join(f"'{key}'" for key in keys)
                message = (
                    f"Structural duplication detected. The following symbols are "
                    f"identical: {locations}. This may violate the 'dry_by_design' principle."
                )
                findings.append(
                    AuditFinding(AuditSeverity.WARNING, message, check_name)
                )

        if not duplicates_found:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "No structural code duplication found.",
                    check_name,
                )
            )

        return findings

--- END OF FILE ./src/system/governance/checks/architecture_checks.py ---

--- START OF FILE ./src/system/governance/checks/base.py ---
# src/system/governance/checks/base.py
"""
Provides a base class for all auditor check containers.
This helps enforce the 'dry_by_design' principle by centralizing
common initialization logic.
"""


class BaseAuditCheck:
    """Base class for a collection of auditor checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        self.context = context

--- END OF FILE ./src/system/governance/checks/base.py ---

--- START OF FILE ./src/system/governance/checks/environment_checks.py ---
# src/system/governance/checks/environment_checks.py
"""Auditor checks related to the system's runtime environment."""
import os

from system.governance.checks.base import BaseAuditCheck
from system.governance.models import AuditFinding, AuditSeverity


class EnvironmentChecks(BaseAuditCheck):
    """Container for environment and runtime configuration checks."""

    # CAPABILITY: audit.check.environment
    def check_runtime_environment(self) -> list[AuditFinding]:
        """Verifies that required environment variables specified in runtime_requirements.yaml are set, returning a list of audit findings for missing variables or configuration issues."""
        """Verifies that required environment variables are set."""
        findings = []
        check_name = "Runtime Environment Validation"

        requirements_path = (
            self.context.intent_dir / "config" / "runtime_requirements.yaml"
        )
        if not requirements_path.exists():
            findings.append(
                AuditFinding(
                    AuditSeverity.WARNING,
                    "runtime_requirements.yaml not found; cannot validate environment.",
                    check_name,
                )
            )
            return findings

        requirements = self.context.load_config(requirements_path, "yaml")
        required_vars = requirements.get("required_environment_variables", [])

        missing_vars = []
        for var in required_vars:
            if var.get("required") and not os.getenv(var.get("name")):
                missing_vars.append(var)

        if missing_vars:
            for var in missing_vars:
                msg = f"Required environment variable '{var.get('name')}' is not set. Description: {var.get('description')}"
                findings.append(AuditFinding(AuditSeverity.ERROR, msg, check_name))
        else:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "All required environment variables are set.",
                    check_name,
                )
            )

        return findings

--- END OF FILE ./src/system/governance/checks/environment_checks.py ---

--- START OF FILE ./src/system/governance/checks/file_checks.py ---
# src/system/governance/checks/file_checks.py
"""Auditor checks related to file existence, format, and structure."""

from pathlib import Path
from typing import List, Set

from core.validation_pipeline import validate_code
from system.governance.models import AuditFinding, AuditSeverity


class FileChecks:
    """Container for file-based constitutional checks."""

    def __init__(self, context):
        """Initialize with a shared auditor context."""
        self.context = context
        self.intent_dir: Path = context.intent_dir
        self.repo_root: Path = context.repo_root

    # CAPABILITY: audit.check.required_files
    def check_required_files(self) -> List[AuditFinding]:
        """Verify that all files declared in meta.yaml exist on disk."""
        findings: List[AuditFinding] = []
        check_name = "Required Intent File Existence"

        required_files = self._get_known_files_from_meta()

        if not required_files:
            findings.append(
                AuditFinding(
                    severity=AuditSeverity.WARNING,
                    message="meta.yaml is empty or missing; cannot check for required files.",
                    check_name=check_name,
                )
            )
            return findings

        missing_count = 0
        for file_rel_path in sorted(required_files):
            full_path = self.repo_root / file_rel_path
            if not full_path.exists():
                missing_count += 1
                findings.append(
                    AuditFinding(
                        severity=AuditSeverity.ERROR,
                        message=f"Missing constitutionally-required file: '{file_rel_path}'",
                        check_name=check_name,
                    )
                )

        if missing_count == 0:
            findings.append(
                AuditFinding(
                    severity=AuditSeverity.SUCCESS,
                    message=f"All {len(required_files)} constitutionally-required files are present.",
                    check_name=check_name,
                )
            )

        return findings

    # CAPABILITY: audit.check.syntax
    def check_syntax(self) -> List[AuditFinding]:
        """Validate syntax of all .intent YAML/JSON files (including proposals)."""
        findings: List[AuditFinding] = []
        check_name = "YAML/JSON Syntax Validity"

        files_to_check = [
            *self.intent_dir.rglob("*.yaml"),
            *self.intent_dir.rglob("*.json"),
        ]

        error_findings = []
        for file_path in files_to_check:
            if not file_path.is_file():
                continue
            try:
                content = file_path.read_text(encoding="utf-8")
                result = validate_code(str(file_path), content, quiet=True)
                if result["status"] == "dirty":
                    for violation in result["violations"]:
                        error_findings.append(
                            AuditFinding(
                                severity=AuditSeverity.ERROR,
                                message=f"Syntax Error: {violation['message']}",
                                check_name=check_name,
                                file_path=str(file_path.relative_to(self.repo_root)),
                            )
                        )
            except UnicodeDecodeError:
                error_findings.append(
                    AuditFinding(
                        severity=AuditSeverity.ERROR,
                        message=f"Unable to read file '{file_path.name}' due to encoding issues",
                        check_name=check_name,
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                )

        if not error_findings:
            findings.append(
                AuditFinding(
                    severity=AuditSeverity.SUCCESS,
                    message=f"Validated syntax for {len(files_to_check)} YAML/JSON files.",
                    check_name=check_name,
                )
            )
        findings.extend(error_findings)
        return findings

    # CAPABILITY: audit.check.orphaned_intent_files
    def check_for_orphaned_intent_files(self) -> List[AuditFinding]:
        """Find .intent files not referenced in meta.yaml."""
        findings: List[AuditFinding] = []
        check_name = "Orphaned Intent Files"
        known_files = self._get_known_files_from_meta()

        if not known_files:
            return findings

        ignore_patterns = {".bak", "proposals", ".example", ".lock"}
        physical_files = {
            str(p.relative_to(self.repo_root)).replace("\\", "/")
            for p in self.intent_dir.rglob("*")
            if p.is_file() and not any(pat in str(p) for pat in ignore_patterns)
        }

        orphaned_files = sorted(physical_files - known_files)

        if orphaned_files:
            for orphan in orphaned_files:
                findings.append(
                    AuditFinding(
                        severity=AuditSeverity.WARNING,
                        message=f"Orphaned intent file: '{orphan}' is not a recognized system file.",
                        check_name=check_name,
                    )
                )
        else:
            findings.append(
                AuditFinding(
                    severity=AuditSeverity.SUCCESS,
                    message="No orphaned or unrecognized intent files found.",
                    check_name=check_name,
                )
            )
        return findings

    def _get_known_files_from_meta(self) -> Set[str]:
        """Build a set of known intent files from .intent/meta.yaml."""
        meta_file_path = self.intent_dir / "meta.yaml"
        if not meta_file_path.exists():
            return set()

        try:
            meta_config = self.context.load_config(meta_file_path, "yaml")
        except Exception:
            return set()

        known_files: Set[str] = set()

        # --- START OF FIX ---
        # Handle special-case root-relative paths first.
        ux_config = (
            meta_config.get("operator_experience", {}).get("guard", {}).get("drift", {})
        )
        if evidence_path := ux_config.get("evidence_path"):
            if isinstance(evidence_path, str):
                known_files.add(evidence_path.replace("\\", "/"))
        # --- END OF FIX ---

        def _recursive_find_paths(data):
            """Recursively find all file paths in meta configuration."""
            if isinstance(data, dict):
                # Exclude the special case we just handled from this recursive search.
                if "operator_experience" in data:
                    data_copy = data.copy()
                    del data_copy["operator_experience"]
                    for value in data_copy.values():
                        _recursive_find_paths(value)
                else:
                    for value in data.values():
                        _recursive_find_paths(value)
            elif isinstance(data, list):
                for item in data:
                    _recursive_find_paths(item)
            elif isinstance(data, str) and "." in data and "/" in data:
                # This part now correctly handles only the .intent-relative paths.
                full_path_str = str(Path(".intent") / data).replace("\\", "/")
                known_files.add(full_path_str)

        _recursive_find_paths(meta_config)

        known_files.add(".intent/meta.yaml")
        known_files.add(".intent/project_manifest.yaml")

        schema_dir = self.intent_dir / "schemas"
        if schema_dir.exists():
            for schema_file in schema_dir.glob("*.json"):
                known_files.add(
                    str(schema_file.relative_to(self.repo_root)).replace("\\", "/")
                )

        return known_files

--- END OF FILE ./src/system/governance/checks/file_checks.py ---

--- START OF FILE ./src/system/governance/checks/health_checks.py ---
# src/system/governance/checks/health_checks.py
"""Auditor checks for codebase health, complexity, and atomicity."""

import ast
import statistics

from radon.visitors import ComplexityVisitor

from system.governance.models import AuditFinding, AuditSeverity


class HealthChecks:
    """Container for codebase health constitutional checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        self.context = context
        self.health_policy = self.context.load_config(
            self.context.intent_dir / "policies" / "code_health_policy.yaml"
        )

    def _get_logical_lines_of_code(self, source_code: str) -> int:
        """Calculates the Logical Lines of Code (LLOC), ignoring comments and blank lines."""
        return len(
            [
                line
                for line in source_code.splitlines()
                if line.strip() and not line.strip().startswith("#")
            ]
        )

    # CAPABILITY: audit.check.codebase_health
    def check_codebase_health(self) -> list[AuditFinding]:
        """Measures code complexity and atomicity against defined policies."""
        findings = []
        check_name = "Codebase Health & Atomicity"

        # --- Policy Thresholds ---
        max_complexity = self.health_policy.get("rules", {}).get(
            "max_cognitive_complexity", 15
        )
        std_dev_threshold = self.health_policy.get("rules", {}).get(
            "outlier_standard_deviations", 2.0
        )

        file_llocs = {}
        complexity_violations = []

        # --- Analysis Phase ---
        for symbol in self.context.symbols_list:
            file_path_str = symbol.get("file")
            if not file_path_str or not file_path_str.endswith(".py"):
                continue

            file_path = self.context.repo_root / file_path_str
            if file_path not in file_llocs:  # Analyze each file only once
                try:
                    source_code = file_path.read_text(encoding="utf-8")
                    file_llocs[file_path] = self._get_logical_lines_of_code(source_code)

                    # Analyze complexity for all functions in the file
                    tree = ast.parse(source_code)
                    visitor = ComplexityVisitor.from_ast(tree)
                    for func in visitor.functions:
                        if func.cognitive_complexity > max_complexity:
                            msg = (
                                f"Function '{func.name}' in '{file_path_str}' has a Cognitive "
                                f"Complexity of {func.cognitive_complexity}, exceeding the policy limit of {max_complexity}."
                            )
                            complexity_violations.append(
                                AuditFinding(AuditSeverity.WARNING, msg, check_name)
                            )
                except Exception:
                    continue  # Skip files that can't be parsed

        # --- Statistical Outlier Detection Phase ---
        if len(file_llocs) < 3:  # Need enough data for meaningful stats
            return complexity_violations  # Return any complexity issues found

        lloc_values = list(file_llocs.values())
        average_lloc = statistics.mean(lloc_values)
        std_dev = statistics.stdev(lloc_values)
        outlier_threshold = average_lloc + (std_dev_threshold * std_dev)

        outlier_findings = []
        for path, lloc in file_llocs.items():
            if lloc > outlier_threshold:
                msg = (
                    f"File '{path.relative_to(self.context.repo_root)}' is a complexity outlier "
                    f"({lloc} LLOC vs. project average of {average_lloc:.0f}). "
                    "This may violate the 'separation_of_concerns' principle."
                )
                outlier_findings.append(
                    AuditFinding(AuditSeverity.WARNING, msg, check_name)
                )

        # --- Reporting Phase ---
        if not complexity_violations and not outlier_findings:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "Codebase complexity and atomicity are within healthy limits.",
                    check_name,
                )
            )
        else:
            findings.extend(complexity_violations)
            findings.extend(outlier_findings)

        return findings

--- END OF FILE ./src/system/governance/checks/health_checks.py ---

--- START OF FILE ./src/system/governance/checks/__init__.py ---
[EMPTY FILE]

--- END OF FILE ./src/system/governance/checks/__init__.py ---

--- START OF FILE ./src/system/governance/checks/proposal_checks.py ---
# src/system/governance/checks/proposal_checks.py
"""Auditor checks for proposal formats and drift in .intent/proposals/."""

from __future__ import annotations

import hashlib
import json
from pathlib import Path
from typing import List

import jsonschema
import yaml

from shared.schemas.manifest_validator import load_schema
from system.governance.checks.base import BaseAuditCheck
from system.governance.models import AuditFinding, AuditSeverity


class ProposalChecks(BaseAuditCheck):
    """Container for proposal-related constitutional checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        super().__init__(context)
        self.proposals_dir: Path = self.context.intent_dir / "proposals"
        try:
            self.proposal_schema = load_schema("proposal.schema.json")
        except Exception:
            self.proposal_schema = None

    # --- Private Helper Methods for Single-Responsibility ---

    def _get_proposal_paths(self) -> list[Path]:
        """Return all cr-* proposals (both YAML and JSON)."""
        if not self.proposals_dir.exists():
            return []
        return sorted(
            list(self.proposals_dir.glob("cr-*.yaml"))
            + list(self.proposals_dir.glob("cr-*.yml"))
            + list(self.proposals_dir.glob("cr-*.json"))
        )

    def _load_proposal(self, path: Path) -> dict:
        """Load proposal preserving its format, raising ValueError on failure."""
        try:
            text = path.read_text(encoding="utf-8")
            if path.suffix.lower() == ".json":
                return json.loads(text) or {}
            return yaml.safe_load(text) or {}
        except Exception as e:
            raise ValueError(f"parse error: {e}") from e

    # --- CHANGE 1: Replaced the old v1 token logic with the secure v2 logic ---
    @staticmethod
    def _expected_token_for_proposal(proposal: dict) -> str:
        """
        Produce a deterministic token for approvals bound to the full proposal intent.
        This logic MUST mirror the token generation in `src/system/admin/utils.py`.
        """
        payload = {
            "version": "v2",
            "target_path": proposal.get("target_path"),
            "action": proposal.get("action"),
            "content": proposal.get("content", ""),
        }
        digest = hashlib.sha256()
        digest.update(json.dumps(payload, sort_keys=True).encode("utf-8"))
        return f"core-proposal-v2:{digest.finalize().hex()}"

    def _validate_single_proposal_schema(
        self, path: Path, validator: jsonschema.Draft7Validator
    ) -> List[AuditFinding]:
        """Validates a single proposal file against the JSON schema."""
        findings = []
        rel_path = str(path.relative_to(self.context.repo_root))
        try:
            data = self._load_proposal(path)
            errors = list(validator.iter_errors(data))
            if errors:
                for err in errors:
                    loc = ".".join(str(p) for p in err.absolute_path) or "<root>"
                    findings.append(
                        AuditFinding(
                            AuditSeverity.ERROR,
                            f"{path.name}: {loc} -> {err.message}",
                            "Proposals: Schema Compliance",
                            rel_path,
                        )
                    )
            else:
                findings.append(
                    AuditFinding(
                        AuditSeverity.SUCCESS,
                        f"{path.name} conforms to proposal.schema.json",
                        "Proposals: Schema Compliance",
                        rel_path,
                    )
                )
        except ValueError as e:
            findings.append(
                AuditFinding(
                    AuditSeverity.ERROR,
                    f"{path.name}: {e}",
                    "Proposals: Schema Compliance",
                    rel_path,
                )
            )
        return findings

    def _validate_single_proposal_signatures(self, path: Path) -> List[AuditFinding]:
        """Validates the signatures of a single proposal file for drift."""
        findings = []
        rel_path = str(path.relative_to(self.context.repo_root))
        try:
            data = self._load_proposal(path)
            # --- CHANGE 2: Call the new v2 token generator method ---
            expected_token = self._expected_token_for_proposal(data)
            signatures = data.get("signatures", [])

            if not signatures:
                findings.append(
                    AuditFinding(
                        AuditSeverity.WARNING,
                        f"{path.name}: no signatures present.",
                        "Proposals: Signature ↔ Content Drift",
                        rel_path,
                    )
                )
                return findings

            mismatches = [s for s in signatures if s.get("token") != expected_token]
            if mismatches:
                identities = ", ".join(
                    s.get("identity", "<unknown>") for s in mismatches
                )
                findings.append(
                    AuditFinding(
                        AuditSeverity.WARNING,
                        f"{path.name}: {len(mismatches)} signature(s) do not match current content (likely edited after signing). Identities: {identities}",
                        "Proposals: Signature ↔ Content Drift",
                        rel_path,
                    )
                )
            else:
                findings.append(
                    AuditFinding(
                        AuditSeverity.SUCCESS,
                        f"{path.name}: all signatures match current content.",
                        "Proposals: Signature ↔ Content Drift",
                        rel_path,
                    )
                )

        except ValueError as e:
            findings.append(
                AuditFinding(
                    AuditSeverity.ERROR,
                    f"{path.name}: {e}",
                    "Proposals: Signature ↔ Content Drift",
                    rel_path,
                )
            )
        return findings

    # --- Public Check Methods (Orchestrators) ---

    # CAPABILITY: audit.check.proposals_schema
    def check_proposal_files_match_schema(self) -> list[AuditFinding]:
        """Validate each cr-*.yaml/json proposal against proposal.schema.json."""
        if not self.proposal_schema:
            return [
                AuditFinding(
                    AuditSeverity.ERROR,
                    "Proposal schema file could not be loaded.",
                    "Proposals: Schema Compliance",
                )
            ]

        paths = self._get_proposal_paths()
        if not paths:
            return [
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "No pending proposals found.",
                    "Proposals: Schema Compliance",
                )
            ]

        validator = jsonschema.Draft7Validator(self.proposal_schema)
        all_findings = []
        for path in paths:
            all_findings.extend(self._validate_single_proposal_schema(path, validator))
        return all_findings

    # CAPABILITY: audit.check.proposals_drift
    def check_signatures_match_content(self) -> list[AuditFinding]:
        """Detect content/signature drift for all pending proposals."""
        paths = self._get_proposal_paths()
        if not paths:
            return []

        all_findings = []
        for path in paths:
            all_findings.extend(self._validate_single_proposal_signatures(path))
        return all_findings

    # CAPABILITY: audit.check.proposals_list
    def list_pending_proposals(self) -> list[AuditFinding]:
        """Emit a friendly summary of pending proposals."""
        paths = self._get_proposal_paths()
        if not paths:
            if self.proposals_dir.exists():
                return [
                    AuditFinding(
                        AuditSeverity.SUCCESS,
                        "No pending proposals.",
                        "Proposals: Pending Summary",
                    )
                ]
            return []

        return [
            AuditFinding(
                AuditSeverity.WARNING,
                f"Pending proposal: {path.name}",
                "Proposals: Pending Summary",
                str(path.relative_to(self.context.repo_root)),
            )
            for path in paths
        ]

--- END OF FILE ./src/system/governance/checks/proposal_checks.py ---

--- START OF FILE ./src/system/governance/checks/quality_checks.py ---
# src/system/governance/checks/quality_checks.py
"""Auditor checks related to code quality and conventions."""

from system.governance.checks.base import BaseAuditCheck
from system.governance.models import AuditFinding, AuditSeverity


class QualityChecks(BaseAuditCheck):
    """Container for code quality constitutional checks."""

    # The __init__ method has been removed.

    # CAPABILITY: audit.check.docstrings
    def check_docstrings_and_intents(self) -> list[AuditFinding]:
        """Finds symbols missing docstrings or having generic intents."""
        findings = []
        check_name = "Docstring & Intent Presence"
        warnings_found = False
        for entry in self.context.symbols_list:
            if entry.get("type") != "ClassDef" and not entry.get("docstring"):
                warnings_found = True
                findings.append(
                    AuditFinding(
                        AuditSeverity.WARNING,
                        f"Missing Docstring in '{entry.get('file')}': Symbol '{entry.get('name')}'",
                        check_name,
                    )
                )
            if "Provides functionality for the" in entry.get("intent", ""):
                warnings_found = True
                findings.append(
                    AuditFinding(
                        AuditSeverity.WARNING,
                        f"Generic Intent in '{entry.get('file')}': Symbol '{entry.get('name')}' has a weak intent statement.",
                        check_name,
                    )
                )
        if not warnings_found:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "All symbols have docstrings and specific intents.",
                    check_name,
                )
            )
        return findings

    # CAPABILITY: audit.check.dead_code
    def check_for_dead_code(self) -> list[AuditFinding]:
        """Detects unreferenced public symbols."""
        findings = []
        check_name = "Dead Code (Unreferenced Symbols)"
        all_called_symbols = set()
        for symbol in self.context.symbols_list:
            all_called_symbols.update(symbol.get("calls", []))

        warnings_found = False
        for symbol in self.context.symbols_list:
            name = symbol["name"]
            if name.startswith(("_", "test_")):
                continue
            if name in all_called_symbols:
                continue
            if symbol.get("entry_point_type"):
                continue
            warnings_found = True
            findings.append(
                AuditFinding(
                    AuditSeverity.WARNING,
                    f"Potentially dead code: Symbol '{name}' in '{symbol['file']}' is unreferenced.",
                    check_name,
                )
            )

        if not warnings_found:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "No unreferenced public symbols found.",
                    check_name,
                )
            )
        return findings

--- END OF FILE ./src/system/governance/checks/quality_checks.py ---

--- START OF FILE ./src/system/governance/checks/security_checks.py ---
# src/system/governance/checks/security_checks.py
"""Auditor checks for security-related policies."""
import re
from pathlib import Path

from system.governance.models import AuditFinding, AuditSeverity


class SecurityChecks:
    """Container for security-related constitutional checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        self.context = context
        self.secrets_policy = self.context.load_config(
            self.context.intent_dir / "policies" / "secrets_management.yaml"
        )

    # CAPABILITY: audit.check.secrets
    def check_for_hardcoded_secrets(self) -> list[AuditFinding]:
        """Scans source code for patterns that look like hardcoded secrets."""
        findings = []
        check_name = "Security: No Hardcoded Secrets"
        rule = next(
            (
                r
                for r in self.secrets_policy.get("rules", [])
                if r["id"] == "no_hardcoded_secrets"
            ),
            None,
        )

        if not rule:
            findings.append(
                AuditFinding(
                    AuditSeverity.WARNING,
                    "Secrets management policy not defined.",
                    check_name,
                )
            )
            return findings

        patterns = rule.get("detection", {}).get("patterns", [])
        exclude_globs = rule.get("detection", {}).get("exclude", [])
        compiled_patterns = [re.compile(p) for p in patterns]

        files_to_scan = [
            Path(s["file"]) for s in self.context.symbols_list if s.get("file")
        ]

        scanned_files_count = 0
        violations_found = 0

        for file_path in set(files_to_scan):
            full_path = self.context.repo_root / file_path
            if any(full_path.match(glob) for glob in exclude_globs):
                continue

            scanned_files_count += 1
            try:
                content = full_path.read_text(encoding="utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    for pattern in compiled_patterns:
                        if pattern.search(line):
                            violations_found += 1
                            findings.append(
                                AuditFinding(
                                    AuditSeverity.ERROR,
                                    f"Potential hardcoded secret found in '{file_path}' on line {i}.",
                                    check_name,
                                    str(file_path),
                                )
                            )
            except Exception:
                continue  # Ignore files that can't be read

        if violations_found == 0:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    f"Scanned {scanned_files_count} files; no hardcoded secrets found.",
                    check_name,
                )
            )

        return findings

--- END OF FILE ./src/system/governance/checks/security_checks.py ---

--- START OF FILE ./src/system/governance/checks/structure_checks.py ---
# src/system/governance/checks/structure_checks.py
"""Auditor checks related to the system's declared structure and relationships."""

from shared.schemas.manifest_validator import validate_manifest_entry
from shared.utils.import_scanner import scan_imports_for_file
from system.governance.checks.base import BaseAuditCheck
from system.governance.models import AuditFinding, AuditSeverity


class StructureChecks(BaseAuditCheck):
    """Container for structural constitutional checks."""

    # CAPABILITY: audit.check.project_manifest
    def check_project_manifest(self) -> list[AuditFinding]:
        """Validates the integrity of project_manifest.yaml."""
        findings = []
        check_name = "Project Manifest Integrity"
        required_keys = ["name", "intent", "required_capabilities", "active_agents"]
        errors_found = False
        for key in required_keys:
            if key not in self.context.project_manifest:
                errors_found = True
                findings.append(
                    AuditFinding(
                        AuditSeverity.ERROR,
                        f"project_manifest.yaml missing required key: '{key}'",
                        check_name,
                    )
                )
        if not errors_found:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "project_manifest.yaml contains all required keys.",
                    check_name,
                )
            )
        return findings

    # CAPABILITY: audit.check.capability_coverage
    def check_capability_coverage(self) -> list[AuditFinding]:
        """Ensures all required capabilities are implemented."""
        findings = []
        check_name = "Capability Coverage"
        required_caps = set(
            self.context.project_manifest.get("required_capabilities", [])
        )
        implemented_caps = {
            f.get("capability")
            for f in self.context.symbols_list
            if f.get("capability") != "unassigned"
        }
        missing = sorted(list(required_caps - implemented_caps))

        for cap in missing:
            findings.append(
                AuditFinding(
                    AuditSeverity.ERROR,
                    f"Missing capability implementation for: {cap}",
                    check_name,
                )
            )

        if not missing:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "All required capabilities are implemented.",
                    check_name,
                )
            )
        return findings

    # CAPABILITY: audit.check.capability_definitions
    def check_capability_definitions(self) -> list[AuditFinding]:
        """Ensures all implemented capabilities are valid."""
        findings = []
        check_name = "Capability Definitions"
        capability_tags_path = (
            self.context.intent_dir / "knowledge" / "capability_tags.yaml"
        )
        defined_tags_data = self.context.load_config(capability_tags_path, "yaml")
        defined_tags = {tag["name"] for tag in defined_tags_data.get("tags", [])}

        implemented_caps = {
            f.get("capability")
            for f in self.context.symbols_list
            if f.get("capability") != "unassigned"
        }

        undefined = sorted(list(implemented_caps - defined_tags))
        for cap in undefined:
            findings.append(
                AuditFinding(
                    AuditSeverity.ERROR,
                    f"Unconstitutional capability: '{cap}' is implemented in the code but not defined in capability_tags.yaml.",
                    check_name,
                )
            )

        if not undefined:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "All implemented capabilities are constitutionally defined.",
                    check_name,
                )
            )
        return findings

    # CAPABILITY: audit.check.knowledge_graph_schema
    def check_knowledge_graph_schema(self) -> list[AuditFinding]:
        """Validates all knowledge graph symbols against the schema."""
        findings = []
        check_name = "Knowledge Graph Schema Compliance"
        error_count = 0
        for key, entry in self.context.symbols_map.items():
            is_valid, validation_errors = validate_manifest_entry(
                entry, "knowledge_graph_entry.schema.json"
            )
            if not is_valid:
                error_count += 1
                for err in validation_errors:
                    findings.append(
                        AuditFinding(
                            AuditSeverity.ERROR,
                            f"Knowledge Graph entry '{key}' schema error: {err}",
                            check_name,
                        )
                    )
        if error_count == 0:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    f"All {len(self.context.symbols_map)} symbols in knowledge graph pass schema validation.",
                    check_name,
                )
            )
        return findings

    # --- THIS IS THE FIX ---
    # Restore the missing capability tag.
    # CAPABILITY: audit.check.domain_integrity
    def check_domain_integrity(self) -> list[AuditFinding]:
        """Checks for domain mismatches and illegal imports."""
        findings = []
        check_name = "Domain Integrity (Location & Imports)"
        errors_found = False
        for entry in self.context.symbols_list:
            file_path = self.context.repo_root / entry.get("file", "")
            if not file_path.exists():
                findings.append(
                    AuditFinding(
                        AuditSeverity.WARNING,
                        f"File '{entry.get('file')}' from knowledge graph not found on disk.",
                        check_name,
                    )
                )
                continue
            declared_domain = entry.get("domain")
            actual_domain = self.context.intent_model.resolve_domain_for_path(
                file_path.relative_to(self.context.repo_root)
            )
            if declared_domain != actual_domain:
                errors_found = True
                findings.append(
                    AuditFinding(
                        AuditSeverity.ERROR,
                        f"Domain Mismatch for '{entry.get('key')}': Declared='{declared_domain}', Actual='{actual_domain}'",
                        check_name,
                    )
                )

            allowed = set(
                self.context.intent_model.get_domain_permissions(actual_domain)
            ) | {actual_domain}
            imports = scan_imports_for_file(file_path)
            for imp in imports:
                if imp.startswith("src."):
                    imp = imp[4:]
                if imp.startswith(("core.", "shared.", "system.", "agents.")):
                    imp_path_parts = imp.split(".")
                    potential_path = self.context.src_dir.joinpath(*imp_path_parts)
                    check_path = potential_path.with_suffix(".py")
                    if not check_path.exists():
                        check_path = potential_path
                    imp_domain = self.context.intent_model.resolve_domain_for_path(
                        check_path
                    )
                    if imp_domain and imp_domain not in allowed:
                        errors_found = True
                        findings.append(
                            AuditFinding(
                                AuditSeverity.ERROR,
                                f"Forbidden Import in '{entry.get('file')}': Domain '{actual_domain}' cannot import '{imp}' from forbidden domain '{imp_domain}'",
                                check_name,
                            )
                        )
        if not errors_found:
            findings.append(
                AuditFinding(
                    AuditSeverity.SUCCESS,
                    "Domain locations and import boundaries are valid.",
                    check_name,
                )
            )
        return findings

--- END OF FILE ./src/system/governance/checks/structure_checks.py ---

--- START OF FILE ./src/system/governance/constitutional_auditor.py ---
# src/system/governance/constitutional_auditor.py
"""
CORE Constitutional Auditor Orchestrator
=======================================
Discovers and runs modular checks to validate the system's integrity.
"""
import importlib
import inspect
import io
import sys
from pathlib import Path
from typing import Callable, List, Optional, Tuple

from dotenv import load_dotenv
from rich.console import Console
from rich.panel import Panel

from core.intent_model import IntentModel
from shared.config_loader import load_config
from shared.logger import getLogger
from shared.path_utils import get_repo_root
from shared.utils.manifest_aggregator import aggregate_manifests
from system.governance.models import AuditFinding, AuditSeverity

log = getLogger(__name__)


# CAPABILITY: alignment_checking
class ConstitutionalAuditor:
    """Orchestrates the discovery and execution of constitutional checks."""

    class _LoggingBridge(io.StringIO):
        """Redirects console output to the logger."""

        def write(self, s: str) -> None:
            """Redirects writes to the logger info stream."""
            if cleaned_s := s.strip():
                log.info(cleaned_s)

    def __init__(self, repo_root_override: Optional[Path] = None):
        """
        Initialize the auditor, loading configuration and knowledge files.

        Args:
            repo_root_override: If provided, use this directory as the repo root (used for canary validation).
        """
        self.repo_root: Path = repo_root_override or get_repo_root()
        self.console = Console(
            file=self._LoggingBridge(), force_terminal=True, color_system="auto"
        )
        self.context = self.AuditorContext(self.repo_root)
        self.findings: List[AuditFinding] = []
        self.checks: List[Tuple[str, Callable[[], List[AuditFinding]]]] = []

        if repo_root_override:
            dotenv_path = self.repo_root / ".env"
            if dotenv_path.exists():
                load_dotenv(dotenv_path=dotenv_path, override=True)
                log.info(f"Loaded environment from {dotenv_path} for canary validation")

        self.checks = self._discover_checks()

    class AuditorContext:
        """Shared state container for audit checks."""

        def __init__(self, repo_root: Path):
            """Initialize context with repository paths and configurations."""
            self.repo_root: Path = repo_root
            self.intent_dir: Path = repo_root / ".intent"
            self.src_dir: Path = repo_root / "src"
            self.intent_model = IntentModel(repo_root)
            self.project_manifest = aggregate_manifests(repo_root)
            self.knowledge_graph = load_config(
                self.intent_dir / "knowledge/knowledge_graph.json", "json"
            )
            self.symbols_map: dict = self.knowledge_graph.get("symbols", {})
            self.symbols_list: list = list(self.symbols_map.values())
            self.load_config = load_config

    def _discover_checks(self) -> List[Tuple[str, Callable[[], List[AuditFinding]]]]:
        """Discover check methods from modules in the 'checks' directory."""
        discovered_checks: List[Tuple[str, Callable[[], List[AuditFinding]]]] = []
        checks_dir = Path(__file__).parent / "checks"

        for check_file in checks_dir.glob("*.py"):
            if check_file.name.startswith("__"):
                continue

            module_name = f"system.governance.checks.{check_file.stem}"
            try:
                module = importlib.import_module(module_name)
                for class_name, class_obj in inspect.getmembers(
                    module, inspect.isclass
                ):
                    if not class_name.endswith("Checks"):
                        continue

                    check_instance = class_obj(self.context)
                    for method_name, method in inspect.getmembers(
                        check_instance, inspect.ismethod
                    ):
                        if method_name.startswith("_"):
                            continue

                        # --- THIS UNCONSTITUTIONAL SHORTCUT HAS BEEN REMOVED ---
                        # The new declarative pattern in the constitution
                        # makes this check pass cleanly, so the exception is
                        # no longer needed. The system is now fully aligned.

                        symbol_key = f"src/system/governance/checks/{check_file.name}::{method_name}"
                        symbol_data = self.context.symbols_map.get(symbol_key, {})
                        if symbol_data.get("capability", "").startswith("audit.check."):
                            check_name = symbol_data.get("intent", method_name)
                            discovered_checks.append((check_name, method))
            except ImportError as e:
                log.error(
                    f"Failed to import check module {module_name}: {e}", exc_info=True
                )

        log.debug(f"Discovered {len(discovered_checks)} audit checks")
        discovered_checks.sort(key=lambda item: item[0].split(":")[0])
        return discovered_checks

    def run_full_audit(self) -> bool:
        """Run all discovered validation checks and return overall status."""
        self.console.print(
            Panel(
                "🧠 CORE Constitutional Integrity Audit",
                style="bold blue",
                expand=False,
            )
        )

        for check_name, check_fn in self.checks:
            short_name = check_name.split(":")[0]
            log.info(f"🔍 Running Check: {short_name}")
            try:
                findings = check_fn()
                if findings:
                    self.findings.extend(findings)
                    for finding in findings:
                        match finding.severity:
                            case AuditSeverity.ERROR:
                                log.error(f"❌ {finding.message}")
                            case AuditSeverity.WARNING:
                                log.warning(f"⚠️ {finding.message}")
                            case AuditSeverity.SUCCESS:
                                log.info(f"✅ {finding.message}")
            except Exception as e:
                log.error(
                    f"💥 Check '{check_name}' failed unexpectedly: {e}", exc_info=True
                )
                self.findings.append(
                    AuditFinding(
                        severity=AuditSeverity.ERROR,
                        message=f"Check failed: {e}",
                        check_name=check_name,
                    )
                )

        all_passed = not any(f.severity == AuditSeverity.ERROR for f in self.findings)
        self._report_final_status(all_passed)
        return all_passed

    def _report_final_status(self, passed: bool) -> None:
        """Print final audit summary to the console."""
        errors = sum(1 for f in self.findings if f.severity == AuditSeverity.ERROR)
        warnings = sum(1 for f in self.findings if f.severity == AuditSeverity.WARNING)

        if passed:
            msg = f"✅ ALL CHECKS PASSED ({warnings} warnings)"
            style = "bold green"
        else:
            msg = f"❌ AUDIT FAILED: {errors} error(s) and {warnings} warning(s) found"
            style = "bold red"

        self.console.print(Panel(msg, style=style, expand=False))


def main() -> None:
    """CLI entry point for the Constitutional Auditor."""
    load_dotenv()
    auditor = ConstitutionalAuditor()
    try:
        success = auditor.run_full_audit()
        sys.exit(0 if success else 1)
    except FileNotFoundError as e:
        log.error(
            f"Required file not found: {e}. Try running the introspection cycle.",
            exc_info=True,
        )
        sys.exit(1)
    except Exception as e:
        log.error(f"Unexpected error during audit: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()

--- END OF FILE ./src/system/governance/constitutional_auditor.py ---

--- START OF FILE ./src/system/governance/__init__.py ---
[EMPTY FILE]

--- END OF FILE ./src/system/governance/__init__.py ---

--- START OF FILE ./src/system/governance/models.py ---
# src/system/governance/models.py
"""
Data models for the Constitutional Auditor.
"""
from dataclasses import dataclass
from enum import Enum
from typing import Optional


class AuditSeverity(Enum):
    """Severity levels for audit findings."""

    ERROR = "error"
    WARNING = "warning"
    SUCCESS = "success"


@dataclass
class AuditFinding:
    """Represents a single audit finding."""

    severity: AuditSeverity
    message: str
    check_name: str
    file_path: Optional[str] = None

--- END OF FILE ./src/system/governance/models.py ---

--- START OF FILE ./src/system/guard/capability_discovery.py ---
# src/system/guard/capability_discovery.py
"""
Intent: Orchestrates the discovery of capabilities from all available sources,
respecting the principle of precedence (live analysis > source scan).
"""
from __future__ import annotations

from pathlib import Path
from typing import Dict, List, Optional

from system.guard.discovery import from_kgb, from_manifest, from_source_scan
from system.guard.models import CapabilityMeta
from system.tools.domain_mapper import DomainMapper


def collect_code_capabilities(
    root: Path,
    include_globs: Optional[List[str]] = None,
    exclude_globs: Optional[List[str]] = None,
    require_kgb: bool = False,
) -> Dict[str, CapabilityMeta]:
    """
    Unified discovery entrypoint that tries the live KnowledgeGraphBuilder first,
    then falls back to a direct source scan.
    """
    if require_kgb:
        caps = from_kgb.collect_from_kgb(root)
        if not caps:
            raise RuntimeError(
                "Strict intent mode: No capabilities found from KnowledgeGraphBuilder."
            )
        return caps

    # Primary Method (non-strict): Use the Knowledge Graph Builder
    caps = from_kgb.collect_from_kgb(root)
    if caps:
        return caps

    # Fallback Method: If KGB finds nothing, scan the source directly.
    domain_mapper = DomainMapper(root)
    include = include_globs or []
    exclude = exclude_globs or ["**/.git/**", "**/.venv/**", "**/__pycache__/**"]
    return from_source_scan.collect_from_source_scan(
        root, include, exclude, domain_mapper=domain_mapper
    )


def load_manifest_capabilities(
    root: Path, explicit_path: Optional[Path] = None
) -> Dict[str, CapabilityMeta]:
    """Loads, parses, and normalizes capabilities from the project's manifest."""
    return from_manifest.load_manifest_capabilities(root, explicit_path)

--- END OF FILE ./src/system/guard/capability_discovery.py ---

--- START OF FILE ./src/system/guard/discovery/from_kgb.py ---
# src/system/guard/discovery/from_kgb.py
"""
Intent: Provides a focused tool for discovering capabilities by running the
live KnowledgeGraphBuilder.
"""
from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, Optional

from system.guard.models import CapabilityMeta
from system.tools.codegraph_builder import KnowledgeGraphBuilder


def _extract_cap_meta_from_node(node: Dict[str, Any]) -> Optional[CapabilityMeta]:
    """Extracts capability metadata from a Knowledge Graph node."""
    cap = node.get("capability")
    if cap and cap != "unassigned":
        return CapabilityMeta(
            capability=str(cap),
            domain=str(node.get("domain")) if node.get("domain") else None,
            owner=str(node.get("agent")) if node.get("agent") else None,
        )
    return None


def collect_from_kgb(root: Path) -> Dict[str, CapabilityMeta]:
    """Uses KnowledgeGraphBuilder (if present) to discover capabilities from the repo."""
    try:
        builder = KnowledgeGraphBuilder(root_path=root)
        graph = builder.build()
        caps: Dict[str, CapabilityMeta] = {}
        if isinstance(graph, dict):
            symbols = graph.get("symbols", {})
            for node in symbols.values():
                if isinstance(node, dict):
                    meta = _extract_cap_meta_from_node(node)
                    if meta:
                        caps[meta.capability] = meta
        return caps
    except Exception:
        # If KGB fails for any reason (e.g., missing constitution), fall back gracefully.
        return {}

--- END OF FILE ./src/system/guard/discovery/from_kgb.py ---

--- START OF FILE ./src/system/guard/discovery/from_manifest.py ---
# src/system/guard/discovery/from_manifest.py
"""
Intent: Provides a focused tool for discovering capabilities from manifest files.
This version is updated to support the modular manifest architecture.
"""
from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, Optional

try:
    import yaml
except ImportError:
    yaml = None

from system.guard.models import CapabilityMeta


def _normalize_cap_list(items: Any) -> Dict[str, CapabilityMeta]:
    """Normalizes various list/dict shapes into a standard {cap: Meta} dictionary."""
    out: Dict[str, CapabilityMeta] = {}
    if isinstance(items, list):
        for it in items:
            if isinstance(it, str):
                out[it] = CapabilityMeta(it)
    return out


def _find_all_manifests(start: Path) -> list[Path]:
    """Locates all manifest.yaml files within the src directory."""
    src_path = start / "src"
    if not src_path.is_dir():
        return []
    return sorted(list(src_path.glob("**/manifest.yaml")))


def load_manifest_capabilities(
    root: Path, explicit_path: Optional[Path] = None
) -> Dict[str, CapabilityMeta]:
    """
    Loads, parses, and aggregates capabilities from all domain-specific manifests.
    """
    if yaml is None:
        raise RuntimeError("PyYAML is required to load manifests.")

    all_caps: Dict[str, CapabilityMeta] = {}
    manifest_paths = _find_all_manifests(root)

    for path in manifest_paths:
        try:
            with path.open("r", encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}

            # Extract capabilities and associate them with their domain
            domain = data.get("domain", "unknown")
            caps_list = data.get("capabilities", [])
            normalized_caps = _normalize_cap_list(caps_list)

            for cap, meta in normalized_caps.items():
                if cap not in all_caps:
                    all_caps[cap] = CapabilityMeta(capability=cap, domain=domain)
        except Exception:
            # Ignore files that fail to parse
            continue

    return all_caps

--- END OF FILE ./src/system/guard/discovery/from_manifest.py ---

--- START OF FILE ./src/system/guard/discovery/from_source_scan.py ---
# src/system/guard/discovery/from_source_scan.py
from __future__ import annotations

import re
from pathlib import Path
from typing import Dict, Iterable, List, Optional

from system.guard.models import CapabilityMeta
from system.tools.domain_mapper import DomainMapper


def _parse_inline_meta(trailing: str) -> Dict[str, str]:
    """Parse inline [key=value] metadata from trailing text."""
    kv = {}
    if not trailing.strip():
        return kv
    # Updated regex to handle comma-separated pairs inside brackets
    pattern = r"([A-Za-z0-9_.\-:/]+)\s*=\s*([^,\]]+)"
    for key, value in re.findall(pattern, trailing):
        kv[key.strip()] = value.strip()
    return kv


def _iter_source_files(
    root: Path, include_globs: List[str], exclude_globs: List[str]
) -> Iterable[Path]:
    """Yields repository files to be scanned."""

    def wanted(p: Path) -> bool:
        """Return True if the path matches include_globs (if specified) or has a .py suffix, and does not match exclude_globs."""
        if any((p.match(g) for g in exclude_globs)):
            return False
        if include_globs:
            return any((p.match(g) for g in include_globs))
        return p.suffix in {".py"}

    for p in root.rglob("*"):
        if p.is_file() and wanted(p):
            yield p


def collect_from_source_scan(
    root: Path,
    include_globs: List[str],
    exclude_globs: List[str],
    domain_mapper: Optional[DomainMapper] = None,
) -> Dict[str, CapabilityMeta]:
    """
    Scans for '# CAPABILITY:' tags with optional inline metadata.
    Now constitution-aware via DomainMapper.
    """
    caps: Dict[str, CapabilityMeta] = {}

    # Create domain mapper if not provided for backward compatibility.
    if domain_mapper is None:
        domain_mapper = DomainMapper(root)

    # Use a regex that can handle both simple and metadata-rich capability tags.
    capability_re = re.compile(r"^\s*#\s*CAPABILITY:\s*([A-Za-z0-9_.\-:/]+)(.*)$")

    for file in _iter_source_files(root, include_globs, exclude_globs):
        try:
            content = file.read_text(encoding="utf-8", errors="ignore")
            for line in content.splitlines():
                m = capability_re.match(line)
                if not m:
                    continue

                cap = m.group(1).strip()
                trailing_text = m.group(2) or ""
                kv = _parse_inline_meta(trailing_text)

                # CRITICAL FIX: Use DomainMapper to determine domain if not provided inline.
                domain = kv.get("domain")
                if domain is None:
                    relative_path = file.relative_to(root)
                    domain = domain_mapper.determine_domain(relative_path)
                    if domain == "unassigned":
                        domain = None  # Standardize "unassigned" to None.

                caps[cap] = CapabilityMeta(
                    capability=cap, domain=domain, owner=kv.get("owner")
                )
        except Exception:
            continue
    return caps

--- END OF FILE ./src/system/guard/discovery/from_source_scan.py ---

--- START OF FILE ./src/system/guard/drift_detector.py ---
# src/system/guard/drift_detector.py
"""
Intent: Compares two sets of capabilities (from manifest and code) to detect
drift and produces a machine-readable report.
"""
from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, List, Optional

from .models import CapabilityMeta, DriftReport


def detect_capability_drift(
    manifest_caps: Dict[str, CapabilityMeta], code_caps: Dict[str, CapabilityMeta]
) -> DriftReport:
    """Computes missing, undeclared, and mismatched capabilities between manifest and code."""
    m_keys = set(manifest_caps.keys())
    c_keys = set(code_caps.keys())

    missing = sorted(list(m_keys - c_keys))
    undeclared = sorted(list(c_keys - m_keys))

    mismatches: List[Dict[str, Dict[str, Optional[str]]]] = []
    for k in sorted(list(m_keys & c_keys)):
        m = manifest_caps[k]
        c = code_caps[k]
        # We only compare the 'domain', as 'owner' is not a field that is
        # declared in the manifest files. This makes the check more robust.
        if m.domain != c.domain:
            mismatches.append(
                {
                    "capability": k,
                    "manifest": {"domain": m.domain, "owner": m.owner},
                    "code": {"domain": c.domain, "owner": c.owner},
                }
            )

    return DriftReport(missing, undeclared, mismatches)


def write_report(report_path: Path, report: DriftReport) -> None:
    """Persists the drift report to disk for evidence and CI."""
    report_path.parent.mkdir(parents=True, exist_ok=True)
    report_path.write_text(json.dumps(report.to_dict(), indent=2), encoding="utf-8")

--- END OF FILE ./src/system/guard/drift_detector.py ---

--- START OF FILE ./src/system/guard/models.py ---
# src/system/guard/models.py
"""
Intent: Provides the shared data models for the governance guard tools,
breaking a potential circular import dependency.
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Optional


@dataclass(frozen=True)
class CapabilityMeta:
    """A minimal, shared data container for capability metadata."""

    capability: str
    domain: Optional[str] = None
    owner: Optional[str] = None


@dataclass
class DriftReport:
    """Structured result for capability drift suitable for JSON emission and CI gating."""

    missing_in_code: List[str]
    undeclared_in_manifest: List[str]
    mismatched_mappings: List[Dict[str, Dict[str, Optional[str]]]]

    def to_dict(self) -> dict:
        """Converts the drift report into a stable JSON-serializable dict."""
        return {
            "missing_in_code": sorted(self.missing_in_code),
            "undeclared_in_manifest": sorted(self.undeclared_in_manifest),
            "mismatched_mappings": self.mismatched_mappings,
        }

--- END OF FILE ./src/system/guard/models.py ---

--- START OF FILE ./src/system/manifest.yaml ---
capabilities:
- alignment_checking
- audit.check.codebase_health
- audit.check.duplication
- audit.check.required_files
- audit.check.secrets
- audit.check.syntax
- audit.check.project_manifest
- audit.check.capability_coverage
- audit.check.capability_definitions
- audit.check.knowledge_graph_schema
- audit.check.domain_integrity
- audit.check.docstrings
- audit.check.dead_code
- audit.check.orphaned_intent_files
- audit.check.environment
- audit.check.proposals_schema
- audit.check.proposals_drift
- audit.check.proposals_list
- scaffold_project
description: Governance tooling, lifecycle setup, CLI utilities
domain: system

--- END OF FILE ./src/system/manifest.yaml ---

--- START OF FILE ./src/system/starter_kits/default/capability_tags.yaml.template ---
# .intent/knowledge/capability_tags.yaml
#
# This is the canonical dictionary of all valid capability tags for this project.
# The CORE ConstitutionalAuditor will verify that any # CAPABILITY tag used in
# the source code is defined in this file.
#
# This file was seeded by CORE's analysis of your repository. You should add
# a 'description' for each capability to clarify its purpose.

tags:
  # - name: your_capability_1
  #   description: "A clear explanation of what this capability does."
  # - name: your_capability_2
  #   description: "Another clear explanation."

--- END OF FILE ./src/system/starter_kits/default/capability_tags.yaml.template ---

--- START OF FILE ./src/system/starter_kits/default/ci.yml.template ---
# Basic CI workflow for a Python project using Poetry
name: CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

permissions:
  contents: read

jobs:
  build_and_test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.11", "3.12"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install Poetry
      run: |
        pipx install poetry

    - name: Install dependencies
      run: |
        poetry install

    - name: Run linter
      run: |
        poetry run ruff check .

    - name: Run formatter check
      run: |
        poetry run black --check .

    - name: Run tests
      run: |
        poetry run pytest

--- END OF FILE ./src/system/starter_kits/default/ci.yml.template ---

--- START OF FILE ./src/system/starter_kits/default/gitignore.template ---
# Python
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.so
.venv/
.env

# CORE-specific artifacts
.intent/knowledge/knowledge_graph.json
reports/
logs/
sandbox/
pending_writes/

# IDE / Editor
.vscode/
.idea/

--- END OF FILE ./src/system/starter_kits/default/gitignore.template ---

--- START OF FILE ./src/system/starter_kits/default/.gitkeep ---
[EMPTY FILE]

--- END OF FILE ./src/system/starter_kits/default/.gitkeep ---

--- START OF FILE ./src/system/starter_kits/default/principles.yaml ---
# .intent/mission/principles.yaml
principles:
  - id: clarity_first
    description: >
      Every function must have a docstring explaining its purpose.
      If a human cannot understand a piece of code in 30 seconds, it must be simplified.
  - id: safe_by_default
    description: >
      Every change must assume rollback or rejection unless explicitly validated.
      No file write or code execution may proceed without confirmation or a safety check.
  - id: no_orphaned_logic
    description: >
      All code must be discoverable and auditable. No function or file should exist
      without being traceable to a manifest or a clear purpose.
  - id: single_source_of_truth
    description: >
      The `.intent/` directory is the single source of truth for the project's
      capabilities, structure, and intent. All governance is derived from these files.

--- END OF FILE ./src/system/starter_kits/default/principles.yaml ---

--- START OF FILE ./src/system/starter_kits/default/project_manifest.yaml ---
# .intent/project_manifest.yaml
name: "new-core-project"
version: "0.1.0"
intent: "A new project, ready to be guided by a clear intent."
required_capabilities: []

--- END OF FILE ./src/system/starter_kits/default/project_manifest.yaml ---

--- START OF FILE ./src/system/starter_kits/default/pyproject.toml.template ---
[tool.poetry]
name = "{project_name}"
version = "0.1.0"
description = "A new project governed by CORE."
authors = ["Your Name <you@example.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.9"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

--- END OF FILE ./src/system/starter_kits/default/pyproject.toml.template ---

--- START OF FILE ./src/system/starter_kits/default/README.md ---
# Welcome to Your New CORE Constitution

This `.intent/` directory is the "Mind" of your new application. It contains the complete, machine-readable definition of your project's goals, rules, and structure. It is the single source of truth that governs the behavior of any AI agents working on this codebase.

This starter kit was generated by CORE to provide a balanced, best-practice foundation for your project.

## Your First Steps

1.  **Review `mission/principles.yaml`**: These are the high-level values of your project. You should edit them to match your own philosophy.
2.  **Review `project_manifest.yaml`**: This file lists the capabilities your application is expected to have. As you add `# CAPABILITY:` tags to your code, this list should grow.
3.  **Run Your First Audit**: Use CORE's tools to run a constitutional audit. This will tell you if your code is in alignment with the rules defined here.

This constitution is now yours. Evolve it, amend it, and use it to guide your project's growth with clarity and purpose.

--- END OF FILE ./src/system/starter_kits/default/README.md ---

--- START OF FILE ./src/system/starter_kits/default/README.md.template ---
# {project_name}

A new project governed by CORE.

--- END OF FILE ./src/system/starter_kits/default/README.md.template ---

--- START OF FILE ./src/system/starter_kits/default/safety_policies.yaml ---
# .intent/policies/safety_policies.yaml
rules:
  - id: no_dangerous_execution
    description: >
      Generated or modified code must not contain calls to dangerous functions
      that enable arbitrary code execution or shell access.
    enforcement: hard
    detection:
      type: substring
      patterns:
        - "eval("
        - "exec("
        - "os.system("
        - "subprocess.run("
        - "subprocess.Popen("
    action: reject
    feedback: "❌ Dangerous execution call detected. This is forbidden by safety policies."
  - id: no_unsafe_imports
    description: >
      Prevent importing modules that enable dangerous operations unless explicitly allowed.
    enforcement: hard
    detection:
      type: import_name
      forbidden:
        - "import pickle"
        - "from subprocess import"
    action: reject
    feedback: "❌ Unsafe import detected. This is forbidden by safety policies."

--- END OF FILE ./src/system/starter_kits/default/safety_policies.yaml ---

--- START OF FILE ./src/system/starter_kits/default/source_structure.yaml ---
# .intent/knowledge/source_structure.yaml
structure:
  - domain: main
    path: src/main
    description: "The primary domain for this application's core logic."
    allowed_imports: [main, shared]
  - domain: shared
    path: src/shared
    description: "Shared utilities and data models."
    allowed_imports: [shared]

--- END OF FILE ./src/system/starter_kits/default/source_structure.yaml ---

--- START OF FILE ./src/system/starter_kits/default/test_main.py.template ---
# A simple starter test for the application.
# The `client` fixture is provided by pytest-fastapi.

# Note: You will need to add `pytest` and `pytest-fastapi` to your
# dev dependencies in pyproject.toml to run this test.
# poetry add --group dev pytest pytest-fastapi

# from fastapi.testclient import TestClient
# from {project_name}.main import app

# client = TestClient(app)


def test_truth():
    """
    This is a placeholder test. It is here to ensure that the test runner
    is correctly configured and can execute. Replace it with real tests.
    """
    assert True is True

# def test_read_main():
#     response = client.get("/")
#     assert response.status_code == 200
#     assert "message" in response.json()

--- END OF FILE ./src/system/starter_kits/default/test_main.py.template ---

--- START OF FILE ./src/system/tools/ast_utils.py ---
# src/system/tools/ast_utils.py
"""
Utilities for parsing and analyzing AST nodes.
"""
import ast
import hashlib
import re
from typing import Dict, List, Optional


def strip_docstrings(node):
    """Recursively remove docstring nodes from an AST tree for structural hashing."""
    if isinstance(
        node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module)
    ):
        if (
            node.body
            and isinstance(node.body[0], ast.Expr)
            and isinstance(node.body[0].value, ast.Constant)
            and isinstance(node.body[0].value.value, str)
        ):
            node.body = node.body[1:]

    for child_node in ast.iter_child_nodes(node):
        strip_docstrings(child_node)
    return node


def calculate_structural_hash(node: ast.AST) -> str:
    """Calculate a hash of the node's structure without docstrings."""
    node_for_hashing = strip_docstrings(ast.parse(ast.unparse(node)))
    structural_string = ast.unparse(node_for_hashing).replace("\n", "").replace(" ", "")
    return hashlib.sha256(structural_string.encode("utf-8")).hexdigest()


def detect_docstring(node: ast.AST) -> Optional[str]:
    """Detects both standard and non-standard docstrings for a node."""
    standard_doc = ast.get_docstring(node)
    if standard_doc:
        return standard_doc

    if (
        node.body
        and isinstance(node.body[0], ast.Expr)
        and isinstance(node.body[0].value, ast.Constant)
        and isinstance(node.body[0].value.value, str)
    ):
        return node.body[0].value.value
    return None


def extract_base_classes(node: ast.ClassDef) -> List[str]:
    """Extract base class names from a class definition."""
    base_classes = []
    for base in node.bases:
        if isinstance(base, ast.Name):
            base_classes.append(base.id)
        elif isinstance(base, ast.Attribute):
            base_classes.append(base.attr)
    return base_classes


def parse_metadata_comment(node: ast.AST, source_lines: List[str]) -> Dict[str, str]:
    """Parses the line immediately preceding a symbol definition for a '# CAPABILITY:' tag."""
    if node.lineno > 1 and node.lineno - 2 < len(source_lines):
        line = source_lines[node.lineno - 2].strip()
        if line.startswith("#"):
            match = re.search(r"CAPABILITY:\s*(\S+)", line, re.IGNORECASE)
            if match:
                return {"capability": match.group(1).strip()}
    return {}


def is_fastapi_assignment(node: ast.AST) -> bool:
    """Check if node is a FastAPI app assignment."""
    return (
        isinstance(node, ast.Assign)
        and isinstance(node.value, ast.Call)
        and isinstance(node.value.func, ast.Name)
        and node.value.func.id == "FastAPI"
        and isinstance(node.targets[0], ast.Name)
    )


def is_main_block(node: ast.AST) -> bool:
    """Check if node is an if __name__ == '__main__' block."""
    return (
        isinstance(node, ast.If)
        and isinstance(node.test, ast.Compare)
        and isinstance(node.test.left, ast.Name)
        and node.test.left.id == "__name__"
        and isinstance(node.test.comparators[0], ast.Constant)
        and node.test.comparators[0].value == "__main__"
    )

--- END OF FILE ./src/system/tools/ast_utils.py ---

--- START OF FILE ./src/system/tools/ast_visitor.py ---
# src/system/tools/ast_visitor.py
"""
Contains specialized AST (Abstract Syntax Tree) visitors for the
KnowledgeGraphBuilder. This module separates the complex logic of tree
traversal from the main orchestration logic of the builder.
"""
import ast
from pathlib import Path
from typing import List


class FunctionCallVisitor(ast.NodeVisitor):
    """An AST visitor that collects the names of all functions being called within a node."""

    def __init__(self):
        self.calls: set[str] = set()

    def visit_Call(self, node: ast.Call):
        """Records function or method calls in `self.calls` and recursively visits child nodes."""
        if isinstance(node.func, ast.Name):
            self.calls.add(node.func.id)
        elif isinstance(node.func, ast.Attribute):
            self.calls.add(node.func.attr)
        self.generic_visit(node)


class ContextAwareVisitor(ast.NodeVisitor):
    """A stateful AST visitor that understands nested class and function contexts."""

    def __init__(self, builder, filepath: Path, source_lines: List[str]):
        """Initialize the instance with the given builder, filepath, source lines, and an empty context stack."""
        self.builder = builder
        self.filepath = filepath
        self.source_lines = source_lines
        self.context_stack: List[str] = []

    def _process_and_visit(self, node, node_type: str):
        """Helper to process a symbol and manage the context stack."""
        parent_key = self.context_stack[-1] if self.context_stack else None

        is_method = False
        if parent_key and parent_key in self.builder.functions:
            if self.builder.functions[parent_key].is_class:
                is_method = True

        symbol_key = self.builder._process_symbol_node(
            node, self.filepath, self.source_lines, parent_key if is_method else None
        )

        if symbol_key:
            self.context_stack.append(symbol_key)
            self.generic_visit(node)
            self.context_stack.pop()
        else:
            self.generic_visit(node)

    def visit_ClassDef(self, node: ast.ClassDef):
        """Processes a class definition node, and visits its children."""
        self._process_and_visit(node, "ClassDef")

    def visit_FunctionDef(self, node: ast.FunctionDef):
        """Processes a function definition node, and visits its children."""
        self._process_and_visit(node, "FunctionDef")

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
        """Processes an async function definition node, and visits its children."""
        self._process_and_visit(node, "AsyncFunctionDef")

--- END OF FILE ./src/system/tools/ast_visitor.py ---

--- START OF FILE ./src/system/tools/change_log_updater.py ---
# src/system/tools/change_log_updater.py

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List

from shared.config_loader import load_config
from shared.logger import getLogger

log = getLogger(__name__)

CHANGE_LOG_PATH = Path(".intent/knowledge/meta_code_change_log.json")
SCHEMA_VERSION = "1.0.0"


def load_existing_log() -> Dict:
    """Loads the existing change log from disk or returns a new structure."""
    data = load_config(CHANGE_LOG_PATH, "json")
    if not data:
        return {"schema_version": SCHEMA_VERSION, "changes": []}
    return data


def append_change_entry(
    task: str,
    step: str,
    modified_files: List[str],
    score: float,
    violations: List[Dict],
):
    """Appends a new, structured entry to the metacode change log."""
    log_data = load_existing_log()
    timestamp = datetime.utcnow().isoformat() + "Z"

    log_data["changes"].append(
        {
            "timestamp": timestamp,
            "task": task,
            "step": step,
            "modified_files": modified_files,
            "score": score,
            "violations": violations,
            "source": "orchestrator",
        }
    )

    CHANGE_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
    CHANGE_LOG_PATH.write_text(json.dumps(log_data, indent=2), encoding="utf-8")
    log.info(f"Appended change log entry at {timestamp}.")


if __name__ == "__main__":
    # Example usage for testing
    append_change_entry(
        task="Add intent guard integration",
        step="Check manifest before file write",
        modified_files=["src/core/cli.py", "src/core/intent_guard.py"],
        score=0.85,
        violations=[],
    )

--- END OF FILE ./src/system/tools/change_log_updater.py ---

--- START OF FILE ./src/system/tools/codegraph_builder.py ---
# src/system/tools/codegraph_builder.py
"""
Main knowledge graph builder orchestrating all components.
"""
from pathlib import Path
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv

from shared.config_loader import load_config
from shared.logger import getLogger
from system.tools.domain_mapper import DomainMapper
from system.tools.entry_point_detector import EntryPointDetector
from system.tools.file_scanner import FileScanner
from system.tools.graph_serializer import GraphSerializer
from system.tools.pattern_matcher import PatternMatcher
from system.tools.project_structure import (
    ProjectStructureError,
    find_project_root,
    get_cli_entry_points,
    get_python_files,
)

log = getLogger(__name__)


class KnowledgeGraphBuilder:
    """Builds a comprehensive JSON representation of the project's code structure and relationships."""

    def __init__(self, root_path: Path, exclude_patterns: Optional[List[str]] = None):
        """Initializes the builder, loading patterns and project configuration."""
        self.root_path = root_path.resolve()
        self.src_root = self.root_path / "src"
        self.exclude_patterns = exclude_patterns or [
            "venv",
            ".venv",
            "__pycache__",
            ".git",
            "tests",
            "work",
        ]
        self.files_scanned = 0
        self.files_failed = 0

        # Initialize components
        cli_entry_points = get_cli_entry_points(self.root_path)
        self.domain_mapper = DomainMapper(self.root_path)
        self.entry_point_detector = EntryPointDetector(self.root_path, cli_entry_points)
        self.file_scanner = FileScanner(self.domain_mapper, self.entry_point_detector)
        self.pattern_matcher = PatternMatcher(self._load_patterns(), self.root_path)

    def _load_patterns(self) -> List[Dict]:
        """Loads entry point detection patterns from the intent file."""
        patterns_path = self.root_path / ".intent/knowledge/entry_point_patterns.yaml"
        if not patterns_path.exists():
            log.warning("entry_point_patterns.yaml not found.")
            return []
        config = load_config(patterns_path, "yaml")
        return config.get("patterns", []) if config else []

    def build(self) -> Dict[str, Any]:
        """Orchestrates the full knowledge graph generation process."""
        log.info(f"Building knowledge graph for directory: {self.src_root}")

        py_files = get_python_files(self.src_root, self.exclude_patterns)
        log.info(f"Found {len(py_files)} Python files to scan in src/")

        # Scan all files
        for pyfile in py_files:
            if self.file_scanner.scan_file(pyfile):
                self.files_scanned += 1
            else:
                self.files_failed += 1

        log.info(
            f"Scanned {self.files_scanned} files ({self.files_failed} failed). "
            f"Applying declarative patterns..."
        )

        # Apply patterns to enhance the function data
        self.pattern_matcher.apply_patterns(self.file_scanner.functions)

        # Build and return the serialized graph
        return GraphSerializer.build_graph_data(
            self.file_scanner.functions, self.files_scanned
        )


def main():
    """CLI entry point to run the knowledge graph builder and save the output."""
    load_dotenv()
    try:
        # Find project root robustly
        try:
            root = find_project_root(Path.cwd())
        except ProjectStructureError:
            log.warning(
                "Could not find pyproject.toml, using current directory as root. "
                "This may cause issues."
            )
            root = Path.cwd()

        # Build the knowledge graph
        builder = KnowledgeGraphBuilder(root)
        graph = builder.build()

        # Save to file
        out_path = root / ".intent/knowledge/knowledge_graph.json"
        GraphSerializer.save_to_file(graph, out_path)

    except Exception as e:
        log.error(f"An error occurred: {e}", exc_info=True)


if __name__ == "__main__":
    main()

--- END OF FILE ./src/system/tools/codegraph_builder.py ---

--- START OF FILE ./src/system/tools/docstring_adder.py ---
# src/system/tools/docstring_adder.py
"""
A tool that finds and adds missing docstrings to the codebase, fulfilling
the 'clarity_first' principle. This is a core capability for CORE's
self-healing and self-improvement loop.
"""
import ast
import asyncio
import json
from typing import Any, Dict

import typer
from rich.progress import track

from core.clients import GeneratorClient
from core.validation_pipeline import validate_code
from shared.logger import getLogger
from shared.path_utils import get_repo_root
from system.tools.codegraph_builder import KnowledgeGraphBuilder

try:
    from rich.progress import track  # nice-to-have progress
except Exception:  # pragma: no cover

    def track(iterable, description=None):
        """Iterate over an iterable, optionally with a description (no-op, yields items unchanged)."""
        # minimal fallback: just pass items through
        for item in iterable:
            yield item


# --- Constants & Setup ---
log = getLogger("docstring_adder")
REPO_ROOT = get_repo_root()
KNOWLEDGE_GRAPH_PATH = REPO_ROOT / ".intent" / "knowledge" / "knowledge_graph.json"
CONCURRENCY_LIMIT = 10


def add_docstring_to_function_line_based(
    source_code: str, line_number: int, docstring: str
) -> str:
    """Surgically inserts a docstring into source code using a line-based method."""
    lines = source_code.splitlines()
    if not lines or line_number < 1 or line_number > len(lines):
        log.error(f"Invalid line number {line_number} for source code")
        return source_code

    target_line_index = line_number - 1
    target_line = lines[target_line_index]
    indentation = len(target_line) - len(target_line.lstrip())
    docstring_indent = " " * (indentation + 4)

    # Sanitize the docstring to prevent breaking the code structure
    docstring = docstring.strip().replace('"""', "'")
    if not docstring:
        log.warning("Empty docstring received")
        return source_code

    formatted_docstring = f'{docstring_indent}"""{docstring}"""'

    if target_line_index + 1 < len(lines) and '"""' in lines[target_line_index + 1]:
        log.warning(f"Docstring already exists at line {line_number}")
        return source_code

    lines.insert(target_line_index + 1, formatted_docstring)
    return "\n".join(lines)


async def generate_and_apply_docstring(
    target: Dict[str, Any], generator: GeneratorClient, dry_run: bool
) -> None:
    """Generates, validates, and applies a docstring for a single symbol."""
    func_name = target.get("name")

    file_rel_path = target.get("file")
    if not isinstance(file_rel_path, str):
        log.error(
            f"Invalid KG entry for '{func_name}': 'file' key is not a string. Entry: {target}"
        )
        return

    file_path = REPO_ROOT / file_rel_path
    line_num = target.get("line_number")

    try:
        if not file_path.exists():
            log.error(f"File not found: {file_path}")
            return

        source_code = file_path.read_text(encoding="utf-8")
        tree = ast.parse(source_code)

        node_to_update = next(
            (
                node
                for node in ast.walk(tree)
                if isinstance(
                    node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                )
                and node.name == func_name
                and node.lineno == line_num
            ),
            None,
        )

        if not node_to_update:
            log.warning(
                f"Could not precisely locate symbol `{func_name}` at line {line_num} in {file_path}. Skipping."
            )
            return

        if ast.get_docstring(node_to_update):
            log.info(f"Symbol `{func_name}` already has a docstring. Skipping.")
            return

        function_source = ast.unparse(node_to_update)
        prompt = (
            f"You are an expert Python programmer specializing in documentation.\n"
            f"Write a clear, concise, single-line docstring for the following symbol. "
            f"Your response must be ONLY the docstring text itself, without quotes.\n\n"
            f"Symbol:\n```python\n{function_source}\n```"
        )

        generated_docstring = await generator.make_request_async(prompt)

        if "Error:" in generated_docstring:
            log.warning(
                f"LLM failed for `{func_name}` in {file_path}: {generated_docstring}"
            )
            return

        new_source_code = add_docstring_to_function_line_based(
            source_code, line_num, generated_docstring
        )

        # --- VALIDATION STEP ---
        validation_result = validate_code(str(file_path), new_source_code, quiet=True)
        if validation_result["status"] == "dirty":
            log.error(
                f"Generated docstring for `{func_name}` in {file_path} resulted in invalid code. Discarding change."
            )
            return

        final_code = validation_result["code"]
        # --- END VALIDATION STEP ---

        if dry_run:
            typer.secho(
                f"\n📄 In {file_path.name}, would add to function `{func_name}`:",
                fg=typer.colors.YELLOW,
            )
            typer.secho(f'   """{generated_docstring.strip()}"""', fg=typer.colors.CYAN)
        else:
            if final_code != source_code:
                file_path.write_text(final_code, encoding="utf-8")
                log.info(f"Added docstring to `{func_name}` in {file_path.name}")

    except Exception as e:
        log.error(f"Failed to process `{func_name}` in {file_path}: {e}", exc_info=True)


async def _async_main(dry_run: bool):
    """The core asynchronous logic for finding and fixing docstrings."""
    log.info("🩺 Starting self-documentation cycle...")

    log.info("   -> Refreshing self-image (Knowledge Graph)...")
    try:
        builder = KnowledgeGraphBuilder(REPO_ROOT)
        graph = builder.build()
        KNOWLEDGE_GRAPH_PATH.write_text(json.dumps(graph, indent=2), encoding="utf-8")
        log.info("   -> Knowledge Graph refreshed.")
    except Exception as e:
        log.error(f"Failed to rebuild knowledge graph, cannot proceed: {e}")
        return

    kg_data = graph
    generator = GeneratorClient()
    symbols = kg_data.get("symbols", {}).values()
    targets = [
        s
        for s in symbols
        if not s.get("docstring")
        and s.get("type") in ["FunctionDef", "AsyncFunctionDef", "ClassDef"]
    ]

    if not targets:
        log.info(
            "✅ No symbols with missing docstrings found. Codebase is fully documented."
        )
        return

    log.info(
        f"Found {len(targets)} symbols requiring docstrings. Generating concurrently..."
    )

    semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)

    async def worker(target):
        """Processes a target by generating and applying a docstring while respecting a semaphore limit."""
        async with semaphore:
            await generate_and_apply_docstring(target, generator, dry_run)

    tasks = [worker(target) for target in targets]
    for f in track(
        asyncio.as_completed(tasks),
        description="Generating docstrings...",
        total=len(tasks),
    ):
        await f

    log.info("🎉 Self-documentation cycle complete.")


# CAPABILITY: add_missing_docstrings
def fix_missing_docstrings(
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--write",
        help="Show what docstrings would be added without writing any files. Use --write to apply.",
    )
):
    """Finds all symbols with missing docstrings and uses an LLM to generate and apply them, with validation."""
    asyncio.run(_async_main(dry_run))

    if not dry_run:
        log.info("🧠 Rebuilding knowledge graph to reflect changes...")
        try:
            builder = KnowledgeGraphBuilder(REPO_ROOT)
            graph = builder.build()
            out_path = REPO_ROOT / ".intent/knowledge/knowledge_graph.json"
            out_path.write_text(json.dumps(graph, indent=2), encoding="utf-8")
            log.info("✅ Knowledge graph successfully updated.")
        except Exception as e:
            log.error(f"Failed to rebuild knowledge graph: {e}", exc_info=True)


if __name__ == "__main__":
    typer.run(fix_missing_docstrings)

--- END OF FILE ./src/system/tools/docstring_adder.py ---

--- START OF FILE ./src/system/tools/domain_mapper.py ---
# src/system/tools/domain_mapper.py
from pathlib import Path
from typing import Dict

# Corrected import to use the project's custom, test-aware logger
from shared.config_loader import load_config
from shared.logger import getLogger

# Configure logging for detailed CI diagnostics
# Using getLogger to integrate with your existing logger setup
log = getLogger(__name__)


class DomainMapper:
    """Maps file paths to their corresponding domains based on directory structure."""

    def __init__(self, root_path: Path):
        """Initialize the domain mapper with the root path of the project."""
        original_root = root_path
        self.root_path = Path(root_path).resolve()

        # INSTRUMENTATION
        log.debug(f"DomainMapper.__init__ - Original root_path input: {original_root}")
        log.debug(f"DomainMapper.__init__ - Final resolved root_path: {self.root_path}")

        self.domain_map_relative: Dict[Path, str] = self._load_domain_map()
        self.sorted_domain_paths = sorted(
            self.domain_map_relative.keys(), key=lambda p: len(p.parts), reverse=True
        )

    def _load_domain_map(self) -> Dict[Path, str]:
        """Load domain mappings from source_structure.yaml."""
        config_path = self.root_path / ".intent" / "knowledge" / "source_structure.yaml"

        try:
            data = load_config(config_path, "yaml")
        except Exception as e:
            log.warning(f"Could not load domain configuration from {config_path}: {e}")
            return {}

        structure = data.get("structure", [])
        if not isinstance(structure, list):
            log.warning("source_structure.yaml is missing a 'structure' list.")
            return {}

        domain_map = {}
        for entry in structure:
            if isinstance(entry, dict) and "path" in entry and "domain" in entry:
                relative_path = Path(entry["path"])
                domain_map[relative_path] = entry["domain"]

        # INSTRUMENTATION
        log.debug(
            f"DomainMapper._load_domain_map - Loaded {len(domain_map)} mappings: {domain_map}"
        )
        return domain_map

    def determine_domain(self, file_path_relative: Path) -> str:
        """Determine the domain for a file given its relative path from root."""
        # INSTRUMENTATION
        log.debug(
            f"determine_domain - Input file_path_relative: '{file_path_relative}' (type: {type(file_path_relative)})"
        )

        if not isinstance(file_path_relative, Path):
            file_path_relative = Path(file_path_relative)

        if file_path_relative.is_absolute():
            try:
                file_path_relative = file_path_relative.relative_to(self.root_path)
            except ValueError:
                log.warning(
                    f"Cannot make {file_path_relative} relative to {self.root_path}"
                )
                return "unassigned"

        # INSTRUMENTATION
        log.debug(
            f"determine_domain - Starting domain matching loop for: '{file_path_relative}'"
        )
        log.debug(
            f"determine_domain - Available domain paths: {self.sorted_domain_paths}"
        )

        for domain_path in self.sorted_domain_paths:
            # INSTRUMENTATION
            log.debug(
                f"determine_domain - ITERATION: Comparing '{file_path_relative}' against '{domain_path}'"
            )
            try:
                # Use is_relative_to for a more robust check in Python 3.9+
                if file_path_relative.is_relative_to(domain_path):
                    domain = self.domain_map_relative[domain_path]
                    log.debug(
                        f"determine_domain - MATCH FOUND: '{file_path_relative}' belongs to domain '{domain}'"
                    )
                    return domain
            except AttributeError:  # Fallback for older Python versions if needed
                if str(file_path_relative).startswith(str(domain_path)):
                    domain = self.domain_map_relative[domain_path]
                    log.debug(
                        f"determine_domain - MATCH FOUND: '{file_path_relative}' belongs to domain '{domain}'"
                    )
                    return domain

            log.debug("determine_domain - NO MATCH on this iteration.")
            continue

        log.debug(
            f"determine_domain - NO MATCH FOUND for '{file_path_relative}'. Returning 'unassigned'."
        )
        return "unassigned"

    def infer_agent_from_path(self, relative_path: Path) -> str:
        """Infer the agent type from the given relative path by checking for predefined keywords, returning the corresponding agent string or 'generic_agent' if no match is found."""
        # ... (rest of the file is unchanged) ...
        path_lower = str(relative_path).lower()
        agent_keywords = {
            "planner": "planner_agent",
            "generator": "generator_agent",
            "core": "core_agent",
            "tool": "tooling_agent",
        }
        for keyword, agent in agent_keywords.items():
            if keyword in path_lower:
                return agent
        if any(x in path_lower for x in ["validator", "guard", "audit"]):
            return "validator_agent"
        return "generic_agent"

--- END OF FILE ./src/system/tools/domain_mapper.py ---

--- START OF FILE ./src/system/tools/entry_point_detector.py ---
# src/system/tools/entry_point_detector.py
"""
Detects entry points in Python code (FastAPI routes, CLI commands, etc.).
"""
import ast
from pathlib import Path
from typing import Optional, Set

from shared.config_loader import load_config
from shared.logger import getLogger
from system.tools.ast_utils import is_fastapi_assignment, is_main_block
from system.tools.ast_visitor import FunctionCallVisitor

log = getLogger(__name__)


class EntryPointDetector:
    """Detects various types of entry points in Python code."""

    def __init__(self, root_path: Path, cli_entry_points: Set[str]):
        """Initialize the instance with root path, CLI entry points, and load patterns."""
        self.root_path = root_path
        self.cli_entry_points = cli_entry_points
        self.fastapi_app_name: Optional[str] = None
        self.patterns = self._load_patterns()

    def _load_patterns(self) -> list:
        """Loads entry point detection patterns from the intent file."""
        patterns_path = self.root_path / ".intent/knowledge/entry_point_patterns.yaml"
        if not patterns_path.exists():
            log.warning("entry_point_patterns.yaml not found.")
            return []
        config = load_config(patterns_path, "yaml")
        return config.get("patterns", []) if config else []

    def detect_in_tree(self, tree: ast.AST) -> Set[str]:
        """Detect entry points in an AST tree and update internal state."""
        main_block_entries = set()

        for node in ast.walk(tree):
            if is_fastapi_assignment(node):
                self.fastapi_app_name = node.targets[0].id
            elif is_main_block(node):
                visitor = FunctionCallVisitor()
                visitor.visit(node)
                main_block_entries.update(visitor.calls)

        return main_block_entries

    def _is_fastapi_route_decorator(self, decorator: ast.AST) -> bool:
        """Check if decorator is a FastAPI route decorator."""
        return (
            isinstance(decorator, ast.Call)
            and isinstance(decorator.func, ast.Attribute)
            and isinstance(decorator.func.value, ast.Name)
            and decorator.func.value.id == self.fastapi_app_name
        )

    def get_entry_point_type(
        self, node: ast.FunctionDef | ast.AsyncFunctionDef
    ) -> Optional[str]:
        """Identifies decorator or CLI-based entry points for a function."""
        for decorator in node.decorator_list:
            if self._is_fastapi_route_decorator(decorator):
                return f"fastapi_route_{decorator.func.attr}"
            elif (
                isinstance(decorator, ast.Name)
                and decorator.id == "asynccontextmanager"
            ):
                return "context_manager"

        if self.fastapi_app_name and node.name == "lifespan":
            return "fastapi_lifespan"
        if node.name in self.cli_entry_points:
            return "cli_entry_point"
        return None

--- END OF FILE ./src/system/tools/entry_point_detector.py ---

--- START OF FILE ./src/system/tools/file_scanner.py ---
# src/system/tools/file_scanner.py
"""
Scans individual Python files and extracts symbol information.
"""
import ast
from pathlib import Path
from typing import Dict, Optional

from shared.logger import getLogger
from system.tools.ast_utils import (
    calculate_structural_hash,
    detect_docstring,
)
from system.tools.ast_visitor import FunctionCallVisitor
from system.tools.domain_mapper import DomainMapper
from system.tools.entry_point_detector import EntryPointDetector
from system.tools.models import FunctionInfo

log = getLogger(__name__)


class FileScanner:
    """Scans Python files and extracts symbol information."""

    def __init__(
        self, domain_mapper: DomainMapper, entry_point_detector: EntryPointDetector
    ):
        """Initializes the scanner with its required helper components."""
        self.domain_mapper = domain_mapper
        self.entry_point_detector = entry_point_detector
        self.functions: Dict[str, FunctionInfo] = {}

    def scan_file(self, filepath: Path) -> bool:
        """Scans a single Python file, parsing its AST to extract all symbols."""
        try:
            content = filepath.read_text(encoding="utf-8")
            source_lines = content.splitlines()
            tree = ast.parse(content, filename=str(filepath))

            # Detect entry points and update detector state
            main_block_entries = self.entry_point_detector.detect_in_tree(tree)
            self.entry_point_detector.cli_entry_points.update(main_block_entries)

            # Process all function and class definitions in the file
            for node in ast.walk(tree):
                if isinstance(
                    node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                ):
                    self.process_symbol_node(node, filepath, source_lines)

            return True

        except UnicodeDecodeError as e:
            log.error(f"Encoding error scanning {filepath}: {e}")
            return False
        except Exception as e:
            log.error(f"Error scanning {filepath}: {e}")
            return False

    def process_symbol_node(
        self,
        node: ast.AST,
        filepath: Path,
        source_lines: list[str],
        parent_key: Optional[str] = None,
    ) -> Optional[str]:
        """Extracts and stores metadata from a single function or class AST node."""
        if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            return None

        # Calculate structural hash
        structural_hash = calculate_structural_hash(node)

        # Extract function calls
        visitor = FunctionCallVisitor()
        visitor.visit(node)

        # Build function info
        key = f"{filepath.relative_to(self.domain_mapper.root_path).as_posix()}::{node.name}"
        doc = detect_docstring(node)
        domain = self.domain_mapper.determine_domain(
            filepath.relative_to(self.domain_mapper.root_path)
        )
        is_class = isinstance(node, ast.ClassDef)

        func_info = FunctionInfo(
            name=node.name,
            qualname=f"{domain}.{node.name}",
            module=filepath.stem,
            filepath=filepath.relative_to(self.domain_mapper.root_path).as_posix(),
            lineno=node.lineno,
            end_lineno=node.end_lineno if hasattr(node, "end_lineno") else None,
            params=(
                {arg.arg: None for arg in node.args.args}
                if hasattr(node, "args")
                else {}
            ),
            returns=None,
            decorators=(
                [d.id for d in node.decorator_list]
                if hasattr(node, "decorator_list")
                else []
            ),
            docstring=doc,
            calls=visitor.calls,
            complexity=None,
            type=node.__class__.__name__,
            tags=[key],  # Add key to tags
        )

        self.functions[key] = func_info

        # Process nested methods for classes
        if is_class:
            for child_node in node.body:
                if isinstance(child_node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    self.process_symbol_node(child_node, filepath, source_lines, key)

        return key

--- END OF FILE ./src/system/tools/file_scanner.py ---

--- START OF FILE ./src/system/tools/graph_serializer.py ---
# src/system/tools/graph_serializer.py
"""
Handles serialization of the knowledge graph to JSON format.
"""
import json
from dataclasses import asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict

from filelock import FileLock

from shared.logger import getLogger
from system.tools.models import FunctionInfo

log = getLogger(__name__)


class GraphSerializer:
    """Handles serialization of knowledge graph data."""

    @staticmethod
    def serialize_functions(functions: Dict[str, FunctionInfo]) -> Dict[str, Any]:
        """Convert FunctionInfo objects to serializable dictionaries."""
        serializable_functions = {
            key: asdict(
                info, dict_factory=lambda x: {k: v for (k, v) in x if v is not None}
            )
            for key, info in functions.items()
        }

        # Sort calls for consistent output
        for data in serializable_functions.values():
            data["calls"] = sorted(list(data["calls"]))

        return serializable_functions

    @staticmethod
    def build_graph_data(
        functions: Dict[str, FunctionInfo], files_scanned: int
    ) -> Dict[str, Any]:
        """Build the complete knowledge graph data structure."""
        serializable_functions = GraphSerializer.serialize_functions(functions)

        return {
            "schema_version": "2.0.0",
            "metadata": {
                "files_scanned": files_scanned,
                "total_symbols": len(functions),
                "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            },
            "symbols": serializable_functions,
        }

    @staticmethod
    def save_to_file(graph_data: Dict[str, Any], output_path: Path) -> None:
        """Save the knowledge graph to a JSON file with file locking."""
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with FileLock(str(output_path) + ".lock"):
            output_path.write_text(json.dumps(graph_data, indent=2), encoding="utf-8")

        log.info(f"✅ Knowledge graph saved to {output_path}")
        log.info(
            f"   -> {graph_data['metadata']['files_scanned']} files, "
            f"{graph_data['metadata']['total_symbols']} symbols"
        )

--- END OF FILE ./src/system/tools/graph_serializer.py ---

--- START OF FILE ./src/system/tools/__init__.py ---
# src/system/tools/__init__.py
# Package marker for src/system/tools — contains CORE's introspection and audit tools.

--- END OF FILE ./src/system/tools/__init__.py ---

--- START OF FILE ./src/system/tools/intent_guard_runner.py ---
# src/system/tools/intent_guard_runner.py
"""
Intent Guard Runner

What it does
------------
1) Reads .intent/meta.yaml to find policy + source map.
2) Parses Python files under enabled domains and collects imports (AST-based).
3) Enforces .intent/policies/intent_guard.yaml:
   - deny-by-default between domains unless allowed
   - explicit forbids (e.g., agents -> system, core -> data)
   - disallow domain cycles in observed edges (if enabled)
   - library forbids (e.g., requests)
   - respects ignored paths and waivers
   - respects domains with enabled: false in source_structure.yaml

Usage
-----
PYTHONPATH=src poetry run python src/system/tools/intent_guard_runner.py check
# optional flags:
#   --format pretty|json  (default: pretty)
#   --no-fail             (exit 0 even if violations; overrides enforcement.mode)

Exit codes
----------
0 on success (or --no-fail); 1 if violations and enforcement.mode == "fail".
"""

from __future__ import annotations

import ast
import json
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Set, Tuple

import yaml

# Optional pretty output
try:
    from rich import box
    from rich.console import Console
    from rich.table import Table
except Exception:  # pragma: no cover
    Console = None
    Table = None
    box = None

# --- Paths -------------------------------------------------------------------


def find_repo_root(start: Optional[Path] = None) -> Path:
    here = (start or Path(__file__).resolve()).parent
    for p in [here] + list(here.parents):
        if (p / ".intent").exists():
            return p
    return Path.cwd()


REPO = find_repo_root()
INTENT = REPO / ".intent"
META = INTENT / "meta.yaml"

# --- Models ------------------------------------------------------------------


@dataclass
class Domain:
    name: str
    path: Path
    enabled: bool


@dataclass
class ImportEdge:
    importer_file: Path
    importer_domain: str
    imported_module: str
    imported_domain: Optional[str]
    lineno: Optional[int]


# --- Helpers -----------------------------------------------------------------


def load_yaml(path: Path) -> dict:
    if not path.exists():
        fail(f"Missing file: {rel(path)}")
    try:
        return yaml.safe_load(path.read_text()) or {}
    except Exception as e:
        fail(f"YAML error in {rel(path)}: {e}")


def rel(p: Path) -> str:
    try:
        return str(p.relative_to(REPO))
    except Exception:
        return str(p)


def info(msg: str) -> None:
    if Console:
        Console().print(f"[bold cyan]INFO[/] {msg}")
    else:
        print(f"INFO {msg}")


def warn(msg: str) -> None:
    if Console:
        Console().print(f"[bold yellow]WARN[/] {msg}")
    else:
        print(f"WARN {msg}")


def fail(msg: str, code: int = 1) -> None:
    if Console:
        Console().print(f"[bold red]ERROR[/] {msg}")
    else:
        print(f"ERROR {msg}", file=sys.stderr)
    sys.exit(code)


def compile_regex_list(patterns: Iterable[str]) -> List[re.Pattern]:
    out = []
    for p in patterns or []:
        try:
            out.append(re.compile(p))
        except re.error as e:
            warn(f"Invalid regex '{p}': {e}")
    return out


def path_matches_any(path: str, patterns: List[re.Pattern]) -> bool:
    return any(r.search(path) for r in patterns)


def is_relative_import(node: ast.AST) -> bool:
    # from .foo import bar  OR  from .. import x
    return isinstance(node, ast.ImportFrom) and (node.level or 0) > 0


# --- Policy & map loading ----------------------------------------------------


def load_paths_from_meta() -> Tuple[Path, Path]:
    meta = load_yaml(META)
    pol = meta.get("policies", {})
    knowledge = meta.get("knowledge", {})
    policy_path = INTENT / pol.get("intent_guard", "policies/intent_guard.yaml")
    source_map_path = INTENT / knowledge.get(
        "source_structure", "knowledge/source_structure.yaml"
    )
    return policy_path, source_map_path


def load_domains(source_map_path: Path) -> Dict[str, Domain]:
    data = load_yaml(source_map_path)
    out: Dict[str, Domain] = {}
    for d in data.get("domains", []):
        name = d.get("domain")
        if not name:
            continue
        path = REPO / str(d.get("path", ""))
        out[name] = Domain(name=name, path=path, enabled=bool(d.get("enabled", True)))
    return out


# --- Scanner -----------------------------------------------------------------


def discover_py_files(
    domains: Dict[str, Domain], ignored: List[re.Pattern]
) -> List[Path]:
    files: List[Path] = []
    for dom in domains.values():
        if not dom.enabled:
            continue
        base = dom.path
        if not base.exists():
            # Don't fail: domain may be declared ahead of time
            continue
        for p in base.rglob("*.py"):
            rp = rel(p)
            if path_matches_any(rp, ignored):
                continue
            files.append(p)
    return files


def top_level_name(mod: str) -> str:
    return mod.split(".", 1)[0]


def resolve_domain_for_module(mod: str, domains: Dict[str, Domain]) -> Optional[str]:
    tl = top_level_name(mod)
    if tl in domains:
        return tl
    return None  # 3rd-party or stdlib


def scan_file_imports(path: Path) -> List[Tuple[str, Optional[int]]]:
    """Return list of (imported_module, lineno). Only absolute imports are returned."""
    try:
        tree = ast.parse(path.read_text(encoding="utf-8"), filename=str(path))
    except SyntaxError as e:
        warn(f"Skipping {rel(path)} due to syntax error: {e}")
        return []
    out: List[Tuple[str, Optional[int]]] = []
    for node in ast.walk(tree):
        if is_relative_import(node):
            continue
        if isinstance(node, ast.Import):
            for alias in node.names:
                if alias.name:
                    out.append((alias.name, getattr(node, "lineno", None)))
        elif isinstance(node, ast.ImportFrom) and node.module:
            out.append((node.module, getattr(node, "lineno", None)))
    return out


def domain_for_file(path: Path, domains: Dict[str, Domain]) -> Optional[str]:
    for name, dom in domains.items():
        try:
            if path.is_relative_to(dom.path):
                return name
        except AttributeError:
            # Python <3.9 compatibility: emulate is_relative_to
            try:
                path.relative_to(dom.path)
                return name
            except Exception:
                pass
    return None


def collect_edges(
    domains: Dict[str, Domain], ignored: List[re.Pattern]
) -> List[ImportEdge]:
    edges: List[ImportEdge] = []
    for f in discover_py_files(domains, ignored):
        importer_dom = domain_for_file(f, domains)
        if not importer_dom:
            continue
        for mod, ln in scan_file_imports(f):
            imported_dom = resolve_domain_for_module(mod, domains)
            edges.append(
                ImportEdge(
                    importer_file=f,
                    importer_domain=importer_dom,
                    imported_module=mod,
                    imported_domain=imported_dom,
                    lineno=ln,
                )
            )
    return edges


# --- Enforcement -------------------------------------------------------------


@dataclass
class Violation:
    kind: str  # "forbidden-import" | "not-allowed" | "cycle" | "forbidden-library"
    message: str
    file: Optional[str] = None
    lineno: Optional[int] = None
    domain_from: Optional[str] = None
    domain_to: Optional[str] = None
    module: Optional[str] = None
    reason: Optional[str] = None


def enforce_policy(
    policy: dict,
    domains: Dict[str, Domain],
    edges: List[ImportEdge],
) -> List[Violation]:
    rules = policy.get("rules") or {}
    imports_rule = rules.get("imports") or {}
    libraries_rule = rules.get("libraries") or {}
    #    subprocess_rule = rules.get("subprocess") or {}  # reserved for future use

    # Respect enabled:false domains?
    respect_enabled = bool(imports_rule.get("respect_source_structure_enabled", False))

    # domain permissions
    domain_perms = imports_rule.get("domains") or {}
    default_policy = (imports_rule.get("default_policy") or "deny").lower()

    # forbids
    forbids = imports_rule.get("forbidden") or []
    forbid_pairs = {
        (f["from"], f["to"]): f.get("rationale")
        for f in forbids
        if "from" in f and "to" in f
    }

    # libraries
    lib_forbidden = set((libraries_rule.get("forbidden") or []))
    ignore_stdlib_prefixes = tuple(libraries_rule.get("ignore_stdlib_prefixes") or [])

    # ignored paths & waivers
    enforcement = policy.get("enforcement") or {}
    ignored_patterns = compile_regex_list(enforcement.get("ignored_paths") or [])
    waivers = enforcement.get("waivers") or []
    waiver_patterns = [
        (compile_regex_list([w.get("path", "")])[0], w.get("reason", ""))
        for w in waivers
        if w.get("path")
    ]

    violations: List[Violation] = []

    # 1) Domain-to-domain import checks
    for e in edges:
        rp = rel(e.importer_file)
        # Skip ignored paths
        if path_matches_any(rp, ignored_patterns):
            continue
        # Skip if either domain is disabled and we respect it
        if respect_enabled:
            if e.imported_domain and not domains[e.imported_domain].enabled:
                continue
            if e.importer_domain and not domains[e.importer_domain].enabled:
                continue

        # 1a) Library forbids (applies to 3rd-party too)
        if e.imported_domain is None:
            top = top_level_name(e.imported_module)
            if top.startswith(ignore_stdlib_prefixes):
                pass
            elif top in lib_forbidden:
                violations.append(
                    Violation(
                        kind="forbidden-library",
                        message=f"Use of forbidden library '{top}'",
                        file=rp,
                        lineno=e.lineno,
                        module=top,
                    )
                )
            # else: policy 'allow' → do nothing
            continue  # only domain edges below

        # 1b) Explicit forbids
        key = (e.importer_domain, e.imported_domain)
        if key in forbid_pairs:
            violations.append(
                Violation(
                    kind="forbidden-import",
                    message=f"{e.importer_domain} → {e.imported_domain} is forbidden",
                    file=rp,
                    lineno=e.lineno,
                    domain_from=e.importer_domain,
                    domain_to=e.imported_domain,
                    module=e.imported_module,
                    reason=forbid_pairs[key],
                )
            )
            continue

        # 1c) Default deny unless allowed
        allowed = set((domain_perms.get(e.importer_domain) or {}).get("may_import", []))
        if e.imported_domain not in allowed:
            if default_policy == "deny" and e.imported_domain != e.importer_domain:
                violations.append(
                    Violation(
                        kind="not-allowed",
                        message=f"{e.importer_domain} may not import {e.imported_domain}",
                        file=rp,
                        lineno=e.lineno,
                        domain_from=e.importer_domain,
                        domain_to=e.imported_domain,
                        module=e.imported_module,
                    )
                )

    # 2) Cycle detection across observed domain edges
    if imports_rule.get("disallow_cycles", False):
        graph: Dict[str, Set[str]] = {}
        for e in edges:
            if (
                e.imported_domain
                and e.importer_domain
                and e.imported_domain != e.importer_domain
            ):
                # Respect enabled: skip disabled edges
                if respect_enabled and (
                    not domains[e.importer_domain].enabled
                    or not domains[e.imported_domain].enabled
                ):
                    continue
                graph.setdefault(e.importer_domain, set()).add(e.imported_domain)
        cycles = find_domain_cycles(graph)
        for cyc in cycles:
            # Represent cycle compactly: A→B→...→A
            cycle_str = " → ".join(cyc + [cyc[0]])
            violations.append(
                Violation(
                    kind="cycle",
                    message=f"Domain import cycle detected: {cycle_str}",
                    domain_from=cyc[0],
                    domain_to=cyc[-1],
                )
            )

    # 3) Apply waivers (suppress by file regex)
    if waiver_patterns:
        filtered: List[Violation] = []
        for v in violations:
            rp = v.file or ""
            waived = any(r.search(rp) for r, _ in waiver_patterns)
            if not waived:
                filtered.append(v)
        violations = filtered

    return violations


def find_domain_cycles(graph: Dict[str, Set[str]]) -> List[List[str]]:
    """Return list of cycles found in a directed graph (domain-level)."""
    visited: Set[str] = set()
    stack: Set[str] = set()
    path: List[str] = []
    cycles: List[List[str]] = []

    def dfs(node: str):
        visited.add(node)
        stack.add(node)
        path.append(node)
        for nbr in graph.get(node, set()):
            if nbr not in visited:
                dfs(nbr)
            elif nbr in stack:
                # Found a cycle; slice path from nbr to end
                try:
                    i = path.index(nbr)
                    cyc = path[i:].copy()
                    if cyc and cyc not in cycles:
                        cycles.append(cyc)
                except ValueError:
                    pass
        stack.remove(node)
        path.pop()

    for n in list(graph.keys()):
        if n not in visited:
            dfs(n)
    return cycles


# --- Reporting ---------------------------------------------------------------


def print_report(violations: List[Violation], fmt: str = "pretty") -> None:
    if not violations:
        if Console:
            Console().print("[bold green]✅ No guard violations.[/]")
        else:
            print("OK: No guard violations.")
        return

    if fmt == "json":
        print(json.dumps([v.__dict__ for v in violations], indent=2))
        return

    # pretty
    if Console and Table:
        console = Console()
        table = Table(title="Intent Guard Violations", box=box.SIMPLE_HEAVY)
        table.add_column("Kind")
        table.add_column("From")
        table.add_column("To/Module")
        table.add_column("File:Line")
        table.add_column("Reason/Msg")
        for v in violations:
            from_d = v.domain_from or "-"
            to = v.domain_to or (v.module or "-")
            fl = f"{v.file}:{v.lineno}" if v.file else "-"
            reason = v.reason or v.message
            table.add_row(v.kind, from_d, to, fl, reason)
        console.print(table)
    else:
        print("Violations:")
        for v in violations:
            print(
                f"- [{v.kind}] {v.domain_from or '-'} -> {v.domain_to or v.module or '-'} "
                f"at {v.file}:{v.lineno} :: {v.reason or v.message}"
            )


# --- CLI ---------------------------------------------------------------------


def parse_args(argv: List[str]) -> Tuple[str, Dict[str, str]]:
    if not argv:
        return "check", {"format": "pretty", "no_fail": "0"}
    cmd = argv[0]
    fmt = "pretty"
    no_fail = "0"
    for a in argv[1:]:
        if a == "--format" and False:
            pass  # reserved
        elif a.startswith("--format="):
            fmt = a.split("=", 1)[1].strip()
        elif a == "--no-fail":
            no_fail = "1"
        elif a in {"pretty", "json"}:
            fmt = a
        else:
            # ignore unknowns to stay friendly
            pass
    if cmd not in {"check"}:
        fail(f"Unknown command '{cmd}'. Use: check [--format=json|pretty] [--no-fail]")
    return cmd, {"format": fmt, "no_fail": no_fail}


def main(argv: Optional[List[str]] = None) -> None:
    cmd, opts = parse_args(argv or sys.argv[1:])
    fmt = opts.get("format", "pretty")
    no_fail_flag = opts.get("no_fail") == "1"

    policy_path, source_map_path = load_paths_from_meta()
    policy = load_yaml(policy_path)
    #    source_map = load_yaml(source_map_path)

    # enforcement.mode: warn|fail
    enforcement = policy.get("enforcement") or {}
    mode_fail = (enforcement.get("mode", "fail").lower() == "fail") and (
        not no_fail_flag
    )

    # compile ignore patterns once
    ignored = compile_regex_list(enforcement.get("ignored_paths") or [])

    domains = load_domains(source_map_path)
    edges = collect_edges(domains, ignored)
    violations = enforce_policy(policy, domains, edges)

    print_report(violations, fmt=fmt)

    if violations and mode_fail:
        sys.exit(1)
    sys.exit(0)


if __name__ == "__main__":
    main()

--- END OF FILE ./src/system/tools/intent_guard_runner.py ---

--- START OF FILE ./src/system/tools/manifest_migrator.py ---
# src/system/tools/manifest_migrator.py
"""
Manifest Migrator / Validator

What it does (safe, self-contained):
1) Reads .intent/meta.yaml to discover constitutional paths.
2) Loads .intent/knowledge/source_structure.yaml to list domains.
3) Validates every manifest in .intent/manifests/ against .intent/schemas/manifest.schema.json.
4) Scaffolds any missing manifests with safe placeholders (capabilities: ["unassigned"]).
5) Detects duplicate capabilities across domains (and can fail on conflicts).
6) Optionally writes a drift report JSON (as defined in meta.yaml if present).

Run examples:
  python src/system/tools/manifest_migrator.py validate
  python src/system/tools/manifest_migrator.py scaffold
  python src/system/tools/manifest_migrator.py check-duplicates --fail-on-conflicts
  python src/system/tools/manifest_migrator.py all --fail-on-conflicts
"""
from __future__ import annotations

import json
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import yaml  # PyYAML

try:
    from jsonschema import Draft7Validator
except Exception:  # pragma: no cover
    print(
        "ERROR: jsonschema is required. Please add `jsonschema` to your dependencies.",
        file=sys.stderr,
    )
    raise

# Optional niceties; if unavailable, we degrade gracefully.
try:
    from rich import box
    from rich.console import Console
    from rich.table import Table

    console = Console()
except Exception:  # pragma: no cover
    console = None
    Table = None
    box = None


# ---------- Utility: repo + constitution paths ----------


def find_repo_root(start: Optional[Path] = None) -> Path:
    """Walk upward until a folder containing .intent/ is found."""
    here = (start or Path(__file__).resolve()).parent
    for p in [here] + list(here.parents):
        if (p / ".intent").exists():
            return p
    # Fallback: current working dir
    return Path.cwd()


REPO_ROOT = find_repo_root()
INTENT_DIR = REPO_ROOT / ".intent"
META_PATH = INTENT_DIR / "meta.yaml"

DEFAULT_SOURCE_STRUCTURE = "knowledge/source_structure.yaml"
DEFAULT_MANIFEST_SCHEMA = "schemas/manifest.schema.json"
DEFAULT_MANIFESTS_DIR = "manifests"
DEFAULT_DRIFT_REPORT = "reports/drift_report.json"


@dataclass
class Paths:
    source_structure: Path
    manifest_schema: Path
    manifests_dir: Path
    drift_report: Optional[Path]


def load_meta() -> Dict:
    if not META_PATH.exists():
        fail(f"Missing constitution index: {rel(META_PATH)}")
    meta = yaml.safe_load(META_PATH.read_text()) or {}

    # Knowledge
    knowledge = meta.get("knowledge", {})
    source_structure_rel = knowledge.get("source_structure", DEFAULT_SOURCE_STRUCTURE)

    # Schemas
    schemas = meta.get("schemas", {})
    manifest_schema_rel = schemas.get("manifest", DEFAULT_MANIFEST_SCHEMA)

    # Manifests dir
    manifests = meta.get("manifests", {})
    manifests_dir_rel = manifests.get("dir", DEFAULT_MANIFESTS_DIR)

    # Reports
    reports = meta.get("reports", {})
    drift_rel = reports.get("drift", DEFAULT_DRIFT_REPORT) if reports else None

    return {
        "source_structure": INTENT_DIR / source_structure_rel,
        "manifest_schema": INTENT_DIR / manifest_schema_rel,
        "manifests_dir": INTENT_DIR / manifests_dir_rel,
        "drift_report": (REPO_ROOT / drift_rel) if drift_rel else None,
    }


def load_paths() -> Paths:
    meta = load_meta()
    paths = Paths(
        source_structure=Path(meta["source_structure"]),
        manifest_schema=Path(meta["manifest_schema"]),
        manifests_dir=Path(meta["manifests_dir"]),
        drift_report=Path(meta["drift_report"]) if meta.get("drift_report") else None,
    )
    # Basic existence checks (manifests_dir may not exist yet)
    if not paths.source_structure.exists():
        fail(f"Missing knowledge map: {rel(paths.source_structure)}")
    if not paths.manifest_schema.exists():
        fail(f"Missing manifest schema: {rel(paths.manifest_schema)}")
    paths.manifests_dir.mkdir(parents=True, exist_ok=True)
    return paths


# ---------- Source structure parsing ----------


@dataclass
class DomainDef:
    name: str
    path: str
    enabled: bool
    description: str


def load_domains(source_structure_path: Path) -> List[DomainDef]:
    data = yaml.safe_load(source_structure_path.read_text()) or {}
    doms = []
    for entry in data.get("domains", []):
        name = entry.get("domain")
        if not name:
            # skip malformed
            continue
        doms.append(
            DomainDef(
                name=name,
                path=str(entry.get("path", "")),
                enabled=bool(entry.get("enabled", True)),
                description=str(entry.get("description", "")) or f"Domain {name}",
            )
        )
    return doms


# ---------- Manifest IO + validation ----------


def schema_validator(schema_path: Path) -> Draft7Validator:
    raw = json.loads(schema_path.read_text())
    return Draft7Validator(raw)


def manifest_path_for(domain: str, manifests_dir: Path) -> Path:
    return manifests_dir / f"{domain}.manifest.json"


def read_manifest(path: Path) -> Dict:
    try:
        return json.loads(path.read_text())
    except Exception as e:
        fail(f"Invalid JSON in {rel(path)}: {e}")


def write_manifest(path: Path, manifest: Dict) -> None:
    txt = json.dumps(manifest, indent=2, ensure_ascii=False)
    path.write_text(txt + "\n")


# ---------- Reporting helpers ----------


def rel(p: Path) -> str:
    try:
        return str(p.relative_to(REPO_ROOT))
    except Exception:
        return str(p)


def info(msg: str) -> None:
    if console:
        console.print(f"[bold cyan]INFO[/] {msg}")
    else:
        print(f"INFO {msg}")


def warn(msg: str) -> None:
    if console:
        console.print(f"[bold yellow]WARN[/] {msg}")
    else:
        print(f"WARN {msg}")


def fail(msg: str, code: int = 1) -> None:
    if console:
        console.print(f"[bold red]ERROR[/] {msg}")
    else:
        print(f"ERROR {msg}", file=sys.stderr)
    sys.exit(code)


# ---------- Core actions ----------


def validate_manifests(
    paths: Paths, fail_on_error: bool = True
) -> Tuple[int, List[str]]:
    """Validate all manifests against the schema. Returns (#errors, error_messages)."""
    validator = schema_validator(paths.manifest_schema)
    errors = 0
    messages: List[str] = []

    domains = load_domains(paths.source_structure)
    for d in domains:
        mpath = manifest_path_for(d.name, paths.manifests_dir)
        if not mpath.exists():
            warn(f"Missing manifest for domain '{d.name}': {rel(mpath)}")
            errors += 1
            messages.append(f"missing: {d.name}")
            continue
        data = read_manifest(mpath)
        problems = sorted(validator.iter_errors(data), key=lambda e: e.path)
        if problems:
            errors += len(problems)
            warn(f"Schema violations in {rel(mpath)}")
            for e in problems:
                loc = "/".join([str(x) for x in e.path]) or "(root)"
                msg = f"  - {loc}: {e.message}"
                messages.append(f"{d.name}: {msg}")
                if console:
                    console.print(f"[dim]{msg}[/]")
                else:
                    print(msg)
        else:
            info(f"OK: {rel(mpath)}")

    if errors and fail_on_error:
        fail(f"Validation failed with {errors} error(s).")
    return errors, messages


def scaffold_missing(paths: Paths) -> List[Path]:
    """Create minimal manifests for domains that don't have one yet."""
    created: List[Path] = []
    domains = load_domains(paths.source_structure)
    for d in domains:
        mpath = manifest_path_for(d.name, paths.manifests_dir)
        if mpath.exists():
            continue
        manifest = {
            "domain": d.name,
            "description": d.description or f"Domain {d.name}",
            "capabilities": [
                "unassigned"
            ],  # placeholder to satisfy schema (minItems: 1)
            "imports": [],  # optional; can be filled later
            "notes": "Scaffolded by manifest_migrator. Replace 'unassigned' with real capabilities.",
            "version": "0.1.0",
            "owners": [],
        }
        write_manifest(mpath, manifest)
        created.append(mpath)
        info(f"Scaffolded: {rel(mpath)}")
    if not created:
        info("No scaffolding needed. All manifests exist.")
    return created


def collect_capabilities(paths: Paths) -> Dict[str, List[str]]:
    """Return {domain: [capabilities...]} for all existing manifests (missing ones count as [])."""
    domains = load_domains(paths.source_structure)
    result: Dict[str, List[str]] = {}
    for d in domains:
        mpath = manifest_path_for(d.name, paths.manifests_dir)
        if not mpath.exists():
            result[d.name] = []
            continue
        m = read_manifest(mpath)
        caps = m.get("capabilities") or []
        # normalize to strings
        result[d.name] = [str(c) for c in caps if isinstance(c, (str, int))]
    return result


def find_duplicate_capabilities(cap_map: Dict[str, List[str]]) -> Dict[str, List[str]]:
    """Return {capability: [domains...]} for capabilities present in >1 domain."""
    reverse: Dict[str, List[str]] = {}
    for domain, caps in cap_map.items():
        for c in caps:
            reverse.setdefault(c, []).append(domain)
    return {cap: doms for cap, doms in reverse.items() if len(doms) > 1}


def check_duplicates(
    paths: Paths, fail_on_conflicts: bool = False
) -> Dict[str, List[str]]:
    cap_map = collect_capabilities(paths)
    dups = find_duplicate_capabilities(cap_map)

    if console and Table:
        table = Table(
            title="Duplicate Capabilities", box=box.SIMPLE_HEAVY if box else None
        )
        table.add_column("Capability")
        table.add_column("Domains")
        if dups:
            for cap, doms in sorted(dups.items()):
                table.add_row(cap, ", ".join(sorted(doms)))
        else:
            table.add_row("—", "No duplicates")
        console.print(table)
    else:
        if dups:
            print("Duplicate capabilities detected:")
            for cap, doms in sorted(dups.items()):
                print(f"  - {cap}: {', '.join(sorted(doms))}")
        else:
            print("No duplicate capabilities.")

    if dups and fail_on_conflicts:
        fail("Conflicting capabilities found across domains.", code=2)

    return dups


def write_drift_report(
    paths: Paths, validation_errors: List[str], duplicates: Dict[str, List[str]]
) -> Optional[Path]:
    if not paths.drift_report:
        return None
    report_path = paths.drift_report
    report_path.parent.mkdir(parents=True, exist_ok=True)
    payload = {
        "validation_errors": validation_errors,
        "duplicates": duplicates,
    }
    report_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False) + "\n")
    info(f"Wrote drift report: {rel(report_path)}")
    return report_path


# ---------- CLI ----------


def parse_args(argv: List[str]) -> Tuple[str, Dict[str, bool]]:
    """
    Minimal arg parser (keeps dependencies light).
    Commands: validate | scaffold | check-duplicates | all
    Flags: --fail-on-conflicts
    """
    if not argv:
        return "all", {}
    cmd = argv[0]
    flags = {"fail-on-conflicts": False}
    for a in argv[1:]:
        if a == "--fail-on-conflicts":
            flags["fail-on-conflicts"] = True
    if cmd not in {"validate", "scaffold", "check-duplicates", "all"}:
        fail(
            f"Unknown command '{cmd}'. Use: validate | scaffold | check-duplicates | all"
        )
    return cmd, flags


def main(argv: Optional[List[str]] = None) -> None:
    cmd, flags = parse_args(argv or sys.argv[1:])
    paths = load_paths()

    # 1) Ensure manifests exist
    if cmd in {"scaffold", "all"}:
        scaffold_missing(paths)

    # 2) Validate
    validation_errors: List[str] = []
    if cmd in {"validate", "all"}:
        errors, messages = validate_manifests(paths, fail_on_error=False)
        validation_errors = messages
        if errors:
            warn(f"Validation found {errors} error(s).")

    # 3) Duplicates
    duplicates: Dict[str, List[str]] = {}
    if cmd in {"check-duplicates", "all"}:
        duplicates = check_duplicates(
            paths, fail_on_conflicts=flags.get("fail-on-conflicts", False)
        )

    # 4) Drift report (optional, only if path configured)
    write_drift_report(paths, validation_errors, duplicates)

    # Exit codes
    if validation_errors or duplicates:
        # Non-zero exit only if asked to fail on conflicts, or if there were schema errors.
        if validation_errors or flags.get("fail-on-conflicts"):
            sys.exit(1)

    # All good
    sys.exit(0)


if __name__ == "__main__":
    main()

--- END OF FILE ./src/system/tools/manifest_migrator.py ---

--- START OF FILE ./src/system/tools/manifest.yaml ---
capabilities:
- manifest_updating
- add_missing_docstrings
description: Internal introspection utilities
domain: tooling

--- END OF FILE ./src/system/tools/manifest.yaml ---

--- START OF FILE ./src/system/tools/models.py ---
# src/system/tools/models.py
"""
Lightweight, JSON-safe data models for code-introspection tooling.

Why this exists:
- Keep all graph/export structures strictly JSON-serializable.
- Avoid non-JSON-native types like `set`, `Path`, or custom objects in public fields.
- Provide `to_dict()` helpers that are stable for downstream writers (reports, graphs).

Key fix in this file:
- `FunctionInfo.calls` is a **List[str]** (not a set). Lists are JSON-safe and
  preserve order; we deduplicate on write to keep outputs tidy.
"""

from __future__ import annotations

from dataclasses import asdict, dataclass, field
from typing import Dict, List, Optional


def _dedupe_seq(seq: List[str]) -> List[str]:
    """Deduplicate while preserving order (stable for reports/JSON)."""
    return list(dict.fromkeys(seq))


@dataclass
class FunctionInfo:
    """
    Minimal, JSON-safe representation of a function for graphing and reports.
    """

    # Identity
    name: str
    qualname: str
    module: str

    # Source
    filepath: str
    lineno: int
    end_lineno: Optional[int] = None

    # Signature / docs
    params: Dict[str, Optional[str]] = field(
        default_factory=dict
    )  # {param_name: annotation or None}
    returns: Optional[str] = None
    decorators: List[str] = field(default_factory=list)
    docstring: Optional[str] = None

    # Relations / metrics
    calls: List[str] = field(default_factory=list)  # 👈 JSON-safe (list), not a set
    complexity: Optional[int] = None  # cyclomatic or similar, optional

    # Misc
    type: Optional[str] = None  # Added to handle function type
    tags: List[str] = field(default_factory=list)  # free-form labels, optional

    def add_call(self, callee_qualname: str) -> None:
        """Record a call edge (duplicates are ok; they are deduped at export)."""
        if not isinstance(callee_qualname, str):
            return
        self.calls.append(callee_qualname)

    def add_tag(self, tag: str) -> None:
        if isinstance(tag, str) and tag:
            self.tags.append(tag)

    def to_dict(self) -> Dict:
        """
        JSON-stable dict. Ensures lists are de-duplicated and primitives only.
        """
        data = asdict(self)
        data["calls"] = _dedupe_seq([str(x) for x in (self.calls or [])])
        data["decorators"] = _dedupe_seq([str(x) for x in (self.decorators or [])])
        data["tags"] = _dedupe_seq([str(x) for x in (self.tags or [])])

        # Normalize params to {name: annotation or None}, all strings
        params_norm: Dict[str, Optional[str]] = {}
        for k, v in (self.params or {}).items():
            key = str(k)
            params_norm[key] = None if v is None else str(v)
        data["params"] = params_norm

        # Ensure primitives for optional fields
        data["returns"] = None if self.returns is None else str(self.returns)
        data["docstring"] = None if self.docstring is None else str(self.docstring)
        data["module"] = str(self.module)
        data["name"] = str(self.name)
        data["qualname"] = str(self.qualname)
        data["filepath"] = str(self.filepath)
        data["lineno"] = int(self.lineno)
        if self.end_lineno is not None:
            data["end_lineno"] = int(self.end_lineno)
        else:
            data["end_lineno"] = None
        if self.complexity is not None:
            data["complexity"] = int(self.complexity)
        else:
            data["complexity"] = None
        data["type"] = None if self.type is None else str(self.type)  # Added for type

        return data


@dataclass
class ModuleInfo:
    """
    Aggregate of functions from a single Python module (file).
    """

    module: str
    filepath: str
    functions: List[FunctionInfo] = field(default_factory=list)

    def add_function(self, fn: FunctionInfo) -> None:
        if isinstance(fn, FunctionInfo):
            self.functions.append(fn)

    def to_dict(self) -> Dict:
        return {
            "module": str(self.module),
            "filepath": str(self.filepath),
            "functions": [f.to_dict() for f in self.functions],
        }


@dataclass
class ImportEdge:
    """
    Simple import edge (module-level), used by guards/graph builders.
    """

    importer: str  # e.g., "core.services.foo"
    imported: str  # e.g., "shared.utils.time"
    lineno: Optional[int] = None

    def to_dict(self) -> Dict:
        return {
            "importer": str(self.importer),
            "imported": str(self.imported),
            "lineno": None if self.lineno is None else int(self.lineno),
        }


__all__ = [
    "FunctionInfo",
    "ModuleInfo",
    "ImportEdge",
]

--- END OF FILE ./src/system/tools/models.py ---

--- START OF FILE ./src/system/tools/pattern_matcher.py ---
# src/system/tools/pattern_matcher.py
"""
A dedicated module for applying declarative patterns to identify non-obvious
entry points in the knowledge graph.
"""
import re
from pathlib import Path
from typing import Any, Dict, List, Set

from shared.logger import getLogger
from system.tools.models import FunctionInfo  # <<< FIX: Import from the new models file

log = getLogger(__name__)


class PatternMatcher:
    """Applies declarative patterns to a list of symbols to identify entry points."""

    def __init__(self, patterns: List[Dict[str, Any]], root_path: Path):
        """
        Initialize the PatternMatcher with a set of rules.

        Args:
            patterns: A list of pattern dictionaries from entry_point_patterns.yaml.
            root_path: The absolute path to the repository root.
        """
        self.patterns: List[Dict[str, Any]] = patterns
        self.root_path: Path = root_path

    def apply_patterns(self, functions: Dict[str, FunctionInfo]) -> None:
        """
        Apply configured patterns to identify entry points in function symbols.

        Args:
            functions: A dictionary of FunctionInfo objects from the KnowledgeGraphBuilder.
        """
        all_base_classes: Set[str] = {
            base for info in functions.values() for base in info.base_classes
        }

        for info in functions.values():
            if info.entry_point_type:  # Skip if already identified
                continue

            for pattern in self.patterns:
                if self._is_match(
                    info, pattern.get("match", {}), all_base_classes, functions
                ):
                    info.entry_point_type = pattern.get("entry_point_type")
                    info.entry_point_justification = pattern.get("name")
                    log.debug(
                        f"Identified entry point: {info.name} as {info.entry_point_type} ({info.entry_point_justification})"
                    )
                    break

    def _is_match(
        self,
        info: FunctionInfo,
        rules: Dict[str, Any],
        all_base_classes: Set[str],
        functions: Dict[str, FunctionInfo],
    ) -> bool:
        """Check if a single symbol matches a set of declarative rules."""
        try:
            if rules.get("has_capability_tag") and info.capability == "unassigned":
                return False

            if rules.get("is_base_class") and (
                not info.is_class or info.name not in all_base_classes
            ):
                return False

            if "name_regex" in rules:
                try:
                    if not re.match(rules["name_regex"], info.name):
                        return False
                except re.error as e:
                    log.error(
                        f"Invalid regex pattern '{rules['name_regex']}' for {info.name}: {e}"
                    )
                    return False

            if "base_class_includes" in rules:
                parent_bases = set(info.base_classes)
                parent_key = info.parent_class_key
                if parent_key and parent_key in functions:
                    parent_bases.update(functions[parent_key].base_classes)
                if rules["base_class_includes"] not in parent_bases:
                    return False

            if "has_decorator" in rules:
                file_path = self.root_path / info.file
                if file_path.exists():
                    try:
                        source_lines = file_path.read_text(
                            encoding="utf-8"
                        ).splitlines()
                        if info.line_number < 2 or info.line_number > len(source_lines):
                            return False
                        decorator_line = source_lines[info.line_number - 2].strip()
                        if f"@{rules['has_decorator']}" not in decorator_line:
                            return False
                    except UnicodeDecodeError as e:
                        log.error(
                            f"Failed to read {file_path} due to encoding error: {e}"
                        )
                        return False
                else:
                    return False

            if "module_path_contains" in rules:
                if rules["module_path_contains"] not in info.file:
                    return False

            if rules.get("is_public_function") and info.name.startswith("_"):
                return False

            return True
        except Exception as e:
            log.error(f"Error evaluating rules for {info.name}: {e}", exc_info=True)
            return False

--- END OF FILE ./src/system/tools/pattern_matcher.py ---

--- START OF FILE ./src/system/tools/project_structure.py ---
# src/system/tools/project_structure.py
"""
Utilities for discovering and working with project structure.
"""
import re
from pathlib import Path
from typing import Set

from shared.logger import getLogger

log = getLogger(__name__)


class ProjectStructureError(Exception):
    """Custom exception for when the project's root cannot be determined."""

    pass


def find_project_root(start_path: Path) -> Path:
    """Traverses upward from a starting path to find the project root, marked by 'pyproject.toml'."""
    current_path = start_path.resolve()
    for path in [current_path, *current_path.parents]:
        if (path / "pyproject.toml").exists():
            return path
    raise ProjectStructureError("Could not find 'pyproject.toml'.")


def get_cli_entry_points(root_path: Path) -> Set[str]:
    """Parses pyproject.toml to find declared command-line entry points."""
    pyproject_path = root_path / "pyproject.toml"
    if not pyproject_path.exists():
        return set()

    try:
        content = pyproject_path.read_text(encoding="utf-8")
        match = re.search(r"\[tool\.poetry\.scripts\]([^\[]*)", content, re.DOTALL)
        return set(re.findall(r'=\s*"[^"]+:(\w+)"', match.group(1))) if match else set()
    except (OSError, UnicodeDecodeError) as e:
        log.warning(f"Could not read pyproject.toml: {e}")
        return set()


def should_exclude_path(path: Path, exclude_patterns: list[str]) -> bool:
    """Determines if a given path should be excluded from scanning."""
    return any(pattern in path.parts for pattern in exclude_patterns)


def get_python_files(src_root: Path, exclude_patterns: list[str]) -> list[Path]:
    """Get all Python files in the source directory, excluding specified patterns."""
    # --- THIS IS THE FIX ---
    # The glob operation does not guarantee a specific order. By converting to a
    # list and sorting it, we ensure that the KnowledgeGraphBuilder always
    # processes files in a deterministic, alphabetical order. This eliminates
    # a potential source of environment-specific inconsistencies.
    all_files = list(src_root.rglob("*.py"))
    all_files.sort()

    return [
        f
        for f in all_files
        if f.name != "__init__.py" and not should_exclude_path(f, exclude_patterns)
    ]

--- END OF FILE ./src/system/tools/project_structure.py ---

--- START OF FILE ./src/system/tools/scaffolder.py ---
# src/system/tools/scaffolder.py
"""
Intent: Provides a reusable Scaffolding service that is fully compliant
with the declared constitution.
"""

import shutil
from pathlib import Path

import typer
import yaml

from shared.config_loader import load_config
from shared.logger import getLogger
from shared.path_utils import get_repo_root

log = getLogger("core_admin.scaffolder")
CORE_ROOT = get_repo_root()
STARTER_KITS_DIR = CORE_ROOT / "src" / "system" / "starter_kits"


class Scaffolder:
    """A reusable service for creating new, constitutionally-governed projects."""

    def __init__(
        self,
        project_name: str,
        profile: str = "default",
        workspace_dir: Path | None = None,
    ):
        """Initializes the Scaffolder with project name, profile, and workspace directory."""
        self.name = project_name
        self.profile = profile

        source_structure = load_config(
            CORE_ROOT / ".intent/knowledge/source_structure.yaml"
        )
        workspace_path_str = source_structure.get("paths", {}).get("workspace", "work")

        self.workspace = workspace_dir or (CORE_ROOT / workspace_path_str)

        self.project_root = self.workspace / self.name
        self.starter_kit_path = STARTER_KITS_DIR / self.profile

        if not self.starter_kit_path.is_dir():
            raise FileNotFoundError(
                f"Starter kit profile '{self.profile}' not found at {self.starter_kit_path}."
            )

    def scaffold_base_structure(self):
        """Creates the base project structure, including tests and CI directories."""
        log.info(f"💾 Creating project structure at {self.project_root}...")
        if self.project_root.exists():
            raise FileExistsError(f"Directory '{self.project_root}' already exists.")

        # --- THIS IS THE MODIFIED SECTION ---
        # Create all necessary directories upfront
        self.project_root.mkdir(parents=True, exist_ok=True)
        (self.project_root / "src").mkdir()
        (self.project_root / "tests").mkdir()  # Create tests directory
        (self.project_root / ".github" / "workflows").mkdir(
            parents=True, exist_ok=True
        )  # Create CI directory
        (self.project_root / "reports").mkdir()
        # --- END OF MODIFIED SECTION ---

        intent_dir = self.project_root / ".intent"
        intent_dir.mkdir()

        # ... (rest of the function is the same) ...

        constitutional_files_to_copy = [
            "principles.yaml",
            "project_manifest.yaml",
            "safety_policies.yaml",
            "source_structure.yaml",
        ]

        for filename in constitutional_files_to_copy:
            source_path = self.starter_kit_path / filename
            if source_path.exists():
                shutil.copy(source_path, intent_dir / filename)

        readme_template = self.starter_kit_path / "README.md"
        if readme_template.exists():
            shutil.copy(readme_template, intent_dir / "README.md")

        for template_path in self.starter_kit_path.glob("*.template"):
            content = template_path.read_text(encoding="utf-8").format(
                project_name=self.name
            )
            target_name = (
                ".gitignore"
                if template_path.name == "gitignore.template"
                else template_path.name.replace(".template", "")
            )
            (self.project_root / target_name).write_text(content, encoding="utf-8")

        manifest_path = intent_dir / "project_manifest.yaml"
        if manifest_path.exists():
            manifest_data = yaml.safe_load(manifest_path.read_text(encoding="utf-8"))
            if manifest_data:
                manifest_data["name"] = self.name
                manifest_path.write_text(
                    yaml.dump(manifest_data, indent=2), encoding="utf-8"
                )

        log.info(f"   -> ✅ Base structure for '{self.name}' created successfully.")

    def write_file(self, relative_path: str, content: str):
        """Writes content to a file within the new project's directory, creating parent directories as needed."""
        target_file = self.project_root / relative_path
        target_file.parent.mkdir(parents=True, exist_ok=True)
        target_file.write_text(content, encoding="utf-8")
        log.info(f"   -> 📄 Wrote agent-generated file: {relative_path}")


def new_project(
    name: str = typer.Argument(
        ...,
        help="The name of the new CORE-governed application to create.",
    ),
    profile: str = typer.Option(
        "default",
        "--profile",
        help="The starter kit profile to use for the new project's constitution.",
    ),
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--write",
        help="Show what will be created without writing files. Use --write to apply.",
    ),
):
    """Scaffolds a new CORE-governed application with the given name, profile, and dry-run option, including base structure and README generation."""
    scaffolder = Scaffolder(project_name=name, profile=profile)
    log.info(
        f"🚀 Scaffolding new CORE application: '{name}' using '{profile}' profile."
    )
    if dry_run:
        log.info("\n💧 Dry Run Mode: No files will be written.")
        typer.secho(
            f"Would create project '{name}' in '{scaffolder.workspace}/' with the '{profile}' starter kit.",
            fg=typer.colors.YELLOW,
        )
    else:
        try:
            scaffolder.scaffold_base_structure()
            readme_template_path = scaffolder.starter_kit_path / "README.md.template"
            if readme_template_path.exists():
                readme_content = readme_template_path.read_text(
                    encoding="utf-8"
                ).format(project_name=name)
                scaffolder.write_file("README.md", readme_content)

        except FileExistsError as e:
            log.error(f"❌ {e}")
            raise typer.Exit(code=1)
        except Exception as e:
            log.error(f"❌ An unexpected error occurred: {e}", exc_info=True)
            raise typer.Exit(code=1)

    log.info(f"\n🎉 Scaffolding for '{name}' complete.")
    typer.secho("\nNext Steps:", bold=True)
    typer.echo(
        f"1. Navigate into your new project: `cd {scaffolder.workspace.relative_to(CORE_ROOT)}/{name}`"
    )
    typer.echo("2. Run `poetry install` to set up the environment.")
    typer.echo(
        f"3. From the CORE directory, run `core-admin byor-init {scaffolder.workspace.relative_to(CORE_ROOT)}/{name}` to perform the first audit."
    )

--- END OF FILE ./src/system/tools/scaffolder.py ---

--- START OF FILE ./src/work/my-first-ai-app/capability_tags.yaml ---
# .intent/knowledge/capability_tags.yaml
#
# This is the canonical dictionary of all valid capability tags for this project.
# The CORE ConstitutionalAuditor will verify that any # CAPABILITY tag used in
# the source code is defined in this file.
#
# This file was seeded by CORE's analysis of your repository. You should add
# a 'description' for each capability to clarify its purpose.

tags:
  # - name: your_capability_1
  #   description: "A clear explanation of what this capability does."
  # - name: your_capability_2
  #   description: "Another clear explanation."

--- END OF FILE ./src/work/my-first-ai-app/capability_tags.yaml ---

--- START OF FILE ./src/work/my-first-ai-app/.gitignore ---
# Python
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.so
.venv/
.env

# CORE-specific artifacts
.intent/knowledge/knowledge_graph.json
reports/
logs/
sandbox/
pending_writes/

# IDE / Editor
.vscode/
.idea/

--- END OF FILE ./src/work/my-first-ai-app/.gitignore ---

--- START OF FILE ./src/work/my-first-ai-app/.intent/principles.yaml ---
# .intent/mission/principles.yaml
principles:
  - id: clarity_first
    description: >
      Every function must have a docstring explaining its purpose.
      If a human cannot understand a piece of code in 30 seconds, it must be simplified.
  - id: safe_by_default
    description: >
      Every change must assume rollback or rejection unless explicitly validated.
      No file write or code execution may proceed without confirmation or a safety check.
  - id: no_orphaned_logic
    description: >
      All code must be discoverable and auditable. No function or file should exist
      without being traceable to a manifest or a clear purpose.
  - id: single_source_of_truth
    description: >
      The `.intent/` directory is the single source of truth for the project's
      capabilities, structure, and intent. All governance is derived from these files.

--- END OF FILE ./src/work/my-first-ai-app/.intent/principles.yaml ---

--- START OF FILE ./src/work/my-first-ai-app/.intent/project_manifest.yaml ---
intent: A new project, ready to be guided by a clear intent.
name: my-first-ai-app
required_capabilities: []
version: 0.1.0

--- END OF FILE ./src/work/my-first-ai-app/.intent/project_manifest.yaml ---

--- START OF FILE ./src/work/my-first-ai-app/.intent/README.md ---
# Welcome to Your New CORE Constitution

This `.intent/` directory is the "Mind" of your new application. It contains the complete, machine-readable definition of your project's goals, rules, and structure. It is the single source of truth that governs the behavior of any AI agents working on this codebase.

This starter kit was generated by CORE to provide a balanced, best-practice foundation for your project.

## Your First Steps

1.  **Review `mission/principles.yaml`**: These are the high-level values of your project. You should edit them to match your own philosophy.
2.  **Review `project_manifest.yaml`**: This file lists the capabilities your application is expected to have. As you add `# CAPABILITY:` tags to your code, this list should grow.
3.  **Run Your First Audit**: Use CORE's tools to run a constitutional audit. This will tell you if your code is in alignment with the rules defined here.

This constitution is now yours. Evolve it, amend it, and use it to guide your project's growth with clarity and purpose.

--- END OF FILE ./src/work/my-first-ai-app/.intent/README.md ---

--- START OF FILE ./src/work/my-first-ai-app/.intent/safety_policies.yaml ---
# .intent/policies/safety_policies.yaml
rules:
  - id: no_dangerous_execution
    description: >
      Generated or modified code must not contain calls to dangerous functions
      that enable arbitrary code execution or shell access.
    enforcement: hard
    detection:
      type: substring
      patterns:
        - "eval("
        - "exec("
        - "os.system("
        - "subprocess.run("
        - "subprocess.Popen("
    action: reject
    feedback: "❌ Dangerous execution call detected. This is forbidden by safety policies."
  - id: no_unsafe_imports
    description: >
      Prevent importing modules that enable dangerous operations unless explicitly allowed.
    enforcement: hard
    detection:
      type: import_name
      forbidden:
        - "import pickle"
        - "from subprocess import"
    action: reject
    feedback: "❌ Unsafe import detected. This is forbidden by safety policies."

--- END OF FILE ./src/work/my-first-ai-app/.intent/safety_policies.yaml ---

--- START OF FILE ./src/work/my-first-ai-app/.intent/source_structure.yaml ---
# .intent/knowledge/source_structure.yaml
structure:
  - domain: main
    path: src/main
    description: "The primary domain for this application's core logic."
    allowed_imports: [main, shared]
  - domain: shared
    path: src/shared
    description: "Shared utilities and data models."
    allowed_imports: [shared]

--- END OF FILE ./src/work/my-first-ai-app/.intent/source_structure.yaml ---

--- START OF FILE ./src/work/my-first-ai-app/pyproject.toml ---
[tool.poetry]
name = "fastapi-hello-world"
version = "0.1.0"
description = "A simple FastAPI web server that returns hello world"
authors = ["Your Name <you@example.com>"]

[tool.poetry.dependencies]
python = ">=3.7"
fastapi = "^0.68.0"
uvicorn = "^0.15.0"

[tool.poetry.dev-dependencies]

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

--- END OF FILE ./src/work/my-first-ai-app/pyproject.toml ---

--- START OF FILE ./src/work/my-first-ai-app/README.md ---
# FastAPI Hello World

A simple FastAPI web server that returns hello world.

## Installation

1. Install Poetry (if not already installed):
   ```bash
   pip install poetry
   ```

2. Install dependencies:
   ```bash
   poetry install
   ```

## Running the Server

```bash
poetry run uvicorn src.main:app --reload
```

Open your browser to [http://localhost:8000](http://localhost:8000) to see the response.

--- END OF FILE ./src/work/my-first-ai-app/README.md ---

--- START OF FILE ./src/work/my-first-ai-app/src/main.py ---
from fastapi import FastAPI

app = FastAPI()


@app.get("/")
def read_root():
    return {"message": "Hello, World"}

--- END OF FILE ./src/work/my-first-ai-app/src/main.py ---

--- START OF FILE ./tests/admin/test_agent_cli.py ---
# tests/admin/test_agent_cli.py
import json
from unittest.mock import MagicMock

import pytest

from system.admin.agent import scaffold_new_application


@pytest.fixture
def mock_scaffolder_deps(mocker, tmp_path):
    """Mocks dependencies for the scaffolding logic."""
    # Mock the Scaffolder class itself to avoid actual file operations
    mock_scaffolder_instance = MagicMock()
    mock_scaffolder_instance.workspace = tmp_path
    mock_scaffolder_instance.project_root = tmp_path / "test-app"
    mocker.patch("system.admin.agent.Scaffolder", return_value=mock_scaffolder_instance)

    # Mock the Orchestrator to return a predictable file structure
    mock_orchestrator = MagicMock()
    mock_orchestrator.make_request.return_value = json.dumps(
        {
            "src/main.py": "print('hello')",
            "pyproject.toml": "[tool.poetry]\nname = 'test-app'",
        }
    )

    mock_file_handler = MagicMock()
    mock_file_handler.repo_path = tmp_path

    return mock_scaffolder_instance, mock_orchestrator, mock_file_handler


def test_scaffold_new_application_logic_success(mock_scaffolder_deps):
    """Tests the core logic of scaffolding a new application."""
    mock_scaffolder, mock_orchestrator, mock_file_handler = mock_scaffolder_deps

    success, message = scaffold_new_application(
        project_name="test-app",
        goal="A simple test app",
        orchestrator=mock_orchestrator,
        file_handler=mock_file_handler,
        initialize_git=False,
    )

    assert success is True
    assert "Successfully scaffolded" in message
    # Verify that the scaffolder was called to create the base and write files
    mock_scaffolder.scaffold_base_structure.assert_called_once()
    assert mock_scaffolder.write_file.call_count == 4  # 2 from LLM, 2 for test/CI


def test_scaffold_new_application_handles_llm_failure(mock_scaffolder_deps):
    """Tests that scaffolding fails gracefully if the LLM returns invalid data."""
    mock_scaffolder, mock_orchestrator, mock_file_handler = mock_scaffolder_deps
    mock_orchestrator.make_request.return_value = "this is not json"

    success, message = scaffold_new_application(
        project_name="test-app",
        goal="A simple test app",
        orchestrator=mock_orchestrator,
        file_handler=mock_file_handler,
    )

    assert success is False
    assert "Scaffolding failed" in message
    mock_scaffolder.scaffold_base_structure.assert_not_called()

--- END OF FILE ./tests/admin/test_agent_cli.py ---

--- START OF FILE ./tests/admin/test_guard_drift_cli.py ---
# tests/admin/test_guard_drift_cli.py
from __future__ import annotations

import json
from pathlib import Path

import pytest
import yaml
from typer.testing import CliRunner

from system.admin import app  # uses core-admin entrypoint

runner = CliRunner()


def write(p: Path, text: str) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(text, encoding="utf-8")


def domain_manifest_yaml(domain: str, capabilities: list[str]):
    """Helper to create a domain-specific manifest file content."""
    return yaml.safe_dump(
        {"domain": domain, "capabilities": capabilities}, sort_keys=False
    )


@pytest.fixture
def drift_test_repo(tmp_path: Path) -> Path:
    """Creates a temporary repository with the basic constitutional files needed for drift tests."""
    # Create the source_structure.yaml to map files to domains
    write(
        tmp_path / ".intent/knowledge/source_structure.yaml",
        yaml.safe_dump(
            {
                "structure": [
                    {"domain": "domain_alpha", "path": "src/domain_alpha"},
                    {"domain": "domain_beta", "path": "src/domain_beta"},
                ]
            }
        ),
    )
    # Boilerplate files for correct KGB initialization
    write(tmp_path / ".intent/knowledge/entry_point_patterns.yaml", "patterns: []")
    write(tmp_path / "pyproject.toml", "[tool.poetry]\nname = 'test-project'")
    return tmp_path


def test_guard_drift_clean_repo(drift_test_repo: Path):
    """Tests that a clean repository with modular manifests passes the drift check."""
    tmp_path = drift_test_repo
    out = tmp_path / "reports" / "drift_report.json"

    write(
        tmp_path / "src" / "domain_alpha" / "mod.py",
        "# CAPABILITY: alpha.cap\ndef alpha_func(): pass",
    )
    write(
        tmp_path / "src" / "domain_beta" / "mod.py",
        "# CAPABILITY: beta.cap\ndef beta_func(): pass",
    )
    write(
        tmp_path / "src" / "domain_alpha" / "manifest.yaml",
        domain_manifest_yaml("domain_alpha", ["alpha.cap"]),
    )
    write(
        tmp_path / "src" / "domain_beta" / "manifest.yaml",
        domain_manifest_yaml("domain_beta", ["beta.cap"]),
    )

    result = runner.invoke(
        app,
        [
            "guard",
            "drift",
            "--root",
            str(tmp_path),
            "--format",
            "json",
            "--output",
            str(out),
        ],
    )

    assert result.exit_code == 0, result.output
    report = json.loads(out.read_text(encoding="utf-8"))
    assert not report["missing_in_code"]
    assert not report["undeclared_in_manifest"]
    assert not report["mismatched_mappings"]


def test_guard_drift_detects_undeclared(drift_test_repo: Path):
    """Tests that a capability in code but not in any manifest is detected."""
    tmp_path = drift_test_repo
    write(
        tmp_path / "src" / "domain_alpha" / "manifest.yaml",
        domain_manifest_yaml("domain_alpha", ["alpha.cap"]),
    )
    write(
        tmp_path / "src" / "domain_alpha" / "mod.py",
        "# CAPABILITY: alpha.cap\ndef alpha_func(): pass\n\n# CAPABILITY: ghost.cap\ndef ghost_func(): pass",
    )
    out = tmp_path / "reports" / "drift_report.json"

    result = runner.invoke(
        app,
        [
            "guard",
            "drift",
            "--root",
            str(tmp_path),
            "--format",
            "json",
            "--fail-on",
            "any",
            "--output",
            str(out),
        ],
    )

    assert result.exit_code == 2, result.output
    report = json.loads(out.read_text(encoding="utf-8"))
    assert "ghost.cap" in report["undeclared_in_manifest"]
    assert len(report["mismatched_mappings"]) == 0


def test_guard_drift_detects_mismatched_domain(drift_test_repo: Path):
    """Tests that a capability in the wrong domain is detected as a mismatch."""
    tmp_path = drift_test_repo
    write(
        tmp_path / "src" / "domain_alpha" / "manifest.yaml",
        domain_manifest_yaml("domain_alpha", ["beta.cap"]),
    )
    write(
        tmp_path / "src" / "domain_beta" / "mod.py",
        "# CAPABILITY: beta.cap\ndef beta_func(): pass",
    )
    out = tmp_path / "reports" / "drift_report.json"

    result = runner.invoke(
        app,
        [
            "guard",
            "drift",
            "--root",
            str(tmp_path),
            "--format",
            "json",
            "--fail-on",
            "any",
            "--output",
            str(out),
        ],
    )

    assert result.exit_code == 2, result.output
    report = json.loads(out.read_text(encoding="utf-8"))
    assert not report["missing_in_code"]
    assert not report["undeclared_in_manifest"]
    assert any(m.get("capability") == "beta.cap" for m in report["mismatched_mappings"])

--- END OF FILE ./tests/admin/test_guard_drift_cli.py ---

--- START OF FILE ./tests/core/test_intent_model.py ---
# tests/core/test_intent_model.py

from pathlib import Path

import pytest

from core.intent_model import IntentModel


# Use a more specific fixture to get the project root
@pytest.fixture(scope="module")
def project_root() -> Path:
    """Fixture to provide the absolute path to the project root."""
    # This assumes the tests are run from the project root, which pytest does.
    return Path.cwd().resolve()


@pytest.fixture(scope="module")
def intent_model(project_root: Path) -> IntentModel:
    """Fixture to provide a loaded IntentModel instance."""
    return IntentModel(project_root)


def test_intent_model_loads_structure(intent_model: IntentModel):
    """Verify that the intent model loads the structure data without crashing."""
    assert intent_model.structure is not None
    assert "core" in intent_model.structure
    assert "agents" in intent_model.structure
    assert isinstance(intent_model.structure["core"], dict)


def test_resolve_domain_for_path_core(intent_model: IntentModel, project_root: Path):
    """Test that a path within the 'core' domain resolves correctly."""
    # Create a dummy path that would exist in the core domain
    core_file_path = project_root / "src" / "core" / "main.py"
    domain = intent_model.resolve_domain_for_path(core_file_path)
    assert domain == "core"


def test_resolve_domain_for_path_agents(intent_model: IntentModel, project_root: Path):
    """Test that a path within the 'agents' domain resolves correctly."""
    agents_file_path = project_root / "src" / "agents" / "planner_agent.py"
    domain = intent_model.resolve_domain_for_path(agents_file_path)
    assert domain == "agents"


def test_resolve_domain_for_path_unassigned(
    intent_model: IntentModel, project_root: Path
):
    """Test that a path outside any defined domain resolves to None."""
    # A path that doesn't fall into any defined source structure domain
    other_file_path = project_root / "README.md"
    domain = intent_model.resolve_domain_for_path(other_file_path)
    # The current implementation might resolve to None or a default.
    # Based on the code, it should be None as it's outside 'src'.
    assert domain is None


def test_get_domain_permissions_core(intent_model: IntentModel):
    """Check the permissions for a domain that has defined allowed_imports."""
    core_permissions = intent_model.get_domain_permissions("core")
    assert isinstance(core_permissions, list)
    assert "shared" in core_permissions


def test_get_domain_permissions_unrestricted(intent_model: IntentModel):
    """Check that a domain without 'allowed_imports' returns an empty list."""
    # Assuming a domain 'policies' might not have explicit imports defined
    # in source_structure.yaml. This may need adjustment if that file changes.
    policy_permissions = intent_model.get_domain_permissions("policies")
    assert isinstance(policy_permissions, list)
    assert policy_permissions == []

--- END OF FILE ./tests/core/test_intent_model.py ---

--- START OF FILE ./tests/governance/test_local_mode_governance.py ---
# tests/governance/test_local_mode_governance.py
"""
Tests to ensure that CORE's governance principles are correctly
reflected in its configuration files.
"""
from shared.config_loader import load_config
from shared.path_utils import get_repo_root


def test_local_fallback_requires_git_checkpoint():
    """Ensure local_mode.yaml correctly enforces Git validation."""
    repo_root = get_repo_root()
    config_path = repo_root / ".intent" / "config" / "local_mode.yaml"

    # Check that the file actually exists before testing its content
    assert config_path.exists(), "local_mode.yaml configuration file is missing."

    config = load_config(config_path)

    # This is a critical safety check: local mode must not bypass Git commits.
    ignore_validation = config.get("apis", {}).get("git", {}).get("ignore_validation")
    assert (
        ignore_validation is False
    ), "CRITICAL: local_mode.yaml is configured to ignore Git validation."

--- END OF FILE ./tests/governance/test_local_mode_governance.py ---

--- START OF FILE ./tests/integration/test_full_run.py ---
# tests/integration/test_full_run.py
"""
An end-to-end integration test for the CORE system.
This test simulates a real user request and verifies the entire
Plan -> Generate -> Execute cycle with the new agent architecture.
"""

from pathlib import Path
from unittest.mock import AsyncMock, MagicMock

import pytest
from fastapi.testclient import TestClient

from core.main import app


@pytest.fixture
def mock_agents(mocker):
    """Mocks the PlannerAgent and ExecutionAgent."""
    # Mock Planner to return a valid plan
    mock_plan = [
        MagicMock(
            step="Create a simple Python file.",
            action="create_file",
            params=MagicMock(file_path="src/hello.py", code=None),
        )
    ]
    mocker.patch("core.main.PlannerAgent.create_execution_plan", return_value=mock_plan)

    # Mock ExecutionAgent to succeed
    mocker.patch(
        "core.main.ExecutionAgent.execute_plan", new_callable=AsyncMock
    ).return_value = (True, "Plan executed successfully.")


@pytest.fixture
def test_git_repo(tmp_path: Path):
    """Creates a temporary, valid Git repository for the test to run in."""
    import subprocess

    subprocess.run(["git", "init"], cwd=tmp_path, check=True)
    (tmp_path / "src").mkdir()
    return tmp_path


def test_execute_goal_end_to_end(mock_agents, test_git_repo, monkeypatch, mocker):
    """Tests the entire /execute_goal endpoint flow with the refactored agents."""
    monkeypatch.chdir(test_git_repo)

    # We still need to mock the services that are initialized in main.py
    mocker.patch("core.main.OrchestratorClient")
    mocker.patch("core.main.GeneratorClient")
    mocker.patch("core.main.GitService")
    mocker.patch("core.main.IntentGuard")

    with TestClient(app) as client:
        response = client.post(
            "/execute_goal", json={"goal": "Create a hello world script"}
        )

        assert response.status_code == 200
        assert response.json()["status"] == "success"

--- END OF FILE ./tests/integration/test_full_run.py ---

--- START OF FILE ./tests/unit/test_agent_utils.py ---
# tests/unit/test_agent_utils.py
import re
import textwrap

import pytest

from agents.utils import CodeEditor


@pytest.fixture
def code_editor():
    """Provides an instance of the CodeEditor."""
    return CodeEditor()


@pytest.fixture
def sample_code():
    """Provides a sample Python code snippet for testing."""
    return textwrap.dedent(
        """
        # A sample file
        import os

        class MyClass:
            def method_one(self):
                \"\"\"This is the first method.\"\"\"
                return 1

        def top_level_function(a, b):
            \"\"\"A function at the top level.\"\"\"
            return a + b
    """
    )


def test_replace_simple_function(code_editor, sample_code):
    """Tests replacing a top-level function with a new version."""
    new_function_code = textwrap.dedent(
        """
        def top_level_function(a, b):
            \"\"\"A modified function.\"\"\"
            # Added a comment
            return a * b  # Changed the operation
    """
    )
    modified_code = code_editor.replace_symbol_in_code(
        sample_code, "top_level_function", new_function_code
    )

    assert "return a * b" in modified_code
    assert "return a + b" not in modified_code
    assert "class MyClass:" in modified_code
    assert "method_one" in modified_code
    assert "# A sample file" in modified_code  # Check that comments are preserved


def test_replace_method_in_class(code_editor, sample_code):
    """Tests replacing a method within a class."""
    new_method_code = textwrap.dedent(
        """
        def method_one(self):
            \"\"\"A new docstring for the method.\"\"\"
            return 100
    """
    )
    modified_code = code_editor.replace_symbol_in_code(
        sample_code, "method_one", new_method_code
    )

    assert "return 100" in modified_code
    # Ensure there's no standalone "return 1" line (avoid substring false positives)
    assert not re.search(r"(?m)^\s*return\s+1\s*$", modified_code)
    assert "top_level_function" in modified_code
    # Crucially, check that the class definition is still present
    assert "class MyClass:" in modified_code


def test_replace_symbol_not_found_raises_error(code_editor, sample_code):
    """Tests that a ValueError is raised if the target symbol doesn't exist."""
    new_code = "def new_func(): return None"
    with pytest.raises(ValueError, match="Symbol 'non_existent_function' not found"):
        code_editor.replace_symbol_in_code(
            sample_code, "non_existent_function", new_code
        )


def test_replace_with_invalid_original_syntax_raises_error(code_editor):
    """Tests that a ValueError is raised if the original code has a syntax error."""
    invalid_original_code = "def top_level_function(a, b) return a + b"
    new_code = "def top_level_function(a,b): return a*b"
    with pytest.raises(
        ValueError, match="Could not parse original code due to syntax error"
    ):
        code_editor.replace_symbol_in_code(
            invalid_original_code, "top_level_function", new_code
        )

--- END OF FILE ./tests/unit/test_agent_utils.py ---

--- START OF FILE ./tests/unit/test_clients.py ---
# tests/unit/test_clients.py
from unittest.mock import MagicMock

import pytest

from core.clients import OrchestratorClient


@pytest.fixture
def set_orchestrator_env(monkeypatch):
    monkeypatch.setenv("ORCHESTRATOR_API_URL", "http://fake-orchestrator.com/api/v1")
    monkeypatch.setenv("ORCHESTRATOR_API_KEY", "fake_orch_key")
    # --- THIS IS THE FIX ---
    # The test now expects the model name to be the new default from config.py.
    monkeypatch.setenv("ORCHESTRATOR_MODEL_NAME", "deepseek-chat")


def test_make_request_sends_correct_chat_payload(set_orchestrator_env, mocker):
    mock_post = mocker.patch("requests.post")
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        "choices": [{"message": {"content": "This is a mock chat response."}}]
    }
    mock_post.return_value = mock_response

    client = OrchestratorClient()
    prompt_text = "Analyze this user request."
    client.make_request(prompt_text, user_id="test_user")

    mock_post.assert_called_once()
    call_kwargs = mock_post.call_args.kwargs

    sent_payload = call_kwargs["json"]
    # The test will now correctly assert the model name.
    assert sent_payload["model"] == "deepseek-chat"

--- END OF FILE ./tests/unit/test_clients.py ---

--- START OF FILE ./tests/unit/test_execution_agent.py ---
# tests/unit/test_execution_agent.py
from unittest.mock import AsyncMock, MagicMock

import pytest

from agents.execution_agent import ExecutionAgent
from agents.models import ExecutionTask, TaskParams
from agents.plan_executor import PlanExecutionError


@pytest.fixture
def mock_dependencies():
    """Mocks all external dependencies for the ExecutionAgent."""
    # Use AsyncMock for any objects that have async methods to be called
    return {
        "generator_client": MagicMock(),
        "prompt_pipeline": MagicMock(),
        "plan_executor": AsyncMock(),
    }


@pytest.mark.asyncio
async def test_execute_plan_success(mock_dependencies):
    """Tests that a valid plan is executed correctly."""
    agent = ExecutionAgent(**mock_dependencies)
    goal = "Test goal"
    plan = [
        ExecutionTask(
            step="Create a file",
            action="create_file",
            params=TaskParams(file_path="test.py"),
        )
    ]

    agent.generator.make_request.return_value = "print('Hello')"
    mock_dependencies["prompt_pipeline"].process.return_value = "enriched_prompt"
    mock_dependencies["plan_executor"].execute_plan.return_value = (True, "Success")

    # Mock the git_service used by the context manager
    agent.executor.git_service = MagicMock()
    agent.executor.git_service.is_git_repo.return_value = (
        False  # Disable git logic for this test
    )

    success, message = await agent.execute_plan(goal, plan)

    assert success is True
    assert message == "✅ Plan executed successfully."
    agent.generator.make_request.assert_called_once()
    mock_dependencies["plan_executor"].execute_plan.assert_awaited_once()
    executed_plan = mock_dependencies["plan_executor"].execute_plan.call_args[0][0]
    assert executed_plan[0].params.code == "print('Hello')"


@pytest.mark.asyncio
async def test_execute_plan_fails_on_code_generation_failure(mock_dependencies):
    """Tests that the process fails if code generation returns nothing."""
    agent = ExecutionAgent(**mock_dependencies)
    goal = "Test goal"
    plan = [
        ExecutionTask(
            step="Create a file",
            action="create_file",
            params=TaskParams(file_path="test.py"),
        )
    ]

    agent.generator.make_request.return_value = ""

    success, message = await agent.execute_plan(goal, plan)

    assert success is False
    assert "Code generation failed" in message
    mock_dependencies["plan_executor"].execute_plan.assert_not_awaited()


@pytest.mark.asyncio
async def test_execute_plan_handles_executor_failure(mock_dependencies):
    """Tests that failures from the PlanExecutor are propagated correctly."""
    agent = ExecutionAgent(**mock_dependencies)
    goal = "Test goal"
    plan = [
        ExecutionTask(
            step="Create a file",
            action="create_file",
            params=TaskParams(file_path="test.py"),
        )
    ]

    agent.generator.make_request.return_value = "print('Hello')"
    mock_dependencies["plan_executor"].execute_plan.side_effect = PlanExecutionError(
        "Validation failed", violations=[{"rule": "E999"}]
    )

    # Mock the git_service used by the context manager
    agent.executor.git_service = MagicMock()
    agent.executor.git_service.is_git_repo.return_value = True
    agent.executor.git_service.get_current_commit.return_value = "dummy_hash"

    success, message = await agent.execute_plan(goal, plan)

    assert success is False
    assert "Plan execution failed: Validation failed" in message

--- END OF FILE ./tests/unit/test_execution_agent.py ---

--- START OF FILE ./tests/unit/test_git_service.py ---
# tests/unit/test_git_service.py
from unittest.mock import MagicMock, call

import pytest

from core.git_service import GitService


@pytest.fixture
def mock_git_service(mocker, tmp_path):
    """Creates a GitService instance with a mocked subprocess.run."""
    (tmp_path / ".git").mkdir()

    mock_run = mocker.patch("subprocess.run")

    # Configure mock for multiple calls: first for status, then for commit
    mock_run.side_effect = [
        MagicMock(stdout=" M my_file.py", stderr="", returncode=0),  # For git status
        MagicMock(stdout="commit success", stderr="", returncode=0),  # For git commit
    ]

    service = GitService(repo_path=str(tmp_path))
    return service, mock_run


def test_git_add(mock_git_service):
    """Tests that the add method calls subprocess.run with the correct arguments."""
    service, mock_run = mock_git_service
    # Reset side_effect for this simple, single-call test
    mock_run.side_effect = None
    mock_run.return_value = MagicMock(stdout="", stderr="", returncode=0)

    file_to_add = "src/core/main.py"
    service.add(file_to_add)

    mock_run.assert_called_once_with(
        ["git", "add", file_to_add],
        cwd=service.repo_path,
        capture_output=True,
        text=True,
        check=True,
    )


def test_git_commit(mock_git_service):
    """Tests that the commit method calls subprocess.run with status and then commit."""
    service, mock_run = mock_git_service
    commit_message = "feat(agent): Test commit"

    service.commit(commit_message)

    # --- THIS IS THE FIX ---
    # Assert that run was called twice
    assert mock_run.call_count == 2

    # Check the calls were made in the correct order with correct arguments
    expected_calls = [
        call(
            ["git", "status", "--porcelain"],
            cwd=service.repo_path,
            capture_output=True,
            text=True,
            check=True,
        ),
        call(
            ["git", "commit", "-m", commit_message],
            cwd=service.repo_path,
            capture_output=True,
            text=True,
            check=True,
        ),
    ]
    mock_run.assert_has_calls(expected_calls)


def test_is_git_repo_true(tmp_path):
    """Tests that is_git_repo returns True when a .git directory exists."""
    (tmp_path / ".git").mkdir()
    service = GitService(repo_path=str(tmp_path))
    assert service.is_git_repo() is True


def test_is_git_repo_false(tmp_path):
    """Tests that GitService raises an error if .git directory is missing on init."""
    with pytest.raises(ValueError):
        GitService(repo_path=str(tmp_path))

--- END OF FILE ./tests/unit/test_git_service.py ---

--- START OF FILE ./tests/unit/test_planner_agent.py ---
# tests/unit/test_planner_agent.py
import json
from unittest.mock import patch

import pytest

from agents.models import ExecutionTask
from agents.plan_executor import PlanExecutionError
from agents.planner_agent import PlannerAgent, PlannerConfig


@pytest.fixture
def mock_dependencies():
    """Mocks all external dependencies for the NEW, simpler PlannerAgent."""
    return {
        "orchestrator_client": patch("core.clients.OrchestratorClient").start(),
        "prompt_pipeline": patch("core.prompt_pipeline.PromptPipeline").start(),
        "config": PlannerConfig(),
    }


def test_create_execution_plan_success(mock_dependencies):
    """Tests that the planner can successfully parse a valid high-level plan."""
    agent = PlannerAgent(**mock_dependencies)
    goal = "Test goal"

    plan_json = json.dumps(
        [
            {
                "step": "A valid step",
                "action": "create_file",
                "params": {"file_path": "src/test.py"},
            }
        ]
    )
    agent.orchestrator.make_request.return_value = f"```json\n{plan_json}\n```"

    plan = agent.create_execution_plan(goal)

    assert len(plan) == 1
    assert isinstance(plan[0], ExecutionTask)
    assert plan[0].action == "create_file"
    agent.prompt_pipeline.process.assert_called_once()


def test_create_execution_plan_fails_on_invalid_action(mock_dependencies):
    """Tests that the planner fails if the plan contains an invalid action."""
    agent = PlannerAgent(**mock_dependencies)
    goal = "Test goal"

    invalid_plan_json = json.dumps(
        [{"step": "Invalid action", "action": "make_coffee", "params": {}}]
    )
    agent.orchestrator.make_request.return_value = f"```json\n{invalid_plan_json}\n```"

    with pytest.raises(PlanExecutionError):
        agent.create_execution_plan(goal)


# Stop patching after tests are done
@pytest.fixture(autouse=True, scope="module")
def stop_patches():
    yield
    patch.stopall()

--- END OF FILE ./tests/unit/test_planner_agent.py ---

--- START OF FILE ./tests/unit/test_test_runner.py ---
# tests/unit/test_test_runner.py
"""
Tests for the core test execution capability in `core/test_runner.py`.
This ensures that the system's own "immune system" is reliable.
"""
from unittest.mock import MagicMock

import pytest

from core.test_runner import run_tests


@pytest.fixture
def mock_subprocess_run(mocker):
    """Mocks the `subprocess.run` function."""
    return mocker.patch("core.test_runner.subprocess.run")


def test_run_tests_success(mock_subprocess_run):
    """
    Verify that run_tests correctly interprets a successful test run (exit code 0).
    """
    # Arrange: Configure the mock to simulate a successful pytest execution
    mock_subprocess_run.return_value = MagicMock(
        returncode=0,
        stdout="============================= 1 passed in 0.01s =============================",
        stderr="",
    )

    # Act
    result = run_tests()

    # Assert
    assert result["exit_code"] == "0"
    assert result["summary"] == "✅ Tests passed"
    mock_subprocess_run.assert_called_once()


def test_run_tests_failure(mock_subprocess_run):
    """
    Verify that run_tests correctly interprets a failed test run (non-zero exit code).
    """
    # Arrange: Configure the mock to simulate a failed pytest execution
    mock_subprocess_run.return_value = MagicMock(
        returncode=1,
        stdout="============================== 1 failed in 0.01s ==============================",
        stderr="AssertionError: assert False",
    )

    # Act
    result = run_tests()

    # Assert
    assert result["exit_code"] == "1"
    assert result["summary"] == "❌ Tests failed"
    mock_subprocess_run.assert_called_once()


def test_run_tests_pytest_not_found(mock_subprocess_run):
    """
    Verify that run_tests handles the case where pytest is not installed.
    """
    # Arrange: Configure the mock to simulate a FileNotFoundError
    mock_subprocess_run.side_effect = FileNotFoundError(
        "No such file or directory: 'pytest'"
    )

    # Act
    result = run_tests()

    # Assert
    assert result["summary"] == "❌ pytest not found"
    assert "pytest is not installed" in result["stderr"]
    mock_subprocess_run.assert_called_once()

--- END OF FILE ./tests/unit/test_test_runner.py ---

