# src/mind/governance/checks/base_check.py
"""
Provides a shared base class for all constitutional audit checks to inherit from.
"""

from __future__ import annotations
from typing import TYPE_CHECKING, ClassVar, List

if TYPE_CHECKING:
    from mind.governance.audit_context import AuditorContext


# ID: 2cb0374b-a487-4dce-bab1-c2ee8a693b0a
class BaseCheck:
    """
    A base class for audit checks, providing a shared context and requiring
    subclasses to declare the constitutional rules they enforce.
    """

    # --- CONSTITUTIONAL ENFORCEMENT CONTRACT ---
    # Every subclass MUST override this attribute to declare which specific
    # policy rule IDs it is responsible for enforcing. This creates a traceable
    # link between the constitution (the law) and the checks (the enforcement).
    policy_rule_ids: ClassVar[List[str]] = []

    def __init__(self, context: AuditorContext):
        """
        Initializes the check with a shared auditor context.
        This common initializer serves the 'dry_by_design' principle.
        """
        self.context = context
        self.repo_root = context.repo_path
        self.intent_path = context.intent_path
        self.src_dir = context.src_dir

        # Future enhancement: You could add logic here to validate that
        # subclasses have indeed overridden `policy_rule_ids`.
        if not self.policy_rule_ids:
            print(
                f"Warning: Check '{self.__class__.__name__}' does not enforce any policy rules."
            )# src/mind/governance/checks/capability_coverage.py
"""
A constitutional audit check to ensure that all capabilities declared in the
project manifest are implemented in the database, enforcing the 'knowledge.database_ssot' rule.
"""

from __future__ import annotations

from mind.governance.audit_context import AuditorContext
# Import the BaseCheck to inherit from it
from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 979ce56f-7f3c-40e7-8736-ce219bab6ad8
# Inherit from BaseCheck
class CapabilityCoverageCheck(BaseCheck):
    """
    Verifies that every capability in the manifest has a corresponding
    implementation entry in the database's symbols table.
    """

    # Fulfills the contract from BaseCheck, linking this check directly
    # to the data_governance policy.
    policy_rule_ids = ["knowledge.database_ssot"]

    # The __init__ method is no longer needed; it is handled by BaseCheck.

    # ID: e0730fb8-2616-42b2-915b-48f30ff4ac17
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings: list[AuditFinding] = []

        manifest_path = self.context.mind_path / "project_manifest.yaml"
        if not manifest_path.exists():
            # This is a structural issue, not a direct rule violation from the
            # core policies yet. We can give it a more specific check_id
            # related to overall structural compliance.
            findings.append(
                AuditFinding(
                    check_id="structural_compliance.manifest.missing",
                    severity=AuditSeverity.ERROR,
                    message=(
                        "The project_manifest.yaml file is missing from .intent/mind/."
                    ),
                    file_path=str(manifest_path.relative_to(self.context.repo_path)),
                )
            )
            return findings

        manifest_content = self.context._load_yaml(manifest_path)
        declared_capabilities: set[str] = set(
            manifest_content.get("capabilities", [])
        )

        # SSOT-correct logic: The database is the source of truth.
        implemented_capabilities: set[str] = {
            s["capability"]
            for s in self.context.knowledge_graph.get("symbols", {}).values()
            if s.get("capability")
        }

        missing_implementations = declared_capabilities - implemented_capabilities

        for cap_key in sorted(missing_implementations):
            findings.append(
                AuditFinding(
                    # The check_id is now the exact ID from the constitution.
                    check_id="knowledge.database_ssot",
                    # The severity now matches the policy's enforcement level.
                    severity=AuditSeverity.ERROR,
                    message=(
                        f"Violation of 'knowledge.database_ssot': Capability '{cap_key}' "
                        "is declared in the manifest but has no implementation in the database (SSOT)."
                    ),
                    file_path=str(manifest_path.relative_to(self.context.repo_path)),
                )
            )

        return findings# src/mind/governance/checks/coverage_check.py
"""
Constitutional enforcement of test coverage requirements.

Verifies that the codebase meets the coverage requirements defined in the
quality_assurance policy.
"""

from __future__ import annotations

import json
import subprocess
from typing import Any

from mind.governance.audit_context import AuditorContext
# Import the BaseCheck to inherit from it
from mind.governance.checks.base_check import BaseCheck
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity


logger = getLogger(__name__)


# ID: f09915fb-02c8-49d4-b5c5-19cd5e955df4
# Inherit from BaseCheck
class CoverageGovernanceCheck(BaseCheck):
    """
    Enforces constitutional test coverage requirements.

    This check verifies that:
    1. Overall coverage meets the minimum threshold.
    2. Critical paths meet their specific higher thresholds.
    3. No significant coverage regressions have occurred.
    """

    # Fulfills the contract from BaseCheck. These are the primary rules in the
    # quality_assurance policy that this check is responsible for enforcing.
    policy_rule_ids = [
        "coverage.minimum_threshold",
        "coverage.no_untested_commits",
    ]

    def __init__(self, context: AuditorContext) -> None:
        """Initializes the check using the context provided by BaseCheck."""
        super().__init__(context)
        # Get the policy from the shared context instead of loading it manually.
        policy = self.context.policies.get("quality_assurance", {})
        coverage_cfg = policy.get("coverage_requirements", {})

        self.minimum_threshold: float = coverage_cfg.get("minimum_threshold", 75.0)
        self.critical_paths: list[str] = coverage_cfg.get("critical_paths", [])
        self.exclusions: list[str] = coverage_cfg.get("exclusions", [])

    # ID: a8126c8d-f9b8-40d5-a098-4aa5065f656c
    # The original was async, we keep it that way assuming the auditor can handle it.
    async def execute(self) -> list[AuditFinding]:
        """
        Executes the coverage check and returns audit findings.
        """
        findings: list[AuditFinding] = []

        coverage_data = self._measure_coverage()
        if not coverage_data:
            return [
                AuditFinding(
                    check_id="coverage.minimum_threshold",
                    severity=AuditSeverity.ERROR,
                    message="Failed to measure test coverage",
                    file_path="N/A",
                    context={"error": "Could not run pytest coverage"},
                )
            ]

        overall_coverage = coverage_data.get("overall_percent", 0.0)

        # 1) Enforce coverage.minimum_threshold
        if overall_coverage < self.minimum_threshold:
            findings.append(
                AuditFinding(
                    check_id="coverage.minimum_threshold",
                    severity=AuditSeverity.ERROR,
                    message=(
                        f"Coverage {overall_coverage}% below constitutional minimum "
                        f"{self.minimum_threshold}%"
                    ),
                    file_path="N/A",
                    context={
                        "current": overall_coverage,
                        "required": self.minimum_threshold,
                        "delta": overall_coverage - self.minimum_threshold,
                        "action": "Trigger autonomous remediation",
                    },
                )
            )

        # 2) Enforce critical path thresholds
        for path_spec in self._iter_critical_path_specs():
            path_pattern, required = self._parse_path_spec(path_spec)
            actual = self._get_path_coverage(coverage_data, path_pattern)
            if actual is not None and actual < required:
                findings.append(
                    AuditFinding(
                        # Structured check_id to show it's a specific violation
                        # of the minimum threshold rule.
                        check_id="coverage.minimum_threshold.critical_path",
                        severity=AuditSeverity.ERROR,
                        message=(
                            f"Critical path '{path_pattern}' coverage {actual}% "
                            f"below required {required}%"
                        ),
                        file_path=path_pattern,
                        context={
                            "current": actual,
                            "required": required,
                            "delta": actual - required,
                        },
                    )
                )

        # 3) Enforce coverage.no_untested_commits (regression check)
        regression = self._check_regression(coverage_data)
        if regression:
            findings.append(regression)

        return findings

    def _measure_coverage(self) -> dict[str, Any] | None:
        """Runs pytest with coverage and returns parsed results."""
        try:
            # Use self.repo_root from BaseCheck for consistency
            result = subprocess.run(
                [
                    "poetry", "run", "pytest", "--cov=src",
                    "--cov-report=json", "--cov-report=term", "-q",
                ],
                cwd=self.repo_root,
                capture_output=True, text=True, timeout=300,
            )

            coverage_json = self.repo_root / "coverage.json"
            if coverage_json.exists():
                data = json.loads(coverage_json.read_text())
                totals = data.get("totals", {})
                return {
                    "overall_percent": float(totals.get("percent_covered", 0) or 0),
                    "lines_covered": int(totals.get("covered_lines", 0) or 0),
                    "lines_total": int(totals.get("num_statements", 0) or 0),
                    "files": data.get("files", {}),
                    "timestamp": data.get("meta", {}).get("timestamp"),
                }
            return self._parse_term_output(result.stdout)
        except subprocess.TimeoutExpired:
            logger.error("Coverage measurement timed out after 5 minutes")
            return None
        except Exception as exc:
            logger.error("Failed to measure coverage: %s", exc, exc_info=True)
            return None

    def _parse_term_output(self, output: str) -> dict[str, Any] | None:
        """Fallback parser for terminal coverage output."""
        try:
            for line in output.splitlines():
                if line.startswith("TOTAL"):
                    parts = line.split()
                    if len(parts) >= 4:
                        percent_str = parts[-1].rstrip("%")
                        percent = float(percent_str)
                        total_lines = int(parts[1])
                        missed_lines = int(parts[2])
                        covered_lines = total_lines - missed_lines
                        return {
                            "overall_percent": percent,
                            "lines_total": total_lines,
                            "lines_covered": covered_lines,
                        }
        except Exception as exc:
            logger.debug("Failed to parse coverage output: %s", exc)
        return None

    def _iter_critical_path_specs(self) -> list[str]:
        """Returns the list of critical path specifications."""
        return list(self.critical_paths or [])

    def _parse_path_spec(self, spec: str) -> tuple[str, float]:
        """Parses a path specification like 'src/core/**/*.py: 85%'."""
        parts = spec.split(":", maxsplit=1)
        path = parts[0].strip()
        percent_str = parts[1].strip().rstrip("%") if len(parts) > 1 else "0"
        required = float(percent_str or 0)
        return path, required

    def _get_path_coverage(
        self, coverage_data: dict[str, Any], pattern: str
    ) -> float | None:
        """Gets coverage percentage for files matching a pattern."""
        from fnmatch import fnmatch
        files = coverage_data.get("files", {})
        if not files: return None
        total_lines, covered_lines = 0, 0
        for file_path, file_data in files.items():
            if fnmatch(file_path, pattern):
                summary = file_data.get("summary", {})
                total_lines += int(summary.get("num_statements", 0) or 0)
                covered_lines += int(summary.get("covered_lines", 0) or 0)
        if total_lines == 0: return None
        return round(covered_lines / total_lines * 100, 2)

    def _check_regression(self, coverage_data: dict[str, Any]) -> AuditFinding | None:
        """Checks for significant coverage regressions."""
        # Use self.repo_root from BaseCheck for consistency
        history_file = self.repo_root / "work" / "testing" / "coverage_history.json"
        if not history_file.exists():
            self._save_coverage_history(coverage_data)
            return None
        try:
            history = json.loads(history_file.read_text())
            last_run = history.get("last_run", {})
            last_percent = float(last_run.get("overall_percent", 0) or 0)
            current_percent = float(coverage_data.get("overall_percent", 0) or 0)
            delta = current_percent - last_percent
            self._save_coverage_history(coverage_data)
            if delta < -5.0:
                # This finding correctly uses the constitutional rule ID.
                return AuditFinding(
                    check_id="coverage.no_untested_commits",
                    severity=AuditSeverity.ERROR,
                    message=f"Significant coverage regression: {abs(delta):.1f}% drop",
                    file_path="N/A",
                    context={"previous": last_percent, "current": current_percent, "delta": delta},
                )
        except Exception as exc:
            logger.debug("Could not check coverage regression: %s", exc)
        return None

    def _save_coverage_history(self, coverage_data: dict[str, Any]) -> None:
        """Saves coverage data to history file for regression tracking."""
        try:
            # Use self.repo_root from BaseCheck for consistency
            history_file = self.repo_root / "work" / "testing" / "coverage_history.json"
            history_file.parent.mkdir(parents=True, exist_ok=True)
            history = {
                "last_run": coverage_data,
                "updated_at": coverage_data.get("timestamp"),
            }
            history_file.write_text(json.dumps(history, indent=2))
        except Exception as exc:
            logger.debug("Could not save coverage history: %s", exc)# src/mind/governance/checks/dependency_injection_check.py
"""
A constitutional audit check to enforce the Dependency Injection (DI) policy.

This check is responsible for enforcing all DI-related policy rules
from charter/policies/code_standards.yaml.
"""

from __future__ import annotations

import ast
from pathlib import Path
from typing import Any, Iterable, List

from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity


logger = getLogger(__name__)


# ID: 68fa7a18-3591-46ad-9470-0a0bb8685491
class DependencyInjectionCheck(BaseCheck):
    """
    Ensures that services and features do not directly instantiate their dependencies,
    do not use forbidden global imports, and prefer constructor injection.
    """

    # Fulfills the contract from BaseCheck, now covering ALL DI rules.
    policy_rule_ids = [
        "di.no_direct_instantiation",
        "di.no_global_session_import",
        "di.constructor_injection_preferred",
    ]

    def __init__(self, context: AuditorContext) -> None:
        super().__init__(context)
        code_standards_policy: dict[str, Any] = self.context.policies.get(
            "code_standards", {}
        )
        self.policy: list[dict[str, Any]] = code_standards_policy.get(
            "dependency_injection", []
        )
        # Create a quick lookup map for rules by their ID for efficiency
        self.rules_by_id = {rule.get("id"): rule for rule in self.policy}


    # ID: e0b8b3db-959e-4ac1-bc26-a7f3e1b35bc0
    def execute(self) -> list[AuditFinding]:
        """Runs the DI check by scanning source files for policy violations."""
        findings: list[AuditFinding] = []

        # More robustly check if a specific rule is defined in the policy
        if "di.no_direct_instantiation" in self.rules_by_id:
            rule = self.rules_by_id["di.no_direct_instantiation"]
            findings.extend(self._check_forbidden_instantiations(rule))

        if "di.no_global_session_import" in self.rules_by_id:
            rule = self.rules_by_id["di.no_global_session_import"]
            findings.extend(self._check_forbidden_imports(rule))

        if "di.constructor_injection_preferred" in self.rules_by_id:
            rule = self.rules_by_id["di.constructor_injection_preferred"]
            findings.extend(self._check_constructor_injection(rule))

        return findings

    def _check_forbidden_instantiations(self, rule: dict[str, Any]) -> list[AuditFinding]:
        """Finds direct instantiations of major services."""
        findings: list[AuditFinding] = []
        forbidden_calls: set[str] = set(rule.get("forbidden_instantiations", []))
        if not forbidden_calls: return findings

        scope: list[str] = rule.get("scope", [])
        exclusions: list[str] = rule.get("exclusions", [])

        for file_path in self._get_files_in_scope(scope, exclusions):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):
                        if node.func.id in forbidden_calls:
                            findings.append(
                                AuditFinding(
                                    # Use the rule ID directly from the policy
                                    check_id=rule.get("id"),
                                    severity=AuditSeverity.ERROR,
                                    message=(
                                        f"Direct instantiation of '{node.func.id}' is "
                                        "forbidden. Inject it via the constructor."
                                    ),
                                    file_path=str(file_path.relative_to(self.repo_root)),
                                    line_number=node.lineno,
                                    context={"category": "architectural"},
                                )
                            )
            except (SyntaxError, OSError) as exc:
                logger.debug("Skipping DI scan for %s due to error: %s", file_path, exc)
        return findings

    def _check_forbidden_imports(self, rule: dict[str, Any]) -> list[AuditFinding]:
        """Finds direct imports of forbidden functions like get_session."""
        findings: list[AuditFinding] = []
        forbidden_imports: set[str] = set(rule.get("forbidden_imports", []))
        if not forbidden_imports: return findings

        scope: list[str] = rule.get("scope", [])
        exclusions: list[str] = rule.get("exclusions", [])

        for file_path in self._get_files_in_scope(scope, exclusions):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))
                for node in ast.walk(tree):
                    if isinstance(node, ast.ImportFrom) and node.module in forbidden_imports:
                        findings.append(
                            AuditFinding(
                                # Use the rule ID directly from the policy
                                check_id=rule.get("id"),
                                severity=AuditSeverity.ERROR,
                                message=(
                                    f"Direct import of '{node.module}' is forbidden. "
                                    "Inject the dependency instead."
                                ),
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=node.lineno,
                                context={"category": "architectural"},
                            )
                        )
            except (SyntaxError, OSError) as exc:
                logger.debug("Skipping DI scan for %s due to error: %s", file_path, exc)
        return findings

    def _check_constructor_injection(self, rule: dict[str, Any]) -> list[AuditFinding]:
        """
        Verifies that services prefer constructor injection.

        NOTE: This is a complex check to implement fully via static analysis.
        This serves as a placeholder for future enhancement.
        """
        # TODO: Implement AST logic to detect when services are instantiated
        # as attributes outside of the constructor.
        return []

    def _get_files_in_scope(
        self, scope: Iterable[str], exclusions: Iterable[str]
    ) -> list[Path]:
        """Helper to get all files matching the scope and exclusion globs."""
        scope_patterns = list(scope or [])
        exclusion_patterns = list(exclusions or [])
        if not scope_patterns: return []

        files: list[Path] = []
        for glob_pattern in scope_patterns:
            for file_path in self.repo_root.glob(glob_pattern):
                if not file_path.is_file(): continue
                if any(file_path.match(ex) for ex in exclusion_patterns): continue
                files.append(file_path)

        return list({p.resolve() for p in files})# src/mind/governance/checks/domain_placement.py
"""
A constitutional audit check to ensure capabilities are declared in the
correct domain manifest file, enforcing the 'structural_compliance' rule.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity
from shared.utils.yaml_processor import yaml_processor


logger = getLogger(__name__)


# ID: 0cd8ad5a-ed46-4f18-8335-f95b747d6164
class DomainPlacementCheck(BaseCheck):
    """
    Validates that capability keys declared in a domain manifest file
    match the domain of that file, contributing to the 'structural_compliance' rule.

    Example:
        - File: .intent/mind/knowledge/domains/core.yaml
        - Capability key: "core.introspection.analyze_code" ✅ OK
        - Capability key: "llm.router.select_model"        ❌ Wrong domain
    """

    # Fulfills the contract from BaseCheck.
    policy_rule_ids = [
        "structural_compliance",
    ]

    def __init__(self, context: AuditorContext) -> None:
        super().__init__(context)
        # self.context is set by the parent class.
        self.domains_dir: Path = (
            self.context.mind_path / "knowledge" / "domains"
        )

    # ID: 7eb75aef-6463-450d-8088-e9a64e3d85c8
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by scanning all domain manifests for misplaced capabilities.
        """
        findings: list[AuditFinding] = []

        if not self.domains_dir.is_dir():
            # No domain manifests present yet – nothing to validate.
            return findings

        for domain_file in sorted(self.domains_dir.glob("*.yaml")):
            findings.extend(self._check_domain_file(domain_file))

        return findings

    def _check_domain_file(self, domain_file: Path) -> list[AuditFinding]:
        """Validate a single domain manifest file."""
        findings: list[AuditFinding] = []
        domain_name = domain_file.stem

        try:
            manifest_content: dict[str, Any] | None = yaml_processor.load(domain_file)
        except Exception as exc:
            logger.warning(
                "Failed to load domain manifest %s: %s", domain_file, exc
            )
            return findings

        if not manifest_content:
            return findings

        capabilities = manifest_content.get("tags", [])
        if not isinstance(capabilities, list):
            return findings

        for cap in capabilities:
            if not isinstance(cap, dict): continue
            cap_key = cap.get("key")
            if not cap_key or not isinstance(cap_key, str): continue

            if not cap_key.startswith(f"{domain_name}."):
                findings.append(
                    AuditFinding(
                        # Standardized check_id for better traceability.
                        check_id="structural_compliance.domain_placement",
                        severity=AuditSeverity.ERROR,
                        message=(
                            f"Capability '{cap_key}' is misplaced in '{domain_file.name}'. "
                            f"It should be declared in '{cap_key.split('.')[0]}.yaml'."
                        ),
                        file_path=str(
                            domain_file.relative_to(self.repo_root)
                        ),
                        context={
                            "domain_file": domain_file.name,
                            "expected_domain": cap_key.split(".")[0],
                            "actual_domain": domain_name,
                        },
                    )
                )

        return findings# src/mind/governance/checks/duplication_check.py
"""
A constitutional audit check to find semantically duplicate symbols using a
vector database, providing evidence for refactoring patterns.
"""

from __future__ import annotations

import asyncio
from typing import Any, Dict

from rich.progress import track

from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from services.clients.qdrant_client import QdrantService
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity


logger = getLogger(__name__)


# ID: 13cf4ae9-f18b-410f-a320-399cc713f277
class DuplicationCheck(BaseCheck):
    """
    Enforces the 'dry_by_design' principle by finding semantically similar symbols,
    triggering refactoring patterns like 'extract_function' and 'extract_module'.
    """

    # Fulfills the contract from BaseCheck. This check provides the evidence
    # that informs the need for these refactoring patterns.
    policy_rule_ids = [
        "extract_function",
        "extract_module",
        "introduce_facade",
    ]

    def __init__(self, context: AuditorContext, qdrant_service: QdrantService) -> None:
        super().__init__(context)
        self.qdrant_service = qdrant_service
        self.symbols: Dict[str, Dict[str, Any]] = self.context.knowledge_graph.get(
            "symbols", {}
        )

        # Get ignore configuration from the central context, not by loading it here.
        # This assumes the AuditorContext is responsible for loading the ignore policy.
        ignore_policy = self.context.policies.get("audit_ignore_policy", {})
        self.ignored_symbol_keys = {
            item["key"]
            for item in ignore_policy.get("symbol_ignores", [])
            if isinstance(item, dict) and "key" in item
        }

    async def _check_single_symbol(
        self, symbol: dict[str, Any], threshold: float
    ) -> list[AuditFinding]:
        """Checks a single symbol for duplicates against the Qdrant index."""
        findings: list[AuditFinding] = []
        symbol_key = symbol.get("symbol_path")
        point_id = str(symbol.get("uuid")) if symbol.get("uuid") else None

        if not symbol_key or not point_id or symbol_key in self.ignored_symbol_keys:
            return findings

        try:
            query_vector = await self.qdrant_service.get_vector_by_id(point_id=point_id)
            if not query_vector:
                return findings

            similar_hits = await self.qdrant_service.search_similar(query_vector=query_vector, limit=5)
            for hit in similar_hits:
                payload = hit.get("payload") or {}
                score = float(hit.get("score", 0.0))
                hit_symbol_key = payload.get("chunk_id")

                if not hit_symbol_key or hit_symbol_key == symbol_key or hit_symbol_key in self.ignored_symbol_keys:
                    continue

                if score > threshold:
                    symbol_a, symbol_b = sorted((symbol_key, hit_symbol_key))
                    findings.append(
                        AuditFinding(
                            # A more structured ID indicating a prompt for refactoring.
                            check_id="code_standards.refactoring.semantic_duplication",
                            severity=AuditSeverity.WARNING,
                            message=(
                                "Potential duplicate logic found between "
                                f"'{symbol_a.split('::')[-1]}' and '{symbol_b.split('::')[-1]}'."
                            ),
                            file_path=symbol.get("file_path"),
                            context={
                                "symbol_a": symbol_a,
                                "symbol_b": symbol_b,
                                "similarity": f"{score:.2f}",
                                "suggested_actions": self.policy_rule_ids,
                            },
                        )
                    )
        except Exception as exc:
            logger.warning("Could not perform duplication check for '%s': %s", symbol_key, exc)

        return findings

    # ID: 1da6e2c3-fbd4-4860-b95e-7625f426edba
    async def execute(self, threshold: float = 0.85) -> list[AuditFinding]:
        """Asynchronously runs the duplication check across all vectorized symbols."""
        symbols_to_check = list(self.symbols.values())
        if not symbols_to_check:
            return []

        tasks = [self._check_single_symbol(symbol, threshold) for symbol in symbols_to_check]
        results: list[AuditFinding] = []

        for future in track(
            asyncio.as_completed(tasks),
            description="Checking for duplicate code...",
            total=len(tasks),
        ):
            results.extend(await future)

        # Deduplicate findings by (symbol_a, symbol_b) pair
        unique_findings: dict[tuple[str, str], AuditFinding] = {}
        for finding in results:
            ctx = finding.context or {}
            key_tuple = tuple(sorted((ctx.get("symbol_a", ""), ctx.get("symbol_b", ""))))
            if all(key_tuple) and key_tuple not in unique_findings:
                unique_findings[key_tuple] = finding

        return list(unique_findings.values())# src/mind/governance/checks/environment_checks.py
"""
Audits the system's runtime environment for required configuration, enforcing
the 'operations.runtime.env_vars_defined' constitutional rule.
"""

from __future__ import annotations

import os
from typing import Any, Dict, List

# No longer need 'Any' as we know the context type
from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 0c3965b7-b3f3-4fb6-bbbb-c94a1ffae3fe
class EnvironmentChecks(BaseCheck):
    """
    Ensures that all constitutionally required environment variables are set
    at runtime, as defined in the 'runtime_requirements' policy.
    """

    # Fulfills the contract from BaseCheck, linking this operational check
    # to the core constitution.
    policy_rule_ids = ["operations.runtime.env_vars_defined"]

    def __init__(self, context: AuditorContext) -> None:
        super().__init__(context)
        # This check is governed by the 'runtime_requirements' policy, which
        # should be loaded into the central context.
        self.requirements: Dict[str, Any] = self.context.policies.get(
            "runtime_requirements", {}
        )

    # ID: 0c0e7695-b11e-4ad8-9e74-23d5f79dad00
    def execute(self) -> List[AuditFinding]:
        """
        Verifies that required environment variables are set.
        """
        findings: List[AuditFinding] = []

        required_vars = self.requirements.get("variables", {})
        if not isinstance(required_vars, dict):
            findings.append(
                AuditFinding(
                    # This check_id indicates a misconfiguration of the policy
                    # this check depends on.
                    check_id="operations.runtime.policy_misconfigured",
                    severity=AuditSeverity.ERROR,
                    message=(
                        "runtime_requirements.variables must be a mapping of "
                        "ENV_VAR_NAME -> config dict."
                    ),
                    file_path="mind/runtime_requirements.yaml",
                )
            )
            return findings

        for name, config in required_vars.items():
            if not isinstance(config, dict) or not config.get("required"):
                continue

            if not os.getenv(name):
                description = config.get("description", "No description provided.")
                message = (
                    f"Required environment variable '{name}' is not set. "
                    f"Description: {description}"
                )
                findings.append(
                    AuditFinding(
                        # The finding directly references the constitutional rule.
                        check_id="operations.runtime.env_vars_defined",
                        severity=AuditSeverity.ERROR,
                        message=message,
                        file_path=".env",  # Hint to human where to fix it
                        context={"variable_name": name},
                    )
                )

        return findings# src/mind/governance/checks/file_checks.py
"""
Audits file existence, orphan detection, and SSOT compliance for
constitutional governance files.
"""

from __future__ import annotations

from mind.governance.checks.base_check import BaseCheck
from shared.config import settings
from shared.models import AuditFinding, AuditSeverity
from shared.utils.constitutional_parser import get_all_constitutional_paths

# This maps the legacy file path to the specific constitutional rule it violates.
# This makes the check a direct enforcer of the data_governance policy.
DEPRECATED_KNOWLEDGE_MAP = {
    ".intent/mind/knowledge/cli_registry.yaml": "db.cli_registry_in_db",
    ".intent/mind/knowledge/resource_manifest.yaml": "db.llm_resources_in_db",
    ".intent/mind/knowledge/cognitive_roles.yaml": "db.cognitive_roles_in_db",
}

KNOWN_UNINDEXED_FILES = {
    ".intent/charter/constitution/approvers.yaml.example",
    ".intent/keys/private.key",
}


# ID: 37b5ae2f-c3c2-4db4-9677-f16fd788c908
class FileChecks(BaseCheck):
    """
    Container for file-based constitutional checks, ensuring structural
    integrity and adherence to the Single Source of Truth (SSOT) principle.
    """

    # Explicit Constitutional Linkage:
    # This check directly enforces the data_governance rules for DB as SSOT
    # and contributes to overall structural_compliance from the QA policy.
    policy_rule_ids = [
        "db.cli_registry_in_db",
        "db.llm_resources_in_db",
        "db.cognitive_roles_in_db",
        "structural_compliance", # For orphan/missing file checks
    ]

    # ID: 56481071-3a0c-437d-ba57-533bc03d9ed6
    def execute(self) -> list[AuditFinding]:
        """Runs all file-related checks."""
        meta_content = settings._meta_config
        required_files = get_all_constitutional_paths(meta_content, self.intent_path)

        findings = self._check_for_deprecated_files()
        findings.extend(self._check_required_files(required_files))
        findings.extend(self._check_for_orphaned_intent_files(required_files))
        return findings

    def _check_for_deprecated_files(self) -> list[AuditFinding]:
        """
        Verify that files constitutionally replaced by the database do not exist,
        creating a finding for each specific rule violation.
        """
        findings: list[AuditFinding] = []
        for file_rel_path, rule_id in DEPRECATED_KNOWLEDGE_MAP.items():
            full_path = self.repo_root / file_rel_path
            if full_path.exists():
                findings.append(
                    AuditFinding(
                        # The check_id is now the exact ID from the constitution.
                        check_id=rule_id,
                        severity=AuditSeverity.ERROR,
                        message=(
                            f"Deprecated knowledge file exists: '{file_rel_path}'. "
                            f"Per rule '{rule_id}', the database is the SSOT."
                        ),
                        file_path=file_rel_path,
                    )
                )
        return findings

    def _check_required_files(self, required_files: set[str]) -> list[AuditFinding]:
        """Verify that all files declared in meta.yaml exist on disk."""
        findings: list[AuditFinding] = []
        for file_rel_path in sorted(required_files):
            full_path = self.repo_root / file_rel_path
            if not full_path.exists():
                findings.append(
                    AuditFinding(
                        # This check contributes to the broader structural_compliance rule.
                        check_id="structural_compliance.meta.missing_file",
                        severity=AuditSeverity.ERROR,
                        message=f"File declared in meta.yaml is missing: '{file_rel_path}'",
                        file_path=file_rel_path,
                    )
                )
        return findings

    def _check_for_orphaned_intent_files(
        self, declared_files: set[str]
    ) -> list[AuditFinding]:
        """Find .intent files not referenced in meta.yaml."""
        findings: list[AuditFinding] = []
        all_known_files = declared_files.union(KNOWN_UNINDEXED_FILES)
        if (self.intent_path / "proposals/README.md").exists():
            all_known_files.add(".intent/proposals/README.md")

        physical_files: set[str] = {
            str(p.relative_to(self.repo_root)).replace("\\", "/")
            for p in self.intent_path.rglob("*")
            if p.is_file()
        }
        orphaned_files = sorted(physical_files - all_known_files)

        for orphan in orphaned_files:
            if "prompts" in orphan or "reports" in orphan:
                continue
            findings.append(
                AuditFinding(
                    # This also contributes to the structural_compliance rule.
                    check_id="structural_compliance.meta.orphaned_file",
                    severity=AuditSeverity.WARNING,
                    message=f"Orphaned file in .intent/: '{orphan}'. Add to meta.yaml or remove.",
                    file_path=orphan,
                )
            )
        return findings# src/mind/governance/checks/health_checks.py
"""
Audits codebase health for complexity, atomicity, and line length violations,
enforcing the 'code_quality' constitutional rule.
"""

from __future__ import annotations

import ast
import statistics
from pathlib import Path

from radon.visitors import ComplexityVisitor

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 51dd8f1d-eda6-40e2-9c64-530ce6c290a6
class HealthChecks(BaseCheck):
    """
    Container for codebase health constitutional checks. This check enforces the
    'code_quality' rule from the quality_assurance policy by measuring various
    metrics defined in the 'health_standards' section of the code_standards policy.
    """

    # Explicit Constitutional Linkage:
    # This check contributes directly to the 'code_quality' rule.
    policy_rule_ids = ["code_quality"]

    def __init__(self, context):
        super().__init__(context)
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.health_policy = code_standards_policy.get("health_standards", {})

    # ID: 64bffe32-e6fd-4fd1-a235-aaf764363076
    def execute(self) -> list[AuditFinding]:
        """Measures code complexity and atomicity against defined policies."""
        policy_rules = self.health_policy
        file_line_counts = {}
        all_violations = []
        unique_files = {
            s["file_path"]
            for s in self.context.symbols_list
            if s.get("file_path", "").startswith("src/")
        }
        for file_path_str in sorted(list(unique_files)):
            if not file_path_str.endswith(".py"):
                continue
            file_path = self.repo_root / file_path_str
            logical_lines, violations = self._analyze_python_file(
                file_path, policy_rules
            )
            if logical_lines > 0:
                file_line_counts[file_path] = logical_lines
            all_violations.extend(violations)
        all_violations.extend(
            self._find_file_size_outliers(file_line_counts, policy_rules)
        )
        return all_violations

    def _analyze_python_file(
        self, file_path: Path, rules: dict
    ) -> tuple[int, list[AuditFinding]]:
        """Analyze a single Python file for health violations."""
        try:
            source_code = file_path.read_text(encoding="utf-8")
            logical_lines = self._count_logical_lines(source_code)

            # Note: The finding 'check_id' is now more specific.
            max_lloc = rules.get("max_module_lloc", 300)
            if logical_lines > max_lloc:
                return logical_lines, [
                    AuditFinding(
                        check_id="code_quality.health.module_too_long",
                        severity=AuditSeverity.WARNING,
                        message=f"Module has {logical_lines} lines (limit: {max_lloc}).",
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                ]

            syntax_tree = ast.parse(source_code)
            complexity_visitor = ComplexityVisitor.from_ast(syntax_tree)
            violations = self._check_function_metrics(
                complexity_visitor,
                rules,
                str(file_path.relative_to(self.repo_root)),
            )
            return logical_lines, violations
        except Exception:
            return 0, []

    def _count_logical_lines(self, source_code: str) -> int:
        return sum(
            1
            for line in source_code.splitlines()
            if line.strip() and not line.strip().startswith("#")
        )

    def _check_function_metrics(
        self,
        visitor: ComplexityVisitor,
        rules: dict,
        file_path_str: str,
    ) -> list[AuditFinding]:
        violations = []
        max_complexity = rules.get("max_cognitive_complexity", 15)
        max_func_lloc = rules.get("max_function_lloc", 80)

        for function in visitor.functions:
            if function.cognitive_complexity > max_complexity:
                violations.append(
                    AuditFinding(
                        check_id="code_quality.health.function_too_complex",
                        severity=AuditSeverity.WARNING,
                        message=f"Function '{function.name}' complexity is {function.cognitive_complexity} (limit: {max_complexity}).",
                        file_path=file_path_str,
                        line_number=function.lineno,
                    )
                )
            if function.lloc > max_func_lloc:
                violations.append(
                    AuditFinding(
                        check_id="code_quality.health.function_too_long",
                        severity=AuditSeverity.WARNING,
                        message=f"Function '{function.name}' has {function.lloc} lines (limit: {max_func_lloc}).",
                        file_path=file_path_str,
                        line_number=function.lineno,
                    )
                )
        return violations

    def _find_file_size_outliers(
        self, file_line_counts: dict, rules: dict
    ) -> list[AuditFinding]:
        if len(file_line_counts) < 3:
            return []
        violations = []
        line_count_values = list(file_line_counts.values())
        average_lines = statistics.mean(line_count_values)
        standard_deviation = statistics.stdev(line_count_values)

        stdev_multiplier = rules.get("outlier_standard_deviations", 2.0)
        outlier_threshold = average_lines + (stdev_multiplier * standard_deviation)

        for file_path, line_count in file_line_counts.items():
            if line_count > outlier_threshold:
                violations.append(
                    AuditFinding(
                        check_id="code_quality.health.module_outlier",
                        severity=AuditSeverity.WARNING,
                        message=f"Module size outlier ({line_count} lines vs avg of {average_lines:.0f}). Consider refactoring.",
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                )
        return violations# src/mind/governance/checks/id_coverage_check.py
"""
A constitutional audit check to enforce that every public symbol has an ID tag,
as mandated by the 'linkage.assign_ids' and 'symbols.public_capability_id_and_docstring' rules.
"""

from __future__ import annotations

import ast

from mind.governance.checks.base_check import BaseCheck
from shared.ast_utility import find_symbol_id_and_def_line
from shared.models import AuditFinding, AuditSeverity
from shared.logger import getLogger


logger = getLogger(__name__)


# ID: 3501ed8c-8366-4ad7-9ab4-7dcf4c045c70
class IdCoverageCheck(BaseCheck):
    """
    Ensures every public function/class in `src/` has a valid '# ID:' tag,
    enforcing key operational and code standard policies.
    """

    # Fulfills the contract from BaseCheck. This check verifies that the mandatory
    # workflow step of assigning IDs has been completed and that symbols meet
    # the code standard for public capabilities.
    policy_rule_ids = [
        "linkage.assign_ids",
        "symbols.public_capability_id_and_docstring",
    ]

    # No __init__ is needed as it uses the default from BaseCheck.

    # ID: f69a1a2e-26cd-4cc2-8fdc-7f18e0e77d0c
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by scanning all source files for public symbols missing an ID.
        """
        findings = []
        # Use self.src_dir provided by the BaseCheck for consistency.
        for file_path in self.src_dir.rglob("*.py"):
            try:
                content = file_path.read_text("utf-8")
                source_lines = content.splitlines()
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                        continue
                    if node.name.startswith("_"):
                        continue  # Rule applies only to public symbols

                    id_result = find_symbol_id_and_def_line(node, source_lines)

                    if not id_result.has_id:
                        findings.append(
                            AuditFinding(
                                # The check_id now directly references the constitutional rule.
                                check_id="linkage.assign_ids",
                                severity=AuditSeverity.ERROR,
                                message=f"Public symbol '{node.name}' is missing its required '# ID:' tag.",
                                file_path=str(
                                    file_path.relative_to(self.repo_root)
                                ),
                                line_number=id_result.definition_line_num,
                            )
                        )

            except Exception as exc:
                # Log the error for debugging but don't crash the audit.
                logger.debug("Skipping ID coverage scan for %s due to error: %s", file_path, exc)
                continue

        return findings# src/mind/governance/checks/id_uniqueness_check.py
"""
A constitutional audit check to enforce that every # ID tag is unique, as
mandated by the 'linkage.duplicate_ids' operational rule.
"""

from __future__ import annotations

import re
from collections import defaultdict

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity

# Pre-compiled regex for efficiency to find '# ID: <uuid>'
ID_TAG_REGEX = re.compile(
    r"#\s*ID:\s*([0-9a-fA-F]{8}-([0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12})"
)


# ID: ddaabb9e-5e9a-4574-b458-dbed610e64e5
class IdUniquenessCheck(BaseCheck):
    """
    Scans the entire source code to ensure that every assigned symbol ID (UUID) is unique.
    This prevents data corruption from accidental copy-paste errors during development.
    """

    # Fulfills the contract from BaseCheck, linking this check to the
    # mandatory operational workflow in operations.yaml.
    policy_rule_ids = ["linkage.duplicate_ids"]

    # ID: f2a3b4c5-d6e7-f8a9-b0c1-d2e3f4a5b6c7
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by scanning all Python files in `src/` and returns
        findings for any duplicate UUIDs.
        """
        # A dictionary to store locations of each UUID: {uuid: [("file/path.py", line_num), ...]}
        uuid_locations: dict[str, list[tuple[str, int]]] = defaultdict(list)

        # Use self.src_dir provided by BaseCheck for consistency.
        for file_path in self.src_dir.rglob("*.py"):
            try:
                content = file_path.read_text("utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    match = ID_TAG_REGEX.search(line)
                    if match:
                        found_uuid = match.group(1)
                        # Use self.repo_root for consistency.
                        rel_path = str(file_path.relative_to(self.repo_root))
                        uuid_locations[found_uuid].append((rel_path, i))
            except Exception:
                # Silently ignore files that can't be read or parsed
                continue

        findings = []
        for found_uuid, locations in uuid_locations.items():
            if len(locations) > 1:
                # Found a duplicate!
                locations_str = ", ".join(
                    [f"{path}:{line}" for path, line in locations]
                )
                findings.append(
                    AuditFinding(
                        # The check_id now matches the constitution exactly.
                        check_id="linkage.duplicate_ids",
                        severity=AuditSeverity.ERROR,
                        message=f"Duplicate ID tag found: {found_uuid}",
                        context={"locations": locations_str},
                    )
                )

        return findings# src/mind/governance/checks/import_rules.py
"""
A constitutional audit check to enforce architectural import rules,
enforcing the 'structural_compliance' rule.
"""

from __future__ import annotations

import ast
from pathlib import Path

from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 0690cf39-3739-449e-9228-2c7c8526209b
class ImportRulesCheck(BaseCheck):
    """
    Ensures that code files only import modules from their allowed domains,
    as defined in the source_structure policy.
    """

    # Fulfills the contract from BaseCheck.
    policy_rule_ids = ["structural_compliance"]

    def __init__(self, context: AuditorContext):
        super().__init__(context)
        self.domain_map: dict[str, str] = {}
        self.import_rules: dict[str, set[str]] = {}
        self._load_rules_from_policy()

    def _load_rules_from_policy(self):
        """Loads domain maps and import rules from the source_structure policy."""
        if self.domain_map:
            return

        structure_policy = self.context.policies.get("source_structure", {})
        structure = structure_policy.get("structure", [])

        for domain_info in structure:
            path_str = domain_info.get("path")
            domain_name = domain_info.get("domain")
            if path_str and domain_name:
                self.domain_map[path_str] = domain_name

            allowed_imports = domain_info.get("allowed_imports", [])
            if domain_name:
                self.import_rules.setdefault(domain_name, set()).update(allowed_imports)

    # --- THIS FUNCTION IS NOW A METHOD OF THE CLASS ---
    def _scan_imports(self, file_path: Path, content: str | None = None) -> list[str]:
        """
        Parse a Python file or its content and extract all imported module paths.
        """
        imports = []
        try:
            source = content if content is not None else file_path.read_text(encoding="utf-8")
            tree = ast.parse(source)

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        if node.level > 0:
                            path_parts = list(file_path.parent.parts)
                            # Now this line works because 'self' is defined.
                            base_parts = path_parts[len(self.repo_root.parts):]
                            if node.level > 1:
                                base_parts = base_parts[:-(node.level - 1)]
                            base_import = ".".join(base_parts)
                            imports.append(f"{base_import}.{node.module}")
                        else:
                            imports.append(node.module)
        except Exception:
            pass
        return imports

    def _get_domain_for_path_str(self, file_path_str: str) -> str | None:
        """Finds the domain for a given relative file path string."""
        best_match, best_domain = "", None
        for domain_path_prefix, domain_name in self.domain_map.items():
            if file_path_str.startswith(domain_path_prefix) and len(domain_path_prefix) > len(best_match):
                best_match = domain_path_prefix
                best_domain = domain_name
        return best_domain

    # ID: f1a7dedb-d5e4-442d-8957-b7f974778bc5
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by scanning all source files and validating their imports.
        """
        findings = []
        for file_path in self.src_dir.rglob("*.py"):
            # Call it as a method: self._scan_imports
            findings.extend(self._check_file_imports(file_path, file_content=None))
        return findings

    # ID: 31287af5-d942-4a1d-b06d-d0570026d035
    def execute_on_content(
        self, file_path_str: str, file_content: str
    ) -> list[AuditFinding]:
        """
        Runs the import check on a string of content instead of a file on disk.
        """
        file_path = self.repo_root / file_path_str
        # Call it as a method: self._scan_imports
        return self._check_file_imports(file_path, file_content)

    def _check_file_imports(
        self, file_path: Path, file_content: str | None
    ) -> list[AuditFinding]:
        """Core logic to check imports for a given file path and optional content."""
        findings = []
        file_rel_path_str = str(file_path.relative_to(self.repo_root))
        file_domain = self._get_domain_for_path_str(file_rel_path_str)
        if not file_domain:
            return []

        allowed_imports = self.import_rules.get(file_domain, set()).copy()
        allowed_imports.add(file_domain)

        # Call it as a method: self._scan_imports
        imported_modules = self._scan_imports(file_path, content=file_content)

        for module_str in imported_modules:
            # Reconstruct a path-like string to check the domain of the imported module
            imported_module_as_path = module_str.replace('.', '/')
            if self._get_domain_for_path_str(imported_module_as_path) == file_domain:
                continue

            imported_domain = module_str.split(".")[0]
            if imported_domain not in allowed_imports:
                findings.append(
                    AuditFinding(
                        check_id="structural_compliance.import_violation",
                        severity=AuditSeverity.ERROR,
                        message=(
                            f"Illegal import of '{module_str}' in domain '{file_domain}'. "
                            f"Allowed domains: {sorted(list(allowed_imports))}"
                        ),
                        file_path=file_rel_path_str,
                    )
                )
        return findings# src/mind/governance/checks/__init__.py
"""Provides functionality for the __init__ module."""

from __future__ import annotations
# src/mind/governance/checks/knowledge_differ.py
"""
A service to compare knowledge artifacts between a database source of truth
and legacy YAML files.
"""
from __future__ import annotations
import yaml
from pathlib import Path
from typing import Any
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

class KnowledgeDiffer:
    """Encapsulates the logic for diffing knowledge sources."""

    def __init__(self, session: AsyncSession, repo_root: Path):
        self.session = session
        self.repo_root = repo_root

    async def compare(self, config: dict[str, Any]) -> dict[str, Any]:
        """Compares DB to YAML for a given configuration."""
        yaml_path = self._resolve_yaml(*config["yaml_paths"])
        if not yaml_path:
            return {"status": "passed", "diff": None}

        schema, table = config["table"].split(".")
        db_rows, db_cols = await self._fetch_table(schema, table)
        yaml_items = self._read_yaml(yaml_path, config["yaml_key"])

        yaml_keys = {k for item in yaml_items for k in item}
        compare_fields = sorted(list(yaml_keys.intersection(db_cols)))

        diff = self._diff_records(yaml_items, db_rows, config["primary_key"], compare_fields)

        # A diff is "clean" if all its value lists are empty.
        is_clean = not any(diff.values())
        return {
            "status": "passed" if is_clean else "failed",
            "diff": diff if not is_clean else None,
            "yaml_path": yaml_path,
        }

    async def _fetch_table(self, schema: str, table: str) -> tuple[list[dict], list[str]]:
        """Fetches all rows and column names from a database table."""
        cols_sql = text("SELECT column_name FROM information_schema.columns WHERE table_schema = :s AND table_name = :t ORDER BY ordinal_position")
        result = await self.session.execute(cols_sql, {"s": schema, "t": table})
        cols = [row[0] for row in result.fetchall()]
        if not cols: return [], []

        # --- THIS IS THE FIX ---
        # 1. Build the list of quoted column names safely.
        quoted_cols = ", ".join(f'"{c}"' for c in cols)
        # 2. Use the prepared string in the final SQL statement.
        rows_sql = text(f'SELECT {quoted_cols} FROM "{schema}"."{table}"')
        # --- END OF FIX ---

        rows = (await self.session.execute(rows_sql)).mappings().all()
        return [dict(row) for row in rows], cols

    def _resolve_yaml(self, *candidates: str) -> Path | None:
        """Finds the first existing YAML file from a list of candidates."""
        for rel_path in candidates:
            path = self.repo_root / rel_path
            if path.exists():
                return path
        return None

    def _read_yaml(self, path: Path, key: str) -> list[dict]:
        """Reads a YAML file and extracts items by key."""
        try:
            data = yaml.safe_load(path.read_text("utf-8")) or {}
            items = data.get(key, [])
            return items if isinstance(items, list) else []
        except Exception:
            return []

    def _diff_records(self, yaml_items: list, db_items: list, p_key: str, fields: list) -> dict:
        """Compares YAML and DB records and returns differences."""
        yaml_idx = {str(item.get(p_key)): item for item in yaml_items if item.get(p_key)}
        db_idx = {str(item.get(p_key)): item for item in db_items if item.get(p_key)}

        mismatched = []
        for key, yaml_rec in yaml_idx.items():
            db_rec = db_idx.get(key)
            if db_rec:
                field_diffs = {}
                for field in fields:
                    yaml_val = yaml_rec.get(field)
                    db_val = db_rec.get(field)
                    # Comparing as strings is a simple way to normalize None, '', etc.
                    if str(yaml_val or "") != str(db_val or ""):
                        field_diffs[field] = {"yaml": yaml_val, "db": db_val}
                if field_diffs:
                    mismatched.append({"key": key, "fields": field_diffs})

        return {
            "missing_in_db": sorted(list(set(yaml_idx.keys()) - set(db_idx.keys()))),
            "mismatched": mismatched,
        }# src/mind/governance/checks/knowledge_source_check.py
"""
Compares DB single-source-of-truth tables with their (legacy) YAML exports,
enforcing the database SSOT rules from the data_governance policy.
"""
from __future__ import annotations
from typing import Any
from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
# Import our new engine
from mind.governance.checks.knowledge_differ import KnowledgeDiffer
from services.database.session_manager import get_session
from shared.models import AuditFinding, AuditSeverity

# The configuration remains part of the check, as it's specific to this audit.
TABLE_CONFIGS = {
    "cli_registry": {
        "rule_id": "db.cli_registry_in_db",
        "yaml_paths": [".intent/mind/knowledge/cli_registry.yaml"],
        "table": "core.cli_commands", "yaml_key": "commands", "primary_key": "name",
    },
    "resource_manifest": {
        "rule_id": "db.llm_resources_in_db",
        "yaml_paths": [".intent/mind/knowledge/resource_manifest.yaml"],
        "table": "core.llm_resources", "yaml_key": "llm_resources", "primary_key": "name",
    },
    "cognitive_roles": {
        "rule_id": "db.cognitive_roles_in_db",
        "yaml_paths": [".intent/mind/knowledge/cognitive_roles.yaml"],
        "table": "core.cognitive_roles", "yaml_key": "cognitive_roles", "primary_key": "role",
    },
}

# ID: 81d6e8ed-a6f6-444c-acda-9064896c5111
class KnowledgeSourceCheck(BaseCheck):
    """
    Ensures the database is the Single Source of Truth by detecting drift
    or the presence of legacy YAML knowledge files.
    """

    # Fulfills the contract from BaseCheck.
    policy_rule_ids = [
        "db.ssot_for_operational_data",
        "db.cli_registry_in_db",
        "db.llm_resources_in_db",
        "db.cognitive_roles_in_db",
    ]

    # No __init__ needed, we'll create the differ inside execute.

    # ID: b846d3ab-5762-4bc8-9dfc-f3fa060da29c
    async def execute(self) -> list[AuditFinding]:
        """
        Executes the SSOT check by using the KnowledgeDiffer to compare
        each configured artifact and generating findings for any drift.
        """
        findings: list[AuditFinding] = []
        async with get_session() as session:
            differ = KnowledgeDiffer(session, self.repo_root)
            for config in TABLE_CONFIGS.values():
                result = await differ.compare(config)
                if result["status"] == "failed":
                    findings.extend(self._create_findings_from_result(result, config))
        return findings

    def _create_findings_from_result(self, result: dict, config: dict) -> list[AuditFinding]:
        """Translates a diff result into a list of AuditFinding objects."""
        findings = []
        diff = result.get("diff", {})
        rule_id = config["rule_id"]
        yaml_path = str(result["yaml_path"].relative_to(self.repo_root))

        if diff.get("missing_in_db"):
            keys = ", ".join(diff["missing_in_db"])
            findings.append(AuditFinding(
                check_id=rule_id, severity=AuditSeverity.ERROR,
                message=f"SSOT Violation: Entries exist in legacy file '{yaml_path}' but are missing from the database: {keys}",
                file_path=yaml_path,
            ))

        if diff.get("mismatched"):
            keys = ", ".join([m["key"] for m in diff["mismatched"]])
            findings.append(AuditFinding(
                check_id=rule_id, severity=AuditSeverity.ERROR,
                message=f"SSOT Violation: Entries in legacy file '{yaml_path}' are out of sync with the database: {keys}",
                file_path=yaml_path,
                context={"mismatches": diff["mismatched"]},
            ))

        # If the file exists but the diff is empty, it means the file is a redundant but in-sync copy.
        # This can be a WARNING to encourage its removal.
        if not findings:
             findings.append(AuditFinding(
                check_id=rule_id, severity=AuditSeverity.WARNING,
                message=f"SSOT Redundancy: Legacy file '{yaml_path}' exists. It should be removed as the DB is the SSOT.",
                file_path=yaml_path,
            ))

        return findings# src/mind/governance/checks/legacy_tag_check.py
"""
A constitutional audit check to find and forbid legacy '# CAPABILITY:' tags,
enforcing the 'caps.id_format' rule from the code_standards policy.
"""

from __future__ import annotations

import re
from pathlib import Path

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity

# It's good practice to define constants at the module level.
LEGACY_TAG_PATTERN = re.compile(r"#\s*CAPABILITY:", re.IGNORECASE)
EXCLUDE_DIRS = {".git", ".venv", "__pycache__", ".pytest_cache", ".ruff_cache", "reports"}
EXCLUDE_FILES = {"poetry.lock", "project_context.txt"}
BINARY_EXTENSIONS = {
    ".png", ".jpg", ".jpeg", ".gif", ".ico", ".pyc", ".so",
    ".o", ".zip", ".gz", ".pdf",
}

# ID: 0649c22b-9336-490b-9ffd-25e202924301
class LegacyTagCheck(BaseCheck):
    """
    Scans the codebase to ensure no legacy '# CAPABILITY:' tags remain,
    thereby enforcing the constitutionally mandated '# ID:' format.
    """

    # Fulfills the contract from BaseCheck.
    policy_rule_ids = ["caps.id_format"]

    # ID: 94e602d4-47da-455d-be69-fe7a037bcb2b
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by scanning all non-excluded files for the legacy tag pattern.
        """
        findings = []
        for file_path in self.repo_root.rglob("*"):
            if not self._is_scannable(file_path):
                continue

            try:
                content = file_path.read_text(encoding="utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    if LEGACY_TAG_PATTERN.search(line):
                        findings.append(
                            AuditFinding(
                                # The check_id now matches the constitution exactly.
                                check_id="caps.id_format",
                                severity=AuditSeverity.ERROR,
                                message="Legacy '# CAPABILITY:' tag found. Please replace with '# ID: <uuid>'.",
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=i,
                            )
                        )
            except (UnicodeDecodeError, OSError):
                # Silently ignore files that can't be read (e.g., broken symlinks)
                continue
        return findings

    def _is_scannable(self, file_path: Path) -> bool:
        """Helper method to determine if a file should be scanned."""
        if not file_path.is_file():
            return False
        if file_path.name in EXCLUDE_FILES:
            return False
        if file_path.suffix in BINARY_EXTENSIONS:
            return False
        # Check if any part of the path is in the exclude list
        if any(part in EXCLUDE_DIRS for part in file_path.parts):
            return False
        return True# src/mind/governance/checks/manifest_lint.py
"""
Audits capability manifests for quality issues like placeholder text, enforcing
the 'caps.no_placeholder_text' and 'caps.meaningful_description' rules.
"""

from __future__ import annotations

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity

# Define placeholder strings as a constant for clarity.
# In the future, this could be loaded from the policy file itself.
PLACEHOLDER_SUBSTRINGS = {"tbd", "n/a", "auto-added"}


# ID: ee190b8d-1bf0-4b1a-90e2-abf21ca013c9
class ManifestLintCheck(BaseCheck):
    """
    Checks for placeholder text in capability manifest descriptions to ensure
    all documented capabilities are meaningful.
    """

    # Fulfills the contract from BaseCheck. This check enforces two related
    # rules from the code_standards policy.
    policy_rule_ids = [
        "caps.no_placeholder_text",
        "caps.meaningful_description",
    ]

    # The __init__ is no longer needed; we can get the symbols list
    # directly from self.context in the execute method.

    # ID: 2e114e07-e521-4e56-a56c-f3afc6458f44
    def execute(self) -> list[AuditFinding]:
        """Finds capabilities with placeholder descriptions."""
        findings = []

        # The check's logic is self-contained and doesn't need to load the rule
        # from the policy, as its purpose is to enforce this specific behavior.

        for symbol in self.context.symbols_list:
            # A symbol's "intent" is its description in the manifest.
            description = (symbol.get("intent", "") or "").lower()

            if any(p in description for p in PLACEHOLDER_SUBSTRINGS):
                original_description = symbol.get("intent", "") or ""
                findings.append(
                    AuditFinding(
                        # The check_id now matches the specific constitutional rule.
                        check_id="caps.no_placeholder_text",
                        # The severity now matches the policy's 'error' enforcement.
                        severity=AuditSeverity.ERROR,
                        message=(
                            f"Capability '{symbol.get('key')}' has a forbidden placeholder "
                            f"description: '{original_description}'"
                        ),
                        file_path=symbol.get("file_path"),
                        line_number=symbol.get("line_number"),
                    )
                )
        return findings# src/mind/governance/checks/naming_conventions.py
"""
A constitutional audit check to enforce file and symbol naming conventions
as defined in the code_standards.yaml policy.
"""

from __future__ import annotations

import re
from typing import Any

from mind.governance.audit_context import AuditorContext
# Import the BaseCheck to inherit from it
from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity
from shared.logger import getLogger

logger = getLogger(__name__)


# ID: 7cff5dba-bd63-4e8c-8e3f-8f242a59f28d
# Inherit from BaseCheck
class NamingConventionsCheck(BaseCheck):
    """
    Ensures that file names match the patterns defined in the constitution.
    This check is fully dynamic and reads all configuration from the policy file.
    """

    def __init__(self, context: AuditorContext):
        super().__init__(context)
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.naming_policy = code_standards_policy.get("naming_conventions", {})

        # --- Dynamic Constitutional Linkage ---
        # Because this check is policy-driven, it dynamically discovers all the
        # rules it is responsible for from the policy file itself.
        all_rule_ids = []
        for rules in self.naming_policy.values():
            if isinstance(rules, list):
                for rule in rules:
                    if isinstance(rule, dict) and rule.get("id"):
                        all_rule_ids.append(rule["id"])
        # Fulfills the contract from BaseCheck.
        self.policy_rule_ids = all_rule_ids

    # ID: 6bebb819-1073-4163-8b70-09c2c374f6c8
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by iterating through policy rules and scanning the
        repository file system for violations.
        """
        findings = []
        if not self.naming_policy:
            return findings

        for category, rules in self.naming_policy.items():
            if not isinstance(rules, list):
                continue
            for rule in rules:
                findings.extend(self._process_rule(rule, category))
        return findings

    def _process_rule(self, rule: dict[str, Any], category: str) -> list[AuditFinding]:
        """Processes a single naming convention rule against the file system."""
        findings = []
        scope_glob = rule.get("scope")
        pattern = rule.get("pattern")
        rule_id = rule.get("id")
        exclusions = rule.get("exclusions", [])
        enforcement = rule.get("enforcement", "error")

        if not scope_glob or not pattern or not rule_id:
            return findings  # Skip malformed rules

        try:
            compiled_pattern = re.compile(pattern)
        except re.error:
            # If the regex in the policy is invalid, log it but don't crash.
            logger.warning("Invalid regex pattern for naming rule '%s': %s", rule_id, pattern)
            return findings

        for file_path in self.repo_root.glob(scope_glob):
            if not file_path.is_file():
                continue
            if any(file_path.match(ex) for ex in exclusions):
                continue

            if not compiled_pattern.match(file_path.name):
                findings.append(
                    AuditFinding(
                        check_id=rule_id,
                        severity=AuditSeverity[enforcement.upper()],
                        message=(
                            f"File name '{file_path.name}' violates naming convention '{rule_id}'. "
                            f"Expected pattern: {pattern}"
                        ),
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                )
        return findings# src/mind/governance/checks/orphaned_logic.py
"""
A constitutional audit check to find "orphaned logic" - public symbols
that have not been assigned a capability, enforcing the 'intent_alignment' rule.
"""

from __future__ import annotations
import re
from typing import Any
from mind.governance.audit_context import AuditorContext
# Import the BaseCheck to inherit from it
from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: bc44f537-758e-49a2-9914-fc6355b51f48
# Inherit from BaseCheck
class OrphanedLogicCheck(BaseCheck):
    """
    Ensures that all public symbols are assigned to a capability, enforcing
    the 'intent_alignment' rule by preventing undocumented functionality.
    """

    # Fulfills the contract from BaseCheck.
    policy_rule_ids = ["intent_alignment"]

    def __init__(self, context: AuditorContext):
        super().__init__(context)
        self.symbols = self.context.symbols_map

        # --- Centralized Configuration ---
        # All required policies are retrieved from the single context object.
        # The check is no longer responsible for loading files.
        ignore_policy = self.context.policies.get("audit_ignore_policy", {})
        structure_policy = self.context.policies.get("project_structure", {})

        self.ignored_symbol_keys = {
            item["key"]
            for item in ignore_policy.get("symbol_ignores", [])
            if "key" in item
        }
        self.entry_point_patterns = structure_policy.get("entry_point_patterns", [])

    def _is_entry_point(self, symbol_data: dict[str, Any]) -> bool:
        """Checks if a symbol matches any of the defined entry point patterns."""
        for pattern in self.entry_point_patterns:
            match_rules = pattern.get("match", {})
            # Assume it's a match until a rule fails
            is_a_match = all(
                self._evaluate_match_rule(rule_key, rule_value, symbol_data)
                for rule_key, rule_value in match_rules.items()
            )
            if is_a_match:
                return True
        return False

    def _evaluate_match_rule(self, key: str, value: Any, data: dict) -> bool:
        """Evaluates a single criterion for the entry point pattern matching."""
        if key == "type":
            is_class = data.get("is_class", False)
            return (value == "class" and is_class) or (value == "function" and not is_class)
        if key == "name_regex":
            return bool(re.search(value, data.get("name", "")))
        if key == "module_path_contains":
            return value in data.get("file_path", "")
        # Add other rule evaluations here if needed
        return data.get(key) == value

    # ID: 567318b3-2e45-4383-8af6-9880c3c9576c
    def _find_unassigned_public_symbols(self) -> list[dict[str, Any]]:
        """Finds all public symbols with a null capability key that are not ignored."""
        unassigned = []
        for symbol_key, symbol_data in self.symbols.items():
            is_public = not symbol_data.get("name", "").startswith("_")
            is_unassigned = symbol_data.get("capability") is None
            is_ignored = symbol_key in self.ignored_symbol_keys

            if is_public and is_unassigned and not is_ignored:
                if not self._is_entry_point(symbol_data):
                    symbol_data["key"] = symbol_key
                    unassigned.append(symbol_data)
        return unassigned

    # ID: 2ba01327-4559-427f-b0d4-a0737b7937fc
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any orphaned symbols.
        """
        findings = []
        orphaned_symbols = self._find_unassigned_public_symbols()

        for symbol in orphaned_symbols:
            symbol_key = symbol.get("key", "unknown")
            short_name = symbol_key.split("::")[-1]

            findings.append(
                AuditFinding(
                    # The check_id now matches the constitution.
                    check_id="intent_alignment",
                    severity=AuditSeverity.ERROR,
                    message=f"Orphaned logic found: Public symbol '{short_name}' is not assigned to a capability.",
                    file_path=symbol.get("file_path"),
                    line_number=symbol.get("line_number"),
                    context={"symbol_key": symbol_key},
                )
            )
        return findings# src/mind/governance/checks/security_checks.py
"""
Scans source code for security vulnerabilities based on configurable rules
defined in the data_governance and safety_framework policies.
"""

from __future__ import annotations
import ast
import fnmatch
import re
from pathlib import Path
from typing import Any
from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity

logger = getLogger(__name__)

# ID: 80baca41-4809-456b-985b-9bb9a6cebb7b
class SecurityChecks(BaseCheck):
    """
    A policy-driven check that enforces all security and safety rules
    defined in the constitution.
    """

    def __init__(self, context: AuditorContext):
        """Initializes the check and dynamically discovers the rules it must enforce."""
        super().__init__(context)

        data_gov_policy = self.context.policies.get("data_governance", {})
        safety_framework = self.context.policies.get("safety_framework", {})

        # Consolidate all security-related rules into a single lookup table
        self.rules_by_id: dict[str, Any] = {
            rule["id"]: rule
            for policy in [data_gov_policy, safety_framework]
            for section in ["security_rules", "safety_rules"]
            for rule in policy.get(section, [])
            if isinstance(rule, dict) and rule.get("id")
        }

        # Dynamically set the constitutional contract
        self.policy_rule_ids = list(self.rules_by_id.keys())

    # ID: cb2146e9-2abb-4982-ac11-31f118a10707
    def execute(self) -> list[AuditFinding]:
        """
        Scans source code for violations of any configured security rule.
        """
        findings = []
        # The logic is now a generic loop over all discovered rules.
        for rule_id, rule in self.rules_by_id.items():
            detection_method = rule.get("detection", {}).get("method")
            if detection_method == "regex_scan":
                findings.extend(self._scan_with_regex(rule))
            elif detection_method == "ast_call_scan":
                findings.extend(self._scan_for_dangerous_calls(rule))
            # Add other detection methods here as needed.
        return findings

    def _get_files_to_scan(self, rule: dict[str, Any]) -> list[Path]:
        """Gets a list of Python files to scan, respecting rule exclusions."""
        exclude_globs = rule.get("detection", {}).get("exclude", [])
        files_to_scan = []
        for file_path in self.context.python_files:
            rel_path_str = str(file_path.relative_to(self.repo_root))
            if any(fnmatch.fnmatch(rel_path_str, glob) for glob in exclude_globs):
                continue
            files_to_scan.append(file_path)
        return files_to_scan

    def _scan_with_regex(self, rule: dict[str, Any]) -> list[AuditFinding]:
        """Generic scanner for rules using regex patterns on file content."""
        findings = []
        rule_id = rule["id"]
        patterns = [re.compile(p) for p in rule.get("detection", {}).get("patterns", [])]
        if not patterns:
            return []

        for file_path in self._get_files_to_scan(rule):
            try:
                content = file_path.read_text(encoding="utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    for pattern in patterns:
                        if pattern.search(line):
                            findings.append(AuditFinding(
                                check_id=rule_id,
                                severity=AuditSeverity[rule.get("enforcement", "error").upper()],
                                message=f"Potential security violation of '{rule_id}' found.",
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=i,
                                context={"pattern": pattern.pattern},
                            ))
            except Exception as exc:
                logger.debug("Failed regex scan for rule %s on file %s: %s", rule_id, file_path, exc)
        return findings

    def _scan_for_dangerous_calls(self, rule: dict[str, Any]) -> list[AuditFinding]:
        """Generic scanner for rules looking for dangerous function calls via AST."""
        findings = []
        rule_id = rule["id"]
        patterns = [re.compile(p) for p in rule.get("detection", {}).get("patterns", [])]
        if not patterns:
            return []

        for file_path in self._get_files_to_scan(rule):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))
                for node in ast.walk(tree):
                    if isinstance(node, ast.Call):
                        # ast.unparse is a reliable way to get the function call name
                        call_str = ast.unparse(node.func)
                        for pattern in patterns:
                            if pattern.search(call_str):
                                findings.append(AuditFinding(
                                    check_id=rule_id,
                                    severity=AuditSeverity[rule.get("enforcement", "error").upper()],
                                    message=f"Dangerous call pattern found violating '{rule_id}': '{call_str}'",
                                    file_path=str(file_path.relative_to(self.repo_root)),
                                    line_number=node.lineno,
                                ))
            except Exception as exc:
                logger.debug("Failed AST scan for rule %s on file %s: %s", rule_id, file_path, exc)
        return findings# src/mind/governance/checks/style_checks.py
"""
A policy-driven auditor for code style and convention compliance, as defined
in the consolidated code_standards.yaml.
"""

from __future__ import annotations
import ast
from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity
from shared.logger import getLogger

logger = getLogger(__name__)

# ID: 791f7cd8-0441-4e2e-ac65-aa8d0ab82ac7
class StyleChecks(BaseCheck):
    """
    A policy-driven engine for enforcing all code style and convention rules
    defined in the constitution.
    """

    def __init__(self, context: AuditorContext):
        super().__init__(context)
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.style_rules = code_standards_policy.get("style_rules", [])
        self.rules_by_id = {
            rule["id"]: rule for rule in self.style_rules if isinstance(rule, dict) and "id" in rule
        }
        # Dynamically discover all style rules this check is responsible for.
        self.policy_rule_ids = list(self.rules_by_id.keys())

    # ID: 20cebb25-123b-40d9-999f-4d849eba4228
    def execute(self) -> list[AuditFinding]:
        """Verifies that Python modules adhere to all documented style conventions."""
        findings = []
        # Loop through each constitutional rule this check is responsible for.
        for rule_id, rule in self.rules_by_id.items():
            if rule_id == "style.docstrings_public_apis":
                findings.extend(self._check_public_docstrings(rule))
            # Other rules are acknowledged here but enforced by external tools.
            # This correctly marks them as "covered" by the audit framework.
            elif rule_id in [
                "style.linter_required",
                "style.formatter_required",
                "style.import_order",
                "style.fail_on_style_in_ci",
            ]:
                # These rules are delegated to CI tools like ruff and black.
                # The check fulfills its constitutional duty by acknowledging them.
                pass
        return findings

    def _check_public_docstrings(self, rule: dict) -> list[AuditFinding]:
        """
        Enforces that all public modules, classes, and functions have docstrings.
        """
        findings = []
        severity = AuditSeverity[rule.get("enforcement", "warn").upper()]

        for file_path in self.context.python_files:
            try:
                content = file_path.read_text(encoding="utf-8")
                tree = ast.parse(content, filename=str(file_path))

                # 1. Check for module-level docstring
                if not ast.get_docstring(tree):
                    findings.append(AuditFinding(
                        check_id=rule["id"], severity=severity,
                        message="Missing required module-level docstring.",
                        file_path=str(file_path.relative_to(self.repo_root)),
                        line_number=1,
                    ))

                # 2. Check for public class and function docstrings
                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                        # A public API is one that does not start with an underscore
                        if not node.name.startswith("_"):
                            if not ast.get_docstring(node):
                                findings.append(AuditFinding(
                                    check_id=rule["id"], severity=severity,
                                    message=f"Public API '{node.name}' is missing a docstring.",
                                    file_path=str(file_path.relative_to(self.repo_root)),
                                    line_number=node.lineno,
                                ))
            except Exception as e:
                logger.debug("Could not parse file %s for style check: %s", file_path, e)
        return findings
