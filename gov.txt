# src/mind/governance/checks/base_check.py
"""
Provides a shared base class for all constitutional audit checks to inherit from.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from mind.governance.audit_context import AuditorContext


# ID: 2cb0374b-a487-4dce-bab1-c2ee8a693b0a
class BaseCheck:
    """A base class for audit checks, providing a shared context."""

    def __init__(self, context: AuditorContext):
        """
        Initializes the check with a shared auditor context.
        This common initializer serves the 'dry_by_design' principle.
        """
        self.context = context
        self.repo_root = context.repo_path
        self.intent_path = context.intent_path
        self.src_dir = context.src_dir
# src/mind/governance/checks/capability_coverage.py
"""
A constitutional audit check to ensure that all capabilities declared in the
project manifest are implemented in the database.

Constitutional linkage:
- Policy: data_governance
- Rule:  knowledge.database_ssot
"""

from __future__ import annotations

from mind.governance.audit_context import AuditorContext
from shared.models import AuditFinding, AuditSeverity


# ID: 979ce56f-7f3c-40e7-8736-ce219bab6ad8
class CapabilityCoverageCheck:
    """
    Verifies that every capability in the manifest has a corresponding
    implementation entry in the database's symbols table.
    """

    # Explicit constitutional mapping:
    # This tells the PolicyCoverageService that this check is an implementation
    # of the `knowledge.database_ssot` rule from the data_governance policy.
    policy_rule_id = "knowledge.database_ssot"

    def __init__(self, context: AuditorContext):
        self.context = context

    # ID: e0730fb8-2616-42b2-915b-48f30ff4ac17
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings: list[AuditFinding] = []

        manifest_path = self.context.mind_path / "project_manifest.yaml"
        if not manifest_path.exists():
            # This is a structural misconfiguration of the Mind, not directly
            # covered by a constitutional rule yet, so we keep a local check_id.
            findings.append(
                AuditFinding(
                    check_id="manifest.missing.project_manifest",
                    severity=AuditSeverity.ERROR,
                    message=(
                        "The project_manifest.yaml file is missing from .intent/mind/."
                    ),
                    file_path=str(manifest_path.relative_to(self.context.repo_path)),
                )
            )
            return findings

        manifest_content = self.context._load_yaml(manifest_path)
        declared_capabilities: set[str] = set(
            manifest_content.get("capabilities", [])
        )

        # --- SSOT-CORRECT LOGIC ---
        # The source of truth for implementation is the database, not code comments.
        # The view aliases 'key' to 'capability', so we must use that name here.
        implemented_capabilities: set[str] = {
            s["capability"]
            for s in self.context.knowledge_graph.get("symbols", {}).values()
            if s.get("capability")
        }
        # --- END OF SSOT-CORRECT LOGIC ---

        missing_implementations = declared_capabilities - implemented_capabilities

        for cap_key in sorted(missing_implementations):
            findings.append(
                AuditFinding(
                    check_id="capability.coverage.missing_implementation",
                    severity=AuditSeverity.WARNING,
                    message=(
                        f"Capability '{cap_key}' is declared in the manifest but has "
                        "no implementation linked in the database."
                    ),
                    file_path=str(manifest_path.relative_to(self.context.repo_path)),
                )
            )

        return findings
# src/mind/governance/checks/coverage_check.py
"""
Constitutional enforcement of test coverage requirements.

Verifies that the codebase meets the coverage requirements defined in the
quality_assurance policy:

- coverage.minimum_threshold
- coverage.no_untested_commits
"""

from __future__ import annotations

import json
import subprocess
from typing import Any

from shared.config import settings
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity


logger = getLogger(__name__)


# ID: f09915fb-02c8-49d4-b5c5-19cd5e955df4
class CoverageGovernanceCheck:
    """
    Enforces constitutional test coverage requirements.

    This check verifies that:
    1. Overall coverage meets the minimum threshold (75%)
    2. Critical paths meet their specific higher thresholds
    3. No significant coverage regressions have occurred
    """

    # Explicit constitutional mapping:
    # These are the primary rules in the quality_assurance policy that this
    # check is responsible for enforcing.
    policy_rule_ids = [
        "coverage.minimum_threshold",
        "coverage.no_untested_commits",
    ]

    def __init__(self) -> None:
        self.policy = settings.load("charter.policies.quality_assurance")

        # Align with the current policy structure:
        # quality_assurance.yaml → coverage_requirements: { ... }
        coverage_cfg = self.policy.get("coverage_requirements", {})

        self.minimum_threshold: float = coverage_cfg.get("minimum_threshold", 75)
        self.critical_paths: list[str] = coverage_cfg.get("critical_paths", [])
        self.exclusions: list[str] = coverage_cfg.get("exclusions", [])

    # ID: a8126c8d-f9b8-40d5-a098-4aa5065f656c
    async def execute(self) -> list[AuditFinding]:
        """
        Executes the coverage check and returns audit findings.

        Returns:
            List of AuditFinding objects for any violations
        """
        findings: list[AuditFinding] = []

        coverage_data = self._measure_coverage()
        if not coverage_data:
            return [
                AuditFinding(
                    check_id="coverage.minimum_threshold",
                    severity=AuditSeverity.ERROR,
                    message="Failed to measure test coverage",
                    file_path="N/A",
                    context={"error": "Could not run pytest coverage"},
                )
            ]

        overall_coverage = coverage_data.get("overall_percent", 0.0)

        # 1) Enforce coverage.minimum_threshold
        if overall_coverage < self.minimum_threshold:
            findings.append(
                AuditFinding(
                    check_id="coverage.minimum_threshold",
                    severity=AuditSeverity.ERROR,
                    message=(
                        f"Coverage {overall_coverage}% below constitutional minimum "
                        f"{self.minimum_threshold}%"
                    ),
                    file_path="N/A",
                    context={
                        "current": overall_coverage,
                        "required": self.minimum_threshold,
                        "delta": overall_coverage - self.minimum_threshold,
                        "action": "Trigger autonomous remediation",
                    },
                )
            )

        # 2) Enforce critical path thresholds (still governed by policy config)
        for path_spec in self._iter_critical_path_specs():
            path_pattern, required = self._parse_path_spec(path_spec)
            actual = self._get_path_coverage(coverage_data, path_pattern)
            if actual is not None and actual < required:
                findings.append(
                    AuditFinding(
                        check_id="coverage.critical_path",
                        severity=AuditSeverity.ERROR,
                        message=(
                            f"Critical path '{path_pattern}' coverage {actual}% "
                            f"below required {required}%"
                        ),
                        file_path=path_pattern,
                        context={
                            "current": actual,
                            "required": required,
                            "delta": actual - required,
                        },
                    )
                )

        # 3) Enforce coverage.no_untested_commits (regression check)
        regression = self._check_regression(coverage_data)
        if regression:
            findings.append(regression)

        return findings

    def _measure_coverage(self) -> dict[str, Any] | None:
        """
        Runs pytest with coverage and returns parsed results.

        Returns:
            Dict with coverage metrics or None if measurement fails
        """
        try:
            result = subprocess.run(
                [
                    "poetry",
                    "run",
                    "pytest",
                    "--cov=src",
                    "--cov-report=json",
                    "--cov-report=term",
                    "-q",
                ],
                cwd=settings.REPO_PATH,
                capture_output=True,
                text=True,
                timeout=300,
            )

            coverage_json = settings.REPO_PATH / "coverage.json"
            if coverage_json.exists():
                data = json.loads(coverage_json.read_text())
                totals = data.get("totals", {})
                return {
                    "overall_percent": float(totals.get("percent_covered", 0) or 0),
                    "lines_covered": int(totals.get("covered_lines", 0) or 0),
                    "lines_total": int(totals.get("num_statements", 0) or 0),
                    "files": data.get("files", {}),
                    "timestamp": data.get("meta", {}).get("timestamp"),
                }

            # Fallback: try to parse terminal output if JSON file is missing
            return self._parse_term_output(result.stdout)

        except subprocess.TimeoutExpired:
            logger.error("Coverage measurement timed out after 5 minutes")
            return None
        except Exception as exc:  # noqa: BLE001
            logger.error("Failed to measure coverage: %s", exc, exc_info=True)
            return None

    def _parse_term_output(self, output: str) -> dict[str, Any] | None:
        """
        Fallback parser for terminal coverage output.

        Args:
            output: Terminal output from pytest --cov

        Returns:
            Dict with coverage metrics or None
        """
        try:
            for line in output.splitlines():
                if line.startswith("TOTAL"):
                    parts = line.split()
                    if len(parts) >= 4:
                        percent_str = parts[-1].rstrip("%")
                        percent = float(percent_str)
                        total_lines = int(parts[1])
                        missed_lines = int(parts[2])
                        covered_lines = total_lines - missed_lines
                        return {
                            "overall_percent": percent,
                            "lines_total": total_lines,
                            "lines_covered": covered_lines,
                        }
        except Exception as exc:  # noqa: BLE001
            logger.debug("Failed to parse coverage output: %s", exc)
        return None

    def _iter_critical_path_specs(self) -> list[str]:
        """
        Returns the list of critical path specifications.

        Kept as a separate method so it’s easy to evolve later (e.g. merging
        policy-specified specs with dynamic overrides from DB if needed).
        """
        return list(self.critical_paths or [])

    def _parse_path_spec(self, spec: str) -> tuple[str, float]:
        """
        Parses a path specification like 'src/core/**/*.py: 85%'.

        Args:
            spec: Path specification string

        Returns:
            Tuple of (path_pattern, required_percent)
        """
        parts = spec.split(":", maxsplit=1)
        path = parts[0].strip()
        percent_str = parts[1].strip().rstrip("%") if len(parts) > 1 else "0"
        required = float(percent_str or 0)
        return path, required

    def _get_path_coverage(
        self, coverage_data: dict[str, Any], pattern: str
    ) -> float | None:
        """
        Gets coverage percentage for files matching a pattern.

        Args:
            coverage_data: Coverage data from measurement
            pattern: Glob-style path pattern

        Returns:
            Coverage percentage or None if no matches
        """
        from fnmatch import fnmatch

        files = coverage_data.get("files", {})
        if not files:
            return None

        total_lines = 0
        covered_lines = 0

        for file_path, file_data in files.items():
            if fnmatch(file_path, pattern):
                summary = file_data.get("summary", {})
                total_lines += int(summary.get("num_statements", 0) or 0)
                covered_lines += int(summary.get("covered_lines", 0) or 0)

        if total_lines == 0:
            return None

        return round(covered_lines / total_lines * 100, 2)

    def _check_regression(self, coverage_data: dict[str, Any]) -> AuditFinding | None:
        """
        Checks for significant coverage regressions.

        Args:
            coverage_data: Current coverage data

        Returns:
            AuditFinding if regression detected, None otherwise
        """
        history_file = settings.REPO_PATH / "work" / "testing" / "coverage_history.json"
        if not history_file.exists():
            self._save_coverage_history(coverage_data)
            return None

        try:
            history = json.loads(history_file.read_text())
            last_run = history.get("last_run", {})
            last_percent = float(last_run.get("overall_percent", 0) or 0)
            current_percent = float(coverage_data.get("overall_percent", 0) or 0)
            delta = current_percent - last_percent

            self._save_coverage_history(coverage_data)

            if delta < -5.0:
                # This enforces coverage.no_untested_commits
                return AuditFinding(
                    check_id="coverage.no_untested_commits",
                    severity=AuditSeverity.ERROR,
                    message=f"Significant coverage regression: {abs(delta):.1f}% drop",
                    file_path="N/A",
                    context={
                        "previous": last_percent,
                        "current": current_percent,
                        "delta": delta,
                    },
                )
        except Exception as exc:  # noqa: BLE001
            logger.debug("Could not check coverage regression: %s", exc)

        return None

    def _save_coverage_history(self, coverage_data: dict[str, Any]) -> None:
        """Saves coverage data to history file for regression tracking."""
        try:
            history_file = (
                settings.REPO_PATH / "work" / "testing" / "coverage_history.json"
            )
            history_file.parent.mkdir(parents=True, exist_ok=True)
            history = {
                "last_run": coverage_data,
                "updated_at": coverage_data.get("timestamp"),
            }
            history_file.write_text(json.dumps(history, indent=2))
        except Exception as exc:  # noqa: BLE001
            logger.debug("Could not save coverage history: %s", exc)
# src/mind/governance/checks/dependency_injection_check.py
"""
A constitutional audit check to enforce the Dependency Injection (DI) policy.

This check is responsible for enforcing the following policy rules
from charter/policies/code_standards.yaml:

- di.no_direct_instantiation
- di.no_global_session_import
"""

from __future__ import annotations

import ast
from pathlib import Path
from typing import Any, Iterable, List

from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity


logger = getLogger(__name__)


# ID: 68fa7a18-3591-46ad-9470-0a0bb8685491
class DependencyInjectionCheck(BaseCheck):
    """
    Ensures that services and features do not directly instantiate their dependencies,
    and do not use forbidden global imports like `get_session`.
    """

    # Explicit constitutional mapping:
    # These are the concrete DI rules this check enforces mechanically.
    policy_rule_ids = [
        "di.no_direct_instantiation",
        "di.no_global_session_import",
    ]

    def __init__(self, context: AuditorContext) -> None:
        super().__init__(context)
        # Load from the consolidated code_standards policy
        code_standards_policy: dict[str, Any] = self.context.policies.get(
            "code_standards", {}
        )
        # Expect a list of DI-related rule configs under "dependency_injection"
        self.policy: list[dict[str, Any]] = code_standards_policy.get(
            "dependency_injection", []
        )

    # ID: e0b8b3db-959e-4ac1-bc26-a7f3e1b35bc0
    def execute(self) -> list[AuditFinding]:
        """Runs the DI check by scanning source files for policy violations."""
        findings: list[AuditFinding] = []
        rules = self.policy or []
        if not rules:
            # Nothing to enforce if the policy is empty/undefined
            return findings

        for rule in rules:
            rule_id = rule.get("id")
            if not rule_id:
                continue

            if rule_id == "di.no_direct_instantiation":
                findings.extend(self._check_forbidden_instantiations(rule))
            elif rule_id == "di.no_global_session_import":
                findings.extend(self._check_forbidden_imports(rule))

        return findings

    def _check_forbidden_instantiations(self, rule: dict[str, Any]) -> list[AuditFinding]:
        """Finds direct instantiations of major services."""
        findings: list[AuditFinding] = []

        forbidden_calls: set[str] = set(rule.get("forbidden_instantiations", []))
        if not forbidden_calls:
            return findings

        scope: list[str] = rule.get("scope", [])
        exclusions: list[str] = rule.get("exclusions", [])

        for file_path in self._get_files_in_scope(scope, exclusions):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):
                        if node.func.id in forbidden_calls:
                            findings.append(
                                AuditFinding(
                                    check_id="di.no_direct_instantiation",
                                    severity=AuditSeverity.ERROR,
                                    message=(
                                        f"Direct instantiation of '{node.func.id}' is "
                                        "forbidden. Inject it via the constructor."
                                    ),
                                    file_path=str(
                                        file_path.relative_to(self.repo_root)
                                    ),
                                    line_number=node.lineno,
                                    context={"category": "architectural"},
                                )
                            )
            except SyntaxError as exc:
                logger.debug(
                    "Skipping DI instantiation scan for %s due to SyntaxError: %s",
                    file_path,
                    exc,
                )
            except OSError as exc:
                logger.debug(
                    "Skipping DI instantiation scan for %s due to read error: %s",
                    file_path,
                    exc,
                )

        return findings

    def _check_forbidden_imports(self, rule: dict[str, Any]) -> list[AuditFinding]:
        """Finds direct imports of forbidden functions like get_session."""
        findings: list[AuditFinding] = []

        forbidden_imports: set[str] = set(rule.get("forbidden_imports", []))
        if not forbidden_imports:
            return findings

        scope: list[str] = rule.get("scope", [])
        exclusions: list[str] = rule.get("exclusions", [])

        for file_path in self._get_files_in_scope(scope, exclusions):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if isinstance(node, ast.ImportFrom) and node.module in forbidden_imports:
                        findings.append(
                            AuditFinding(
                                check_id="di.no_global_session_import",
                                severity=AuditSeverity.ERROR,
                                message=(
                                    f"Direct import of '{node.module}' is forbidden. "
                                    "Inject the dependency instead."
                                ),
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=node.lineno,
                                context={"category": "architectural"},
                            )
                        )
            except SyntaxError as exc:
                logger.debug(
                    "Skipping DI import scan for %s due to SyntaxError: %s",
                    file_path,
                    exc,
                )
            except OSError as exc:
                logger.debug(
                    "Skipping DI import scan for %s due to read error: %s",
                    file_path,
                    exc,
                )

        return findings

    def _get_files_in_scope(
        self, scope: Iterable[str], exclusions: Iterable[str]
    ) -> list[Path]:
        """
        Helper to get all files matching the scope and exclusion globs.

        Args:
            scope: Glob patterns for files to include.
            exclusions: Glob patterns for files to exclude.
        """
        scope_patterns = list(scope or [])
        exclusion_patterns = list(exclusions or [])

        if not scope_patterns:
            return []

        files: list[Path] = []
        for glob_pattern in scope_patterns:
            for file_path in self.repo_root.glob(glob_pattern):
                if not file_path.is_file():
                    continue
                if any(file_path.match(ex) for ex in exclusion_patterns):
                    continue
                files.append(file_path)

        # Deduplicate while preserving type Path
        unique_files: List[Path] = list({p.resolve() for p in files})
        return unique_files
# src/mind/governance/checks/domain_placement.py
"""
A constitutional audit check to ensure capabilities are declared in the
correct domain manifest file.

This check contributes to the following constitutional rule from
charter/policies/quality_assurance.yaml:

- structural_compliance
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Iterable, List

from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity
from shared.utils.yaml_processor import yaml_processor


logger = getLogger(__name__)


# ID: 0cd8ad5a-ed46-4f18-8335-f95b747d6164
class DomainPlacementCheck(BaseCheck):
    """
    Validates that capability keys declared in a domain manifest file
    match the domain of that file.

    Example:
        - File: .intent/mind/knowledge/domains/core.yaml
        - Capability key: "core.introspection.analyze_code" ✅ OK
        - Capability key: "llm.router.select_model"        ❌ Wrong domain
    """

    # Constitutional rule(s) this check enforces or contributes to.
    policy_rule_ids = [
        "structural_compliance",
    ]

    def __init__(self, context: AuditorContext) -> None:
        super().__init__(context)
        self.context = context
        self.domains_dir: Path = (
            self.context.mind_path / "knowledge" / "domains"
        )

    # ID: 7eb75aef-6463-450d-8088-e9a64e3d85c8
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings: list[AuditFinding] = []

        if not self.domains_dir.is_dir():
            # No domain manifests present yet – nothing to validate.
            return findings

        for domain_file in sorted(self.domains_dir.glob("*.yaml")):
            findings.extend(self._check_domain_file(domain_file))

        return findings

    def _check_domain_file(self, domain_file: Path) -> list[AuditFinding]:
        """Validate a single domain manifest file."""
        findings: list[AuditFinding] = []
        domain_name = domain_file.stem

        try:
            manifest_content: dict[str, Any] | None = yaml_processor.load(domain_file)
        except Exception as exc:  # pragma: no cover - defensive
            logger.warning(
                "Failed to load domain manifest %s: %s", domain_file, exc
            )
            return findings

        if not manifest_content:
            return findings

        capabilities = manifest_content.get("tags", [])
        if not isinstance(capabilities, list):
            # Malformed structure – we could add a separate finding type later.
            return findings

        for cap in capabilities:
            if not isinstance(cap, dict):
                continue
            cap_key = cap.get("key")
            if not cap_key or not isinstance(cap_key, str):
                continue

            if not cap_key.startswith(f"{domain_name}."):
                findings.append(
                    AuditFinding(
                        check_id="domain.placement.mismatch",
                        severity=AuditSeverity.ERROR,
                        message=(
                            f"Capability '{cap_key}' is misplaced in '{domain_file.name}'. "
                            f"It should be declared in '{cap_key.split('.')[0]}.yaml'."
                        ),
                        file_path=str(
                            domain_file.relative_to(self.context.repo_path)
                        ),
                        context={
                            "domain_file": domain_file.name,
                            "expected_domain": cap_key.split(".")[0],
                            "actual_domain": domain_name,
                        },
                    )
                )

        return findings
# src/mind/governance/checks/duplication_check.py

"""
A constitutional audit check to find semantically duplicate or near-duplicate
symbols (functions/classes) using the Qdrant vector database.

This check contributes to the following refactoring rules from
charter/policies/code_standards.yaml:

- extract_function
- extract_module
- introduce_facade
"""

from __future__ import annotations

import asyncio
from typing import Any, Dict, Iterable, List, Tuple

from rich.progress import track

from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from services.clients.qdrant_client import QdrantService
from shared.config import settings
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity


logger = getLogger(__name__)


# ID: 13cf4ae9-f18b-410f-a320-399cc713f277
class DuplicationCheck(BaseCheck):
    """
    Enforces the 'dry_by_design' principle by finding semantically similar symbols.

    It does not automatically refactor code; instead it provides the evidence
    that should trigger refactoring patterns such as:

    - extract_function
    - extract_module
    - introduce_facade
    """

    # Constitutional rule(s) this check contributes to.
    policy_rule_ids = [
        "extract_function",
        "extract_module",
        "introduce_facade",
    ]

    def __init__(self, context: AuditorContext, qdrant_service: QdrantService) -> None:
        super().__init__(context)
        self.context = context
        self.qdrant_service = qdrant_service
        self.symbols: Dict[str, Dict[str, Any]] = self.context.knowledge_graph.get(
            "symbols", {}
        )

        # Governance: ignore list is defined in audit_ignore_policy.
        try:
            ignore_policy = settings.load(
                "charter.policies.governance.audit_ignore_policy"
            )
        except FileNotFoundError:
            ignore_policy = {}

        self.ignored_symbol_keys = {
            item["key"]
            for item in ignore_policy.get("symbol_ignores", [])
            if isinstance(item, dict) and "key" in item
        }

    async def _check_single_symbol(
        self, symbol: dict[str, Any], threshold: float
    ) -> list[AuditFinding]:
        """
        Checks a single symbol for duplicates against the Qdrant index.

        Args:
            symbol: Symbol metadata record from the knowledge graph.
            threshold: Similarity threshold above which we consider two symbols
                       suspiciously similar.

        Returns:
            A list of AuditFinding instances for potential duplicates.
        """
        findings: list[AuditFinding] = []

        symbol_key = symbol.get("symbol_path")
        point_id = str(symbol.get("uuid")) if symbol.get("uuid") else None

        if not symbol_key or not point_id:
            return findings

        if symbol_key in self.ignored_symbol_keys:
            return findings

        try:
            query_vector = await self.qdrant_service.get_vector_by_id(
                point_id=point_id
            )
            if not query_vector:
                return findings

            similar_hits = await self.qdrant_service.search_similar(
                query_vector=query_vector,
                limit=5,
            )
            for hit in similar_hits:
                payload = hit.get("payload") or {}
                score = float(hit.get("score", 0.0))

                hit_symbol_key = payload.get("chunk_id")
                if (
                    not hit_symbol_key
                    or hit_symbol_key == symbol_key
                    or hit_symbol_key in self.ignored_symbol_keys
                ):
                    continue

                if score > threshold:
                    symbol_a, symbol_b = sorted((symbol_key, hit_symbol_key))
                    findings.append(
                        AuditFinding(
                            check_id="code.style.semantic-duplication",
                            severity=AuditSeverity.WARNING,
                            message=(
                                "Potential duplicate logic found between "
                                f"'{symbol_a.split('::')[-1]}' and "
                                f"'{symbol_b.split('::')[-1]}'."
                            ),
                            file_path=symbol.get("file_path"),
                            context={
                                "symbol_a": symbol_a,
                                "symbol_b": symbol_b,
                                "similarity": f"{score:.2f}",
                                "suggested_actions": [
                                    "extract_function",
                                    "extract_module",
                                    "introduce_facade",
                                ],
                            },
                        )
                    )
        except Exception as exc:
            logger.warning(
                "Could not perform duplication check for '%s': %s",
                symbol_key,
                exc,
            )

        return findings

    # ID: 1da6e2c3-fbd4-4860-b95e-7625f426edba
    async def execute(self, threshold: float = 0.8) -> list[AuditFinding]:
        """
        Asynchronously runs the duplication check across all vectorized symbols.

        Args:
            threshold: Similarity threshold above which a pair is treated
                       as a potential duplication.

        Returns:
            A de-duplicated list of AuditFinding instances.
        """
        symbols_to_check = list(self.symbols.values())
        if not symbols_to_check:
            return []

        tasks = [
            self._check_single_symbol(symbol, threshold)
            for symbol in symbols_to_check
        ]

        results: list[AuditFinding] = []

        for future in track(
            asyncio.as_completed(tasks),
            description="Checking for duplicate code...",
            total=len(tasks),
        ):
            results.extend(await future)

        # Deduplicate findings by (symbol_a, symbol_b) pair so we don't
        # report A-B and B-A separately.
        unique_findings: dict[tuple[str, str], AuditFinding] = {}
        for finding in results:
            ctx = finding.context or {}
            symbol_a = ctx.get("symbol_a")
            symbol_b = ctx.get("symbol_b")
            if not symbol_a or not symbol_b:
                continue
            key_tuple = tuple(sorted((symbol_a, symbol_b)))
            if key_tuple not in unique_findings:
                unique_findings[key_tuple] = finding

        return list(unique_findings.values())
# src/mind/governance/checks/environment_checks.py
"""
Audits the system's runtime environment for required configuration.

This check enforces the requirements defined in the runtime_requirements
policy (usually mind/runtime_requirements.yaml as declared in meta.yaml).

It is currently governed by that policy rather than a specific
charter/policies/* rule, so it does not yet contribute to the
central policy coverage matrix.
"""

from __future__ import annotations

import os
from typing import Any, Dict, List

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 0c3965b7-b3f3-4fb6-bbbb-c94a1ffae3fe
class EnvironmentChecks(BaseCheck):
    """Container for environment and runtime configuration checks."""

    # NOTE: This check is governed by runtime_requirements, not a charter rule yet.
    # Once we add an explicit rule like `operations.runtime.environment_variables`
    # or similar to charter/policies, we can wire it up in policy_coverage_service.
    governed_policy_id: str = "runtime_requirements"

    def __init__(self, context: Any) -> None:
        super().__init__(context)
        # runtime_requirements is expected to be loaded into context.policies
        self.requirements: Dict[str, Any] = self.context.policies.get(
            self.governed_policy_id, {}
        )

    # ID: 0c0e7695-b11e-4ad8-9e74-23d5f79dad00
    def execute(self) -> List[AuditFinding]:
        """
        Verifies that required environment variables specified in
        runtime_requirements are set.

        Returns:
            A list of AuditFinding instances for missing required variables.
        """
        findings: List[AuditFinding] = []

        required_vars = self.requirements.get("variables", {})
        if not isinstance(required_vars, dict):
            # Misconfigured policy – fail soft but visible.
            findings.append(
                AuditFinding(
                    check_id="environment.runtime_requirements.misconfigured",
                    severity=AuditSeverity.ERROR,
                    message=(
                        "runtime_requirements.variables must be a mapping of "
                        "ENV_VAR_NAME -> config dict."
                    ),
                    file_path="mind/runtime_requirements.yaml",
                )
            )
            return findings

        for name, config in required_vars.items():
            if not isinstance(config, dict):
                continue

            is_required = bool(config.get("required"))
            if not is_required:
                continue

            if os.getenv(name):
                continue

            description = config.get("description", "No description provided.")
            message = (
                f"Required environment variable '{name}' is not set. "
                f"Description: {description}"
            )

            findings.append(
                AuditFinding(
                    check_id="environment.variable.missing",
                    severity=AuditSeverity.ERROR,
                    message=message,
                    file_path=".env",  # Hint to human where to fix it
                )
            )

        return findings
# src/mind/governance/checks/file_checks.py
"""
Audits file existence and orphan detection for constitutional governance files.
"""

from __future__ import annotations

from mind.governance.checks.base_check import BaseCheck
from shared.config import settings
from shared.models import AuditFinding, AuditSeverity
from shared.utils.constitutional_parser import get_all_constitutional_paths

KNOWN_UNINDEXED_FILES = {
    ".intent/charter/constitution/approvers.yaml.example",
    ".intent/keys/private.key",
}

DEPRECATED_KNOWLEDGE_FILES = [
    ".intent/knowledge/cli_registry.yaml",
    ".intent/knowledge/resource_manifest.yaml",
    ".intent/knowledge/cognitive_roles.yaml",
]


# ID: 37b5ae2f-c3c2-4db4-9677-f16fd788c908
class FileChecks(BaseCheck):
    """Container for file-based constitutional checks."""

    # ID: 56481071-3a0c-437d-ba57-533bc03d9ed6
    def execute(self) -> list[AuditFinding]:
        """Runs all file-related checks."""
        meta_content = settings._meta_config
        required_files = get_all_constitutional_paths(meta_content, self.intent_path)
        findings = self._check_required_files(required_files)
        findings.extend(self._check_for_orphaned_intent_files(required_files))
        findings.extend(self._check_for_deprecated_files())
        return findings

    def _check_for_deprecated_files(self) -> list[AuditFinding]:
        """Verify that files constitutionally replaced by the database do not exist."""
        findings: list[AuditFinding] = []
        for file_rel_path in DEPRECATED_KNOWLEDGE_FILES:
            full_path = self.repo_root / file_rel_path
            if full_path.exists():
                findings.append(
                    AuditFinding(
                        check_id="config.ssot.deprecated-file",
                        severity=AuditSeverity.ERROR,
                        message=f"Deprecated knowledge file exists: '{file_rel_path}'. The database is the SSOT.",
                        file_path=file_rel_path,
                    )
                )
        return findings

    def _check_required_files(self, required_files: set[str]) -> list[AuditFinding]:
        """Verify that all files declared in meta.yaml exist on disk."""
        findings: list[AuditFinding] = []
        for file_rel_path in sorted(required_files):
            full_path = self.repo_root / file_rel_path
            if not full_path.exists():
                findings.append(
                    AuditFinding(
                        check_id="config.meta.missing-file",
                        severity=AuditSeverity.ERROR,
                        message=f"File declared in meta.yaml is missing: '{file_rel_path}'",
                        file_path=file_rel_path,
                    )
                )
        return findings

    def _check_for_orphaned_intent_files(
        self, declared_files: set[str]
    ) -> list[AuditFinding]:
        """Find .intent files not referenced in meta.yaml."""
        findings: list[AuditFinding] = []
        all_known_files = declared_files.union(KNOWN_UNINDEXED_FILES)
        if (self.intent_path / "proposals/README.md").exists():
            all_known_files.add(".intent/proposals/README.md")
        physical_files: set[str] = {
            str(p.relative_to(self.repo_root)).replace("\\", "/")
            for p in self.intent_path.rglob("*")
            if p.is_file()
        }
        orphaned_files = sorted(physical_files - all_known_files)
        for orphan in orphaned_files:
            if "prompts" in orphan or "reports" in orphan:
                continue
            findings.append(
                AuditFinding(
                    check_id="config.meta.orphaned-file",
                    severity=AuditSeverity.WARNING,
                    message=f"Orphaned file in .intent/: '{orphan}'. Add to meta.yaml or remove.",
                    file_path=orphan,
                )
            )
        return findings
# src/mind/governance/checks/health_checks.py
"""
Audits codebase health for complexity, atomicity, and line length violations.
"""

from __future__ import annotations

import ast
import statistics
from pathlib import Path

from radon.visitors import ComplexityVisitor

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 51dd8f1d-eda6-40e2-9c64-530ce6c290a6
class HealthChecks(BaseCheck):
    """Container for codebase health constitutional checks."""

    def __init__(self, context):
        super().__init__(context)
        # CORRECTED: Load from the consolidated code_standards policy
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.health_policy = code_standards_policy.get("health_standards", {})

    # ID: 64bffe32-e6fd-4fd1-a235-aaf764363076
    def execute(self) -> list[AuditFinding]:
        """Measures code complexity and atomicity against defined policies."""
        # The 'rules' key is now directly on self.health_policy
        policy_rules = self.health_policy
        file_line_counts = {}
        all_violations = []
        unique_files = {
            s["file_path"]
            for s in self.context.symbols_list
            if s.get("file_path", "").startswith("src/")
        }
        for file_path_str in sorted(list(unique_files)):
            if not file_path_str.endswith(".py"):
                continue
            file_path = self.repo_root / file_path_str
            logical_lines, violations = self._analyze_python_file(
                file_path, policy_rules
            )
            if logical_lines > 0:
                file_line_counts[file_path] = logical_lines
            all_violations.extend(violations)
        all_violations.extend(
            self._find_file_size_outliers(file_line_counts, policy_rules)
        )
        return all_violations

    def _analyze_python_file(
        self, file_path: Path, rules: dict
    ) -> tuple[int, list[AuditFinding]]:
        """Analyze a single Python file for health violations."""
        try:
            source_code = file_path.read_text(encoding="utf-8")
            logical_lines = self._count_logical_lines(source_code)
            if logical_lines > rules.get("max_module_lloc", 300):
                return logical_lines, [
                    AuditFinding(
                        check_id="code.complexity.module-too-long",
                        severity=AuditSeverity.WARNING,
                        message=f"Module has {logical_lines} lines (limit: {rules.get('max_module_lloc', 300)}).",
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                ]
            syntax_tree = ast.parse(source_code)
            complexity_visitor = ComplexityVisitor.from_ast(syntax_tree)
            violations = self._check_function_metrics(
                complexity_visitor,
                rules,
                str(file_path.relative_to(self.repo_root)),
            )
            return logical_lines, violations
        except Exception:
            return 0, []

    def _count_logical_lines(self, source_code: str) -> int:
        return sum(
            1
            for line in source_code.splitlines()
            if line.strip() and not line.strip().startswith("#")
        )

    def _check_function_metrics(
        self,
        visitor: ComplexityVisitor,
        rules: dict,
        file_path_str: str,
    ) -> list[AuditFinding]:
        violations = []
        for function in visitor.functions:
            if function.cognitive_complexity > rules.get(
                "max_cognitive_complexity", 15
            ):
                violations.append(
                    AuditFinding(
                        check_id="code.complexity.function-too-complex",
                        severity=AuditSeverity.WARNING,
                        message=f"Function '{function.name}' complexity is {function.cognitive_complexity} (limit: {rules.get('max_cognitive_complexity', 15)}).",
                        file_path=file_path_str,
                    )
                )
            if function.lloc > rules.get("max_function_lloc", 80):
                violations.append(
                    AuditFinding(
                        check_id="code.complexity.function-too-long",
                        severity=AuditSeverity.WARNING,
                        message=f"Function '{function.name}' has {function.lloc} lines (limit: {rules.get('max_function_lloc', 80)}).",
                        file_path=file_path_str,
                    )
                )
        return violations

    def _find_file_size_outliers(
        self, file_line_counts: dict, rules: dict
    ) -> list[AuditFinding]:
        if len(file_line_counts) < 3:
            return []
        violations = []
        line_count_values = list(file_line_counts.values())
        average_lines = statistics.mean(line_count_values)
        standard_deviation = statistics.stdev(line_count_values)
        outlier_threshold = average_lines + (
            rules.get("outlier_standard_deviations", 2.0) * standard_deviation
        )
        for file_path, line_count in file_line_counts.items():
            if line_count > outlier_threshold:
                violations.append(
                    AuditFinding(
                        check_id="code.complexity.module-outlier",
                        severity=AuditSeverity.WARNING,
                        message=f"Module size outlier ({line_count} lines vs avg of {average_lines:.0f}). Consider refactoring.",
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                )
        return violations
# src/mind/governance/checks/id_coverage_check.py
"""
A constitutional audit check to enforce that every public symbol in the codebase
has a registered ID in the database.
"""

from __future__ import annotations

import ast

from mind.governance.checks.base_check import BaseCheck
from shared.ast_utility import find_symbol_id_and_def_line
from shared.models import AuditFinding, AuditSeverity


# ID: 3501ed8c-8366-4ad7-9ab4-7dcf4c045c70
class IdCoverageCheck(BaseCheck):
    """
    Ensures every public function/class in `src/` has a valid, DB-registered ID tag.
    """

    # ID: f69a1a2e-26cd-4cc2-8fdc-7f18e0e77d0c
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings = []
        for file_path in self.context.src_dir.rglob("*.py"):
            try:
                content = file_path.read_text("utf-8")
                source_lines = content.splitlines()
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if not isinstance(
                        node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                    ):
                        continue

                    # Rule 1: Must be a public symbol
                    if node.name.startswith("_"):
                        continue

                    # Use the robust utility to find the ID and definition line
                    id_result = find_symbol_id_and_def_line(node, source_lines)

                    if not id_result.has_id:
                        findings.append(
                            AuditFinding(
                                check_id="linkage.id.missing-tag",
                                severity=AuditSeverity.ERROR,
                                message=f"Public symbol '{node.name}' is missing its required '# ID:' tag.",
                                file_path=str(
                                    file_path.relative_to(self.context.repo_path)
                                ),
                                line_number=id_result.definition_line_num,
                            )
                        )

            except Exception:
                # Silently ignore files that cannot be parsed
                continue

        return findings
# src/mind/governance/checks/id_uniqueness_check.py
"""
A constitutional audit check to enforce that every # ID tag is unique across the codebase.
"""

from __future__ import annotations

import re
from collections import defaultdict

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity

# Pre-compiled regex for efficiency to find '# ID: <uuid>'
ID_TAG_REGEX = re.compile(
    r"#\s*ID:\s*([0-9a-fA-F]{8}-([0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12})"
)


# ID: ddaabb9e-5e9a-4574-b458-dbed610e64e5
class IdUniquenessCheck(BaseCheck):
    """
    Scans the entire source code to ensure that every assigned symbol ID (UUID) is unique.
    This prevents data corruption from accidental copy-paste errors during development.
    """

    # ID: f2a3b4c5-d6e7-f8a9-b0c1-d2e3f4a5b6c7
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by scanning all Python files in `src/` and returns findings for any duplicate UUIDs.
        """
        # A dictionary to store locations of each UUID: {uuid: [("file/path.py", line_num), ...]}
        uuid_locations: dict[str, list[tuple[str, int]]] = defaultdict(list)

        src_dir = self.context.repo_path / "src"
        for file_path in src_dir.rglob("*.py"):
            try:
                content = file_path.read_text("utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    match = ID_TAG_REGEX.search(line)
                    if match:
                        found_uuid = match.group(1)
                        rel_path = str(file_path.relative_to(self.context.repo_path))
                        uuid_locations[found_uuid].append((rel_path, i))
            except Exception:
                # Silently ignore files that can't be read or parsed
                continue

        findings = []
        for found_uuid, locations in uuid_locations.items():
            if len(locations) > 1:
                # Found a duplicate!
                locations_str = ", ".join(
                    [f"{path}:{line}" for path, line in locations]
                )
                findings.append(
                    AuditFinding(
                        check_id="linkage.id.duplicate",
                        severity=AuditSeverity.ERROR,
                        message=f"Duplicate ID tag found: {found_uuid}",
                        context={"locations": locations_str},
                    )
                )

        return findings
# src/mind/governance/checks/import_rules.py
"""
A constitutional audit check to enforce architectural import rules as
defined in the source_structure.yaml manifest.
"""

from __future__ import annotations

import ast
from pathlib import Path

from sqlalchemy import text

from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from services.database.session_manager import get_session
from shared.models import AuditFinding, AuditSeverity


def _scan_imports(file_path: Path, content: str | None = None) -> list[str]:
    """
    Parse a Python file or its content and extract all imported module paths.
    """
    imports = []
    try:
        source = (
            content if content is not None else file_path.read_text(encoding="utf-8")
        )
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    if node.level > 0:
                        base = ".".join(file_path.parts[1:-1])
                        if node.level > 1:
                            base = ".".join(base.split(".")[: -(node.level - 1)])
                        imports.append(f"{base}.{node.module}")
                    else:
                        imports.append(node.module)

    except Exception:
        pass

    return imports


# ID: 0690cf39-3739-449e-9228-2c7c8526209b
class ImportRulesCheck(BaseCheck):
    """
    Ensures that code files only import modules from their allowed domains.
    This check now reads its configuration from the database.
    """

    def __init__(self, context: AuditorContext):
        super().__init__(context)
        self.domain_map: dict[str, str] = {}
        self.import_rules: dict[str, set[str]] = {}

    async def _load_rules_from_db(self):
        """Loads domain maps and import rules from the database."""
        if self.domain_map:
            return

        async with get_session() as session:
            await session.execute(text("SELECT key FROM core.domains"))

        structure = self.context.source_structure.get("structure", [])
        for domain_info in structure:
            path_str = domain_info.get("path")
            domain_name = domain_info.get("domain")
            if path_str and domain_name:
                self.domain_map[path_str] = domain_name

        for domain_info in structure:
            domain_name = domain_info.get("domain")
            allowed_imports = domain_info.get("allowed_imports", [])
            if domain_name:
                self.import_rules[domain_name] = set(allowed_imports)

    def _get_domain_for_path_str(self, file_path_str: str) -> str | None:
        """Finds the domain for a given relative file path string."""
        for domain_path_prefix, domain_name in self.domain_map.items():
            if file_path_str.startswith(domain_path_prefix):
                return domain_name
        return None

    # ID: f1a7dedb-d5e4-442d-8957-b7f974778bc5
    async def execute(self) -> list[AuditFinding]:
        """
        Runs the check by scanning all source files and validating their imports.
        """
        await self._load_rules_from_db()
        findings = []
        # Use self.src_dir provided by the BaseCheck
        for file_path in self.src_dir.rglob("*.py"):
            findings.extend(self._check_file_imports(file_path, file_content=None))
        return findings

    # ID: 31287af5-d942-4a1d-b06d-d0570026d035
    async def execute_on_content(
        self, file_path_str: str, file_content: str
    ) -> list[AuditFinding]:
        """
        Runs the import check on a string of content instead of a file on disk.
        """
        await self._load_rules_from_db()
        # Use self.repo_root provided by the BaseCheck
        file_path = self.repo_root / file_path_str
        return self._check_file_imports(file_path, file_content)

    def _check_file_imports(
        self, file_path: Path, file_content: str | None
    ) -> list[AuditFinding]:
        """Core logic to check imports for a given file path and optional content."""
        findings = []
        # Use self.repo_root provided by the BaseCheck
        file_rel_path_str = str(file_path.relative_to(self.repo_root))
        file_domain = self._get_domain_for_path_str(file_rel_path_str)
        if not file_domain:
            return []

        allowed_imports_for_domain = self.import_rules.get(file_domain, set())
        imported_modules = _scan_imports(file_path, content=file_content)

        for module_str in imported_modules:
            imported_package = module_str.split(".")[0]

            if not any(
                imported_package.startswith(p)
                for p in ["src", "cli", "core", "features", "services", "shared"]
            ):
                continue

            if imported_package in allowed_imports_for_domain:
                continue

            if imported_package == file_domain:
                continue

            findings.append(
                AuditFinding(
                    check_id="architecture.import_violation",
                    severity=AuditSeverity.ERROR,
                    message=f"Illegal import of '{module_str}' in domain '{file_domain}'. Allowed: {sorted(list(allowed_imports_for_domain))}",
                    file_path=file_rel_path_str,
                )
            )
        return findings
# src/mind/governance/checks/__init__.py
"""Provides functionality for the __init__ module."""

from __future__ import annotations
# src/mind/governance/checks/knowledge_source_check.py
"""
Compares DB single-source-of-truth tables with their (legacy) YAML exports.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

import yaml
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, async_sessionmaker

# Configuration
TABLE_CONFIGS = {
    "cli_registry": {
        "yaml_paths": [
            ".intent/mind/knowledge/cli_registry.yaml",
            ".intent/mind/knowledge/cli_registry.yml",
        ],
        "table": "core.cli_commands",
        "yaml_key": "commands",
        "primary_key": "name",
        "preferred_order": ["name", "module", "entrypoint", "enabled"],
    },
    "resource_manifest": {
        "yaml_paths": [
            ".intent/mind/knowledge/resource_manifest.yaml",
            ".intent/mind/knowledge/resource_manifest.yml",
        ],
        "table": "core.llm_resources",
        "yaml_key": "llm_resources",
        "primary_key": "name",
        "preferred_order": ["name", "provider", "model", "enabled"],
    },
    "cognitive_roles": {
        "yaml_paths": [
            ".intent/mind/knowledge/cognitive_roles.yaml",
            ".intent/mind/knowledge/cognitive_roles.yml",
        ],
        "table": "core.cognitive_roles",
        "yaml_key": "cognitive_roles",
        "primary_key": "role",
        "preferred_order": ["name", "description", "enabled"],
    },
}

FIELD_PRIORITY = [
    "name",
    "role",
    "module",
    "entrypoint",
    "provider",
    "model",
    "description",
    "enabled",
]


@dataclass
# ID: 55de1540-39da-4a5d-9e40-b0614cfe655f
class CheckResult:
    name: str
    passed: bool
    details: dict[str, Any]


# ID: 81d6e8ed-a6f6-444c-acda-9064896c5111
class KnowledgeSourceCheck:
    """
    Compares DB single-source-of-truth tables with their (legacy) YAML exports under:
      .intent/mind/knowledge/{cli_registry, resource_manifest, cognitive_roles}.yaml

    Behavior:
      - If a YAML file is missing and `require_yaml_exports=False` (default), that section is SKIPPED.
      - If a YAML file exists, it is compared with the DB rows (adaptive to actual DB columns).
      - Any drift in an existing YAML file FAILS the check.

    Set `require_yaml_exports=True` to enforce the presence of YAML exports.
    """

    NAME = "knowledge_source_check"

    def __init__(
        self,
        repo_root: Path,
        engine: AsyncEngine,
        session_factory: async_sessionmaker[AsyncSession],
        reports_dir: Path | None = None,
        require_yaml_exports: bool = False,
    ) -> None:
        self.repo_root = repo_root
        self.engine = engine
        self.session_factory = session_factory
        self.reports_dir = reports_dir or repo_root / "reports" / "knowledge_ssot"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        self.require_yaml_exports = require_yaml_exports

    # ---------- Public API ----------
    # ID: b846d3ab-5762-4bc8-9dfc-f3fa060da29c
    async def execute(self) -> CheckResult:
        """Execute the knowledge source check and return results."""
        # Resolve YAML paths
        yaml_paths = {
            section: self._resolve_yaml(*config["yaml_paths"])
            for section, config in TABLE_CONFIGS.items()
        }

        # Fetch all database tables
        section_results = {}
        async with self.session_factory() as session:
            for section, config in TABLE_CONFIGS.items():
                schema, table = config["table"].split(".")
                db_rows, db_cols = await self._fetch_table(
                    session, schema, table, config["preferred_order"]
                )

                section_results[section] = await self._compare_section(
                    label=section,
                    yaml_path=yaml_paths[section],
                    db_rows=db_rows,
                    db_cols=db_cols,
                    yaml_key=config["yaml_key"],
                    primary_key=config["primary_key"],
                )

        # Determine overall pass/fail status
        passed = self._determine_overall_status(section_results)

        # Build and save report
        report = self._build_report(passed, yaml_paths, section_results)
        self._save_report(report)

        return CheckResult(name=self.NAME, passed=passed, details=report)

    # ---------- Section comparison ----------
    async def _compare_section(
        self,
        *,
        label: str,
        yaml_path: Path | None,
        db_rows: list[dict[str, Any]],
        db_cols: list[str],
        yaml_key: str,
        primary_key: str,
    ) -> dict[str, Any]:
        """Compare a single section (YAML vs DB)."""
        # Handle missing YAML file
        if yaml_path is None:
            return self._handle_missing_yaml()

        # Load and compare
        yaml_items = self._read_yaml(yaml_path, yaml_key)
        compare_fields = self._determine_compare_fields(yaml_items, db_cols)
        diff = self._diff_records(yaml_items, db_rows, primary_key, compare_fields)

        status = "passed" if self._is_diff_clean(diff) else "failed"
        return {
            "status": status,
            "yaml": str(yaml_path),
            "compare_fields": list(compare_fields),
            "diff": diff,
        }

    def _handle_missing_yaml(self) -> dict[str, Any]:
        """Handle the case where a YAML file is missing."""
        if self.require_yaml_exports:
            return {
                "status": "failed",
                "reason": "yaml_missing_and_required",
                "diff": {
                    "missing_in_db": [],
                    "missing_in_yaml": [],
                    "mismatched": [],
                },
            }
        return {"status": "skipped", "reason": "yaml_missing", "diff": None}

    # ---------- Database operations ----------
    async def _fetch_table(
        self,
        session: AsyncSession,
        schema: str,
        table: str,
        preferred_order: list[str],
    ) -> tuple[list[dict[str, Any]], list[str]]:
        """Fetch all rows and columns from a database table."""
        cols = await self._list_columns(session, schema, table)
        if not cols:
            return [], []

        # Query the table
        select_cols = ", ".join([f'"{c}"' for c in cols])
        sql = text(f'SELECT {select_cols} FROM "{schema}"."{table}"')
        rows = (await session.execute(sql)).mappings().all()

        # Order columns consistently
        ordered_cols = self._order_columns(cols, preferred_order)
        data = [{k: dict(r).get(k) for k in ordered_cols} for r in rows]

        return data, ordered_cols

    async def _list_columns(
        self, session: AsyncSession, schema: str, table: str
    ) -> list[str]:
        """Get the list of columns for a table from information_schema."""
        sql = text(
            """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = :schema AND table_name = :table
            ORDER BY ordinal_position
            """
        )
        rows = (
            await session.execute(sql, {"schema": schema, "table": table})
        ).mappings()
        return [r["column_name"] for r in rows]

    # ---------- YAML operations ----------
    def _resolve_yaml(self, *candidate_rel_paths: str) -> Path | None:
        """Find the first existing YAML file from a list of candidates."""
        for rel in candidate_rel_paths:
            p = self.repo_root / rel
            if p.exists():
                return p
        return None

    def _read_yaml(self, path: Path, key: str) -> list[dict[str, Any]]:
        """Read a YAML file and extract items by key."""
        try:
            data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
            if not isinstance(data, dict):
                return []

            items = data.get(key, [])
            return items if isinstance(items, list) else []
        except Exception:
            return []

    # ---------- Comparison logic ----------
    def _determine_compare_fields(
        self, yaml_items: list[dict[str, Any]], db_cols: list[str]
    ) -> tuple[str, ...]:
        """Determine which fields to compare based on YAML and DB columns."""
        yaml_keys = set()
        for item in yaml_items:
            if isinstance(item, dict):
                yaml_keys.update(item.keys())

        # Include primary key possibilities
        common_keys = (yaml_keys & set(db_cols)) | {"name", "role"}
        return self._order_fields(common_keys)

    def _diff_records(
        self,
        yaml_items: list[dict[str, Any]],
        db_items: list[dict[str, Any]],
        primary_key: str,
        compare_fields: tuple[str, ...],
    ) -> dict[str, Any]:
        """Compare YAML and DB records and return differences."""
        # Build indexes by primary key
        yaml_index = self._build_index(yaml_items, primary_key)
        db_index = self._build_index(db_items, primary_key)

        # Find missing records
        missing_in_db = sorted(set(yaml_index.keys()) - set(db_index.keys()))
        missing_in_yaml = sorted(set(db_index.keys()) - set(yaml_index.keys()))

        # Find mismatched records
        mismatched = self._find_mismatches(yaml_index, db_index, compare_fields)

        return {
            "missing_in_db": missing_in_db,
            "missing_in_yaml": missing_in_yaml,
            "mismatched": mismatched,
        }

    def _build_index(
        self, items: list[dict[str, Any]], key: str
    ) -> dict[str, dict[str, Any]]:
        """Build an index of items by their primary key."""
        return {
            str(item.get(key)).strip(): item
            for item in items
            if isinstance(item, dict) and item.get(key) is not None
        }

    def _find_mismatches(
        self,
        yaml_index: dict[str, dict[str, Any]],
        db_index: dict[str, dict[str, Any]],
        compare_fields: tuple[str, ...],
    ) -> list[dict[str, Any]]:
        """Find records that exist in both but have different field values."""
        mismatched = []
        common_keys = set(yaml_index.keys()) & set(db_index.keys())

        for key in sorted(common_keys):
            yaml_record = yaml_index[key]
            db_record = db_index[key]

            field_diffs = self._compare_records(yaml_record, db_record, compare_fields)

            if field_diffs:
                mismatched.append({"name": key, "fields": field_diffs})

        return mismatched

    def _compare_records(
        self,
        yaml_record: dict[str, Any],
        db_record: dict[str, Any],
        compare_fields: tuple[str, ...],
    ) -> dict[str, dict[str, Any]]:
        """Compare two records field by field."""
        diffs = {}

        for field in compare_fields:
            # Skip fields not present in either record
            if field not in yaml_record and field not in db_record:
                continue

            yaml_val = yaml_record.get(field)
            db_val = db_record.get(field)

            # Normalize: treat empty strings and None as equivalent
            if self._values_equivalent(yaml_val, db_val):
                continue

            if yaml_val != db_val:
                diffs[field] = {"yaml": yaml_val, "db": db_val}

        return diffs

    @staticmethod
    def _values_equivalent(val1: Any, val2: Any) -> bool:
        """Check if two values are equivalent (treating None and empty string as same)."""
        return (val1 is None or val1 == "") and (val2 is None or val2 == "")

    @staticmethod
    def _is_diff_clean(diff: dict[str, Any]) -> bool:
        """Check if a diff shows no differences."""
        return (
            not diff["missing_in_db"]
            and not diff["missing_in_yaml"]
            and not diff["mismatched"]
        )

    # ---------- Utility functions ----------
    @staticmethod
    def _order_columns(cols: list[str], preferred: list[str]) -> list[str]:
        """Order columns with preferred ones first, rest alphabetically."""
        return [c for c in preferred if c in cols] + [
            c for c in cols if c not in preferred
        ]

    @staticmethod
    def _order_fields(fields: set) -> tuple[str, ...]:
        """Order fields with priority fields first, rest alphabetically."""
        ordered = [f for f in FIELD_PRIORITY if f in fields] + [
            f for f in sorted(fields) if f not in FIELD_PRIORITY
        ]
        return tuple(ordered)

    def _determine_overall_status(
        self, section_results: dict[str, dict[str, Any]]
    ) -> bool:
        """Determine if the overall check passed based on section results."""
        any_failed = any(
            result.get("status") == "failed" for result in section_results.values()
        )

        if self.require_yaml_exports:
            any_skipped = any(
                result.get("status") == "skipped" for result in section_results.values()
            )
            return not any_failed and not any_skipped

        return not any_failed

    def _build_report(
        self,
        passed: bool,
        yaml_paths: dict[str, Path | None],
        section_results: dict[str, dict[str, Any]],
    ) -> dict[str, Any]:
        """Build the complete report structure."""
        return {
            "check": self.NAME,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "passed": passed,
            "require_yaml_exports": self.require_yaml_exports,
            "sources": {
                "yaml_paths": {
                    k: str(v) if isinstance(v, Path) else None
                    for k, v in yaml_paths.items()
                },
                "db_tables": {
                    section: config["table"]
                    for section, config in TABLE_CONFIGS.items()
                },
            },
            "sections": section_results,
        }

    def _save_report(self, report: dict[str, Any]) -> None:
        """Save the report to a timestamped JSON file."""
        report_path = self.reports_dir / (
            datetime.utcnow().strftime("%Y%m%d_%H%M%S") + ".json"
        )
        report_path.write_text(json.dumps(report, indent=2), encoding="utf-8")
# src/mind/governance/checks/legacy_tag_check.py
"""Provides functionality for the legacy_tag_check module."""

from __future__ import annotations

import re

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 0649c22b-9336-490b-9ffd-25e202924301
class LegacyTagCheck(BaseCheck):
    # ID: 94e602d4-47da-455d-be69-fe7a037bcb2b
    def execute(self) -> list[AuditFinding]:
        findings = []
        pattern = re.compile(r"#\s*CAPABILITY:", re.IGNORECASE)
        exclude_dirs = {
            ".git",
            ".venv",
            "__pycache__",
            ".pytest_cache",
            ".ruff_cache",
            "reports",
        }
        exclude_files = {"poetry.lock", "project_context.txt"}
        binary_extensions = {
            ".png",
            ".jpg",
            ".jpeg",
            ".gif",
            ".ico",
            ".pyc",
            ".so",
            ".o",
            ".zip",
            ".gz",
            ".pdf",
        }

        # --- THIS IS THE FIX ---
        # The loop now correctly uses self.repo_root, which is set by the BaseCheck parent class.
        for file_path in self.repo_root.rglob("*"):
            if not file_path.is_file():
                continue

            if any(part in exclude_dirs for part in file_path.parts):
                continue
            if file_path.name in exclude_files:
                continue
            if file_path.suffix in binary_extensions:
                continue

            try:
                content = file_path.read_text(encoding="utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    if pattern.search(line):
                        findings.append(
                            AuditFinding(
                                check_id="style.no_legacy_capability_tags",
                                severity=AuditSeverity.ERROR,
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=i,
                            )
                        )
            except UnicodeDecodeError:
                continue
            except Exception:
                continue

        return findings
# src/mind/governance/checks/manifest_lint.py
"""
Audits capability manifests for quality issues like placeholder text.
"""

from __future__ import annotations

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: ee190b8d-1bf0-4b1a-90e2-abf21ca013c9
class ManifestLintCheck(BaseCheck):
    """Checks for placeholder text in capability manifests."""

    def __init__(self, context):
        super().__init__(context)
        # CORRECTED: Load from the consolidated code_standards policy
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.linter_rules = code_standards_policy.get("capability_rules", [])

    # ID: 2e114e07-e521-4e56-a56c-f3afc6458f44
    def execute(self) -> list[AuditFinding]:
        """Finds capabilities with placeholder descriptions."""
        findings = []
        rule = next(
            (r for r in self.linter_rules if r.get("id") == "caps.no_placeholder_text"),
            None,
        )
        if not rule:
            return []

        for symbol in self.context.symbols_list:
            description = symbol.get("intent", "") or ""
            if any(
                f.lower() in description.lower() for f in ["TBD", "N/A", "Auto-added"]
            ):
                findings.append(
                    AuditFinding(
                        check_id="manifest.lint.placeholder",
                        severity=AuditSeverity.WARNING,
                        message=f"Capability '{symbol.get('key')}' has a placeholder description: '{description}'",
                        file_path=symbol.get("file_path"),
                        line_number=symbol.get("line_number"),
                    )
                )
        return findings
# src/mind/governance/checks/naming_conventions.py
"""
A constitutional audit check to enforce file and symbol naming conventions
as defined in the code_standards.yaml policy.
"""

from __future__ import annotations

import re
from typing import Any

from mind.governance.audit_context import AuditorContext
from shared.models import AuditFinding, AuditSeverity


# ID: 7cff5dba-bd63-4e8c-8e3f-8f242a59f28d
class NamingConventionsCheck:
    """
    Ensures that file names match the patterns defined in the constitution.
    This check is now fully dynamic and reads all configuration from the policy file.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.naming_policy = code_standards_policy.get("naming_conventions", {})

    # REFACTORED: The main execute method is now simpler.
    # ID: 6bebb819-1073-4163-8b70-09c2c374f6c8
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by iterating through policy rules and scanning the
        repository file system for violations.
        """
        findings = []
        if not self.naming_policy:
            return findings

        # Iterate through categories like 'intent' and 'code'
        for category, rules in self.naming_policy.items():
            if not isinstance(rules, list):
                continue

            # Iterate through the list of rule objects in each category
            for rule in rules:
                findings.extend(self._process_rule(rule, category))
        return findings

    # NEW: Helper method to process a single rule. This is much easier to test.
    def _process_rule(self, rule: dict[str, Any], category: str) -> list[AuditFinding]:
        """Processes a single naming convention rule against the file system."""
        findings = []
        scope_glob = rule.get("scope")
        pattern = rule.get("pattern")
        rule_id = rule.get("id", f"naming.{category}.unnamed")
        exclusions = rule.get("exclusions", [])
        enforcement = rule.get("enforcement", "error")

        if not scope_glob or not pattern:
            return findings  # Skip malformed rules

        try:
            compiled_pattern = re.compile(pattern)
        except re.error:
            # If the regex in the policy is invalid, skip it.
            # This prevents the auditor from crashing on a bad policy.
            return findings

        for file_path in self.context.repo_root.glob(scope_glob):
            if not file_path.is_file():
                continue

            # Check against exclusions using glob patterns for robustness.
            if any(file_path.match(ex) for ex in exclusions):
                continue

            # The core logic: check if the file's name matches the required pattern.
            if not compiled_pattern.match(file_path.name):
                findings.append(
                    AuditFinding(
                        check_id=rule_id,
                        severity=AuditSeverity[enforcement.upper()],
                        message=f"File name '{file_path.name}' violates naming convention '{rule_id}'. Expected pattern: {pattern}",
                        file_path=str(file_path.relative_to(self.context.repo_root)),
                    )
                )
        return findings
# src/mind/governance/checks/orphaned_logic.py
"""
A constitutional audit check to find "orphaned logic" - public symbols
that have not been assigned a capability ID in the database.
"""

from __future__ import annotations

import re
from typing import Any

from mind.governance.audit_context import AuditorContext
from shared.config import settings
from shared.models import AuditFinding, AuditSeverity


# ID: bc44f537-758e-49a2-9914-fc6355b51f48
class OrphanedLogicCheck:
    """
    Ensures that all public symbols are assigned a capability, preventing
    undocumented or untracked functionality. This check respects the
    `audit_ignore_policy.yaml` and the new `project_structure.yaml`.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        self.symbols = self.context.symbols_map

        # CORRECTED: Load from the new thematic governance_framework policy (if needed, but not here)
        # For audit_ignore_policy, it was a standalone file. Let's assume it's still loaded
        # into the main context correctly for now. If not, we'll fix it.
        # This assumes your AuditorContext change correctly loads audit_ignore_policy.
        # If not, the line would be:
        # governance_policy = self.context.policies.get("governance_framework", {})
        # This part seems to have a dependency on a file not merged yet, let's keep it simple.
        ignore_policy = self.context.policies.get("audit_ignore_policy", {})
        if not ignore_policy:
            # Fallback if the old key is gone, try the new consolidated one
            governance_policy = self.context.policies.get("governance_framework", {})
            # We need to define where ignores live now. Let's assume they are standalone.
            # This part of the code reveals a gap in the new structure! Let's assume
            # audit_ignore_policy remains standalone for now as it's highly dynamic.
            # So we will try to load it directly.
            try:
                ignore_policy = settings.load(
                    "charter.policies.governance.audit_ignore_policy"
                )
            except FileNotFoundError:
                ignore_policy = {}  # Fails gracefully

        self.ignored_symbol_keys = {
            item["key"]
            for item in ignore_policy.get("symbol_ignores", [])
            if "key" in item
        }

        # CORRECTED: Load entry point patterns from project_structure.yaml
        project_structure_policy = self.context.policies.get("project_structure", {})
        if not project_structure_policy:
            project_structure_policy = settings.load("mind.knowledge.project_structure")

        self.entry_point_patterns = project_structure_policy.get(
            "entry_point_patterns", []
        )

    def _is_entry_point(self, symbol_data: dict[str, Any]) -> bool:
        """Checks if a symbol matches any of the defined entry point patterns."""
        for pattern in self.entry_point_patterns:
            match_rules = pattern.get("match", {})
            is_match = True
            for rule_key, rule_value in match_rules.items():
                symbol_value = symbol_data.get(rule_key)

                if rule_key == "type":
                    is_class = symbol_data.get("is_class", False)
                    if (rule_value == "class" and not is_class) or (
                        rule_value == "function" and is_class
                    ):
                        is_match = False
                        break
                elif rule_key == "name_regex":
                    if not re.search(rule_value, symbol_data.get("name", "")):
                        is_match = False
                        break
                elif rule_key == "module_path_contains":
                    if rule_value not in symbol_data.get("file_path", ""):
                        is_match = False
                        break
                elif rule_key == "has_capability_tag":
                    if rule_value and not symbol_data.get("capability"):
                        is_match = False
                        break
                elif rule_key == "is_public_function":
                    if rule_value and symbol_data.get("name", "").startswith("_"):
                        is_match = False
                        break
                elif symbol_value is None:
                    is_match = False
                    break
            if is_match:
                return True
        return False

    # ID: 567318b3-2e45-4383-8af6-9880c3c9576c
    def find_unassigned_public_symbols(self) -> list[dict[str, Any]]:
        """Finds all public symbols with a null capability key that are not ignored."""
        unassigned = []
        for symbol_key, symbol_data in self.symbols.items():
            is_public = symbol_data.get("is_public", False)
            is_unassigned = symbol_data.get("capability") is None
            is_ignored = symbol_key in self.ignored_symbol_keys
            is_entry_point = self._is_entry_point(symbol_data)

            if is_public and is_unassigned and not is_ignored and not is_entry_point:
                symbol_data["key"] = symbol_key
                unassigned.append(symbol_data)
        return unassigned

    # ID: 2ba01327-4559-427f-b0d4-a0737b7937fc
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any orphaned symbols.
        """
        findings = []
        orphaned_symbols = self.find_unassigned_public_symbols()

        for symbol in orphaned_symbols:
            symbol_key = symbol.get("key", "unknown")
            short_name = symbol_key.split("::")[-1]

            findings.append(
                AuditFinding(
                    check_id="linkage.capability.unassigned",
                    severity=AuditSeverity.ERROR,
                    message=f"Public symbol '{short_name}' is not assigned to a capability in the database.",
                    file_path=symbol.get("file_path"),
                    line_number=symbol.get("line_number"),
                    context={"symbol_key": symbol_key},
                )
            )
        return findings
# src/mind/governance/checks/security_checks.py
"""
Scans source code for hardcoded secrets and other security vulnerabilities
based on configurable detection patterns and exclusion rules.
"""

from __future__ import annotations

import ast
import fnmatch
import re
from pathlib import Path

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 80baca41-4809-456b-985b-9bb9a6cebb7b
class SecurityChecks(BaseCheck):
    """Container for security-related constitutional checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        super().__init__(context)
        # CORRECTED: Load from the new consolidated policy files
        data_gov_policy = self.context.policies.get("data_governance", {})
        safety_framework = self.context.policies.get("safety_framework", {})

        self.secrets_rules = data_gov_policy.get("security_rules", [])
        self.safety_rules = safety_framework.get("safety_rules", [])

    # ID: cb2146e9-2abb-4982-ac11-31f118a10707
    def execute(self) -> list[AuditFinding]:
        """Scans source code for hardcoded secrets and other security vulnerabilities."""
        findings = []
        findings.extend(self._check_for_hardcoded_secrets())
        findings.extend(self._check_dangerous_calls())
        findings.extend(self._check_unsafe_imports())
        return findings

    def _get_files_to_scan(self, rule: dict) -> list[Path]:
        """Gets a list of Python files to scan, respecting rule exclusions."""
        exclude_globs = rule.get("scope", {}).get("exclude", [])
        exclude_paths = [exc.get("path") for exc in exclude_globs if exc.get("path")]

        files_to_scan = []
        for file_path in self.context.python_files:
            if not file_path.is_file():
                continue
            rel_path_str = str(file_path.relative_to(self.repo_root))
            if any(fnmatch.fnmatch(rel_path_str, glob) for glob in exclude_paths):
                continue
            files_to_scan.append(file_path)
        return files_to_scan

    def _check_for_hardcoded_secrets(self) -> list[AuditFinding]:
        """Scans for hardcoded secrets."""
        rule = next(
            (
                r
                for r in self.secrets_rules
                if r.get("id") == "secrets.no_hardcoded_secrets"
            ),
            None,
        )
        if not rule:
            return []

        findings = []
        patterns = [
            re.compile(p) for p in rule.get("detection", {}).get("patterns", [])
        ]
        exclude_globs = rule.get("detection", {}).get("exclude", [])

        for file_path in self.context.python_files:
            rel_path_str = str(file_path.relative_to(self.repo_root))
            if any(fnmatch.fnmatch(rel_path_str, glob) for glob in exclude_globs):
                continue

            try:
                content = file_path.read_text(encoding="utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    for pattern in patterns:
                        if pattern.search(line):
                            findings.append(
                                AuditFinding(
                                    check_id="security.secrets.hardcoded",
                                    severity=AuditSeverity.ERROR,
                                    message=f"Potential hardcoded secret found on line {i}.",
                                    file_path=str(
                                        file_path.relative_to(self.repo_root)
                                    ),
                                    line_number=i,
                                )
                            )
            except Exception:
                continue
        return findings

    def _check_dangerous_calls(self) -> list[AuditFinding]:
        """Scans for dangerous function calls based on the safety policy."""
        rule = next(
            (
                r
                for r in self.safety_rules
                if r.get("id") == "safety.no_dangerous_execution"
            ),
            None,
        )
        if not rule:
            return []

        findings = []
        patterns = [
            re.compile(p) for p in rule.get("detection", {}).get("patterns", [])
        ]
        files_to_scan = self._get_files_to_scan(rule)

        for file_path in files_to_scan:
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))
                for node in ast.walk(tree):
                    if isinstance(node, ast.Call):
                        call_str = ast.unparse(node.func)
                        for pattern in patterns:
                            if pattern.search(call_str):
                                findings.append(
                                    AuditFinding(
                                        check_id="security.dangerous.call",
                                        severity=AuditSeverity.ERROR,
                                        message=f"Use of dangerous call pattern: '{call_str}'",
                                        file_path=str(
                                            file_path.relative_to(self.repo_root)
                                        ),
                                        line_number=node.lineno,
                                    )
                                )
            except Exception:
                continue
        return findings

    def _check_unsafe_imports(self) -> list[AuditFinding]:
        """Scans for forbidden imports based on the safety policy."""
        rule = next(
            (r for r in self.safety_rules if r.get("id") == "safety.no_unsafe_imports"),
            None,
        )
        if not rule:
            return []

        findings = []
        forbidden_imports = set(rule.get("detection", {}).get("forbidden", []))
        files_to_scan = self._get_files_to_scan(rule)

        for file_path in files_to_scan:
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            if alias.name in forbidden_imports:
                                findings.append(
                                    AuditFinding(
                                        check_id="security.dangerous.import",
                                        severity=AuditSeverity.ERROR,
                                        message=f"Import of forbidden module: '{alias.name}'",
                                        file_path=str(
                                            file_path.relative_to(self.repo_root)
                                        ),
                                        line_number=node.lineno,
                                    )
                                )
                    elif (
                        isinstance(node, ast.ImportFrom)
                        and node.module in forbidden_imports
                    ):
                        findings.append(
                            AuditFinding(
                                check_id="security.dangerous.import",
                                severity=AuditSeverity.ERROR,
                                message=f"Import from forbidden module: '{node.module}'",
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=node.lineno,
                            )
                        )
            except Exception:
                continue
        return findings
# src/mind/governance/checks/style_checks.py
"""
Auditor checks for code style and convention compliance, as defined in
the consolidated code_standards.yaml.
"""

from __future__ import annotations

import ast

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 791f7cd8-0441-4e2e-ac65-aa8d0ab82ac7
class StyleChecks(BaseCheck):
    """Container for code style and convention constitutional checks."""

    def __init__(self, context):
        super().__init__(context)
        # CORRECTED: Load from the consolidated code_standards policy
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.style_rules = code_standards_policy.get("style_rules", [])

    # ID: 20cebb25-123b-40d9-999f-4d849eba4228
    def execute(self) -> list[AuditFinding]:
        """Verifies that Python modules adhere to documented style conventions."""
        findings = []
        rules = {rule.get("id"): rule for rule in self.style_rules}
        files_to_check = {
            s["file_path"]
            for s in self.context.symbols_list
            if s.get("file_path", "").endswith(".py")
        }
        for file_rel_path in sorted(list(files_to_check)):
            file_abs_path = self.repo_root / file_rel_path
            try:
                source_code = file_abs_path.read_text(encoding="utf-8")
                tree = ast.parse(source_code)
                if "style.docstrings_public_apis" in rules:
                    has_docstring = (
                        tree.body
                        and isinstance(tree.body[0], ast.Expr)
                        and isinstance(tree.body[0].value, ast.Constant)
                    )
                    if not has_docstring:
                        findings.append(
                            AuditFinding(
                                check_id="code.style.missing-module-docstring",
                                severity=AuditSeverity.WARNING,
                                message="Missing required module-level docstring.",
                                file_path=file_rel_path,
                            )
                        )
            except Exception as e:
                findings.append(
                    AuditFinding(
                        check_id="code.parser.error",
                        severity=AuditSeverity.ERROR,
                        message=f"Could not parse file: {e}",
                        file_path=file_rel_path,
                    )
                )
        return findings
