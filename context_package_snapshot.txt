# src/shared/context.py

"""
Defines the CoreContext, a dataclass that holds singleton instances of all major
services, enabling explicit dependency injection throughout the application.
"""

from __future__ import annotations

from collections.abc import Callable
from dataclasses import dataclass, field
from typing import Any


@dataclass
# ID: 9f1dd7c7-1cb2-435d-bd07-b7d436c9459f
class CoreContext:
    """
    A container for shared services, passed explicitly to commands.

    NOTE: Fields are typed as 'Any' to avoid cross-domain imports from here.
    Concrete types are created/wired in the CLI or API composition roots.
    """

    git_service: Any
    cognitive_service: Any
    knowledge_service: Any
    qdrant_service: Any
    auditor_context: Any
    file_handler: Any
    planner_config: Any
    _is_test_mode: bool = False

    # Factory used to create a ContextService instance.
    # This is injected from composition roots to keep shared/context decoupled
    # from services and database layers.
    context_service_factory: Callable[[], Any] | None = field(
        default=None,
        repr=False,
    )

    _context_service: Any = field(default=None, init=False, repr=False)

    @property
    # ID: 11a1768b-d222-40af-99d7-0d45d300e2ba
    def context_service(self) -> Any:
        """
        Get or create ContextService instance.

        Provides constitutional governance for all LLM context via ContextPackages.
        The actual construction is delegated to a factory configured in the
        composition root (CLI/API).
        """
        if self._context_service is None:
            if self.context_service_factory is None:
                raise RuntimeError(
                    "ContextService factory is not configured on CoreContext. "
                    "This should be wired in the composition root (CLI/API).",
                )
            self._context_service = self.context_service_factory()

        return self._context_service
# src/services/context/service.py

"""ContextService - Main orchestrator for context packet lifecycle.

Integrates builder, validator, redactor, cache, and database.
"""

from __future__ import annotations

import logging
from collections.abc import Callable
from contextlib import AbstractAsyncContextManager
from pathlib import Path
from typing import Any

from .builder import ContextBuilder
from .cache import ContextCache
from .database import ContextDatabase
from .providers.ast import ASTProvider
from .providers.db import DBProvider
from .providers.vectors import VectorProvider
from .redactor import ContextRedactor
from .serializers import ContextSerializer
from .validator import ContextValidator

logger = logging.getLogger(__name__)

SessionFactory = Callable[[], AbstractAsyncContextManager]


# ID: 6fee4321-e9f8-4234-b9f0-dbe2c49ec016
class ContextService:
    """Main service for ContextPackage lifecycle management."""

    def __init__(
        self,
        qdrant_client: Any | None = None,
        cognitive_service: Any | None = None,
        config: dict[str, Any] | None = None,
        project_root: str = ".",
        session_factory: SessionFactory | None = None,
    ):
        """Initialize context service with dependencies.

        Args:
            qdrant_client: Qdrant client instance.
            cognitive_service: CognitiveService for embeddings.
            config: Configuration dict.
            project_root: Project root directory.
            session_factory: Callable that returns an async DB session context
                manager. If None, DB persistence and stats are skipped.
        """
        self.config = config or {}
        self.project_root = Path(project_root)
        self.cognitive_service = cognitive_service
        self._session_factory = session_factory

        # Initialize providers without a database session.
        self.db_provider = DBProvider()
        self.vector_provider = VectorProvider(qdrant_client, cognitive_service)
        self.ast_provider = ASTProvider(project_root)

        # Initialize components
        self.builder = ContextBuilder(
            self.db_provider,
            self.vector_provider,
            self.ast_provider,
            self.config,
        )
        self.validator = ContextValidator()
        self.redactor = ContextRedactor()
        self.cache = ContextCache(self.config.get("cache_dir", "work/context_cache"))
        self.database = ContextDatabase()

    # ID: 498ac646-47e9-4e86-83b0-e25923ff9ef5
    async def build_for_task(
        self,
        task_spec: dict[str, Any],
        use_cache: bool = True,
    ) -> dict[str, Any]:
        """Build complete context packet for task.

        Full pipeline:
        1. Check cache
        2. Build from providers
        3. Validate
        4. Redact
        5. Compute hashes
        6. Persist to disk & DB
        7. Cache result

        Args:
            task_spec: Task specification
            use_cache: Whether to use cached packets

        Returns:
            Complete, validated, redacted ContextPackage
        """
        logger.info("Building context for task %s", task_spec.get("task_id"))

        if use_cache:
            cache_key = ContextSerializer.compute_cache_key(task_spec)
            cached = self.cache.get(cache_key)
            if cached:
                logger.info("Using cached packet")
                return cached

        packet = await self.builder.build_for_task(task_spec)

        is_valid, errors = self.validator.validate(packet)
        if not is_valid:
            error_msg = f"Validation failed: {errors}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        packet = self.redactor.redact(packet)

        packet["provenance"]["packet_hash"] = ContextSerializer.compute_packet_hash(
            packet
        )
        packet["provenance"]["cache_key"] = ContextSerializer.compute_cache_key(
            task_spec
        )

        task_id = task_spec["task_id"]
        output_dir = self.project_root / "work" / "context_packets" / task_id
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / "context.yaml"

        ContextSerializer.to_yaml(packet, str(output_path))
        file_size = output_path.stat().st_size

        # Persist metadata to DB if a session factory is available.
        if self._session_factory is not None:
            async with self._session_factory() as db:
                self.database.db = db
                await self.database.save_packet_metadata(
                    packet,
                    str(output_path),
                    file_size,
                )
        else:
            logger.debug(
                "No session_factory configured; skipping DB persistence "
                "for packet %s",
                packet["header"]["packet_id"],
            )

        if use_cache:
            cache_key = packet["provenance"]["cache_key"]
            self.cache.put(cache_key, packet)

        logger.info("Built and persisted packet %s", packet["header"]["packet_id"])
        return packet

    # ID: 1548660f-ebc3-41b0-9427-83f527dbf9b9
    async def load_packet(self, task_id: str) -> dict[str, Any] | None:
        """Load packet from disk by task ID.

        Args:
            task_id: Task identifier

        Returns:
            ContextPackage dict or None if not found
        """
        packet_path = (
            self.project_root / "work" / "context_packets" / task_id / "context.yaml"
        )

        if not packet_path.exists():
            logger.warning("Packet not found for task %s", task_id)
            return None

        return ContextSerializer.from_yaml(str(packet_path))

    # ID: 7eb62236-0835-4856-9ac1-1c421f526535
    def validate_packet(self, packet: dict[str, Any]) -> tuple[bool, list[str]]:
        """Validate a packet against schema.

        Args:
            packet: ContextPackage dict

        Returns:
            Tuple of (is_valid, errors)
        """
        return self.validator.validate(packet)

    # ID: d95ad2a7-1376-4b70-a799-3ce6e33e508c
    async def get_task_packets(self, task_id: str) -> list[dict[str, Any]]:
        """Get all packets for a task from database.

        Args:
            task_id: Task identifier

        Returns:
            List of packet metadata dicts
        """
        if self._session_factory is None:
            logger.warning(
                "No session_factory configured; cannot load task packets for %s",
                task_id,
            )
            return []

        async with self._session_factory() as db:
            self.database.db = db
            return await self.database.get_packets_for_task(task_id)

    # ID: ab7a9ff9-c733-4867-8d4a-fac12672096d
    async def get_stats(self) -> dict[str, Any]:
        """Get service statistics.

        Returns:
            Statistics dict
        """
        if self._session_factory is None:
            logger.warning(
                "No session_factory configured; returning empty stats "
                "because no session_factory is configured.",
            )
            return {}

        async with self._session_factory() as db:
            self.database.db = db
            return await self.database.get_stats()

    # ID: 0a767d59-acbc-4c3c-a372-4ef9bf991d2c
    def clear_cache(self) -> int:
        """Clear all cached packets.

        Returns:
            Number of entries removed
        """
        return self.cache.clear_all()
# src/services/context/builder.py

"""ContextBuilder - Assembles governed context packets."""

from __future__ import annotations

import ast
import json
import logging
import uuid
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

import yaml

from .serializers import ContextSerializer

logger = logging.getLogger(__name__)


# --- START OF FINAL FIX: Correctly parse ALL symbols, including methods ---
def _parse_python_file(filepath: str) -> list[dict]:
    """
    Parses a Python file and extracts metadata for ALL functions and classes,
    including methods nested within classes.
    """
    try:
        with open(filepath, encoding="utf-8") as f:
            source = f.read()
        tree = ast.parse(source, filename=filepath)

        symbols = []
        # The fix is to use ast.walk(), which traverses the entire tree,
        # instead of iterating over tree.body, which only contains top-level nodes.
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                # We can add further filtering here if needed, but for now,
                # we want to find ALL symbols to ensure the context is complete.
                signature = ast.get_source_segment(source, node).split("\n")[0]
                lines = source.splitlines()
                end_lineno = getattr(node, "end_lineno", node.lineno)
                code = "\n".join(lines[node.lineno - 1 : end_lineno])
                docstring = ast.get_docstring(node) or ""

                symbols.append(
                    {
                        "name": node.name,
                        "signature": signature,
                        "code": code,
                        "docstring": docstring,
                    }
                )
        return symbols
    except Exception as e:
        logger.error(f"Failed to parse {filepath}: {e}")
        return []


# --- END OF FINAL FIX ---


# ID: 67d2b587-1115-41a1-8bfd-6911901a9f32
class ContextBuilder:
    def __init__(self, db_provider, vector_provider, ast_provider, config):
        self.db = db_provider
        self.vectors = vector_provider
        self.ast = ast_provider
        self.config = config or {}
        self.version = "0.2.0"
        self.policy = self._load_policy()
        self._knowledge_graph = self._load_knowledge_graph()

    def _load_policy(self) -> dict[str, Any]:
        policy_path = Path(".intent/context/policy.yaml")
        if policy_path.exists():
            with open(policy_path, encoding="utf-8") as f:
                return yaml.safe_load(f)
        return {}

    def _load_knowledge_graph(self) -> dict[str, Any]:
        """Loads the knowledge graph from its canonical JSON file."""
        kg_path = Path("knowledge_graph.json")
        if not kg_path.exists():
            kg_path = Path("reports/knowledge_graph.json")
            if not kg_path.exists():
                logger.warning(
                    "knowledge_graph.json not found. Graph traversal will be disabled."
                )
                return {"symbols": {}}
        try:
            with open(kg_path, encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, OSError) as e:
            logger.error(f"Failed to load knowledge_graph.json: {e}")
            return {"symbols": {}}

    # ID: 6565818e-0b0e-4aff-a9b8-069289c7f9a8
    async def build_for_task(self, task_spec: dict[str, Any]) -> dict[str, Any]:
        start_time = datetime.now(UTC)
        logger.info(f"Building context for task {task_spec.get('task_id')}")

        packet_id = str(uuid.uuid4())
        created_at = start_time.isoformat()

        scope_spec = task_spec.get("scope", {})
        packet = {
            "header": {
                "packet_id": packet_id,
                "task_id": task_spec["task_id"],
                "task_type": task_spec["task_type"],
                "created_at": created_at,
                "builder_version": self.version,
                "privacy": task_spec.get("privacy", "local_only"),
            },
            "problem": {
                "summary": task_spec.get("summary", ""),
                "intent_ref": task_spec.get("intent_ref"),
                "acceptance": task_spec.get("acceptance", []),
            },
            "scope": {
                "include": scope_spec.get("include", []),
                "exclude": scope_spec.get("exclude", []),
                "globs": scope_spec.get("globs", []),
                "roots": scope_spec.get("roots", []),
                "traversal_depth": scope_spec.get("traversal_depth", 0),
            },
            "constraints": self._build_constraints(task_spec),
            "context": [],
            "invariants": self._default_invariants(),
            "policy": {"redactions_applied": [], "remote_allowed": False, "notes": ""},
            "provenance": {
                "inputs": {},
                "build_stats": {},
                "cache_key": "",
                "packet_hash": "",
            },
        }

        context_items = await self._collect_context(packet, task_spec)
        context_items = self._apply_constraints(context_items, packet["constraints"])

        for item in context_items:
            item["tokens_est"] = self._estimate_item_tokens(item)

        packet["context"] = context_items

        duration_ms = int((datetime.now(UTC) - start_time).total_seconds() * 1000)
        packet["provenance"]["build_stats"] = {
            "duration_ms": duration_ms,
            "items_collected": len(context_items),
            "items_filtered": 0,
            "tokens_total": sum(item.get("tokens_est", 0) for item in context_items),
        }
        packet["provenance"]["cache_key"] = ContextSerializer.compute_cache_key(
            task_spec
        )

        logger.info(
            f"Built packet {packet_id} with {len(context_items)} items in {duration_ms}ms"
        )
        return packet

    async def _collect_context(self, packet: dict, task_spec: dict) -> list[dict]:
        items = []
        scope = packet["scope"]
        max_items = packet["constraints"]["max_items"]

        if self.db:
            seed_items = await self.db.get_symbols_for_scope(scope, max_items // 2)
            items.extend(seed_items)

        if self.vectors and task_spec.get("summary"):
            vec_items = await self.vectors.search_similar(
                task_spec["summary"], top_k=max_items // 3
            )
            items.extend(vec_items)

        traversal_depth = scope.get("traversal_depth", 0)
        if traversal_depth > 0 and self._knowledge_graph.get("symbols") and items:
            logger.info(f"Traversing knowledge graph to depth {traversal_depth}.")
            related_items = self._traverse_graph(
                list(items), traversal_depth, max_items - len(items)
            )
            items.extend(related_items)

        items = await self._force_add_code_item(items, task_spec)

        seen_keys = set()
        unique_items = []
        for item in items:
            key = (item.get("name"), item.get("path"), item.get("item_type"))
            if key not in seen_keys:
                seen_keys.add(key)
                unique_items.append(item)

        return unique_items

    def _traverse_graph(
        self, seed_items: list[dict], depth: int, limit: int
    ) -> list[dict]:
        if not self._knowledge_graph.get("symbols"):
            return []

        all_symbols = self._knowledge_graph["symbols"]
        related_symbol_keys = set()

        queue = {
            item.get("metadata", {}).get("symbol_path")
            for item in seed_items
            if item.get("metadata", {}).get("symbol_path")
        }

        for _ in range(depth):
            if not queue or len(related_symbol_keys) >= limit:
                break

            next_queue = set()
            for symbol_key in queue:
                symbol_data = all_symbols.get(symbol_key)
                if symbol_data:
                    for callee_name in symbol_data.get("calls", []):
                        if callee_name not in related_symbol_keys:
                            related_symbol_keys.add(callee_name)
                            next_queue.add(callee_name)

                for caller_key, caller_data in all_symbols.items():
                    if symbol_key and symbol_key.split("::")[-1] in caller_data.get(
                        "calls", []
                    ):
                        if caller_key not in related_symbol_keys:
                            related_symbol_keys.add(caller_key)
                            next_queue.add(caller_key)
            queue = next_queue

        related_items = []
        for key in list(related_symbol_keys)[:limit]:
            symbol_data = all_symbols.get(key) or self._find_symbol_by_qualname(key)
            if symbol_data:
                related_items.append(self._format_symbol_as_context_item(symbol_data))

        logger.info(f"Found {len(related_items)} related symbols via graph traversal.")
        return related_items

    def _find_symbol_by_qualname(self, qualname: str) -> dict | None:
        """Finds a symbol in the knowledge graph by its qualified name."""
        for symbol in self._knowledge_graph.get("symbols", {}).values():
            if symbol.get("qualname") == qualname:
                return symbol
        return None

    def _format_symbol_as_context_item(self, symbol_data: dict) -> dict:
        """Formats a symbol dictionary from the knowledge graph into a context item."""
        return {
            "name": symbol_data.get("qualname"),
            "path": symbol_data.get("file_path"),
            "item_type": "symbol",
            "signature": str(symbol_data.get("parameters", [])),
            "summary": symbol_data.get("intent") or symbol_data.get("docstring"),
            "source": "db_graph_traversal",
        }

    async def _force_add_code_item(self, items: list, task_spec: dict) -> list:
        target_file_str = task_spec.get("target_file")
        target_symbol = task_spec.get("target_symbol")
        if not target_file_str or not target_symbol:
            return items

        if any(
            i.get("item_type") == "code" and i.get("name") == target_symbol
            for i in items
        ):
            return items

        full_path = Path.cwd() / target_file_str
        if not full_path.exists():
            logger.warning(f"File not found: {full_path}")
            return items

        logger.info(f"FORCE-ADDING CODE ITEM for '{target_symbol}'")

        symbols = _parse_python_file(str(full_path))
        for sym in symbols:
            if sym["name"] == target_symbol:
                item = {
                    "name": sym["name"],
                    "path": target_file_str,
                    "item_type": "code",
                    "content": sym["code"],
                    "summary": sym["docstring"][:200],
                    "source": "builtin_ast",
                    "signature": sym.get("signature", ""),
                }
                items.append(item)
                logger.info(f"Added CODE item with content: {target_symbol}")
                break
        return items

    def _apply_constraints(self, items: list, constraints: dict) -> list:
        max_items = constraints.get("max_items", 50)
        max_tokens = constraints.get("max_tokens", 100000)
        if len(items) > max_items:
            items = items[:max_items]
        total = 0
        filtered = []
        for item in items:
            tok = self._estimate_item_tokens(item)
            if total + tok > max_tokens:
                break
            filtered.append(item)
            total += tok
        return filtered

    def _estimate_item_tokens(self, item: dict) -> int:
        text = " ".join([item.get("content", ""), item.get("summary", "")])
        return ContextSerializer.estimate_tokens(text)

    def _build_constraints(self, task_spec: dict) -> dict:
        constraints = task_spec.get("constraints", {})
        return {
            "max_tokens": constraints.get("max_tokens", 100000),
            "max_items": constraints.get("max_items", 50),
            "forbidden_paths": [],
            "forbidden_calls": [],
        }

    def _default_invariants(self) -> list[str]:
        return [
            "All symbols must have signatures",
            "No filesystem operations in snippets",
            "No network calls in snippets",
            "All paths must be relative",
        ]
