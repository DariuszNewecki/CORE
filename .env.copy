# .env
# This file is the single source of truth for runtime secrets and configuration.
# It is constitutionally governed by .intent/mind/config/runtime_requirements.yaml.
# After editing, run `poetry run core-admin manage dotenv sync --write` to apply changes.

# ======================================================================
# 1. CORE SYSTEM & RUNTIME CONFIGURATION
# ======================================================================
CORE_ENV="development"
REPO_PATH="/opt/dev/CORE"
LLM_ENABLED="true"
LOG_LEVEL="INFO"
CORE_ACTION_LOG_PATH="logs/action_log.jsonl"
KEY_STORAGE_DIR=".intent/keys"


# ======================================================================
# 2. GLOBAL CONCURRENCY FALLBACKS
# These are safe, conservative defaults for any resource that is not
# specifically tuned below.
# ======================================================================
# Default max time in seconds for any LLM request to complete.
LLM_REQUEST_TIMEOUT=300

# Default number of parallel requests for any un-tuned resource.
CORE_MAX_CONCURRENT_REQUESTS=2

# Default delay between requests for any un-tuned resource.
LLM_SECONDS_BETWEEN_REQUESTS=2


# ======================================================================
# 3. DATABASE & VECTOR STORE CONFIGURATION
# ======================================================================
DATABASE_URL="postgresql+asyncpg://core:core@192.168.20.23:5432/core"
QDRANT_URL="http://192.168.20.22:6333"
QDRANT_COLLECTION_NAME="core_capabilities"


# ======================================================================
# 4. LLM RESOURCE CONFIGURATION (WITH PER-RESOURCE TUNING)
# ======================================================================

# --- Resource: ollama_local ---
OLLAMA_LOCAL_API_URL="http://192.168.20.100:11434"
OLLAMA_LOCAL_API_KEY=""
OLLAMA_LOCAL_MODEL_NAME="llama3.2:1b"
# Tuning for local GPU: 1 job at a time, no delay.
OLLAMA_LOCAL_MAX_CONCURRENT_REQUESTS=1
OLLAMA_LOCAL_SECONDS_BETWEEN_REQUESTS=0

# --- Resource: deepseek_chat ---
DEEPSEEK_CHAT_API_URL="https://api.deepseek.com"
DEEPSEEK_CHAT_API_KEY="sk-7f916bd5fcd04bea9baa4cf8dfd6e97e"
DEEPSEEK_CHAT_MODEL_NAME="deepseek-chat"
# Tuning for cloud API: few jobs, polite delay.
DEEPSEEK_CHAT_MAX_CONCURRENT_REQUESTS=2
DEEPSEEK_CHAT_SECONDS_BETWEEN_REQUESTS=1

# --- Resource: deepseek_coder ---
DEEPSEEK_CODER_API_URL="https://api.deepseek.com"
DEEPSEEK_CODER_API_KEY="sk-7f916bd5fcd04bea9baa4cf8dfd6e97e"
DEEPSEEK_CODER_MODEL_NAME="deepseek-coder"
# Tuning for cloud API: few jobs, polite delay.
DEEPSEEK_CODER_MAX_CONCURRENT_REQUESTS=3
DEEPSEEK_CODER_SECONDS_BETWEEN_REQUESTS=1

# --- Resource: anthropic_claude_sonnet ---
ANTHROPIC_CLAUDE_SONNET_API_URL="https://api.anthropic.com/"
ANTHROPIC_CLAUDE_SONNET_API_KEY="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"
ANTHROPIC_CLAUDE_SONNET_MODEL_NAME="claude-sonnet-4-20250514"
# Tuning for cloud API: few jobs, polite delay.
ANTHROPIC_CLAUDE_SONNET_MAX_CONCURRENT_REQUESTS=2
ANTHROPIC_CLAUDE_SONNET_SECONDS_BETWEEN_REQUESTS=1


# ======================================================================
# 5. EMBEDDING RESOURCE CONFIGURATION
# ======================================================================
LOCAL_EMBEDDING_API_URL="http://192.168.20.100:11434"
LOCAL_EMBEDDING_API_KEY=""
LOCAL_EMBEDDING_MODEL_NAME="nomic-embed-text"
LOCAL_EMBEDDING_DIM=768
EMBED_MODEL_REVISION="2025-09-15"
# Tuning for local embedding: slightly more parallel jobs are okay.
LOCAL_EMBEDDING_MAX_CONCURRENT_REQUESTS=2
