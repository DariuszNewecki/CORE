# Generated by CORE AccumulativeTestService
# Source: src/shared/legacy_models.py
# Symbols: 1

from unittest.mock import Mock, patch

import pytest

from shared.legacy_models import LegacyLlmResource


def test_LegacyLlmResource():
    """Test basic functionality, edge cases, and correctness of LegacyLlmResource."""

    # Test basic instantiation with required fields
    resource = LegacyLlmResource(
        name="test_model",
        env_prefix="TEST_MODEL"
    )
    assert resource.name == "test_model"
    assert resource.env_prefix == "TEST_MODEL"
    assert resource.provided_capabilities == []
    assert resource.performance_metadata is None

    # Test instantiation with all fields
    resource = LegacyLlmResource(
        name="another_model",
        provided_capabilities=["text_generation", "embeddings"],
        env_prefix="ANOTHER_MODEL",
        performance_metadata={"tokens_per_second": 100, "latency_ms": 50}
    )
    assert resource.name == "another_model"
    assert resource.provided_capabilities == ["text_generation", "embeddings"]
    assert resource.env_prefix == "ANOTHER_MODEL"
    assert resource.performance_metadata == {"tokens_per_second": 100, "latency_ms": 50}

    # Test that provided_capabilities defaults to empty list
    resource = LegacyLlmResource(name="default_caps", env_prefix="DEFAULT_CAPS")
    assert resource.provided_capabilities == []

    # Test that performance_metadata defaults to None
    resource = LegacyLlmResource(name="no_perf", env_prefix="NO_PERF")
    assert resource.performance_metadata is None

    # Test with empty provided_capabilities list
    resource = LegacyLlmResource(
        name="empty_caps",
        provided_capabilities=[],
        env_prefix="EMPTY_CAPS"
    )
    assert resource.provided_capabilities == []

    # Test with complex performance_metadata
    complex_metadata = {
        "metrics": {"accuracy": 0.95, "throughput": 1000},
        "config": {"batch_size": 32, "precision": "float16"},
        "timestamps": ["2024-01-01T00:00:00"]
    }
    resource = LegacyLlmResource(
        name="complex_metadata",
        env_prefix="COMPLEX_METADATA",
        performance_metadata=complex_metadata
    )
    assert resource.performance_metadata == complex_metadata

    # Test that name and env_prefix are required
    with pytest.raises(Exception):
        LegacyLlmResource()  # Missing required fields

    # Test type validation - name must be string
    with pytest.raises(Exception):
        LegacyLlmResource(name=123, env_prefix="TEST")

    # Test type validation - env_prefix must be string
    with pytest.raises(Exception):
        LegacyLlmResource(name="test", env_prefix=456)

    # Test type validation - provided_capabilities must be list of strings
    with pytest.raises(Exception):
        LegacyLlmResource(
            name="test",
            env_prefix="TEST",
            provided_capabilities=["valid", 123]  # Mixed types
        )

    # Test that performance_metadata can be empty dict
    resource = LegacyLlmResource(
        name="empty_metadata",
        env_prefix="EMPTY_METADATA",
        performance_metadata={}
    )
    assert resource.performance_metadata == {}

    # Test equality comparison
    resource1 = LegacyLlmResource(
        name="model1",
        env_prefix="MODEL1",
        provided_capabilities=["cap1", "cap2"]
    )
    resource2 = LegacyLlmResource(
        name="model1",
        env_prefix="MODEL1",
        provided_capabilities=["cap1", "cap2"]
    )
    resource3 = LegacyLlmResource(
        name="model2",
        env_prefix="MODEL2",
        provided_capabilities=["cap1"]
    )
    assert resource1 == resource2
    assert resource1 != resource3

    # Test string representation
    resource = LegacyLlmResource(name="repr_test", env_prefix="REPR_TEST")
    assert "repr_test" in str(resource)
    assert "LegacyLlmResource" in repr(resource)
