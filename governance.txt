# src/mind/governance/audit_context.py

"""
AuditorContext: central view of constitutional artifacts and the knowledge graph
for governance checks and audits.
"""

from __future__ import annotations

from pathlib import Path
from services.knowledge.knowledge_service import KnowledgeService
from shared.config import settings
from shared.logger import getLogger
from shared.models import AuditFinding
from typing import Any
import os


logger = getLogger(__name__)


# ID: 2dc8a2b7-b3f7-4050-bb95-8e3f1648d419
class AuditorContext:
    """
    Provides access to '.intent' artifacts and the in-memory knowledge graph.
    This version is constitutionally-aware and loads policies via meta.yaml.
    """

    def __init__(self, repo_path: Path):
        self.repo_path = Path(repo_path).resolve()
        self.intent_path = self.repo_path / ".intent"
        self.mind_path = self.intent_path / "mind"
        self.charter_path = self.intent_path / "charter"
        self.src_dir = self.repo_path / "src"
        self.last_findings: list[AuditFinding] = []
        self.meta: dict[str, Any] = settings._meta_config
        self.policies: dict[str, Any] = self._load_policies()
        self.source_structure: dict[str, Any] = settings.load(
            "mind.knowledge.project_structure"
        )
        self.knowledge_graph: dict[str, Any] = {"symbols": {}}
        self.symbols_list: list = []
        self.symbols_map: dict = {}
        logger.debug("AuditorContext initialized.")

    # ID: b6970345-7493-4c25-abe6-0fdaf3143e14
    async def load_knowledge_graph(self) -> None:
        """Load the knowledge graph from the service (async)."""
        service = KnowledgeService(self.repo_path)
        self.knowledge_graph = await service.get_graph()
        self.symbols_map = self.knowledge_graph.get("symbols", {})
        self.symbols_list = list(self.symbols_map.values())
        logger.info(f"Loaded knowledge graph with {len(self.symbols_list)} symbols.")

    def _load_policies(self) -> dict[str, Any]:
        """
        Loads all policy files as defined in the meta.yaml index.
        This is the new, constitutionally-aware method.
        """
        policies_to_load = settings._meta_config.get("charter", {}).get("policies", {})
        if not policies_to_load:
            logger.warning("No policies are defined in meta.yaml.")
            return {}
        loaded_policies: dict[str, Any] = {}
        for logical_name, file_rel_path in policies_to_load.items():
            try:
                logical_path = f"charter.policies.{logical_name}"
                loaded_policies[logical_name] = settings.load(logical_path)
            except (FileNotFoundError, OSError, ValueError) as e:
                logger.error(
                    f"Failed to load constitutionally-defined policy '{logical_name}': {e}"
                )
                loaded_policies[logical_name] = {}
        return loaded_policies

    @property
    # ID: 38c486bf-9050-4814-b665-188118e16114
    def python_files(self) -> list[Path]:
        paths: list[Path] = []
        for root, dirs, files in os.walk(self.repo_path):
            dirs[:] = [
                d
                for d in dirs
                if d
                not in {".git", "__pycache__", ".pytest_cache", ".venv", "node_modules"}
            ]
            for name in files:
                if name.endswith(".py"):
                    paths.append(Path(root) / name)
        return paths


__all__ = ["AuditorContext"]
# src/mind/governance/auditor.py
"""
Constitutional Auditor — The primary orchestration engine for all governance checks.
"""

from __future__ import annotations

import asyncio
import importlib
import inspect
import json
import pkgutil
from collections.abc import MutableMapping
from typing import Any

from mind.governance import checks
from mind.governance.audit_context import AuditorContext
from mind.governance.audit_postprocessor import (
    EntryPointAllowList,
    apply_entry_point_downgrade_and_report,
)
from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding
from shared.path_utils import get_repo_root

# --- Configuration for the Auditor ---
REPORTS_DIR = get_repo_root() / "reports"
FINDINGS_FILENAME = "audit_findings.json"
PROCESSED_FINDINGS_FILENAME = "audit_findings.processed.json"
SYMBOL_INDEX_FILENAME = "symbol_index.json"
DOWNGRADE_SEVERITY_TO = "info"
DEAD_SYMBOL_RULE_IDS = {"linkage.capability.unassigned"}


# ID: 420dc6e1-2b67-476f-aa6a-9cddd839304c
class ConstitutionalAuditor:
    """
    Orchestrates the constitutional audit by discovering and running all checks.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    def _discover_checks(self) -> list[type[BaseCheck]]:
        """Dynamically discovers all BaseCheck subclasses in the checks package."""
        check_classes = []
        for _, name, _ in pkgutil.iter_modules(checks.__path__):
            module = importlib.import_module(f"mind.governance.checks.{name}")
            for item_name, item in inspect.getmembers(module, inspect.isclass):
                if (
                    issubclass(item, BaseCheck)
                    and item is not BaseCheck
                    and not inspect.isabstract(item)
                ):
                    check_classes.append(item)
        return check_classes

    async def _run_all_checks(self) -> tuple[list[AuditFinding], int]:
        """Instantiates and runs all discovered checks, collecting their findings."""
        all_findings: list[AuditFinding] = []
        check_classes = self._discover_checks()

        for check_class in check_classes:
            # === START OF FIX ===
            # Inject dependencies based on the check's specific needs.
            if check_class.__name__ == "DuplicationCheck":
                # The DuplicationCheck has a special dependency on QdrantService.
                # We get it from the context and pass it in during instantiation.
                if not hasattr(self.context, "qdrant_service"):
                    # This is an internal error state, but we handle it gracefully.
                    all_findings.append(
                        AuditFinding(
                            check_id="auditor.internal.error",
                            severity="error",
                            message="AuditorContext is missing qdrant_service. Cannot run DuplicationCheck.",
                        )
                    )
                    continue
                check_instance = check_class(self.context, self.context.qdrant_service)
            else:
                # Standard checks only need the context.
                check_instance = check_class(self.context)
            # === END OF FIX ===

            if inspect.iscoroutinefunction(check_instance.execute):
                findings = await check_instance.execute()
            else:
                findings = await asyncio.to_thread(check_instance.execute)
            all_findings.extend(findings)

        unassigned_count = len(
            [f for f in all_findings if f.check_id == "linkage.capability.unassigned"]
        )

        return all_findings, unassigned_count

    # ID: 0c34d8c4-1530-4095-be43-bec35f36d538
    async def run_full_audit_async(self) -> list[MutableMapping[str, Any]]:
        """
        The main entry point for running a full, orchestrated constitutional audit.
        """
        await self.context.load_knowledge_graph()
        raw_findings_objects, unassigned_count = await self._run_all_checks()
        raw_findings = [f.as_dict() for f in raw_findings_objects]

        symbol_index = {
            key: {
                "entry_point_type": data.get("entry_point_type"),
                "pattern_name": data.get("pattern_name"),
                "entry_point_justification": data.get("entry_point_justification"),
            }
            for key, data in self.context.symbols_map.items()
        }

        (REPORTS_DIR / FINDINGS_FILENAME).write_text(json.dumps(raw_findings, indent=2))
        (REPORTS_DIR / SYMBOL_INDEX_FILENAME).write_text(
            json.dumps(symbol_index, indent=2)
        )

        processed_findings = apply_entry_point_downgrade_and_report(
            findings=raw_findings,
            symbol_index=symbol_index,
            reports_dir=REPORTS_DIR,
            allow_list=EntryPointAllowList.default(),
            dead_rule_ids=DEAD_SYMBOL_RULE_IDS,
            downgrade_to=DOWNGRADE_SEVERITY_TO,
            write_reports=True,
        )

        (REPORTS_DIR / PROCESSED_FINDINGS_FILENAME).write_text(
            json.dumps(processed_findings, indent=2)
        )

        return processed_findings


# === START OF FIX: REMOVE REDUNDANT AND VIOLATING HELPER FUNCTIONS ===
# The functions below created new instances of ConstitutionalAuditor, which
# violated the DI policy. The correct pattern is to create the instance
# in the CLI layer (which is already being done) and call its methods.
# These functions are no longer needed.
# === END OF FIX ===
# src/mind/governance/audit_postprocessor.py
"""
Post-processing utilities for Constitutional Auditor findings.

This module provides:
  1) Severity downgrade for "dead-public-symbol" findings when the symbol
     has an allowed `entry_point_type` (as declared in
     .intent/mind/knowledge/entry_point_patterns.yaml).
  2) Auto-generated reports of all symbols auto-ignored-by-pattern to keep
     human visibility without polluting audit_ignore_policy.yaml.

Usage (programmatic):
    from src.mind.governance.audit_postprocessor import (
        EntryPointAllowList,
        apply_entry_point_downgrade_and_report,
    )

    processed_findings = apply_entry_point_downgrade_and_report(
        findings=raw_findings,
        symbol_index=symbol_index,  # dict[str, dict] with entry_point_type, etc.
        reports_dir="reports",
        allow_list=EntryPointAllowList.default(),
        dead_rule_ids={"dead_public_symbol", "dead-public-symbol"},
        downgrade_to="info",  # or "warn"
        write_reports=True,
    )

Usage (CLI; optional):
    python -m src.features.governance.audit_postprocessor \
        --in findings.json --symbols symbols.json --out findings.processed.json --reports reports

Expectations:
  - `findings` is a list[dict] with keys like:
        rule_id: str
        severity: str  ("error"|"warn"|"info")
        symbol_key: str  (e.g., "src/foo.py::Foo.bar")
        message: str
        [any other fields are preserved]

  - `symbol_index` is a dict[str, dict] mapping symbol_key -> metadata dict, e.g.:
        {
          "entry_point_type": "cli_wrapper" | "data_model" | ...,
          "entry_point_justification": "matched pattern X",
          "pattern_name": "cli_wrapper_function",
          ...
        }

  - Caller persists the returned list if using programmatic API.

Notes:
  - This is intentionally narrow-scoped and duck-typed to avoid coupling
    to specific internal Finding/Symbol classes.
"""

from __future__ import annotations

import argparse
import json
import sys
from collections.abc import Iterable, Mapping, MutableMapping, Sequence
from datetime import UTC, datetime
from pathlib import Path


# ID: 34bd4ecc-62ce-4d54-b72b-bfd2b14324ed
class EntryPointAllowList:
    """
    Allow-list of entry_point_type values for which we downgrade "dead-public-symbol"
    findings. This mirrors the generalized patterns you codified in
    `.intent/mind/knowledge/entry_point_patterns.yaml`.

    You can extend/override via constructor or by using .default() and modifying the set.
    """

    def __init__(self, allowed_types: Iterable[str]) -> None:
        self.allowed = {t.strip() for t in allowed_types if t and t.strip()}

    @classmethod
    # ID: f789f14f-26bc-4cc4-b889-17c55c6c5f77
    def default(cls) -> EntryPointAllowList:
        return cls(
            allowed_types=[
                # Structural/data constructs
                "data_model",
                "enum",
                "magic_method",
                "visitor_method",
                "base_class",
                "boilerplate_method",
                # CLI & wrappers
                "cli_command",
                "cli_wrapper",
                "registry_accessor",
                # Orchestration/factories
                "orchestrator",
                "factory",
                # Providers/adapters/clients
                "provider_method",
                "client_surface",
                "client_adapter",
                "io_handler",
                "git_adapter",
                "utility_function",
                # Knowledge & governance pipelines
                "knowledge_core",
                "governance_check",
                "auditor_pipeline",
                # Capabilities
                "capability",
            ]
        )

    def __contains__(self, entry_point_type: str | None) -> bool:
        return bool(entry_point_type) and entry_point_type in self.allowed


def _now_iso() -> str:
    return datetime.now(UTC).strftime("%Y-%m-%dT%H:%M:%SZ")


def _safe_symbol_meta(
    symbol_index: Mapping[str, Mapping[str, object]], symbol_key: str
) -> Mapping[str, object]:
    return symbol_index.get(symbol_key, {}) or {}


# ID: b96e63c3-67b3-44b2-a19a-197368a8aba0
def apply_entry_point_downgrade_and_report(
    *,
    findings: Sequence[MutableMapping[str, object]],
    symbol_index: Mapping[str, Mapping[str, object]],
    reports_dir: str | Path = "reports",
    allow_list: EntryPointAllowList | None = None,
    dead_rule_ids: Iterable[str] = ("dead_public_symbol", "dead-public-symbol"),
    downgrade_to: str = "info",  # could be "warn" if you want a gentle nudge
    write_reports: bool = True,
) -> list[MutableMapping[str, object]]:
    """
    Process a list of findings and:
      - Downgrade severity for dead-public-symbol findings whose symbol entry_point_type
        is allowed by policy.
      - Generate a report listing all auto-ignored symbols (grouped by pattern/type).

    Returns a new list of findings (mutating the original items in place).
    """
    allow = allow_list or EntryPointAllowList.default()
    dead_ids = {r.strip() for r in dead_rule_ids if r and r.strip()}
    processed: list[MutableMapping[str, object]] = []
    auto_ignored: list[dict[str, object]] = []

    for f in findings:
        # Duck-typed access:
        rule_id = str(f.get("rule_id", "") or "")
        symbol_key = str(f.get("symbol_key", "") or "")
        severity = str(f.get("severity", "") or "").lower()

        if rule_id in dead_ids and symbol_key:
            meta = _safe_symbol_meta(symbol_index, symbol_key)
            ep_type = str(meta.get("entry_point_type", "") or "")
            pattern_name = str(meta.get("pattern_name", "") or "")
            justification = str(meta.get("entry_point_justification", "") or "")

            if ep_type in allow:
                # Downgrade severity (only if current is higher)
                if severity in {"error", "warn"}:
                    f["severity"] = downgrade_to
                # Track for auto-ignored report
                auto_ignored.append(
                    {
                        "symbol_key": symbol_key,
                        "entry_point_type": ep_type,
                        "pattern_name": pattern_name or None,
                        "justification": justification or None,
                        "original_rule_id": rule_id,
                        "downgraded_to": f["severity"],
                    }
                )

        processed.append(f)

    if write_reports:
        _write_reports(reports_dir, auto_ignored)

    return processed


def _write_reports(
    reports_dir: str | Path, auto_ignored: Sequence[Mapping[str, object]]
) -> None:
    """
    Emit both JSON and Markdown summaries of auto-ignored-by-pattern symbols.
    These are ephemeral audit artifacts (not part of the Constitution).
    """
    reports_path = Path(reports_dir)
    reports_path.mkdir(parents=True, exist_ok=True)

    ts = _now_iso()
    json_path = reports_path / "audit_auto_ignored.json"
    md_path = reports_path / "audit_auto_ignored.md"

    payload = {
        "generated_at": ts,
        "total_auto_ignored": len(auto_ignored),
        "items": list(auto_ignored),
    }
    json_path.write_text(
        json.dumps(payload, indent=2, ensure_ascii=False), encoding="utf-8"
    )

    # Markdown summary grouped by entry_point_type then pattern_name
    grouped: dict[str, dict[str, list[str]]] = {}
    for item in auto_ignored:
        ep = str(item.get("entry_point_type") or "unknown")
        pat = str(item.get("pattern_name") or "—")
        grouped.setdefault(ep, {}).setdefault(pat, []).append(
            str(item.get("symbol_key") or "")
        )

    lines = [
        "# Audit Auto-Ignored Symbols",
        "",
        f"- Generated: `{ts}`",
        f"- Total auto-ignored: **{len(auto_ignored)}**",
        "",
    ]

    for ep_type in sorted(grouped.keys()):
        lines.append(f"## {ep_type}")
        for pattern_name in sorted(grouped[ep_type].keys()):
            syms = grouped[ep_type][pattern_name]
            lines.append(f"### Pattern: {pattern_name}  _(n={len(syms)})_")
            for s in sorted(syms):
                lines.append(f"- `{s}`")
            lines.append("")  # blank line

    md_path.write_text("\n".join(lines), encoding="utf-8")


# -----------------------------
# Optional CLI entrypoint
# -----------------------------
def _load_json(path: Path) -> object:
    return json.loads(path.read_text(encoding="utf-8"))


def _save_json(path: Path, data: object) -> None:
    path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")


# ID: a373b218-70a0-40fb-89e3-6815b9f76d2b
def main(argv: list[str] | None = None) -> int:
    """
    Minimal CLI to post-process existing auditor outputs.

    Example:
      python -m src.features.governance.audit_postprocessor \
        --in reports/audit_findings.json \
        --symbols reports/symbol_index.json \
        --out reports/audit_findings.processed.json \
        --reports reports \
        --downgrade-to info
    """
    parser = argparse.ArgumentParser(description="Audit findings post-processor")
    parser.add_argument(
        "--in", dest="in_path", required=True, help="Input findings JSON path"
    )
    parser.add_argument(
        "--symbols", dest="symbols_path", required=True, help="Symbol index JSON path"
    )
    parser.add_argument(
        "--out",
        dest="out_path",
        required=True,
        help="Output (processed findings) JSON path",
    )
    parser.add_argument(
        "--reports", dest="reports_dir", default="reports", help="Reports directory"
    )
    parser.add_argument(
        "--downgrade-to",
        dest="downgrade_to",
        default="info",
        choices=["info", "warn"],
        help="Target severity for allowed entry points",
    )
    parser.add_argument(
        "--dead-rule-id",
        dest="dead_rule_ids",
        action="append",
        default=None,
        help="Add/override dead-public-symbol rule id(s). Can be passed multiple times.",
    )

    args = parser.parse_args(argv or sys.argv[1:])

    in_path = Path(args.in_path)
    symbols_path = Path(args.symbols_path)
    out_path = Path(args.out_path)
    reports_dir = Path(args.reports_dir)

    findings_obj = _load_json(in_path)
    symbols_obj = _load_json(symbols_path)

    if not isinstance(findings_obj, list):
        print("ERROR: findings JSON must be a list of objects.", file=sys.stderr)
        return 2
    if not isinstance(symbols_obj, dict):
        print(
            "ERROR: symbols JSON must be an object mapping symbol_key to metadata.",
            file=sys.stderr,
        )
        return 2

    processed = apply_entry_point_downgrade_and_report(
        findings=findings_obj,  # type: ignore[arg-type]
        symbol_index=symbols_obj,  # type: ignore[arg-type]
        reports_dir=reports_dir,
        allow_list=EntryPointAllowList.default(),
        dead_rule_ids=args.dead_rule_ids
        or ("dead_public_symbol", "dead-public-symbol"),
        downgrade_to=args.downgrade_to,
        write_reports=True,
    )

    _save_json(out_path, processed)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
# src/mind/governance/checks/base_check.py
"""
Provides a shared base class for all constitutional audit checks to inherit from.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from mind.governance.audit_context import AuditorContext


# ID: 2cb0374b-a487-4dce-bab1-c2ee8a693b0a
class BaseCheck:
    """A base class for audit checks, providing a shared context."""

    def __init__(self, context: AuditorContext):
        """
        Initializes the check with a shared auditor context.
        This common initializer serves the 'dry_by_design' principle.
        """
        self.context = context
        self.repo_root = context.repo_path
        self.intent_path = context.intent_path
        self.src_dir = context.src_dir
# src/mind/governance/checks/capability_coverage.py
"""
A constitutional audit check to ensure that all capabilities declared in the
project manifest are implemented in the database.
"""

from __future__ import annotations

from mind.governance.audit_context import AuditorContext
from shared.models import AuditFinding, AuditSeverity


# ID: 979ce56f-7f3c-40e7-8736-ce219bab6ad8
class CapabilityCoverageCheck:
    """
    Verifies that every capability in the manifest has a corresponding
    implementation entry in the database's symbols table.
    """

    def __init__(self, context: AuditorContext):
        self.context = context

    # ID: e0730fb8-2616-42b2-915b-48f30ff4ac17
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings = []

        manifest_path = self.context.mind_path / "project_manifest.yaml"
        if not manifest_path.exists():
            findings.append(
                AuditFinding(
                    check_id="manifest.missing.project_manifest",
                    severity=AuditSeverity.ERROR,
                    message="The project_manifest.yaml file is missing from .intent/mind/.",
                    file_path=str(manifest_path.relative_to(self.context.repo_path)),
                )
            )
            return findings

        manifest_content = self.context._load_yaml(manifest_path)
        declared_capabilities: set[str] = set(manifest_content.get("capabilities", []))

        # --- THIS IS THE CORRECT LOGIC ---
        # The source of truth for implementation is the database, not code comments.
        # The view aliases 'key' to 'capability', so we must use that name here.
        implemented_capabilities: set[str] = {
            s["capability"]
            for s in self.context.knowledge_graph.get("symbols", {}).values()
            if s.get("capability")
        }
        # --- END OF CORRECT LOGIC ---

        missing_implementations = declared_capabilities - implemented_capabilities

        for cap_key in sorted(list(missing_implementations)):
            findings.append(
                AuditFinding(
                    check_id="capability.coverage.missing_implementation",
                    severity=AuditSeverity.WARNING,
                    message=f"Capability '{cap_key}' is declared in the manifest but has no implementation linked in the database.",
                    file_path=str(manifest_path.relative_to(self.context.repo_path)),
                )
            )

        return findings
# src/mind/governance/checks/coverage_check.py

"""
Constitutional enforcement of test coverage requirements.
Verifies that the codebase meets the minimum coverage threshold defined
in the quality_assurance_policy.
"""

from __future__ import annotations

from shared.config import settings
from shared.logger import getLogger
from shared.models.audit_models import AuditFinding, AuditSeverity
from typing import Any
import json
import subprocess


logger = getLogger(__name__)


# ID: f09915fb-02c8-49d4-b5c5-19cd5e955df4
class CoverageGovernanceCheck:
    """
    Enforces constitutional test coverage requirements.

    This check verifies that:
    1. Overall coverage meets the minimum threshold (75%)
    2. Critical paths meet their specific higher thresholds
    3. No significant coverage regressions have occurred
    """

    def __init__(self):
        self.policy = settings.load("charter.policies.quality_assurance")
        self.config = self.policy.get("coverage_config", {})
        self.minimum_threshold = self.config.get("minimum_threshold", 75)
        self.critical_paths = self.config.get("critical_paths", [])
        self.exclusions = self.config.get("exclusions", [])

    # ID: a8126c8d-f9b8-40d5-a098-4aa5065f656c
    async def execute(self) -> list[AuditFinding]:
        """
        Executes the coverage check and returns audit findings.

        Returns:
            List of AuditFinding objects for any violations
        """
        findings: list[AuditFinding] = []
        coverage_data = self._measure_coverage()
        if not coverage_data:
            return [
                AuditFinding(
                    check_id="coverage.minimum_threshold",
                    severity=AuditSeverity.ERROR,
                    message="Failed to measure test coverage",
                    file_path="N/A",
                    context={"error": "Could not run pytest coverage"},
                )
            ]
        overall_coverage = coverage_data.get("overall_percent", 0)
        if overall_coverage < self.minimum_threshold:
            findings.append(
                AuditFinding(
                    check_id="coverage.minimum_threshold",
                    severity=AuditSeverity.ERROR,
                    message=f"Coverage {overall_coverage}% below constitutional minimum {self.minimum_threshold}%",
                    file_path="N/A",
                    context={
                        "current": overall_coverage,
                        "required": self.minimum_threshold,
                        "delta": overall_coverage - self.minimum_threshold,
                        "action": "Trigger autonomous remediation",
                    },
                )
            )
        for path_spec in self.critical_paths:
            path_pattern, required = self._parse_path_spec(path_spec)
            actual = self._get_path_coverage(coverage_data, path_pattern)
            if actual is not None and actual < required:
                findings.append(
                    AuditFinding(
                        check_id="coverage.critical_path",
                        severity=AuditSeverity.ERROR,
                        message=f"Critical path '{path_pattern}' coverage {actual}% below required {required}%",
                        file_path=path_pattern,
                        context={
                            "current": actual,
                            "required": required,
                            "delta": actual - required,
                        },
                    )
                )
        regression = self._check_regression(coverage_data)
        if regression:
            findings.append(regression)
        return findings

    def _measure_coverage(self) -> dict[str, Any] | None:
        """
        Runs pytest with coverage and returns parsed results.

        Returns:
            Dict with coverage metrics or None if measurement fails
        """
        try:
            result = subprocess.run(
                [
                    "poetry",
                    "run",
                    "pytest",
                    "--cov=src",
                    "--cov-report=json",
                    "--cov-report=term",
                    "-q",
                ],
                cwd=settings.REPO_PATH,
                capture_output=True,
                text=True,
                timeout=300,
            )
            coverage_json = settings.REPO_PATH / "coverage.json"
            if coverage_json.exists():
                data = json.loads(coverage_json.read_text())
                totals = data.get("totals", {})
                return {
                    "overall_percent": totals.get("percent_covered", 0),
                    "lines_covered": totals.get("covered_lines", 0),
                    "lines_total": totals.get("num_statements", 0),
                    "files": data.get("files", {}),
                    "timestamp": data.get("meta", {}).get("timestamp"),
                }
            return self._parse_term_output(result.stdout)
        except subprocess.TimeoutExpired:
            logger.error("Coverage measurement timed out after 5 minutes")
            return None
        except Exception as e:
            logger.error(f"Failed to measure coverage: {e}", exc_info=True)
            return None

    def _parse_term_output(self, output: str) -> dict[str, Any] | None:
        """
        Fallback parser for terminal coverage output.

        Args:
            output: Terminal output from pytest --cov

        Returns:
            Dict with coverage metrics or None
        """
        try:
            for line in output.splitlines():
                if line.startswith("TOTAL"):
                    parts = line.split()
                    if len(parts) >= 4:
                        percent_str = parts[-1].rstrip("%")
                        return {
                            "overall_percent": float(percent_str),
                            "lines_total": int(parts[1]),
                            "lines_covered": int(parts[1]) - int(parts[2]),
                        }
        except Exception as e:
            logger.debug(f"Failed to parse coverage output: {e}")
        return None

    def _parse_path_spec(self, spec: str) -> tuple[str, float]:
        """
        Parses a path specification like 'src/core/**/*.py: 85%'.

        Args:
            spec: Path specification string

        Returns:
            Tuple of (path_pattern, required_percent)
        """
        parts = spec.split(":")
        path = parts[0].strip()
        percent = float(parts[1].strip().rstrip("%"))
        return (path, percent)

    def _get_path_coverage(self, coverage_data: dict, pattern: str) -> float | None:
        """
        Gets coverage percentage for files matching a pattern.

        Args:
            coverage_data: Coverage data from measurement
            pattern: Glob-style path pattern

        Returns:
            Coverage percentage or None if no matches
        """
        files = coverage_data.get("files", {})
        if not files:
            return None
        from fnmatch import fnmatch

        total_lines = 0
        covered_lines = 0
        for file_path, file_data in files.items():
            if fnmatch(file_path, pattern):
                summary = file_data.get("summary", {})
                total_lines += summary.get("num_statements", 0)
                covered_lines += summary.get("covered_lines", 0)
        if total_lines == 0:
            return None
        return round(covered_lines / total_lines * 100, 2)

    def _check_regression(self, coverage_data: dict) -> AuditFinding | None:
        """
        Checks for significant coverage regressions.

        Args:
            coverage_data: Current coverage data

        Returns:
            AuditFinding if regression detected, None otherwise
        """
        history_file = settings.REPO_PATH / "work" / "testing" / "coverage_history.json"
        if not history_file.exists():
            self._save_coverage_history(coverage_data)
            return None
        try:
            history = json.loads(history_file.read_text())
            last_run = history.get("last_run", {})
            last_percent = last_run.get("overall_percent", 0)
            current_percent = coverage_data.get("overall_percent", 0)
            delta = current_percent - last_percent
            self._save_coverage_history(coverage_data)
            if delta < -5.0:
                return AuditFinding(
                    check_id="coverage.no_untested_commits",
                    severity=AuditSeverity.ERROR,
                    message=f"Significant coverage regression: {abs(delta):.1f}% drop",
                    file_path="N/A",
                    context={
                        "previous": last_percent,
                        "current": current_percent,
                        "delta": delta,
                    },
                )
        except Exception as e:
            logger.debug(f"Could not check coverage regression: {e}")
        return None

    def _save_coverage_history(self, coverage_data: dict) -> None:
        """Saves coverage data to history file for regression tracking."""
        try:
            history_file = (
                settings.REPO_PATH / "work" / "testing" / "coverage_history.json"
            )
            history_file.parent.mkdir(parents=True, exist_ok=True)
            history = {
                "last_run": coverage_data,
                "updated_at": coverage_data.get("timestamp"),
            }
            history_file.write_text(json.dumps(history, indent=2))
        except Exception as e:
            logger.debug(f"Could not save coverage history: {e}")
# src/mind/governance/checks/dependency_injection_check.py
"""
A constitutional audit check to enforce the Dependency Injection (DI) policy.
"""

from __future__ import annotations

import ast
from pathlib import Path

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 68fa7a18-3591-46ad-9470-0a0bb8685491
class DependencyInjectionCheck(BaseCheck):
    """
    Ensures that services and features do not directly instantiate their dependencies,
    and do not use forbidden global imports like `get_session`.
    """

    def __init__(self, context):
        super().__init__(context)
        # CORRECTED: Load from the consolidated code_standards policy
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.policy = code_standards_policy.get("dependency_injection", [])

    # ID: e0b8b3db-959e-4ac1-bc26-a7f3e1b35bc0
    def execute(self) -> list[AuditFinding]:
        """Runs the DI check by scanning source files for policy violations."""
        findings = []
        rules = self.policy
        if not rules:
            return findings

        for rule in rules:
            if rule.get("id") == "di.no_direct_instantiation":
                findings.extend(self._check_forbidden_instantiations(rule))
            elif rule.get("id") == "di.no_global_session_import":
                findings.extend(self._check_forbidden_imports(rule))

        return findings

    def _check_forbidden_instantiations(self, rule: dict) -> list[AuditFinding]:
        """Finds direct instantiations of major services."""
        findings = []
        forbidden_calls = set(rule.get("forbidden_instantiations", []))
        scope = rule.get("scope", [])
        exclusions = rule.get("exclusions", [])

        for file_path in self._get_files_in_scope(scope, exclusions):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):
                        if node.func.id in forbidden_calls:
                            findings.append(
                                AuditFinding(
                                    check_id=rule["id"],
                                    severity=AuditSeverity.ERROR,
                                    message=f"Direct instantiation of '{node.func.id}' is forbidden. Inject it via the constructor.",
                                    file_path=str(
                                        file_path.relative_to(self.repo_root)
                                    ),
                                    line_number=node.lineno,
                                    context={"category": "architectural"},
                                )
                            )
            except Exception:
                continue
        return findings

    def _check_forbidden_imports(self, rule: dict) -> list[AuditFinding]:
        """Finds direct imports of forbidden functions like get_session."""
        findings = []
        forbidden_imports = set(rule.get("forbidden_imports", []))
        scope = rule.get("scope", [])
        exclusions = rule.get("exclusions", [])

        for file_path in self._get_files_in_scope(scope, exclusions):
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if (
                        isinstance(node, ast.ImportFrom)
                        and node.module in forbidden_imports
                    ):
                        findings.append(
                            AuditFinding(
                                check_id=rule["id"],
                                severity=AuditSeverity.ERROR,
                                message=f"Direct import of '{node.module}' is forbidden. Inject the dependency instead.",
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=node.lineno,
                                context={"category": "architectural"},
                            )
                        )
            except Exception:
                continue
        return findings

    def _get_files_in_scope(
        self, scope: list[str], exclusions: list[str]
    ) -> list[Path]:
        """Helper to get all files matching the scope and exclusion globs."""
        files = []
        for glob_pattern in scope:
            for file_path in self.repo_root.glob(glob_pattern):
                if file_path.is_file() and not any(
                    file_path.match(ex) for ex in exclusions
                ):
                    files.append(file_path)
        return list(set(files))
# src/mind/governance/checks/domain_placement.py
"""
A constitutional audit check to ensure capabilities are declared in the
correct domain manifest file.
"""

from __future__ import annotations

from mind.governance.audit_context import AuditorContext
from shared.models import AuditFinding, AuditSeverity
from shared.utils.yaml_processor import yaml_processor


# ID: 0cd8ad5a-ed46-4f18-8335-f95b747d6164
class DomainPlacementCheck:
    """
    Validates that capability keys declared in a domain manifest file
    match the domain of that file.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        self.domains_dir = self.context.mind_path / "knowledge" / "domains"

    # ID: 7eb75aef-6463-450d-8088-e9a64e3d85c8
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings = []
        if not self.domains_dir.is_dir():
            return findings

        for domain_file in self.domains_dir.glob("*.yaml"):
            domain_name = domain_file.stem
            manifest_content = yaml_processor.load(domain_file)
            if not manifest_content:
                continue

            capabilities = manifest_content.get("tags", [])
            if not isinstance(capabilities, list):
                continue

            for cap in capabilities:
                if isinstance(cap, dict) and "key" in cap:
                    cap_key = cap["key"]
                    if not cap_key.startswith(f"{domain_name}."):
                        findings.append(
                            AuditFinding(
                                check_id="domain.placement.mismatch",
                                severity=AuditSeverity.ERROR,
                                message=f"Capability '{cap_key}' is misplaced in '{domain_file.name}'. It should be in a '{cap_key.split('.')[0]}.yaml' manifest.",
                                file_path=str(
                                    domain_file.relative_to(self.context.repo_path)
                                ),
                            )
                        )
        return findings
# src/mind/governance/checks/duplication_check.py

"""
A constitutional audit check to find semantically duplicate or near-duplicate
symbols (functions/classes) using the Qdrant vector database.
"""

from __future__ import annotations

from mind.governance.audit_context import AuditorContext
from rich.progress import track
from services.clients.qdrant_client import QdrantService
from shared.config import settings
from shared.logger import getLogger
from shared.models import AuditFinding, AuditSeverity
from typing import Any
import asyncio


logger = getLogger(__name__)


# ID: 13cf4ae9-f18b-410f-a320-399cc713f277
class DuplicationCheck:
    """
    Enforces the 'dry_by_design' principle by finding semantically similar symbols.
    """

    def __init__(self, context: AuditorContext, qdrant_service: QdrantService):
        self.context = context
        self.qdrant_service = qdrant_service
        self.symbols = self.context.knowledge_graph.get("symbols", {})
        try:
            ignore_policy = settings.load(
                "charter.policies.governance.audit_ignore_policy"
            )
        except FileNotFoundError:
            ignore_policy = {}
        self.ignored_symbol_keys = {
            item["key"]
            for item in ignore_policy.get("symbol_ignores", [])
            if "key" in item
        }

    async def _check_single_symbol(
        self, symbol: dict[str, Any], threshold: float
    ) -> list[AuditFinding]:
        """Checks a single symbol for duplicates against the Qdrant index."""
        findings = []
        symbol_key = symbol.get("symbol_path")
        point_id = str(symbol.get("uuid")) if symbol.get("uuid") else None
        if not point_id or symbol_key in self.ignored_symbol_keys:
            return []
        try:
            query_vector = await self.qdrant_service.get_vector_by_id(point_id=point_id)
            if not query_vector:
                return []
            similar_hits = await self.qdrant_service.search_similar(
                query_vector=query_vector, limit=5
            )
            for hit in similar_hits:
                if not hit.get("payload"):
                    continue
                hit_symbol_key = hit["payload"].get("chunk_id")
                if (
                    not hit_symbol_key
                    or hit_symbol_key == symbol_key
                    or hit_symbol_key in self.ignored_symbol_keys
                ):
                    continue
                if hit["score"] > threshold:
                    symbol_a, symbol_b = sorted((symbol_key, hit_symbol_key))
                    findings.append(
                        AuditFinding(
                            check_id="code.style.semantic-duplication",
                            severity=AuditSeverity.WARNING,
                            message=f"Potential duplicate logic found between '{symbol_a.split('::')[-1]}' and '{symbol_b.split('::')[-1]}'.",
                            file_path=symbol.get("file_path"),
                            context={
                                "symbol_a": symbol_a,
                                "symbol_b": symbol_b,
                                "similarity": f"{hit['score']:.2f}",
                            },
                        )
                    )
        except Exception as e:
            logger.warning(
                f"Could not perform duplication check for '{symbol_key}': {e}"
            )
        return findings

    # ID: 1da6e2c3-fbd4-4860-b95e-7625f426edba
    async def execute(self, threshold: float = 0.8) -> list[AuditFinding]:
        """
        Asynchronously runs the duplication check across all vectorized symbols.
        """
        symbols_to_check = list(self.symbols.values())
        if not symbols_to_check:
            return []
        tasks = [
            self._check_single_symbol(symbol, threshold) for symbol in symbols_to_check
        ]
        results = []
        for future in track(
            asyncio.as_completed(tasks),
            description="Checking for duplicate code...",
            total=len(tasks),
        ):
            results.extend(await future)
        unique_findings = {}
        for finding in results:
            key_tuple = tuple(
                sorted((finding.context["symbol_a"], finding.context["symbol_b"]))
            )
            if key_tuple not in unique_findings:
                unique_findings[key_tuple] = finding
        return list(unique_findings.values())
# src/mind/governance/checks/environment_checks.py
"""
Audits the system's runtime environment for required configuration.
"""

from __future__ import annotations

import os

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 0c3965b7-b3f3-4fb6-bbbb-c94a1ffae3fe
class EnvironmentChecks(BaseCheck):
    """Container for environment and runtime configuration checks."""

    def __init__(self, context):
        super().__init__(context)
        self.requirements = self.context.policies.get("runtime_requirements", {})

    # ID: 0c0e7695-b11e-4ad8-9e74-23d5f79dad00
    def execute(self) -> list[AuditFinding]:
        """
        Verifies that required environment variables specified in
        runtime_requirements.yaml are set.
        """
        findings = []
        required_vars = self.requirements.get("variables", {})

        for name, config in required_vars.items():
            if config.get("required") and not os.getenv(name):
                msg = (
                    f"Required environment variable '{name}' is not set. "
                    f"Description: {config.get('description', 'No description.')}"
                )
                findings.append(
                    AuditFinding(
                        check_id="environment.variable.missing",
                        severity=AuditSeverity.ERROR,
                        message=msg,
                        file_path=".env",
                    )
                )
        return findings
# src/mind/governance/checks/file_checks.py
"""
Audits file existence and orphan detection for constitutional governance files.
"""

from __future__ import annotations

from mind.governance.checks.base_check import BaseCheck
from shared.config import settings
from shared.models import AuditFinding, AuditSeverity
from shared.utils.constitutional_parser import get_all_constitutional_paths

KNOWN_UNINDEXED_FILES = {
    ".intent/charter/constitution/approvers.yaml.example",
    ".intent/keys/private.key",
}

DEPRECATED_KNOWLEDGE_FILES = [
    ".intent/knowledge/cli_registry.yaml",
    ".intent/knowledge/resource_manifest.yaml",
    ".intent/knowledge/cognitive_roles.yaml",
]


# ID: 37b5ae2f-c3c2-4db4-9677-f16fd788c908
class FileChecks(BaseCheck):
    """Container for file-based constitutional checks."""

    # ID: 56481071-3a0c-437d-ba57-533bc03d9ed6
    def execute(self) -> list[AuditFinding]:
        """Runs all file-related checks."""
        meta_content = settings._meta_config
        required_files = get_all_constitutional_paths(meta_content, self.intent_path)
        findings = self._check_required_files(required_files)
        findings.extend(self._check_for_orphaned_intent_files(required_files))
        findings.extend(self._check_for_deprecated_files())
        return findings

    def _check_for_deprecated_files(self) -> list[AuditFinding]:
        """Verify that files constitutionally replaced by the database do not exist."""
        findings: list[AuditFinding] = []
        for file_rel_path in DEPRECATED_KNOWLEDGE_FILES:
            full_path = self.repo_root / file_rel_path
            if full_path.exists():
                findings.append(
                    AuditFinding(
                        check_id="config.ssot.deprecated-file",
                        severity=AuditSeverity.ERROR,
                        message=f"Deprecated knowledge file exists: '{file_rel_path}'. The database is the SSOT.",
                        file_path=file_rel_path,
                    )
                )
        return findings

    def _check_required_files(self, required_files: set[str]) -> list[AuditFinding]:
        """Verify that all files declared in meta.yaml exist on disk."""
        findings: list[AuditFinding] = []
        for file_rel_path in sorted(required_files):
            full_path = self.repo_root / file_rel_path
            if not full_path.exists():
                findings.append(
                    AuditFinding(
                        check_id="config.meta.missing-file",
                        severity=AuditSeverity.ERROR,
                        message=f"File declared in meta.yaml is missing: '{file_rel_path}'",
                        file_path=file_rel_path,
                    )
                )
        return findings

    def _check_for_orphaned_intent_files(
        self, declared_files: set[str]
    ) -> list[AuditFinding]:
        """Find .intent files not referenced in meta.yaml."""
        findings: list[AuditFinding] = []
        all_known_files = declared_files.union(KNOWN_UNINDEXED_FILES)
        if (self.intent_path / "proposals/README.md").exists():
            all_known_files.add(".intent/proposals/README.md")
        physical_files: set[str] = {
            str(p.relative_to(self.repo_root)).replace("\\", "/")
            for p in self.intent_path.rglob("*")
            if p.is_file()
        }
        orphaned_files = sorted(physical_files - all_known_files)
        for orphan in orphaned_files:
            if "prompts" in orphan or "reports" in orphan:
                continue
            findings.append(
                AuditFinding(
                    check_id="config.meta.orphaned-file",
                    severity=AuditSeverity.WARNING,
                    message=f"Orphaned file in .intent/: '{orphan}'. Add to meta.yaml or remove.",
                    file_path=orphan,
                )
            )
        return findings
# src/mind/governance/checks/health_checks.py
"""
Audits codebase health for complexity, atomicity, and line length violations.
"""

from __future__ import annotations

import ast
import statistics
from pathlib import Path

from radon.visitors import ComplexityVisitor

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 51dd8f1d-eda6-40e2-9c64-530ce6c290a6
class HealthChecks(BaseCheck):
    """Container for codebase health constitutional checks."""

    def __init__(self, context):
        super().__init__(context)
        # CORRECTED: Load from the consolidated code_standards policy
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.health_policy = code_standards_policy.get("health_standards", {})

    # ID: 64bffe32-e6fd-4fd1-a235-aaf764363076
    def execute(self) -> list[AuditFinding]:
        """Measures code complexity and atomicity against defined policies."""
        # The 'rules' key is now directly on self.health_policy
        policy_rules = self.health_policy
        file_line_counts = {}
        all_violations = []
        unique_files = {
            s["file_path"]
            for s in self.context.symbols_list
            if s.get("file_path", "").startswith("src/")
        }
        for file_path_str in sorted(list(unique_files)):
            if not file_path_str.endswith(".py"):
                continue
            file_path = self.repo_root / file_path_str
            logical_lines, violations = self._analyze_python_file(
                file_path, policy_rules
            )
            if logical_lines > 0:
                file_line_counts[file_path] = logical_lines
            all_violations.extend(violations)
        all_violations.extend(
            self._find_file_size_outliers(file_line_counts, policy_rules)
        )
        return all_violations

    def _analyze_python_file(
        self, file_path: Path, rules: dict
    ) -> tuple[int, list[AuditFinding]]:
        """Analyze a single Python file for health violations."""
        try:
            source_code = file_path.read_text(encoding="utf-8")
            logical_lines = self._count_logical_lines(source_code)
            if logical_lines > rules.get("max_module_lloc", 300):
                return logical_lines, [
                    AuditFinding(
                        check_id="code.complexity.module-too-long",
                        severity=AuditSeverity.WARNING,
                        message=f"Module has {logical_lines} lines (limit: {rules.get('max_module_lloc', 300)}).",
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                ]
            syntax_tree = ast.parse(source_code)
            complexity_visitor = ComplexityVisitor.from_ast(syntax_tree)
            violations = self._check_function_metrics(
                complexity_visitor,
                rules,
                str(file_path.relative_to(self.repo_root)),
            )
            return logical_lines, violations
        except Exception:
            return 0, []

    def _count_logical_lines(self, source_code: str) -> int:
        return sum(
            1
            for line in source_code.splitlines()
            if line.strip() and not line.strip().startswith("#")
        )

    def _check_function_metrics(
        self,
        visitor: ComplexityVisitor,
        rules: dict,
        file_path_str: str,
    ) -> list[AuditFinding]:
        violations = []
        for function in visitor.functions:
            if function.cognitive_complexity > rules.get(
                "max_cognitive_complexity", 15
            ):
                violations.append(
                    AuditFinding(
                        check_id="code.complexity.function-too-complex",
                        severity=AuditSeverity.WARNING,
                        message=f"Function '{function.name}' complexity is {function.cognitive_complexity} (limit: {rules.get('max_cognitive_complexity', 15)}).",
                        file_path=file_path_str,
                    )
                )
            if function.lloc > rules.get("max_function_lloc", 80):
                violations.append(
                    AuditFinding(
                        check_id="code.complexity.function-too-long",
                        severity=AuditSeverity.WARNING,
                        message=f"Function '{function.name}' has {function.lloc} lines (limit: {rules.get('max_function_lloc', 80)}).",
                        file_path=file_path_str,
                    )
                )
        return violations

    def _find_file_size_outliers(
        self, file_line_counts: dict, rules: dict
    ) -> list[AuditFinding]:
        if len(file_line_counts) < 3:
            return []
        violations = []
        line_count_values = list(file_line_counts.values())
        average_lines = statistics.mean(line_count_values)
        standard_deviation = statistics.stdev(line_count_values)
        outlier_threshold = average_lines + (
            rules.get("outlier_standard_deviations", 2.0) * standard_deviation
        )
        for file_path, line_count in file_line_counts.items():
            if line_count > outlier_threshold:
                violations.append(
                    AuditFinding(
                        check_id="code.complexity.module-outlier",
                        severity=AuditSeverity.WARNING,
                        message=f"Module size outlier ({line_count} lines vs avg of {average_lines:.0f}). Consider refactoring.",
                        file_path=str(file_path.relative_to(self.repo_root)),
                    )
                )
        return violations
# src/mind/governance/checks/id_coverage_check.py
"""
A constitutional audit check to enforce that every public symbol in the codebase
has a registered ID in the database.
"""

from __future__ import annotations

import ast

from mind.governance.checks.base_check import BaseCheck
from shared.ast_utility import find_symbol_id_and_def_line
from shared.models import AuditFinding, AuditSeverity


# ID: 3501ed8c-8366-4ad7-9ab4-7dcf4c045c70
class IdCoverageCheck(BaseCheck):
    """
    Ensures every public function/class in `src/` has a valid, DB-registered ID tag.
    """

    # ID: f69a1a2e-26cd-4cc2-8fdc-7f18e0e77d0c
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any violations.
        """
        findings = []
        for file_path in self.context.src_dir.rglob("*.py"):
            try:
                content = file_path.read_text("utf-8")
                source_lines = content.splitlines()
                tree = ast.parse(content, filename=str(file_path))

                for node in ast.walk(tree):
                    if not isinstance(
                        node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)
                    ):
                        continue

                    # Rule 1: Must be a public symbol
                    if node.name.startswith("_"):
                        continue

                    # Use the robust utility to find the ID and definition line
                    id_result = find_symbol_id_and_def_line(node, source_lines)

                    if not id_result.has_id:
                        findings.append(
                            AuditFinding(
                                check_id="linkage.id.missing-tag",
                                severity=AuditSeverity.ERROR,
                                message=f"Public symbol '{node.name}' is missing its required '# ID:' tag.",
                                file_path=str(
                                    file_path.relative_to(self.context.repo_path)
                                ),
                                line_number=id_result.definition_line_num,
                            )
                        )

            except Exception:
                # Silently ignore files that cannot be parsed
                continue

        return findings
# src/mind/governance/checks/id_uniqueness_check.py
"""
A constitutional audit check to enforce that every # ID tag is unique across the codebase.
"""

from __future__ import annotations

import re
from collections import defaultdict

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity

# Pre-compiled regex for efficiency to find '# ID: <uuid>'
ID_TAG_REGEX = re.compile(
    r"#\s*ID:\s*([0-9a-fA-F]{8}-([0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12})"
)


# ID: ddaabb9e-5e9a-4574-b458-dbed610e64e5
class IdUniquenessCheck(BaseCheck):
    """
    Scans the entire source code to ensure that every assigned symbol ID (UUID) is unique.
    This prevents data corruption from accidental copy-paste errors during development.
    """

    # ID: f2a3b4c5-d6e7-f8a9-b0c1-d2e3f4a5b6c7
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by scanning all Python files in `src/` and returns findings for any duplicate UUIDs.
        """
        # A dictionary to store locations of each UUID: {uuid: [("file/path.py", line_num), ...]}
        uuid_locations: dict[str, list[tuple[str, int]]] = defaultdict(list)

        src_dir = self.context.repo_path / "src"
        for file_path in src_dir.rglob("*.py"):
            try:
                content = file_path.read_text("utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    match = ID_TAG_REGEX.search(line)
                    if match:
                        found_uuid = match.group(1)
                        rel_path = str(file_path.relative_to(self.context.repo_path))
                        uuid_locations[found_uuid].append((rel_path, i))
            except Exception:
                # Silently ignore files that can't be read or parsed
                continue

        findings = []
        for found_uuid, locations in uuid_locations.items():
            if len(locations) > 1:
                # Found a duplicate!
                locations_str = ", ".join(
                    [f"{path}:{line}" for path, line in locations]
                )
                findings.append(
                    AuditFinding(
                        check_id="linkage.id.duplicate",
                        severity=AuditSeverity.ERROR,
                        message=f"Duplicate ID tag found: {found_uuid}",
                        context={"locations": locations_str},
                    )
                )

        return findings
# src/mind/governance/checks/import_rules.py
"""
A constitutional audit check to enforce architectural import rules as
defined in the source_structure.yaml manifest.
"""

from __future__ import annotations

import ast
from pathlib import Path

from sqlalchemy import text

from mind.governance.audit_context import AuditorContext
from mind.governance.checks.base_check import BaseCheck
from services.database.session_manager import get_session
from shared.models import AuditFinding, AuditSeverity


def _scan_imports(file_path: Path, content: str | None = None) -> list[str]:
    """
    Parse a Python file or its content and extract all imported module paths.
    """
    imports = []
    try:
        source = (
            content if content is not None else file_path.read_text(encoding="utf-8")
        )
        tree = ast.parse(source)

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    if node.level > 0:
                        base = ".".join(file_path.parts[1:-1])
                        if node.level > 1:
                            base = ".".join(base.split(".")[: -(node.level - 1)])
                        imports.append(f"{base}.{node.module}")
                    else:
                        imports.append(node.module)

    except Exception:
        pass

    return imports


# ID: 0690cf39-3739-449e-9228-2c7c8526209b
class ImportRulesCheck(BaseCheck):
    """
    Ensures that code files only import modules from their allowed domains.
    This check now reads its configuration from the database.
    """

    def __init__(self, context: AuditorContext):
        super().__init__(context)
        self.domain_map: dict[str, str] = {}
        self.import_rules: dict[str, set[str]] = {}

    async def _load_rules_from_db(self):
        """Loads domain maps and import rules from the database."""
        if self.domain_map:
            return

        async with get_session() as session:
            await session.execute(text("SELECT key FROM core.domains"))

        structure = self.context.source_structure.get("structure", [])
        for domain_info in structure:
            path_str = domain_info.get("path")
            domain_name = domain_info.get("domain")
            if path_str and domain_name:
                self.domain_map[path_str] = domain_name

        for domain_info in structure:
            domain_name = domain_info.get("domain")
            allowed_imports = domain_info.get("allowed_imports", [])
            if domain_name:
                self.import_rules[domain_name] = set(allowed_imports)

    def _get_domain_for_path_str(self, file_path_str: str) -> str | None:
        """Finds the domain for a given relative file path string."""
        for domain_path_prefix, domain_name in self.domain_map.items():
            if file_path_str.startswith(domain_path_prefix):
                return domain_name
        return None

    # ID: f1a7dedb-d5e4-442d-8957-b7f974778bc5
    async def execute(self) -> list[AuditFinding]:
        """
        Runs the check by scanning all source files and validating their imports.
        """
        await self._load_rules_from_db()
        findings = []
        # Use self.src_dir provided by the BaseCheck
        for file_path in self.src_dir.rglob("*.py"):
            findings.extend(self._check_file_imports(file_path, file_content=None))
        return findings

    # ID: 31287af5-d942-4a1d-b06d-d0570026d035
    async def execute_on_content(
        self, file_path_str: str, file_content: str
    ) -> list[AuditFinding]:
        """
        Runs the import check on a string of content instead of a file on disk.
        """
        await self._load_rules_from_db()
        # Use self.repo_root provided by the BaseCheck
        file_path = self.repo_root / file_path_str
        return self._check_file_imports(file_path, file_content)

    def _check_file_imports(
        self, file_path: Path, file_content: str | None
    ) -> list[AuditFinding]:
        """Core logic to check imports for a given file path and optional content."""
        findings = []
        # Use self.repo_root provided by the BaseCheck
        file_rel_path_str = str(file_path.relative_to(self.repo_root))
        file_domain = self._get_domain_for_path_str(file_rel_path_str)
        if not file_domain:
            return []

        allowed_imports_for_domain = self.import_rules.get(file_domain, set())
        imported_modules = _scan_imports(file_path, content=file_content)

        for module_str in imported_modules:
            imported_package = module_str.split(".")[0]

            if not any(
                imported_package.startswith(p)
                for p in ["src", "cli", "core", "features", "services", "shared"]
            ):
                continue

            if imported_package in allowed_imports_for_domain:
                continue

            if imported_package == file_domain:
                continue

            findings.append(
                AuditFinding(
                    check_id="architecture.import_violation",
                    severity=AuditSeverity.ERROR,
                    message=f"Illegal import of '{module_str}' in domain '{file_domain}'. Allowed: {sorted(list(allowed_imports_for_domain))}",
                    file_path=file_rel_path_str,
                )
            )
        return findings
# src/mind/governance/checks/__init__.py
"""Provides functionality for the __init__ module."""

from __future__ import annotations
# src/mind/governance/checks/knowledge_source_check.py
"""
Compares DB single-source-of-truth tables with their (legacy) YAML exports.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

import yaml
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, async_sessionmaker

# Configuration
TABLE_CONFIGS = {
    "cli_registry": {
        "yaml_paths": [
            ".intent/mind/knowledge/cli_registry.yaml",
            ".intent/mind/knowledge/cli_registry.yml",
        ],
        "table": "core.cli_commands",
        "yaml_key": "commands",
        "primary_key": "name",
        "preferred_order": ["name", "module", "entrypoint", "enabled"],
    },
    "resource_manifest": {
        "yaml_paths": [
            ".intent/mind/knowledge/resource_manifest.yaml",
            ".intent/mind/knowledge/resource_manifest.yml",
        ],
        "table": "core.llm_resources",
        "yaml_key": "llm_resources",
        "primary_key": "name",
        "preferred_order": ["name", "provider", "model", "enabled"],
    },
    "cognitive_roles": {
        "yaml_paths": [
            ".intent/mind/knowledge/cognitive_roles.yaml",
            ".intent/mind/knowledge/cognitive_roles.yml",
        ],
        "table": "core.cognitive_roles",
        "yaml_key": "cognitive_roles",
        "primary_key": "role",
        "preferred_order": ["name", "description", "enabled"],
    },
}

FIELD_PRIORITY = [
    "name",
    "role",
    "module",
    "entrypoint",
    "provider",
    "model",
    "description",
    "enabled",
]


@dataclass
# ID: 55de1540-39da-4a5d-9e40-b0614cfe655f
class CheckResult:
    name: str
    passed: bool
    details: dict[str, Any]


# ID: 81d6e8ed-a6f6-444c-acda-9064896c5111
class KnowledgeSourceCheck:
    """
    Compares DB single-source-of-truth tables with their (legacy) YAML exports under:
      .intent/mind/knowledge/{cli_registry, resource_manifest, cognitive_roles}.yaml

    Behavior:
      - If a YAML file is missing and `require_yaml_exports=False` (default), that section is SKIPPED.
      - If a YAML file exists, it is compared with the DB rows (adaptive to actual DB columns).
      - Any drift in an existing YAML file FAILS the check.

    Set `require_yaml_exports=True` to enforce the presence of YAML exports.
    """

    NAME = "knowledge_source_check"

    def __init__(
        self,
        repo_root: Path,
        engine: AsyncEngine,
        session_factory: async_sessionmaker[AsyncSession],
        reports_dir: Path | None = None,
        require_yaml_exports: bool = False,
    ) -> None:
        self.repo_root = repo_root
        self.engine = engine
        self.session_factory = session_factory
        self.reports_dir = reports_dir or repo_root / "reports" / "knowledge_ssot"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        self.require_yaml_exports = require_yaml_exports

    # ---------- Public API ----------
    # ID: b846d3ab-5762-4bc8-9dfc-f3fa060da29c
    async def execute(self) -> CheckResult:
        """Execute the knowledge source check and return results."""
        # Resolve YAML paths
        yaml_paths = {
            section: self._resolve_yaml(*config["yaml_paths"])
            for section, config in TABLE_CONFIGS.items()
        }

        # Fetch all database tables
        section_results = {}
        async with self.session_factory() as session:
            for section, config in TABLE_CONFIGS.items():
                schema, table = config["table"].split(".")
                db_rows, db_cols = await self._fetch_table(
                    session, schema, table, config["preferred_order"]
                )

                section_results[section] = await self._compare_section(
                    label=section,
                    yaml_path=yaml_paths[section],
                    db_rows=db_rows,
                    db_cols=db_cols,
                    yaml_key=config["yaml_key"],
                    primary_key=config["primary_key"],
                )

        # Determine overall pass/fail status
        passed = self._determine_overall_status(section_results)

        # Build and save report
        report = self._build_report(passed, yaml_paths, section_results)
        self._save_report(report)

        return CheckResult(name=self.NAME, passed=passed, details=report)

    # ---------- Section comparison ----------
    async def _compare_section(
        self,
        *,
        label: str,
        yaml_path: Path | None,
        db_rows: list[dict[str, Any]],
        db_cols: list[str],
        yaml_key: str,
        primary_key: str,
    ) -> dict[str, Any]:
        """Compare a single section (YAML vs DB)."""
        # Handle missing YAML file
        if yaml_path is None:
            return self._handle_missing_yaml()

        # Load and compare
        yaml_items = self._read_yaml(yaml_path, yaml_key)
        compare_fields = self._determine_compare_fields(yaml_items, db_cols)
        diff = self._diff_records(yaml_items, db_rows, primary_key, compare_fields)

        status = "passed" if self._is_diff_clean(diff) else "failed"
        return {
            "status": status,
            "yaml": str(yaml_path),
            "compare_fields": list(compare_fields),
            "diff": diff,
        }

    def _handle_missing_yaml(self) -> dict[str, Any]:
        """Handle the case where a YAML file is missing."""
        if self.require_yaml_exports:
            return {
                "status": "failed",
                "reason": "yaml_missing_and_required",
                "diff": {
                    "missing_in_db": [],
                    "missing_in_yaml": [],
                    "mismatched": [],
                },
            }
        return {"status": "skipped", "reason": "yaml_missing", "diff": None}

    # ---------- Database operations ----------
    async def _fetch_table(
        self,
        session: AsyncSession,
        schema: str,
        table: str,
        preferred_order: list[str],
    ) -> tuple[list[dict[str, Any]], list[str]]:
        """Fetch all rows and columns from a database table."""
        cols = await self._list_columns(session, schema, table)
        if not cols:
            return [], []

        # Query the table
        select_cols = ", ".join([f'"{c}"' for c in cols])
        sql = text(f'SELECT {select_cols} FROM "{schema}"."{table}"')
        rows = (await session.execute(sql)).mappings().all()

        # Order columns consistently
        ordered_cols = self._order_columns(cols, preferred_order)
        data = [{k: dict(r).get(k) for k in ordered_cols} for r in rows]

        return data, ordered_cols

    async def _list_columns(
        self, session: AsyncSession, schema: str, table: str
    ) -> list[str]:
        """Get the list of columns for a table from information_schema."""
        sql = text(
            """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = :schema AND table_name = :table
            ORDER BY ordinal_position
            """
        )
        rows = (
            await session.execute(sql, {"schema": schema, "table": table})
        ).mappings()
        return [r["column_name"] for r in rows]

    # ---------- YAML operations ----------
    def _resolve_yaml(self, *candidate_rel_paths: str) -> Path | None:
        """Find the first existing YAML file from a list of candidates."""
        for rel in candidate_rel_paths:
            p = self.repo_root / rel
            if p.exists():
                return p
        return None

    def _read_yaml(self, path: Path, key: str) -> list[dict[str, Any]]:
        """Read a YAML file and extract items by key."""
        try:
            data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
            if not isinstance(data, dict):
                return []

            items = data.get(key, [])
            return items if isinstance(items, list) else []
        except Exception:
            return []

    # ---------- Comparison logic ----------
    def _determine_compare_fields(
        self, yaml_items: list[dict[str, Any]], db_cols: list[str]
    ) -> tuple[str, ...]:
        """Determine which fields to compare based on YAML and DB columns."""
        yaml_keys = set()
        for item in yaml_items:
            if isinstance(item, dict):
                yaml_keys.update(item.keys())

        # Include primary key possibilities
        common_keys = (yaml_keys & set(db_cols)) | {"name", "role"}
        return self._order_fields(common_keys)

    def _diff_records(
        self,
        yaml_items: list[dict[str, Any]],
        db_items: list[dict[str, Any]],
        primary_key: str,
        compare_fields: tuple[str, ...],
    ) -> dict[str, Any]:
        """Compare YAML and DB records and return differences."""
        # Build indexes by primary key
        yaml_index = self._build_index(yaml_items, primary_key)
        db_index = self._build_index(db_items, primary_key)

        # Find missing records
        missing_in_db = sorted(set(yaml_index.keys()) - set(db_index.keys()))
        missing_in_yaml = sorted(set(db_index.keys()) - set(yaml_index.keys()))

        # Find mismatched records
        mismatched = self._find_mismatches(yaml_index, db_index, compare_fields)

        return {
            "missing_in_db": missing_in_db,
            "missing_in_yaml": missing_in_yaml,
            "mismatched": mismatched,
        }

    def _build_index(
        self, items: list[dict[str, Any]], key: str
    ) -> dict[str, dict[str, Any]]:
        """Build an index of items by their primary key."""
        return {
            str(item.get(key)).strip(): item
            for item in items
            if isinstance(item, dict) and item.get(key) is not None
        }

    def _find_mismatches(
        self,
        yaml_index: dict[str, dict[str, Any]],
        db_index: dict[str, dict[str, Any]],
        compare_fields: tuple[str, ...],
    ) -> list[dict[str, Any]]:
        """Find records that exist in both but have different field values."""
        mismatched = []
        common_keys = set(yaml_index.keys()) & set(db_index.keys())

        for key in sorted(common_keys):
            yaml_record = yaml_index[key]
            db_record = db_index[key]

            field_diffs = self._compare_records(yaml_record, db_record, compare_fields)

            if field_diffs:
                mismatched.append({"name": key, "fields": field_diffs})

        return mismatched

    def _compare_records(
        self,
        yaml_record: dict[str, Any],
        db_record: dict[str, Any],
        compare_fields: tuple[str, ...],
    ) -> dict[str, dict[str, Any]]:
        """Compare two records field by field."""
        diffs = {}

        for field in compare_fields:
            # Skip fields not present in either record
            if field not in yaml_record and field not in db_record:
                continue

            yaml_val = yaml_record.get(field)
            db_val = db_record.get(field)

            # Normalize: treat empty strings and None as equivalent
            if self._values_equivalent(yaml_val, db_val):
                continue

            if yaml_val != db_val:
                diffs[field] = {"yaml": yaml_val, "db": db_val}

        return diffs

    @staticmethod
    def _values_equivalent(val1: Any, val2: Any) -> bool:
        """Check if two values are equivalent (treating None and empty string as same)."""
        return (val1 is None or val1 == "") and (val2 is None or val2 == "")

    @staticmethod
    def _is_diff_clean(diff: dict[str, Any]) -> bool:
        """Check if a diff shows no differences."""
        return (
            not diff["missing_in_db"]
            and not diff["missing_in_yaml"]
            and not diff["mismatched"]
        )

    # ---------- Utility functions ----------
    @staticmethod
    def _order_columns(cols: list[str], preferred: list[str]) -> list[str]:
        """Order columns with preferred ones first, rest alphabetically."""
        return [c for c in preferred if c in cols] + [
            c for c in cols if c not in preferred
        ]

    @staticmethod
    def _order_fields(fields: set) -> tuple[str, ...]:
        """Order fields with priority fields first, rest alphabetically."""
        ordered = [f for f in FIELD_PRIORITY if f in fields] + [
            f for f in sorted(fields) if f not in FIELD_PRIORITY
        ]
        return tuple(ordered)

    def _determine_overall_status(
        self, section_results: dict[str, dict[str, Any]]
    ) -> bool:
        """Determine if the overall check passed based on section results."""
        any_failed = any(
            result.get("status") == "failed" for result in section_results.values()
        )

        if self.require_yaml_exports:
            any_skipped = any(
                result.get("status") == "skipped" for result in section_results.values()
            )
            return not any_failed and not any_skipped

        return not any_failed

    def _build_report(
        self,
        passed: bool,
        yaml_paths: dict[str, Path | None],
        section_results: dict[str, dict[str, Any]],
    ) -> dict[str, Any]:
        """Build the complete report structure."""
        return {
            "check": self.NAME,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "passed": passed,
            "require_yaml_exports": self.require_yaml_exports,
            "sources": {
                "yaml_paths": {
                    k: str(v) if isinstance(v, Path) else None
                    for k, v in yaml_paths.items()
                },
                "db_tables": {
                    section: config["table"]
                    for section, config in TABLE_CONFIGS.items()
                },
            },
            "sections": section_results,
        }

    def _save_report(self, report: dict[str, Any]) -> None:
        """Save the report to a timestamped JSON file."""
        report_path = self.reports_dir / (
            datetime.utcnow().strftime("%Y%m%d_%H%M%S") + ".json"
        )
        report_path.write_text(json.dumps(report, indent=2), encoding="utf-8")
# src/mind/governance/checks/legacy_tag_check.py
"""Provides functionality for the legacy_tag_check module."""

from __future__ import annotations

import re

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 0649c22b-9336-490b-9ffd-25e202924301
class LegacyTagCheck(BaseCheck):
    # ID: 94e602d4-47da-455d-be69-fe7a037bcb2b
    def execute(self) -> list[AuditFinding]:
        findings = []
        pattern = re.compile(r"#\s*CAPABILITY:", re.IGNORECASE)
        exclude_dirs = {
            ".git",
            ".venv",
            "__pycache__",
            ".pytest_cache",
            ".ruff_cache",
            "reports",
        }
        exclude_files = {"poetry.lock", "project_context.txt"}
        binary_extensions = {
            ".png",
            ".jpg",
            ".jpeg",
            ".gif",
            ".ico",
            ".pyc",
            ".so",
            ".o",
            ".zip",
            ".gz",
            ".pdf",
        }

        # --- THIS IS THE FIX ---
        # The loop now correctly uses self.repo_root, which is set by the BaseCheck parent class.
        for file_path in self.repo_root.rglob("*"):
            if not file_path.is_file():
                continue

            if any(part in exclude_dirs for part in file_path.parts):
                continue
            if file_path.name in exclude_files:
                continue
            if file_path.suffix in binary_extensions:
                continue

            try:
                content = file_path.read_text(encoding="utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    if pattern.search(line):
                        findings.append(
                            AuditFinding(
                                check_id="style.no_legacy_capability_tags",
                                severity=AuditSeverity.ERROR,
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=i,
                            )
                        )
            except UnicodeDecodeError:
                continue
            except Exception:
                continue

        return findings
# src/mind/governance/checks/manifest_lint.py
"""
Audits capability manifests for quality issues like placeholder text.
"""

from __future__ import annotations

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: ee190b8d-1bf0-4b1a-90e2-abf21ca013c9
class ManifestLintCheck(BaseCheck):
    """Checks for placeholder text in capability manifests."""

    def __init__(self, context):
        super().__init__(context)
        # CORRECTED: Load from the consolidated code_standards policy
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.linter_rules = code_standards_policy.get("capability_rules", [])

    # ID: 2e114e07-e521-4e56-a56c-f3afc6458f44
    def execute(self) -> list[AuditFinding]:
        """Finds capabilities with placeholder descriptions."""
        findings = []
        rule = next(
            (r for r in self.linter_rules if r.get("id") == "caps.no_placeholder_text"),
            None,
        )
        if not rule:
            return []

        for symbol in self.context.symbols_list:
            description = symbol.get("intent", "") or ""
            if any(
                f.lower() in description.lower() for f in ["TBD", "N/A", "Auto-added"]
            ):
                findings.append(
                    AuditFinding(
                        check_id="manifest.lint.placeholder",
                        severity=AuditSeverity.WARNING,
                        message=f"Capability '{symbol.get('key')}' has a placeholder description: '{description}'",
                        file_path=symbol.get("file_path"),
                        line_number=symbol.get("line_number"),
                    )
                )
        return findings
# src/mind/governance/checks/naming_conventions.py
"""
A constitutional audit check to enforce file and symbol naming conventions
as defined in the code_standards.yaml policy.
"""

from __future__ import annotations

import re
from typing import Any

from mind.governance.audit_context import AuditorContext
from shared.models import AuditFinding, AuditSeverity


# ID: 7cff5dba-bd63-4e8c-8e3f-8f242a59f28d
class NamingConventionsCheck:
    """
    Ensures that file names match the patterns defined in the constitution.
    This check is now fully dynamic and reads all configuration from the policy file.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.naming_policy = code_standards_policy.get("naming_conventions", {})

    # REFACTORED: The main execute method is now simpler.
    # ID: 6bebb819-1073-4163-8b70-09c2c374f6c8
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check by iterating through policy rules and scanning the
        repository file system for violations.
        """
        findings = []
        if not self.naming_policy:
            return findings

        # Iterate through categories like 'intent' and 'code'
        for category, rules in self.naming_policy.items():
            if not isinstance(rules, list):
                continue

            # Iterate through the list of rule objects in each category
            for rule in rules:
                findings.extend(self._process_rule(rule, category))
        return findings

    # NEW: Helper method to process a single rule. This is much easier to test.
    def _process_rule(self, rule: dict[str, Any], category: str) -> list[AuditFinding]:
        """Processes a single naming convention rule against the file system."""
        findings = []
        scope_glob = rule.get("scope")
        pattern = rule.get("pattern")
        rule_id = rule.get("id", f"naming.{category}.unnamed")
        exclusions = rule.get("exclusions", [])
        enforcement = rule.get("enforcement", "error")

        if not scope_glob or not pattern:
            return findings  # Skip malformed rules

        try:
            compiled_pattern = re.compile(pattern)
        except re.error:
            # If the regex in the policy is invalid, skip it.
            # This prevents the auditor from crashing on a bad policy.
            return findings

        for file_path in self.context.repo_root.glob(scope_glob):
            if not file_path.is_file():
                continue

            # Check against exclusions using glob patterns for robustness.
            if any(file_path.match(ex) for ex in exclusions):
                continue

            # The core logic: check if the file's name matches the required pattern.
            if not compiled_pattern.match(file_path.name):
                findings.append(
                    AuditFinding(
                        check_id=rule_id,
                        severity=AuditSeverity[enforcement.upper()],
                        message=f"File name '{file_path.name}' violates naming convention '{rule_id}'. Expected pattern: {pattern}",
                        file_path=str(file_path.relative_to(self.context.repo_root)),
                    )
                )
        return findings
# src/mind/governance/checks/orphaned_logic.py
"""
A constitutional audit check to find "orphaned logic" - public symbols
that have not been assigned a capability ID in the database.
"""

from __future__ import annotations

import re
from typing import Any

from mind.governance.audit_context import AuditorContext
from shared.config import settings
from shared.models import AuditFinding, AuditSeverity


# ID: bc44f537-758e-49a2-9914-fc6355b51f48
class OrphanedLogicCheck:
    """
    Ensures that all public symbols are assigned a capability, preventing
    undocumented or untracked functionality. This check respects the
    `audit_ignore_policy.yaml` and the new `project_structure.yaml`.
    """

    def __init__(self, context: AuditorContext):
        self.context = context
        self.symbols = self.context.symbols_map

        # CORRECTED: Load from the new thematic governance_framework policy (if needed, but not here)
        # For audit_ignore_policy, it was a standalone file. Let's assume it's still loaded
        # into the main context correctly for now. If not, we'll fix it.
        # This assumes your AuditorContext change correctly loads audit_ignore_policy.
        # If not, the line would be:
        # governance_policy = self.context.policies.get("governance_framework", {})
        # This part seems to have a dependency on a file not merged yet, let's keep it simple.
        ignore_policy = self.context.policies.get("audit_ignore_policy", {})
        if not ignore_policy:
            # Fallback if the old key is gone, try the new consolidated one
            governance_policy = self.context.policies.get("governance_framework", {})
            # We need to define where ignores live now. Let's assume they are standalone.
            # This part of the code reveals a gap in the new structure! Let's assume
            # audit_ignore_policy remains standalone for now as it's highly dynamic.
            # So we will try to load it directly.
            try:
                ignore_policy = settings.load(
                    "charter.policies.governance.audit_ignore_policy"
                )
            except FileNotFoundError:
                ignore_policy = {}  # Fails gracefully

        self.ignored_symbol_keys = {
            item["key"]
            for item in ignore_policy.get("symbol_ignores", [])
            if "key" in item
        }

        # CORRECTED: Load entry point patterns from project_structure.yaml
        project_structure_policy = self.context.policies.get("project_structure", {})
        if not project_structure_policy:
            project_structure_policy = settings.load("mind.knowledge.project_structure")

        self.entry_point_patterns = project_structure_policy.get(
            "entry_point_patterns", []
        )

    def _is_entry_point(self, symbol_data: dict[str, Any]) -> bool:
        """Checks if a symbol matches any of the defined entry point patterns."""
        for pattern in self.entry_point_patterns:
            match_rules = pattern.get("match", {})
            is_match = True
            for rule_key, rule_value in match_rules.items():
                symbol_value = symbol_data.get(rule_key)

                if rule_key == "type":
                    is_class = symbol_data.get("is_class", False)
                    if (rule_value == "class" and not is_class) or (
                        rule_value == "function" and is_class
                    ):
                        is_match = False
                        break
                elif rule_key == "name_regex":
                    if not re.search(rule_value, symbol_data.get("name", "")):
                        is_match = False
                        break
                elif rule_key == "module_path_contains":
                    if rule_value not in symbol_data.get("file_path", ""):
                        is_match = False
                        break
                elif rule_key == "has_capability_tag":
                    if rule_value and not symbol_data.get("capability"):
                        is_match = False
                        break
                elif rule_key == "is_public_function":
                    if rule_value and symbol_data.get("name", "").startswith("_"):
                        is_match = False
                        break
                elif symbol_value is None:
                    is_match = False
                    break
            if is_match:
                return True
        return False

    # ID: 567318b3-2e45-4383-8af6-9880c3c9576c
    def find_unassigned_public_symbols(self) -> list[dict[str, Any]]:
        """Finds all public symbols with a null capability key that are not ignored."""
        unassigned = []
        for symbol_key, symbol_data in self.symbols.items():
            is_public = symbol_data.get("is_public", False)
            is_unassigned = symbol_data.get("capability") is None
            is_ignored = symbol_key in self.ignored_symbol_keys
            is_entry_point = self._is_entry_point(symbol_data)

            if is_public and is_unassigned and not is_ignored and not is_entry_point:
                symbol_data["key"] = symbol_key
                unassigned.append(symbol_data)
        return unassigned

    # ID: 2ba01327-4559-427f-b0d4-a0737b7937fc
    def execute(self) -> list[AuditFinding]:
        """
        Runs the check and returns a list of findings for any orphaned symbols.
        """
        findings = []
        orphaned_symbols = self.find_unassigned_public_symbols()

        for symbol in orphaned_symbols:
            symbol_key = symbol.get("key", "unknown")
            short_name = symbol_key.split("::")[-1]

            findings.append(
                AuditFinding(
                    check_id="linkage.capability.unassigned",
                    severity=AuditSeverity.ERROR,
                    message=f"Public symbol '{short_name}' is not assigned to a capability in the database.",
                    file_path=symbol.get("file_path"),
                    line_number=symbol.get("line_number"),
                    context={"symbol_key": symbol_key},
                )
            )
        return findings
# src/mind/governance/checks/security_checks.py
"""
Scans source code for hardcoded secrets and other security vulnerabilities
based on configurable detection patterns and exclusion rules.
"""

from __future__ import annotations

import ast
import fnmatch
import re
from pathlib import Path

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 80baca41-4809-456b-985b-9bb9a6cebb7b
class SecurityChecks(BaseCheck):
    """Container for security-related constitutional checks."""

    def __init__(self, context):
        """Initializes the check with a shared auditor context."""
        super().__init__(context)
        # CORRECTED: Load from the new consolidated policy files
        data_gov_policy = self.context.policies.get("data_governance", {})
        safety_framework = self.context.policies.get("safety_framework", {})

        self.secrets_rules = data_gov_policy.get("security_rules", [])
        self.safety_rules = safety_framework.get("safety_rules", [])

    # ID: cb2146e9-2abb-4982-ac11-31f118a10707
    def execute(self) -> list[AuditFinding]:
        """Scans source code for hardcoded secrets and other security vulnerabilities."""
        findings = []
        findings.extend(self._check_for_hardcoded_secrets())
        findings.extend(self._check_dangerous_calls())
        findings.extend(self._check_unsafe_imports())
        return findings

    def _get_files_to_scan(self, rule: dict) -> list[Path]:
        """Gets a list of Python files to scan, respecting rule exclusions."""
        exclude_globs = rule.get("scope", {}).get("exclude", [])
        exclude_paths = [exc.get("path") for exc in exclude_globs if exc.get("path")]

        files_to_scan = []
        for file_path in self.context.python_files:
            if not file_path.is_file():
                continue
            rel_path_str = str(file_path.relative_to(self.repo_root))
            if any(fnmatch.fnmatch(rel_path_str, glob) for glob in exclude_paths):
                continue
            files_to_scan.append(file_path)
        return files_to_scan

    def _check_for_hardcoded_secrets(self) -> list[AuditFinding]:
        """Scans for hardcoded secrets."""
        rule = next(
            (
                r
                for r in self.secrets_rules
                if r.get("id") == "secrets.no_hardcoded_secrets"
            ),
            None,
        )
        if not rule:
            return []

        findings = []
        patterns = [
            re.compile(p) for p in rule.get("detection", {}).get("patterns", [])
        ]
        exclude_globs = rule.get("detection", {}).get("exclude", [])

        for file_path in self.context.python_files:
            rel_path_str = str(file_path.relative_to(self.repo_root))
            if any(fnmatch.fnmatch(rel_path_str, glob) for glob in exclude_globs):
                continue

            try:
                content = file_path.read_text(encoding="utf-8")
                for i, line in enumerate(content.splitlines(), 1):
                    for pattern in patterns:
                        if pattern.search(line):
                            findings.append(
                                AuditFinding(
                                    check_id="security.secrets.hardcoded",
                                    severity=AuditSeverity.ERROR,
                                    message=f"Potential hardcoded secret found on line {i}.",
                                    file_path=str(
                                        file_path.relative_to(self.repo_root)
                                    ),
                                    line_number=i,
                                )
                            )
            except Exception:
                continue
        return findings

    def _check_dangerous_calls(self) -> list[AuditFinding]:
        """Scans for dangerous function calls based on the safety policy."""
        rule = next(
            (
                r
                for r in self.safety_rules
                if r.get("id") == "safety.no_dangerous_execution"
            ),
            None,
        )
        if not rule:
            return []

        findings = []
        patterns = [
            re.compile(p) for p in rule.get("detection", {}).get("patterns", [])
        ]
        files_to_scan = self._get_files_to_scan(rule)

        for file_path in files_to_scan:
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))
                for node in ast.walk(tree):
                    if isinstance(node, ast.Call):
                        call_str = ast.unparse(node.func)
                        for pattern in patterns:
                            if pattern.search(call_str):
                                findings.append(
                                    AuditFinding(
                                        check_id="security.dangerous.call",
                                        severity=AuditSeverity.ERROR,
                                        message=f"Use of dangerous call pattern: '{call_str}'",
                                        file_path=str(
                                            file_path.relative_to(self.repo_root)
                                        ),
                                        line_number=node.lineno,
                                    )
                                )
            except Exception:
                continue
        return findings

    def _check_unsafe_imports(self) -> list[AuditFinding]:
        """Scans for forbidden imports based on the safety policy."""
        rule = next(
            (r for r in self.safety_rules if r.get("id") == "safety.no_unsafe_imports"),
            None,
        )
        if not rule:
            return []

        findings = []
        forbidden_imports = set(rule.get("detection", {}).get("forbidden", []))
        files_to_scan = self._get_files_to_scan(rule)

        for file_path in files_to_scan:
            try:
                content = file_path.read_text("utf-8")
                tree = ast.parse(content, filename=str(file_path))
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            if alias.name in forbidden_imports:
                                findings.append(
                                    AuditFinding(
                                        check_id="security.dangerous.import",
                                        severity=AuditSeverity.ERROR,
                                        message=f"Import of forbidden module: '{alias.name}'",
                                        file_path=str(
                                            file_path.relative_to(self.repo_root)
                                        ),
                                        line_number=node.lineno,
                                    )
                                )
                    elif (
                        isinstance(node, ast.ImportFrom)
                        and node.module in forbidden_imports
                    ):
                        findings.append(
                            AuditFinding(
                                check_id="security.dangerous.import",
                                severity=AuditSeverity.ERROR,
                                message=f"Import from forbidden module: '{node.module}'",
                                file_path=str(file_path.relative_to(self.repo_root)),
                                line_number=node.lineno,
                            )
                        )
            except Exception:
                continue
        return findings
# src/mind/governance/checks/style_checks.py
"""
Auditor checks for code style and convention compliance, as defined in
the consolidated code_standards.yaml.
"""

from __future__ import annotations

import ast

from mind.governance.checks.base_check import BaseCheck
from shared.models import AuditFinding, AuditSeverity


# ID: 791f7cd8-0441-4e2e-ac65-aa8d0ab82ac7
class StyleChecks(BaseCheck):
    """Container for code style and convention constitutional checks."""

    def __init__(self, context):
        super().__init__(context)
        # CORRECTED: Load from the consolidated code_standards policy
        code_standards_policy = self.context.policies.get("code_standards", {})
        self.style_rules = code_standards_policy.get("style_rules", [])

    # ID: 20cebb25-123b-40d9-999f-4d849eba4228
    def execute(self) -> list[AuditFinding]:
        """Verifies that Python modules adhere to documented style conventions."""
        findings = []
        rules = {rule.get("id"): rule for rule in self.style_rules}
        files_to_check = {
            s["file_path"]
            for s in self.context.symbols_list
            if s.get("file_path", "").endswith(".py")
        }
        for file_rel_path in sorted(list(files_to_check)):
            file_abs_path = self.repo_root / file_rel_path
            try:
                source_code = file_abs_path.read_text(encoding="utf-8")
                tree = ast.parse(source_code)
                if "style.docstrings_public_apis" in rules:
                    has_docstring = (
                        tree.body
                        and isinstance(tree.body[0], ast.Expr)
                        and isinstance(tree.body[0].value, ast.Constant)
                    )
                    if not has_docstring:
                        findings.append(
                            AuditFinding(
                                check_id="code.style.missing-module-docstring",
                                severity=AuditSeverity.WARNING,
                                message="Missing required module-level docstring.",
                                file_path=file_rel_path,
                            )
                        )
            except Exception as e:
                findings.append(
                    AuditFinding(
                        check_id="code.parser.error",
                        severity=AuditSeverity.ERROR,
                        message=f"Could not parse file: {e}",
                        file_path=file_rel_path,
                    )
                )
        return findings
# src/mind/governance/constitutional_monitor.py

"""
Constitutional Monitor - Mind-layer orchestrator for constitutional compliance auditing.

This module provides high-level constitutional governance operations by coordinating
between AuditorContext and remediation handlers. It implements the Mind layer's
responsibility for decision-making about constitutional violations.

ID: 8f4a3b2c-9d1e-4f5a-8b2c-3d4e5f6a7b8c
"""

from __future__ import annotations

from dataclasses import dataclass
from mind.governance.audit_context import AuditorContext
from pathlib import Path
from shared.logger import getLogger
from shared.utils.header_tools import HeaderTools
from typing import Protocol
import asyncio


logger = getLogger(__name__)


# ID: c5cb0280-d917-4098-a200-43de6a15de29
class KnowledgeGraphBuilderProtocol(Protocol):

    # ID: f9b3a36a-3c6e-4eff-a645-48c5c5135573
    async def build_and_sync(self) -> None: ...


@dataclass
# ID: e0d28190-86da-4719-9a2d-a38dcedadfa1
class Violation:
    """Represents a single constitutional violation."""

    file_path: str
    policy_id: str
    description: str
    severity: str
    remediation_handler: str | None = None


@dataclass
# ID: c909253f-28a4-4b29-9c50-cb4b30df3dba
class AuditReport:
    """Results of a constitutional audit."""

    policy_category: str
    violations: list[Violation]
    total_files_scanned: int
    compliant_files: int

    @property
    # ID: c603c676-9db7-4c0f-99d8-06a581270f22
    def has_violations(self) -> bool:
        return len(self.violations) > 0


@dataclass
# ID: 2eb4e4cf-9dbe-4806-8618-4e819ac6b89a
class RemediationResult:
    """Results of constitutional remediation."""

    success: bool
    fixed_count: int
    failed_count: int
    error: str | None = None


# ID: 40dae6d4-c0e7-45a6-bd53-4768a19aff60
class ConstitutionalMonitor:
    """
    Mind-layer orchestrator for constitutional compliance and remediation.

    This class coordinates between AuditorContext and autonomous remediation,
    using the HeaderTools for actual header manipulation.
    """

    def __init__(
        self,
        repo_path: Path | str,
        knowledge_builder: KnowledgeGraphBuilderProtocol | None = None,
    ):
        """
        Initialize the constitutional monitor.

        Args:
            repo_path: Root path of the repository to monitor
            knowledge_builder: Optional knowledge graph builder for post-remediation updates
        """
        self.repo_path = Path(repo_path)
        self.auditor = AuditorContext(self.repo_path)
        self.knowledge_builder = knowledge_builder
        logger.info(f"ConstitutionalMonitor initialized for {self.repo_path}")

    # ID: 25eeb765-56da-4101-86e4-65d9fb4ea68b
    def audit_headers(self) -> AuditReport:
        """
        Audit all Python files for header compliance.

        Returns:
            AuditReport containing all header violations found
        """
        logger.info("Starting constitutional header audit...")
        all_py_files = [
            str(p.relative_to(self.repo_path))
            for p in (self.repo_path / "src").rglob("*.py")
        ]
        logger.info(f"Scanning {len(all_py_files)} files for header compliance...")
        violation_objects = []
        for file_path_str in all_py_files:
            file_path = self.repo_path / file_path_str
            try:
                original_content = file_path.read_text(encoding="utf-8")
                header = HeaderTools.parse(original_content)
                correct_location_comment = f"# {file_path_str}"
                is_compliant = (
                    header.location == correct_location_comment
                    and header.module_description is not None
                    and header.has_future_import
                )
                if not is_compliant:
                    violations = []
                    if header.location != correct_location_comment:
                        violations.append("incorrect file location comment")
                    if not header.module_description:
                        violations.append("missing module docstring")
                    if not header.has_future_import:
                        violations.append("missing __future__ import")
                    violation_objects.append(
                        Violation(
                            file_path=file_path_str,
                            policy_id="header_compliance",
                            description=f"Header violations: {', '.join(violations)}",
                            severity="medium",
                            remediation_handler="fix_header",
                        )
                    )
            except Exception as e:
                logger.warning(f"Could not process {file_path_str}: {e}")
        compliant = len(all_py_files) - len(violation_objects)
        logger.info(
            f"Header audit complete: {len(violation_objects)} violations across {len(all_py_files)} files"
        )
        return AuditReport(
            policy_category="header_compliance",
            violations=violation_objects,
            total_files_scanned=len(all_py_files),
            compliant_files=compliant,
        )

    # ID: 585abcab-4b96-4889-ba96-0b408db0755a
    def remediate_violations(self, audit_report: AuditReport) -> RemediationResult:
        """
        Trigger autonomous remediation for constitutional violations.

        Args:
            audit_report: The audit report containing violations to fix

        Returns:
            RemediationResult with success status and counts
        """
        if not audit_report.violations:
            logger.info("No violations to remediate")
            return RemediationResult(success=True, fixed_count=0, failed_count=0)
        logger.info(
            f"Starting remediation for {len(audit_report.violations)} violations..."
        )
        fixed_count = 0
        failed_count = 0
        for violation in audit_report.violations:
            try:
                if violation.remediation_handler == "fix_header":
                    success = self._remediate_header_violation(violation)
                    if success:
                        fixed_count += 1
                    else:
                        failed_count += 1
                else:
                    logger.warning(
                        f"No remediation handler for {violation.remediation_handler}"
                    )
                    failed_count += 1
            except Exception as e:
                logger.error(f"Failed to remediate {violation.file_path}: {e}")
                failed_count += 1
        if fixed_count > 0 and self.knowledge_builder:
            logger.info("🧠 Rebuilding knowledge graph to reflect all changes...")
            asyncio.run(self.knowledge_builder.build_and_sync())
            logger.info("✅ Knowledge graph successfully updated.")
        logger.info(f"Remediation complete: {fixed_count} fixed, {failed_count} failed")
        return RemediationResult(
            success=failed_count == 0,
            fixed_count=fixed_count,
            failed_count=failed_count,
            error=None if failed_count == 0 else f"{failed_count} violations failed",
        )

    def _remediate_header_violation(self, violation: Violation) -> bool:
        """
        Fix a single header violation using HeaderTools.

        Args:
            violation: The violation to fix

        Returns:
            True if successfully fixed, False otherwise
        """
        try:
            file_path = self.repo_path / violation.file_path
            original_content = file_path.read_text(encoding="utf-8")
            header = HeaderTools.parse(original_content)
            correct_location_comment = f"# {violation.file_path}"
            header.location = correct_location_comment
            if not header.module_description:
                header.module_description = (
                    f'"""Provides functionality for the {file_path.stem} module."""'
                )
            header.has_future_import = True
            corrected_code = HeaderTools.reconstruct(header)
            if corrected_code != original_content:
                file_path.write_text(corrected_code, "utf-8")
                logger.info(f"Fixed header in {violation.file_path}")
                return True
            else:
                logger.debug(f"No changes needed for {violation.file_path}")
                return True
        except Exception as e:
            logger.error(f"Failed to fix header in {violation.file_path}: {e}")
            return False
# src/mind/governance/__init__.py
"""Provides functionality for the __init__ module."""

from __future__ import annotations
# src/mind/governance/key_management_service.py

"""
Intent: Key management commands for the CORE Admin CLI.
Provides Ed25519 key generation and helper output for approver configuration.
"""

from __future__ import annotations

from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ed25519
from datetime import UTC, datetime
from shared.config import settings
from shared.logger import getLogger
import os
import typer
import yaml


logger = getLogger(__name__)
log = logger  # keep tests and tools happy


# ID: f8491062-091f-49e6-acbf-9b3ee994409e
def keygen(
    identity: str = typer.Argument(
        ..., help="Identity for the key pair (e.g., 'your.name@example.com')."
    )
) -> None:
    """Intent: Generate a new Ed25519 key pair and print an approver YAML block."""
    logger.info(f"🔑 Generating new key pair for identity: {identity}")
    key_storage_dir = settings.REPO_PATH / settings.KEY_STORAGE_DIR
    key_storage_dir.mkdir(parents=True, exist_ok=True)
    private_key_path = key_storage_dir / "private.key"
    if private_key_path.exists():
        typer.confirm(
            "⚠️ A private key already exists. Overwriting it will invalidate your old identity. Continue?",
            abort=True,
        )
    private_key = ed25519.Ed25519PrivateKey.generate()
    public_key = private_key.public_key()
    pem_private = private_key.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption(),
    )
    private_key_path.write_bytes(pem_private)
    os.chmod(private_key_path, 384)
    pem_public = public_key.public_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PublicFormat.SubjectPublicKeyInfo,
    )
    logger.info(f"\n✅ Private key saved securely to: {private_key_path}")
    logger.info(
        "\n📋 Add the following YAML block to '.intent/constitution/approvers.yaml' under 'approvers':\n"
    )
    approver_data = {
        "identity": identity,
        "public_key": pem_public.decode("utf-8"),
        "created_at": datetime.now(UTC).isoformat(),
        "role": "maintainer",
        "description": "Primary maintainer",
    }
    print(yaml.dump([approver_data], indent=2, sort_keys=False))
# src/mind/governance/micro_proposal_validator.py

"""Provides functionality for the micro_proposal_validator module."""

from __future__ import annotations

from fnmatch import fnmatch
from shared.config import settings
from shared.logger import getLogger
from typing import Any


logger = getLogger(__name__)


def _default_policy() -> dict[str, Any]:
    """
    Safe defaults:
      - allow typical repo paths
      - forbid anything under .intent/**
    """
    return {
        "rules": [
            {
                "id": "safe_paths",
                "allowed_paths": [
                    "src/**",
                    "tests/**",
                    "docs/**",
                    "**/*.md",
                    "**/*.py",
                ],
                "forbidden_paths": [".intent/**"],
            }
        ]
    }


# ID: 6928ebf9-9495-4193-a1aa-ef064f6bb189
class MicroProposalValidator:
    """
    Minimal, deterministic validator:
      - no file I/O
      - enforces allowed/forbidden paths
      - wording matches test expectations
    """

    def __init__(self):
        self.policy: dict[str, Any] = settings.load(
            "charter.policies.agent.micro_proposal_policy"
        )
        rule = next(
            (r for r in self.policy.get("rules", []) if r.get("id") == "safe_paths"), {}
        )
        self._allowed: list[str] = list(rule.get("allowed_paths", []) or [])
        self._forbidden: list[str] = list(rule.get("forbidden_paths", []) or [])

    def _path_ok(self, file_path: str) -> tuple[bool, str]:
        for pat in self._forbidden:
            if fnmatch(file_path, pat):
                return (False, f"Path '{file_path}' is explicitly forbidden by policy")
        if self._allowed and (
            not any(fnmatch(file_path, pat) for pat in self._allowed)
        ):
            return (False, f"Path '{file_path}' not in allowed paths")
        return (True, "ok")

    # ID: a74c44cb-be1f-41fa-ad5c-13bd09602fd7
    def validate(self, plan: list[Any]) -> tuple[bool, str]:
        """
        Lightweight validation used before execution.
        Accepts Pydantic objects (with .model_dump()) or plain dicts.
        """
        if not isinstance(plan, list) or not plan:
            return (False, "Plan is empty")
        for idx, step in enumerate(plan, 1):
            step_dict = step.model_dump() if hasattr(step, "model_dump") else dict(step)
            action = step_dict.get("action") or step_dict.get("name")
            if not action:
                return (False, f"Step {idx} missing action")
            params = step_dict.get("parameters") or step_dict.get("params") or {}
            file_path = params.get("file_path")
            if isinstance(file_path, str):
                ok, msg = self._path_ok(file_path)
                if not ok:
                    return (False, msg)
        return (True, "")
# src/mind/governance/policy_coverage_service.py

"""
Provides a service to perform a meta-audit on the constitution itself,
checking for policy coverage and structural integrity.
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import UTC, datetime
from pathlib import Path
from typing import Any
import hashlib
import json

from pydantic import BaseModel

from shared.config import settings
from shared.logger import getLogger


logger = getLogger(__name__)


# ID: 8d0b79bd-5211-4248-895f-84f407a4125f
class PolicyCoverageReport(BaseModel):
    report_id: str
    generated_at_utc: str
    repo_root: str
    summary: dict[str, int]
    records: list[dict[str, Any]]
    exit_code: int


@dataclass
class _PolicyRef:
    """Internal helper to track discovered policies."""

    id: str
    path: Path
    status: str = "active"
    title: str | None = None


# ID: cf6f0b85-af88-4e4b-a73a-6199ed98c7a3
class PolicyCoverageService:
    """
    Runs a meta-audit on the constitution to ensure all active policies
    are well-formed and covered by the governance model.
    """

    def __init__(self, repo_root: Path | None = None):
        self.repo_root: Path = Path(repo_root or settings.REPO_PATH)
        self.enforcement_model_policy = self._load_enforcement_model_policy()
        self.enforcement_model = self._load_enforcement_model()

    # ID: 2f2ff1f7-3d32-4e3e-9c37-4b8b3d5fa8d9
    def _load_enforcement_model_policy(self) -> dict[str, Any]:
        """
        Load the enforcement model policy content.

        Supports:
        - Legacy path: 'charter.policies.governance.enforcement_model_policy'
        - Current layout: enforcement model embedded in 'safety_framework.yaml'
          referenced as 'charter.policies.safety_framework' in meta.yaml.
        """
        legacy_logical_path = "charter.policies.governance.enforcement_model_policy"

        # Try legacy location first (for backward compatibility).
        try:
            return settings.load(legacy_logical_path)
        except FileNotFoundError:
            logger.info(
                "Legacy enforcement model path '%s' not found; "
                "falling back to 'charter.policies.safety_framework'",
                legacy_logical_path,
            )

        # New layout: enforcement levels live in safety_framework.yaml
        try:
            return settings.load("charter.policies.safety_framework")
        except FileNotFoundError:
            logger.warning(
                "safety_framework policy not found in meta.yaml; "
                "enforcement model will be empty."
            )
            return {}

    def _load_enforcement_model(self) -> dict[str, int]:
        """
        Loads and parses the enforcement model from the pre-loaded policy content.

        Legacy schema:
          enforcement_model_policy["levels"]["error"]["ci_behavior"]

        Current schema (safety_framework.yaml):
          enforcement_model_policy["enforcement_levels"]["error"]["ci_behavior"]
        """
        policy = self.enforcement_model_policy or {}

        # Legacy field name
        levels = policy.get("levels")
        # New field name from safety_framework.yaml
        if not isinstance(levels, dict):
            levels = policy.get("enforcement_levels", {})

        if not isinstance(levels, dict):
            logger.warning(
                "No 'levels' or 'enforcement_levels' found in enforcement_model_policy; "
                "using default enforcement model."
            )
            return {"error": 0, "warn": 0, "info": 0}

        error_cfg = levels.get("error") or {}
        ci_behavior = error_cfg.get("ci_behavior")

        # If CI fails on error, we use exit code 1; otherwise 0 (warn/info stay 0).
        error_exit = 1 if ci_behavior == "fail" else 0
        return {"error": error_exit, "warn": 0, "info": 0}

    def _discover_active_policies(self) -> list[_PolicyRef]:
        """
        Discovers all active policies by reading the meta.yaml index via settings.

        Uses the current flattened structure under:
          meta.yaml → charter → policies → <policy_id>: "<relative_path>"
        """
        refs: list[_PolicyRef] = []

        charter = settings._meta_config.get("charter", {}) or {}
        policies_in_meta = charter.get("policies", {}) or {}

        if not isinstance(policies_in_meta, dict):
            logger.warning(
                "meta.yaml charter.policies is not a dict; found %r",
                type(policies_in_meta),
            )
            return refs

        for logical_id in policies_in_meta.keys():
            logical_path = f"charter.policies.{logical_id}"
            try:
                full_path = settings.get_path(logical_path)
            except FileNotFoundError:
                logger.warning(
                    "Logical policy path '%s' not found in meta.yaml; skipping",
                    logical_path,
                )
                continue

            if not full_path.exists():
                logger.warning("Policy file does not exist on disk: %s", full_path)
                continue

            refs.append(_PolicyRef(id=logical_id, path=full_path))

        return refs

    @staticmethod
    def _extract_rules(policy_data: dict[str, Any]) -> list[tuple[str, str]]:
        """
        Extracts and normalizes rule definitions from a policy file.

        We treat any dict that:
        - has an 'id' field
        - is NOT part of the 'audit_checks' section

        as a policy rule, and read its 'enforcement' if present.

        This picks up things like:
          - style_rules
          - file_header_rules
          - capability_rules
          - dependency_injection rules
          - naming_conventions.* entries
          - coverage_requirements.rules
        """
        rules: list[tuple[str, str]] = []

        def walk(node: Any, path: str) -> None:
            if isinstance(node, dict):
                # treat as rule node if it declares an id and is not under audit_checks
                if "id" in node and "audit_checks" not in path:
                    rule_id = str(node["id"])
                    enforcement = str(node.get("enforcement", "warn")).lower()
                    rules.append((rule_id, enforcement))

                for key, value in node.items():
                    child_path = f"{path}.{key}" if path else key
                    walk(value, child_path)

            elif isinstance(node, list):
                for idx, item in enumerate(node):
                    child_path = f"{path}[{idx}]"
                    walk(item, child_path)

        walk(policy_data, "")

        if not rules:
            # Fallback: treat policy as a single "present" rule if nothing else was found.
            rules.append(("__policy_present__", "warn"))

        return rules

    @staticmethod
    def _collect_audit_bindings(
        policy_id: str, policy_data: dict[str, Any]
    ) -> dict[str, list[str]]:
        """
        Collect bindings between policy rules and audit checks from the policy itself.

        We expect structures like:

          audit_checks:
            - id: header_compliance
              ...
              policy_ref:
                policy: code_standards
                rule: layout.src_module_header
        """
        bindings: dict[str, list[str]] = {}
        checks = policy_data.get("audit_checks", []) or []

        if not isinstance(checks, list):
            return bindings

        for entry in checks:
            if not isinstance(entry, dict):
                continue

            check_id = str(entry.get("id", "")).strip()
            if not check_id:
                continue

            policy_ref = entry.get("policy_ref") or {}
            if not isinstance(policy_ref, dict):
                continue

            ref_policy = str(policy_ref.get("policy", "")).strip()
            ref_rule = str(policy_ref.get("rule", "")).strip()

            if not ref_rule or ref_policy != policy_id:
                continue

            bindings.setdefault(ref_rule, []).append(check_id)

        return bindings

    # ID: c1de13bd-6e47-4b6e-b42d-f6d332ef5016
    def run(self) -> PolicyCoverageReport:
        """
        Executes the policy coverage audit and returns a structured report.

        For each rule we report:
          - policy_id
          - policy_path
          - rule_id
          - enforcement
          - covered: does any audit_check bind to this rule?
          - check_ids: list of check IDs that cover the rule
        """
        policies = self._discover_active_policies()
        records: list[dict[str, Any]] = []
        failures: list[tuple[str, str]] = []

        for policy_ref in policies:
            policy_data = settings.load(f"charter.policies.{policy_ref.id}")
            rules = self._extract_rules(policy_data)
            bindings = self._collect_audit_bindings(policy_ref.id, policy_data)

            for rule_id, level in rules:
                check_ids = bindings.get(rule_id, [])
                is_covered = bool(check_ids)

                records.append(
                    {
                        "policy_id": policy_ref.id,
                        "policy_path": str(policy_ref.path.relative_to(self.repo_root)),
                        "rule_id": rule_id,
                        "enforcement": level,
                        "covered": is_covered,
                        "check_ids": check_ids,
                    }
                )

                if not is_covered and level in ("error", "warn", "info"):
                    failures.append((rule_id, level))

        exit_code = 0
        for _, level in failures:
            exit_code = max(exit_code, self.enforcement_model.get(level, 0))

        report_dict = {
            "generated_at_utc": datetime.now(UTC).isoformat(),
            "repo_root": str(self.repo_root),
            "summary": {
                "policies_seen": len(policies),
                "rules_found": len(records),
                "uncovered_rules": len(failures),
            },
            "records": records,
            "exit_code": exit_code,
        }

        report_json = json.dumps(report_dict, sort_keys=True, separators=(",", ":"))
        report_id = hashlib.sha256(report_json.encode("utf-8")).hexdigest()
        report_dict["report_id"] = report_id

        return PolicyCoverageReport(**report_dict)
# src/mind/governance/policy_gate.py
"""Provides functionality for the policy_gate module."""

from __future__ import annotations

from collections.abc import Iterable, Mapping
from dataclasses import dataclass
from fnmatch import fnmatch
from pathlib import Path

try:
    # Prefer your shared exception if present
    from shared.exceptions import PolicyViolation  # type: ignore
except Exception:  # pragma: no cover
    # ID: da8adaec-6f04-43f8-af55-c74f1297408a
    class PolicyViolation(RuntimeError):
        pass


@dataclass(frozen=True)
# ID: a295c1de-3832-47fb-b9b5-7291dc2f8ddb
class ActionStep:
    """
    Minimal, execution-agnostic view of an action step.
    Only the fields needed for policy checks are required.
    """

    name: str  # e.g. "file.format.black"
    target_path: str | None  # repo-relative path, if any
    metadata: Mapping[str, object]  # free-form, e.g. {"evidence": {...}}


@dataclass(frozen=True)
# ID: 1902366c-e06c-4535-aa72-b276cadd813b
class MicroProposalPolicy:
    """
    Minimal view of the runtime policy. Keep it tolerant to your policy YAML.
    """

    allowed_actions: Iterable[str]  # list of glob patterns
    allowed_paths: Iterable[str]  # list of glob patterns (repo-relative)
    required_evidence: Mapping[str, Iterable[str]]  # action_name -> evidence keys

    @classmethod
    # ID: c1514f13-8715-4a4f-a1b5-8e7288bee62c
    def from_dict(cls, d: Mapping[str, object]) -> MicroProposalPolicy:
        return cls(
            allowed_actions=tuple(d.get("allowed_actions", []) or []),
            allowed_paths=tuple(d.get("allowed_paths", []) or []),
            required_evidence=dict(d.get("required_evidence", {}) or {}),
        )


def _match_any(value: str, patterns: Iterable[str]) -> bool:
    # Empty patterns means "no restriction" (i.e., allow anything)
    ps = tuple(patterns)
    if not ps:
        return True
    return any(fnmatch(value, p) for p in ps)


def _require_evidence(step: ActionStep, policy: MicroProposalPolicy) -> None:
    required = policy.required_evidence.get(step.name, [])
    if not required:
        return
    ev = step.metadata.get("evidence", {}) if step.metadata else {}
    missing = [k for k in required if k not in (ev or {})]
    if missing:
        raise PolicyViolation(
            f"Policy requires evidence {missing} for action '{step.name}', none/missing provided."
        )


# ID: 91dcc541-3458-4fd1-9e33-d95a2a101d6d
def enforce_step(
    *,
    step: ActionStep,
    policy: MicroProposalPolicy,
    repo_root: Path,
) -> None:
    """
    Enforce: allowed_actions, allowed_paths, required_evidence.
    - If a field isn't constrained in policy, it doesn't block.
    - Raises PolicyViolation on any breach.
    """
    # 1) action whitelist (glob-friendly)
    if not _match_any(step.name, policy.allowed_actions):
        raise PolicyViolation(
            f"Action '{step.name}' is not permitted by policy.allowed_actions."
        )

    # 2) path whitelist (repo-relative, glob-friendly)
    if step.target_path:
        rel = str(Path(step.target_path).as_posix())
        if not _match_any(rel, policy.allowed_paths):
            raise PolicyViolation(
                f"Target path '{rel}' is not permitted by policy.allowed_paths."
            )

        # Guard against path traversal outside repo root
        abs_target = (repo_root / rel).resolve()
        if (
            repo_root.resolve() not in abs_target.parents
            and abs_target != repo_root.resolve()
        ):
            raise PolicyViolation(
                f"Target path '{rel}' resolves outside repository root."
            )

    # 3) evidence requirements
    _require_evidence(step, policy)
# src/mind/governance/policy_loader.py

"""
Centralized loaders for constitution-backed policies used by agents and services.
- Avoids hardcoding actions/params in code.
- Keeps a single source of truth for Planner/ExecutionAgent validation.
"""

from __future__ import annotations

from pathlib import Path
from shared.config import settings
from shared.logger import getLogger
from typing import Any
import yaml


logger = getLogger(__name__)

CONSTITUTION_DIR = Path(".intent/charter")
GOVERNANCE_DIR = CONSTITUTION_DIR / "policies" / "governance"
AGENT_DIR = CONSTITUTION_DIR / "policies" / "agent"


def _load_policy_yaml(path: Path) -> dict[str, Any]:
    """
    Loads and performs basic validation on a policy YAML file.
    Resolves relative paths based on settings.REPO_PATH.
    """
    if not path.is_absolute():
        path = settings.REPO_PATH / path
    if not path.exists():
        msg = f"Policy file not found: {path}"
        logger.error(msg)
        raise ValueError(msg)
    try:
        with path.open("r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
        if not isinstance(data, dict):
            msg = f"Policy file must be a dictionary: {path}"
            logger.error(msg)
            raise ValueError(msg)
        return data
    except Exception as e:
        msg = f"Failed to load policy YAML: {path} ({e})"
        logger.error(msg)
        raise ValueError(msg) from e


# ID: 5477bdaa-1466-405a-a8a8-50d15020ebf9
def load_available_actions() -> dict[str, Any]:
    """
    Load the canonical list of available actions for the PlannerAgent.
    """
    policy_path = GOVERNANCE_DIR / "available_actions_policy.yaml"
    policy = _load_policy_yaml(policy_path)
    actions = policy.get("actions")
    if not isinstance(actions, list) or not actions:
        raise ValueError("'actions' must be a non-empty list in the policy.")
    return policy


# ID: d921aae8-c492-4e39-9aba-d5d2ad89af09
def load_micro_proposal_policy() -> dict[str, Any]:
    """
    Load the Micro-Proposal Policy for autonomous path guardrails.
    """
    policy_path = AGENT_DIR / "micro_proposal_policy.yaml"
    policy = _load_policy_yaml(policy_path)
    rules = policy.get("rules")
    if not isinstance(rules, list) or not rules:
        raise ValueError("'rules' must be a non-empty list in the policy.")
    return policy


__all__ = ["load_available_actions", "load_micro_proposal_policy"]
# src/mind/governance/policy_resolver.py

"""Provides functionality for the policy_resolver module."""

from __future__ import annotations

import glob
import os
import yaml


POLICY_ROOT = os.getenv("CORE_POLICY_ROOT", ".intent")


def _scan() -> list[str]:
    return glob.glob(os.path.join(POLICY_ROOT, "**", "*_policy.yaml"), recursive=True)


# ID: c4fd0016-61be-4591-ae8c-38ad05fc4d97
def resolve_policy(*, policy_id: str | None = None, filename: str | None = None) -> str:
    """
    Resolve a policy by YAML 'id' or by filename (basename only).
    Does NOT depend on old directory layout. Raises ValueError if not found.
    """
    candidates = _scan()

    if filename:
        base = os.path.basename(filename)
        for p in candidates:
            if os.path.basename(p) == base:
                return p

    if policy_id:
        for p in candidates:
            try:
                with open(p, encoding="utf-8") as f:
                    data = yaml.safe_load(f) or {}
                if data.get("id") == policy_id:
                    return p
            except Exception:
                pass

    raise ValueError(
        f"Policy not found (policy_id={policy_id!r}, filename={filename!r}) under {POLICY_ROOT}"
    )
# src/mind/governance/runtime_validator.py

"""
Provides a service to run the project's test suite against proposed code changes
in a safe, isolated "canary" environment.
"""

from __future__ import annotations

from pathlib import Path
from rich.console import Console
from shared.logger import getLogger
import asyncio
import shutil
import tempfile


logger = getLogger(__name__)
console = Console()


# ID: 5b29cb98-514b-4887-a51f-b5eff79fe624
class RuntimeValidatorService:
    """A service to test code changes in an isolated environment."""

    def __init__(self, repo_root: Path):
        self.repo_root = repo_root
        self.ignore_patterns = shutil.ignore_patterns(
            ".git",
            ".venv",
            "venv",
            "__pycache__",
            ".pytest_cache",
            ".ruff_cache",
            "work",
        )

    # ID: 98669e0a-8295-408c-ab06-740690de43af
    async def run_tests_in_canary(
        self, file_path_str: str, new_code_content: str
    ) -> tuple[bool, str]:
        """
        Creates a temporary copy of the project, applies the new code, and runs pytest.

        Returns:
            A tuple of (passed: bool, details: str).
        """
        with tempfile.TemporaryDirectory() as tmpdir:
            canary_path = Path(tmpdir) / "canary_repo"
            logger.info(f"Creating canary test environment at {canary_path}...")
            try:
                shutil.copytree(
                    self.repo_root, canary_path, ignore=self.ignore_patterns
                )
                target_file = canary_path / file_path_str
                target_file.parent.mkdir(parents=True, exist_ok=True)
                target_file.write_text(new_code_content, encoding="utf-8")
                logger.info("Running test suite in canary environment...")
                proc = await asyncio.create_subprocess_exec(
                    "poetry",
                    "run",
                    "pytest",
                    cwd=canary_path,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                stdout, stderr = await proc.communicate()
                if proc.returncode == 0:
                    logger.info("✅ Canary tests PASSED.")
                    return (True, "All tests passed in the isolated environment.")
                else:
                    logger.warning("❌ Canary tests FAILED.")
                    error_details = f"Pytest failed with exit code {proc.returncode}.\n\nSTDOUT:\n{stdout.decode()}\n\nSTDERR:\n{stderr.decode()}"
                    return (False, error_details)
            except Exception as e:
                logger.error(f"Error during canary test run: {e}", exc_info=True)
                return (False, f"An unexpected exception occurred: {str(e)}")
