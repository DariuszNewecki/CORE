action: replace_file
content: "from typing import Optional, List, Dict, Any\nfrom core.clients.cognitive_service\
  \ import CognitiveService\nfrom core.config import settings\nfrom core.logging import\
  \ logger\n\nclass LLMService:\n    \"\"\"\n    Service class for handling LLM operations\
  \ using the CognitiveService client.\n    \"\"\"\n    \n    def __init__(self, cognitive_service:\
  \ Optional[CognitiveService] = None):\n        \"\"\"\n        Initialize the LLM\
  \ service with a CognitiveService client.\n        \n        Args:\n           \
  \ cognitive_service: Optional CognitiveService instance. If not provided,\n    \
  \                          a new one will be created using settings.\n        \"\
  \"\"\n        self.cognitive_service = cognitive_service or CognitiveService(\n\
  \            api_key=settings.COGNITIVE_SERVICE_API_KEY,\n            endpoint=settings.COGNITIVE_SERVICE_ENDPOINT,\n\
  \            deployment_name=settings.COGNITIVE_SERVICE_DEPLOYMENT_NAME\n      \
  \  )\n    \n    async def generate_response(self, prompt: str, context: Optional[str]\
  \ = None) -> str:\n        \"\"\"\n        Generate a response from the LLM based\
  \ on the given prompt and optional context.\n        \n        Args:\n         \
  \   prompt: The input prompt for the LLM\n            context: Optional context\
  \ to provide additional information\n            \n        Returns:\n          \
  \  The generated response from the LLM\n        \"\"\"\n        try:\n         \
  \   if context:\n                full_prompt = f\"Context: {context}\\n\\nPrompt:\
  \ {prompt}\"\n            else:\n                full_prompt = prompt\n        \
  \    \n            response = await self.cognitive_service.generate_text(full_prompt)\n\
  \            return response.strip()\n        \n        except Exception as e:\n\
  \            logger.error(f\"Error generating LLM response: {e}\")\n           \
  \ raise\n    \n    async def generate_batch_responses(self, prompts: List[str])\
  \ -> List[str]:\n        \"\"\"\n        Generate responses for multiple prompts\
  \ in a batch.\n        \n        Args:\n            prompts: List of prompts to\
  \ process\n            \n        Returns:\n            List of generated responses\n\
  \        \"\"\"\n        try:\n            responses = await self.cognitive_service.generate_batch_text(prompts)\n\
  \            return [resp.strip() for resp in responses]\n        \n        except\
  \ Exception as e:\n            logger.error(f\"Error generating batch LLM responses:\
  \ {e}\")\n            raise\n    \n    async def get_embeddings(self, text: str)\
  \ -> List[float]:\n        \"\"\"\n        Get embeddings for the given text.\n\
  \        \n        Args:\n            text: Input text to generate embeddings for\n\
  \            \n        Returns:\n            List of embedding values\n        \"\
  \"\"\n        try:\n            embeddings = await self.cognitive_service.get_embeddings(text)\n\
  \            return embeddings\n        \n        except Exception as e:\n     \
  \       logger.error(f\"Error getting embeddings: {e}\")\n            raise\n  \
  \  \n    async def analyze_sentiment(self, text: str) -> Dict[str, Any]:\n     \
  \   \"\"\"\n        Analyze sentiment of the given text.\n        \n        Args:\n\
  \            text: Input text to analyze\n            \n        Returns:\n     \
  \       Dictionary containing sentiment analysis results\n        \"\"\"\n     \
  \   try:\n            sentiment = await self.cognitive_service.analyze_sentiment(text)\n\
  \            return sentiment\n        \n        except Exception as e:\n      \
  \      logger.error(f\"Error analyzing sentiment: {e}\")\n            raise\n\n\
  # Create a singleton instance for easy import\nllm_service = LLMService()"
justification: Replace BaseLLMClient imports with CognitiveService imports to maintain
  functionality while using the updated architecture
target_path: src/services/llm_service.py
