# .intent/charter/patterns/workflow_patterns.yaml
id: workflow_patterns
version: "1.0.0"
title: "Workflow Orchestration Patterns"
description: |
  Defines canonical patterns for multi-step workflows in CORE.
  Workflows compose commands, agents, and services into reliable
  end-to-end processes.

patterns:
  - pattern_id: "linear_workflow"
    type: "sequential"
    purpose: "Execute steps in fixed order with failure handling"

    applies_to:
      - "dev-sync (make command)"
      - "CI pipeline"
      - "Deployment sequence"

    characteristics:
      - "Steps execute in strict sequence"
      - "Each step must succeed before next"
      - "Fail fast on errors"
      - "Clear progress indication"

    implementation_requirements:
      structure:
        - "Define steps as discrete operations"
        - "Each step has clear success/failure criteria"
        - "Steps are idempotent where possible"
        - "State is checkpointed between steps"

      error_handling:
        - "Stop on first failure"
        - "Report which step failed"
        - "Provide clear error message"
        - "Support resume from checkpoint (optional)"

      progress_tracking:
        - "Show step number and total"
        - "Indicate step name/purpose"
        - "Show elapsed time"
        - "Estimate time remaining"

    makefile_example: |
      dev-sync: ## Complete development synchronization
          @echo "ðŸ”„ CORE Development Sync (9 steps)"
          @echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          @echo "ðŸ“‹ Step 1/9: Assigning IDs..."
          @$(CORE_ADMIN) fix ids --write || exit 1

          @echo "ðŸ“ Step 2/9: Fixing headers..."
          @$(CORE_ADMIN) fix headers --write || exit 1

          @echo "ðŸŽ¨ Step 3/9: Formatting code..."
          @$(CORE_ADMIN) fix code-style || exit 1

          @echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          @echo "âœ… Dev-sync complete!"

      # Note: || exit 1 ensures fail-fast behavior

    python_example: |
      class LinearWorkflow:
          """Execute steps sequentially with failure handling."""

          def __init__(self, steps: List[WorkflowStep]):
              self._steps = steps

          async def execute(self) -> WorkflowResult:
              """Execute all steps in sequence."""
              results = []

              for i, step in enumerate(self._steps, 1):
                  logger.info(f"Step {i}/{len(self._steps)}: {step.name}")

                  try:
                      result = await step.execute()
                      results.append(result)
                  except Exception as e:
                      return WorkflowResult(
                          success=False,
                          failed_step=i,
                          error=str(e),
                          partial_results=results
                      )

              return WorkflowResult(success=True, results=results)

  - pattern_id: "dag_workflow"
    type: "dependency_based"
    purpose: "Execute steps respecting dependencies, parallelizing where possible"

    applies_to:
      - "Build systems"
      - "Multi-file code generation"
      - "Batch processing with dependencies"

    characteristics:
      - "Steps have explicit dependencies"
      - "Independent steps run in parallel"
      - "Respects topological order"
      - "Maximizes throughput"

    implementation_requirements:
      dependency_declaration:
        - "Each step declares prerequisites"
        - "Circular dependencies detected at validation"
        - "Missing dependencies reported"

      execution_strategy:
        - "Topological sort determines order"
        - "Steps with no pending deps run immediately"
        - "Failed step blocks dependents only"
        - "Independent branches proceed"

      resource_management:
        - "Limit parallel execution (max workers)"
        - "Balance CPU vs I/O bound tasks"
        - "Clean up on failure"

    example_implementation: |
      class DAGWorkflow:
          """Execute workflow as directed acyclic graph."""

          def __init__(self, steps: Dict[str, WorkflowStep]):
              self._steps = steps
              self._graph = self._build_dependency_graph()

          async def execute(self, max_parallel: int = 4) -> WorkflowResult:
              """Execute with dependency-based parallelization."""

              # Validate no cycles
              self._validate_acyclic()

              # Track completion
              completed = set()
              failed = set()
              results = {}

              while len(completed) + len(failed) < len(self._steps):
                  # Find ready steps (deps satisfied)
                  ready = self._get_ready_steps(completed, failed)

                  if not ready:
                      break  # Stuck - some step failed

                  # Execute in parallel (up to max)
                  batch_results = await self._execute_parallel(
                      ready[:max_parallel]
                  )

                  for step_id, result in batch_results.items():
                      if result.success:
                          completed.add(step_id)
                          results[step_id] = result
                      else:
                          failed.add(step_id)

              return WorkflowResult(
                  success=len(failed) == 0,
                  results=results
              )

  - pattern_id: "saga_workflow"
    type: "transactional"
    purpose: "Execute workflow with compensating actions for rollback"

    applies_to:
      - "Database migrations"
      - "Multi-service deployments"
      - "Critical constitutional changes"

    characteristics:
      - "Each step has compensating action"
      - "Rollback on any failure"
      - "Maintains consistency"
      - "All-or-nothing semantics"

    implementation_requirements:
      step_definition:
        - "Forward action: what to do"
        - "Compensating action: how to undo"
        - "Both must be idempotent"

      execution_flow:
        forward_phase:
          - "Execute steps in order"
          - "Track successful steps"
          - "Stop on first failure"

        rollback_phase:
          - "Execute compensating actions in reverse"
          - "Best effort - log failures but continue"
          - "Restore to consistent state"

      audit_trail:
        - "Log every forward action"
        - "Log every rollback action"
        - "Record final state"

    example_implementation: |
      class SagaWorkflow:
          """Workflow with automatic rollback on failure."""

          @dataclass
          class SagaStep:
              name: str
              forward: Callable
              compensate: Callable

          def __init__(self, steps: List[SagaStep]):
              self._steps = steps

          async def execute(self) -> WorkflowResult:
              """Execute with automatic rollback on failure."""
              completed = []

              # Forward phase
              for step in self._steps:
                  try:
                      logger.info(f"Executing: {step.name}")
                      await step.forward()
                      completed.append(step)
                  except Exception as e:
                      logger.error(f"Failed: {step.name}: {e}")

                      # Rollback phase
                      await self._rollback(completed)
                      return WorkflowResult(success=False, error=e)

              return WorkflowResult(success=True)

          async def _rollback(self, completed: List[SagaStep]) -> None:
              """Execute compensating actions in reverse."""
              for step in reversed(completed):
                  try:
                      logger.info(f"Compensating: {step.name}")
                      await step.compensate()
                  except Exception as e:
                      logger.error(f"Compensation failed: {step.name}: {e}")
                      # Continue rollback despite failure

  - pattern_id: "event_driven_workflow"
    type: "reactive"
    purpose: "React to events with appropriate workflows"

    applies_to:
      - "File watcher triggers"
      - "Constitutional violations"
      - "Git hooks"
      - "CI/CD triggers"

    characteristics:
      - "Event triggers workflow"
      - "Workflow tailored to event type"
      - "Asynchronous execution"
      - "Decoupled components"

    implementation_requirements:
      event_handling:
        - "Subscribe to event sources"
        - "Filter relevant events"
        - "Route to appropriate workflow"
        - "Handle events asynchronously"

      workflow_selection:
        - "Map event type to workflow"
        - "Extract context from event"
        - "Pass context to workflow"

      error_handling:
        - "Failed workflows don't crash system"
        - "Retry transient failures"
        - "Dead letter queue for repeated failures"

    example_implementation: |
      class EventDrivenWorkflowEngine:
          """React to events with appropriate workflows."""

          def __init__(self):
              self._handlers: Dict[str, Callable] = {}

          def register_handler(
              self,
              event_type: str,
              workflow: Callable
          ) -> None:
              """Register workflow for event type."""
              self._handlers[event_type] = workflow

          async def handle_event(self, event: Event) -> None:
              """Process event with registered workflow."""
              workflow = self._handlers.get(event.type)

              if not workflow:
                  logger.warning(f"No handler for {event.type}")
                  return

              try:
                  await workflow(event.data)
              except Exception as e:
                  logger.error(f"Workflow failed for {event.type}: {e}")
                  await self._retry_or_dlq(event, e)

      # Usage example
      engine = EventDrivenWorkflowEngine()

      engine.register_handler(
          "file_modified",
          lambda data: run_linter(data["file_path"])
      )

      engine.register_handler(
          "constitutional_violation",
          lambda data: trigger_remediation(data["violation"])
      )

  - pattern_id: "autonomous_workflow"
    type: "self_improving"
    purpose: "Workflow that improves itself through feedback"

    applies_to:
      - "A2/A3 autonomous development loops"
      - "Self-healing workflows"
      - "Coverage remediation"

    characteristics:
      - "Detects its own failures"
      - "Analyzes failure patterns"
      - "Proposes improvements"
      - "Tests improvements"
      - "Adopts successful changes"

    implementation_requirements:
      feedback_loop:
        1_execute: "Run workflow"
        2_measure: "Collect success metrics"
        3_analyze: "Identify failure patterns"
        4_improve: "Generate improvement proposal"
        5_validate: "Test proposal in safe environment"
        6_adopt: "Apply if successful"

      constitutional_bounds:
        - "Improvements must pass policy check"
        - "Human approval for major changes"
        - "Rollback if performance degrades"

      learning_storage:
        - "Track success/failure history"
        - "Store learned patterns"
        - "Version workflow strategies"

    example_implementation: |
      class AutonomousWorkflow:
          """Self-improving workflow with feedback loop."""

          async def execute_with_learning(
              self,
              task: Task
          ) -> WorkflowResult:
              """Execute and learn from results."""

              # Execute current strategy
              result = await self._execute_current_strategy(task)

              # Record outcome
              await self._record_outcome(task, result)

              # Check if improvement needed
              if self._should_improve():
                  # Analyze failures
                  patterns = await self._analyze_failure_patterns()

                  # Generate improvement proposal
                  proposal = await self._generate_improvement(patterns)

                  # Validate proposal
                  if await self._validate_proposal(proposal):
                      # Test in safe environment
                      if await self._test_improvement(proposal):
                          # Adopt if successful
                          await self._adopt_improvement(proposal)

              return result

          async def _generate_improvement(
              self,
              patterns: List[FailurePattern]
          ) -> ImprovementProposal:
              """Use LLM to propose workflow improvements."""

              context = ContextPackage(
                  failure_patterns=patterns,
                  current_strategy=self._strategy,
                  success_history=self._history
              )

              proposal = await self._planner_agent.propose_improvement(
                  context
              )

              # Constitutional check
              await self._guard.validate_proposal(proposal)

              return proposal

workflow_composition:
  nesting:
    description: "Workflows can contain sub-workflows"
    example: |
      # dev-sync (linear workflow)
      #   â”œâ”€ Phase 1: Code Quality
      #   â”‚   â””â”€ fix code-style (runs black + ruff in parallel - DAG)
      #   â”œâ”€ Phase 2: Constitutional
      #   â”‚   â””â”€ check audit (linear sequence of checks)
      #   â””â”€ Phase 3: Knowledge
      #       â””â”€ vectorize (autonomous workflow with learning)

    guidelines:
      - "Sub-workflows are opaque to parent"
      - "Parent only sees success/failure"
      - "Sub-workflow errors propagate up"

  conditional_execution:
    description: "Steps can be conditional based on context"
    example: |
      class ConditionalWorkflow:
          async def execute(self, context: Context) -> WorkflowResult:
              # Always run
              await self._step1()

              # Conditional
              if context.environment == "production":
                  await self._run_integration_tests()

              # Conditional with different actions
              if context.has_schema_changes:
                  await self._run_migration()
              else:
                  await self._sync_data()

activity_tracking:
  purpose: "Make workflows visible and debuggable"

  timeline_logging:
    what_to_log:
      - "Workflow started (name, trigger, context)"
      - "Step started (name, inputs)"
      - "Step completed (duration, result)"
      - "Step failed (error, partial results)"
      - "Workflow completed (duration, summary)"

    structured_format: |
      {
        "timestamp": "2025-01-15T10:30:00Z",
        "workflow_id": "dev-sync-abc123",
        "workflow_name": "dev-sync",
        "event": "step_started",
        "step": "fix_headers",
        "step_number": 2,
        "total_steps": 9,
        "context": {...}
      }

  visualization:
    - "Timeline view of workflow execution"
    - "Gantt chart for parallel workflows"
    - "Dependency graph for DAG workflows"
    - "Heat map of failure points"

error_handling_strategies:
  retry_policy:
    transient_errors:
      strategy: "Exponential backoff"
      max_attempts: 3
      backoff_factor: 2.0

    permanent_errors:
      strategy: "Fail immediately"
      notify: "Human operator"

  partial_success:
    description: "Some workflows can succeed partially"
    example: "Batch processing - 95/100 items succeeded"
    handling:
      - "Report which items failed"
      - "Allow retry of failed items only"
      - "Don't re-process successful items"

  graceful_degradation:
    description: "Workflow continues with reduced functionality"
    example: "Vectorization fails but DB sync continues"
    implementation:
      - "Mark optional steps clearly"
      - "Continue when optional steps fail"
      - "Report degraded functionality"

circuit_breaker:
  purpose: "Prevent infinite loops in self-healing and auto-remediation workflows"

  rationale: |
    If a fixer and a linter disagree due to configuration mismatch, they could
    fight forever in a loop. The circuit breaker detects this and escalates to
    human intervention before wasting resources or masking real issues.

  rules:
    max_attempts_per_file:
      limit: 3
      window: "1 hour"
      description: "Maximum remediation attempts on same file within time window"

    max_attempts_per_issue:
      limit: 5
      window: "24 hours"
      description: "Maximum attempts to fix same issue type globally"

  detection_criteria:
    - "Same file modified by auto-fix multiple times"
    - "Same check failing repeatedly after remediation"
    - "Oscillating state (Aâ†’Bâ†’Aâ†’B pattern)"
    - "Resource usage spike from retry loops"

  actions_on_breach:
    immediate:
      - action: "quarantine"
        target: "file or check causing loop"
        duration: "24 hours"

      - action: "stop_remediation"
        scope: "affected file/check only"
        message: "Circuit breaker triggered"

    escalation:
      - action: "create_incident_report"
        includes:
          - "File path or check ID"
          - "Number of attempts"
          - "Timeline of changes"
          - "Diff between oscillating states"

      - action: "notify_human"
        method: "log to incident report"
        priority: "medium"
        message: "Auto-remediation loop detected, human review required"

    recovery:
      - action: "reset_after_window"
        description: "Circuit resets automatically after 24 hours"

      - action: "manual_override"
        command: "core-admin manage circuit-breaker reset --file <path>"
        requires: "Human approval"

  implementation_example: |
    class CircuitBreaker:
        def __init__(self, max_attempts=3, window_hours=1):
            self._attempts = {}  # file_path -> [(timestamp, state_hash), ...]

        def check_and_record(self, file_path: str, state_hash: str) -> bool:
            """Returns False if circuit breaker should trigger."""
            now = datetime.now()
            cutoff = now - timedelta(hours=self.window_hours)

            # Get recent attempts for this file
            recent = [
                (ts, h) for ts, h in self._attempts.get(file_path, [])
                if ts > cutoff
            ]

            # Check for oscillation (Aâ†’Bâ†’A pattern)
            if len(recent) >= 3:
                hashes = [h for _, h in recent[-3:]]
                if hashes[0] == hashes[2] and hashes[0] != hashes[1]:
                    return False  # Oscillation detected

            # Check attempt count
            if len(recent) >= self.max_attempts:
                return False  # Too many attempts

            # Record this attempt
            if file_path not in self._attempts:
                self._attempts[file_path] = []
            self._attempts[file_path].append((now, state_hash))

            return True  # OK to proceed

  monitoring:
    metrics:
      - "Circuit breaker trigger count"
      - "Files currently quarantined"
      - "Average time to resolution"
      - "Most frequent trigger patterns"

    dashboard:
      - "List of quarantined files"
      - "Recent circuit breaker events"
      - "Oscillation patterns detected"

  testing:
    unit_tests:
      - "Verify trigger after N attempts"
      - "Verify oscillation detection"
      - "Verify time window enforcement"

    integration_tests:
      - "Simulate fixer/linter conflict"
      - "Verify quarantine prevents further attempts"
      - "Verify incident report generation"

    chaos_engineering:
      - "Intentionally create oscillating fixes"
      - "Verify system doesn't waste resources"
      - "Verify human notification works"

monitoring_and_observability:
  metrics:
    - "Workflow execution count"
    - "Success/failure rate"
    - "Average duration"
    - "Step-level durations"
    - "Resource usage"

  alerting:
    - "Success rate below threshold"
    - "Duration exceeds baseline"
    - "Repeated failures"
    - "Resource exhaustion"

  debugging:
    - "Detailed execution logs"
    - "Checkpoint states"
    - "Input/output at each step"
    - "Replay capability for failed workflows"

testing_workflows:
  unit_tests:
    - "Test individual steps"
    - "Mock dependencies"
    - "Verify error handling"

  integration_tests:
    - "Test full workflow with test data"
    - "Verify step sequencing"
    - "Test rollback mechanisms"

  chaos_engineering:
    - "Inject random failures"
    - "Verify recovery mechanisms"
    - "Test timeout handling"

migration_plan:
  phase_1_inventory:
    - "List all existing workflows (Makefile + Python)"
    - "Identify which pattern each should follow"
    - "Document current behavior"

  phase_2_standardization:
    - "Add structured logging to all workflows"
    - "Implement progress tracking"
    - "Add error handling to make targets"

  phase_3_enhancement:
    - "Add activity tracking"
    - "Implement workflow visualization"
    - "Add pattern validation to CI"

  phase_4_autonomy:
    - "Add learning to autonomous workflows"
    - "Implement improvement proposals"
    - "Enable self-optimization"
