# .intent/charter/patterns/agent_patterns.yaml
id: agent_patterns
version: "1.0.0"
title: "Agent Architecture Patterns"
description: |
  Defines canonical patterns for autonomous agents in CORE's Will layer.
  Agents transform context into decisions through LLM-powered reasoning
  while respecting constitutional boundaries.

patterns:
  - pattern_id: "cognitive_agent"
    type: "reasoning"
    purpose: "Transform context into decisions via LLM reasoning"

    applies_to:
      - "PlannerAgent"
      - "CoderAgent"
      - "ReviewerAgent"
      - "TaggerAgent"

    architecture:
      inputs:
        - "ContextPackage: Rich context about the problem"
        - "Constitution: Policy constraints"
        - "Task: Specific objective"

      processing:
        - "Prompt construction from context"
        - "LLM invocation with temperature/model selection"
        - "Response parsing and validation"

      outputs:
        - "Decision: Structured result (code, plan, tags, etc.)"
        - "Reasoning: Explanation of decision"
        - "Confidence: Certainty level"

    implementation_requirements:
      structure: |
        class AgentName:
            """One-line description of agent's purpose."""

            def __init__(
                self,
                llm_client: LLMClient,
                constitutional_guard: ConstitutionalGuard
            ):
                self._llm = llm_client
                self._guard = constitutional_guard

            async def execute(
                self,
                context: ContextPackage,
                task: TaskSpec
            ) -> AgentDecision:
                """Main entry point for agent execution."""
                # 1. Validate constitutional compliance
                # 2. Build prompt from context
                # 3. Invoke LLM
                # 4. Parse and validate response
                # 5. Return structured decision

      prompt_management:
        - "Prompts stored in .intent/mind/prompts/{agent_name}.md"
        - "Use Jinja2 templating with ContextPackage"
        - "Include few-shot examples in prompts"
        - "Version prompts (track in git)"

      response_handling:
        - "MUST validate LLM output format"
        - "MUST handle malformed responses gracefully"
        - "SHOULD provide partial results when possible"
        - "MUST log raw LLM responses for debugging"

      error_handling:
        - "LLM timeout -> return 'uncertain' decision"
        - "Invalid response -> retry with clarified prompt"
        - "Constitutional violation -> block and report"
        - "Max retries exceeded -> escalate to human"

    constitutional_integration:
      before_execution:
        - "Check if task violates safety policies"
        - "Verify agent has authority for task type"
        - "Validate inputs meet quality thresholds"

      during_execution:
        - "Enforce token limits"
        - "Monitor for policy violations in responses"
        - "Track autonomy level restrictions"

      after_execution:
        - "Audit decision against policies"
        - "Log decision for transparency"
        - "Update autonomy metrics"

    example_implementation: |
      class CoderAgent:
          """Generates code based on specifications and context."""

          PROMPT_PATH = ".intent/mind/prompts/coder_agent.md"

          def __init__(
              self,
              llm_client: LLMClient,
              guard: ConstitutionalGuard
          ):
              self._llm = llm_client
              self._guard = guard

          async def execute(
              self,
              context: ContextPackage,
              spec: CodeGenSpec
          ) -> CodeDecision:
              """Generate code for given specification."""

              # 1. Constitutional pre-check
              await self._guard.validate_task(
                  agent="coder",
                  task_type="code_generation",
                  context=context
              )

              # 2. Build prompt
              prompt = self._build_prompt(context, spec)

              # 3. Invoke LLM with retries
              for attempt in range(3):
                  try:
                      response = await self._llm.complete(
                          prompt=prompt,
                          model="deepseek-coder",
                          temperature=0.2,
                          max_tokens=4000
                      )

                      # 4. Parse response
                      decision = self._parse_code_response(response)

                      # 5. Constitutional post-check
                      await self._guard.validate_decision(decision)

                      # 6. Audit and return
                      await self._audit_decision(decision, context)
                      return decision

                  except MalformedResponseError:
                      if attempt == 2:
                          raise
                      prompt = self._clarify_prompt(prompt, response)

          def _build_prompt(
              self,
              context: ContextPackage,
              spec: CodeGenSpec
          ) -> str:
              """Construct prompt from template and context."""
              template = self._load_prompt_template()
              return template.render(
                  context=context,
                  spec=spec,
                  examples=self._get_few_shot_examples(spec)
              )

  - pattern_id: "orchestrator_agent"
    type: "coordination"
    purpose: "Coordinate multiple agents to achieve complex goals"

    applies_to:
      - "AutonomousDeveloper"
      - "SelfHealingOrchestrator"
      - "MicroProposalExecutor"

    architecture:
      responsibilities:
        - "Break down complex tasks into subtasks"
        - "Select appropriate agents for each subtask"
        - "Manage execution flow and dependencies"
        - "Aggregate results from multiple agents"
        - "Handle failures and retries"

      decision_making:
        - "Determine execution order"
        - "Decide when to parallelize"
        - "Choose which agent for which task"
        - "Determine when to stop/continue"

    implementation_requirements:
      structure: |
        class OrchestratorName:
            """Coordinates multiple agents for complex objectives."""

            def __init__(
                self,
                planner: PlannerAgent,
                executor_registry: Dict[str, Agent],
                guard: ConstitutionalGuard
            ):
                self._planner = planner
                self._executors = executor_registry
                self._guard = guard

            async def execute(
                self,
                objective: Objective,
                context: ContextPackage
            ) -> OrchestratorResult:
                """Execute multi-agent workflow."""
                # 1. Plan execution strategy
                # 2. Execute steps with appropriate agents
                # 3. Handle failures and retries
                # 4. Aggregate and validate results

      execution_flow:
        planning_phase:
          - "Use planner agent to decompose objective"
          - "Identify dependencies between subtasks"
          - "Estimate resources and time"

        execution_phase:
          - "Execute tasks respecting dependencies"
          - "Parallelize independent tasks"
          - "Collect intermediate results"

        validation_phase:
          - "Verify all subtasks completed"
          - "Validate aggregate result"
          - "Check constitutional compliance"

      error_handling:
        strategies:
          - "Retry failed subtasks with backoff"
          - "Skip non-critical failures"
          - "Rollback on critical failures"
          - "Escalate after max retries"

    state_management:
      execution_state:
        - "Track which steps completed"
        - "Store intermediate results"
        - "Record failures and retries"

      persistence:
        - "Save state after each step"
        - "Enable resume from checkpoint"
        - "Clean up state on completion"

    example_implementation: |
      class AutonomousDeveloper:
          """Orchestrates end-to-end autonomous development."""

          async def execute(
              self,
              feature_request: str,
              context: ContextPackage
          ) -> DevelopmentResult:
              """Autonomously implement a feature."""

              # 1. Plan
              plan = await self._planner.create_plan(
                  request=feature_request,
                  context=context
              )

              # 2. Execute steps
              results = []
              for step in plan.steps:
                  agent = self._select_agent(step.type)

                  result = await self._execute_with_retry(
                      agent=agent,
                      step=step,
                      context=context
                  )

                  results.append(result)

                  # Update context with result
                  context = context.with_new_info(result)

              # 3. Validate
              return await self._validate_results(results)

  - pattern_id: "validator_agent"
    type: "verification"
    purpose: "Verify outputs meet quality and policy standards"

    applies_to:
      - "CodeReviewerAgent"
      - "TestValidator"
      - "PolicyComplianceChecker"

    architecture:
      inputs:
        - "Artifact to validate (code, plan, etc.)"
        - "Validation criteria"
        - "Context for informed judgments"

      outputs:
        - "Pass/Fail decision"
        - "Specific violations found"
        - "Suggested improvements"
        - "Confidence in assessment"

    implementation_requirements:
      validation_process:
        1_parse: "Parse artifact into analyzable form"
        2_check_syntax: "Verify structural correctness"
        3_check_semantics: "Verify logical correctness"
        4_check_policy: "Verify constitutional compliance"
        5_check_quality: "Verify quality standards"

      reporting:
        - "List all violations (not just first)"
        - "Provide line numbers and context"
        - "Suggest specific fixes"
        - "Rank by severity"

    example_implementation: |
      class CodeReviewerAgent:
          """Reviews generated code for quality and compliance."""

          async def execute(
              self,
              code: str,
              context: ContextPackage
          ) -> ReviewDecision:
              """Review code and return assessment."""

              violations = []

              # Syntax check
              violations.extend(self._check_syntax(code))

              # LLM-based semantic review
              semantic_issues = await self._llm_review(code, context)
              violations.extend(semantic_issues)

              # Policy compliance
              policy_violations = await self._guard.check_code(code)
              violations.extend(policy_violations)

              return ReviewDecision(
                  approved=len(violations) == 0,
                  violations=violations,
                  suggestions=self._generate_suggestions(violations)
              )

  - pattern_id: "learning_agent"
    type: "adaptive"
    purpose: "Improve performance through experience"

    applies_to:
      - "PlacementOptimizer (learns from successful placements)"
      - "PromptEvolver (learns from LLM performance)"
      - "StrategySelector (learns which approaches work)"

    architecture:
      feedback_loop:
        1_execute: "Perform task with current strategy"
        2_measure: "Collect performance metrics"
        3_analyze: "Identify what worked/failed"
        4_adapt: "Update strategy based on learnings"

      memory:
        - "Store successful patterns"
        - "Store failure patterns"
        - "Track performance trends"

    implementation_requirements:
      experience_tracking:
        - "Log all decisions with context"
        - "Record outcomes (success/failure)"
        - "Capture performance metrics"

      learning_mechanism:
        - "Identify correlations (context -> outcome)"
        - "Update strategy based on data"
        - "A/B test new strategies"

      safety:
        - "Validate learned strategies against policies"
        - "Gradual rollout of new strategies"
        - "Rollback if performance degrades"

    example_implementation: |
      class PlacementOptimizer:
          """Learns optimal file placement strategies."""

          async def learn_from_placement(
              self,
              context: ContextPackage,
              placement: FilePlacement,
              outcome: PlacementOutcome
          ) -> None:
              """Update strategy based on placement result."""

              # 1. Record experience
              await self._db.save_placement_record(
                  context=context,
                  decision=placement,
                  outcome=outcome
              )

              # 2. Analyze patterns
              if outcome.success:
                  patterns = self._extract_success_patterns(
                      context, placement
                  )
                  await self._db.strengthen_patterns(patterns)
              else:
                  await self._db.weaken_patterns(
                      self._extract_failure_patterns(context, placement)
                  )

              # 3. Update strategy
              if self._should_update_strategy():
                  new_strategy = await self._derive_strategy()
                  await self._validate_and_deploy(new_strategy)

agent_communication:
  context_package:
    purpose: "Standard format for inter-agent communication"
    structure:
      - "Problem description"
      - "Relevant code/files"
      - "Domain knowledge"
      - "Previous decisions"
      - "Success criteria"

    usage:
      - "All agents accept ContextPackage"
      - "Agents enrich context with results"
      - "Context flows through orchestration chain"

  decision_format:
    base_structure: |
      @dataclass
      class AgentDecision:
          decision: Any  # Agent-specific result
          reasoning: str  # Why this decision
          confidence: float  # 0.0 - 1.0
          alternatives: List[Any]  # Other options considered
          metadata: dict  # Agent-specific data

    specializations:
      - "CodeDecision(AgentDecision)"
      - "PlanDecision(AgentDecision)"
      - "ReviewDecision(AgentDecision)"

constitutional_constraints:
  autonomy_lanes:
    purpose: "Define what agents can do autonomously"
    levels:
      micro_proposals:
        - "Small, reversible changes"
        - "No human approval needed"
        - "Examples: fix typos, add docstrings"

      major_changes:
        - "Significant modifications"
        - "Human review required"
        - "Examples: refactor modules, change APIs"

      critical_operations:
        - "System-wide impact"
        - "Multiple human approvals"
        - "Examples: schema migrations, policy changes"

  enforcement:
    - "Agents declare required lane before execution"
    - "Guard validates agent has authority"
    - "Actions outside lane are blocked"

testing_agents:
  unit_tests:
    - "Mock LLM client with canned responses"
    - "Test prompt construction logic"
    - "Test response parsing logic"
    - "Test error handling paths"

  integration_tests:
    - "Use real LLM with test prompts"
    - "Verify constitutional compliance"
    - "Test agent composition in orchestrators"

  evaluation:
    - "Track success rate over time"
    - "Compare against baseline (manual approach)"
    - "A/B test prompt variations"

migration_checklist:
  audit_existing_agents:
    - "List all agent classes"
    - "Identify which pattern each follows"
    - "Document missing elements (context, validation, etc.)"

  standardization:
    - "Move prompts to .intent/mind/prompts/"
    - "Add ContextPackage support"
    - "Integrate constitutional guards"
    - "Implement structured decisions"

  validation:
    - "Each agent declares pattern in docstring"
    - "Pattern checker validates implementation"
    - "Add to CI checks"
