# .intent/charter/standards/architecture/vector_service_standards.yaml
id: standard_architecture_vector_service_standards
version: "1.0.0"
title: "Architecture Standard â€“ Vector Service Standards"
type: "standard_architecture"
status: active

owners:
  accountable: "Architecture Owner"
  responsible:
    - "Core Maintainer"

review:
  frequency: "12 months"

schema_id: "standard.architecture.vector_service_standards"

policy_id: vector_service_standards
category: infrastructure
layer: body
depends_on:
  - dependency_injection_policy
  - knowledge_graph_integrity

# =============================================================================
# PHILOSOPHICAL FOUNDATION
# =============================================================================

philosophy: |
  Vector databases are the semantic memory of CORE. Like PostgreSQL serves as
  the single source of truth for structured data, Qdrant serves as the single
  source of truth for semantic understanding.

  Inconsistent interaction patterns with vector storage create:
  - Redundant embeddings (wasted compute & money)
  - Drift between DB and vector store
  - Unpredictable behavior across features
  - Technical debt that compounds exponentially

  This policy establishes that ALL vector operations MUST follow standardized
  patterns for hashing, service method usage, and payload structures.

  The goal: When any developer adds vectorization, they use the same tools
  the same way, creating a predictable, maintainable semantic layer.

# =============================================================================
# CONSTITUTIONAL REQUIREMENTS
# =============================================================================

requirements:
  deterministic_id_generation:
    mandate: |
      Qdrant Point IDs MUST be deterministic and stable across process restarts.
      Using Python's built-in `hash()` function for persistent IDs is PROHIBITED
      because it is randomized per process (salted).

    implementation:
      - "Use `shared.universal.get_deterministic_id(string)` for integer IDs"
      - "Use `uuid.uuid5` for UUIDs (if needed)"
      - "Input to ID generation must be a stable unique identifier (e.g., symbol path)"

    prohibited:
      - "hash(str) -> Randomized per run"
      - "uuid.uuid4() -> Random every time"
      - "random.randint() -> Random"

    validation:
      - "Code review must reject `hash()` usage for DB IDs"
      - "Vectors must remain accessible across server restarts"

    examples:
      compliant: |
        from shared.universal import get_deterministic_id

        # Stable across runs
        point_id = get_deterministic_id(item.item_id)

      non_compliant: |
        # VIOLATION: Changes every time python restarts
        point_id = hash(item.item_id)

  mandatory_hashing:
    mandate: |
      ALL vectorization operations MUST compute and store content hashes
      to enable deduplication and change detection.

    implementation:
      - "Compute SHA-256 hash of normalized content before embedding"
      - "Store hash in payload as 'content_sha256' field"
      - "Check existing hashes before generating new embeddings"
      - "Skip re-vectorization if hash matches"

    validation:
      - "Every vector payload MUST include content_sha256"
      - "Hash collisions trigger explicit handling"
      - "Missing hashes are audit violations"

    examples:
      compliant: |
        normalized = normalize_text(source_code)
        content_hash = hashlib.sha256(normalized.encode()).hexdigest()

        # Check if already vectorized
        existing_hash = await get_stored_hash(vector_id)
        if existing_hash == content_hash:
            return  # Skip, already current

        # Vectorize and store with hash
        vector = await embed(normalized)
        await upsert(vector, {"content_sha256": content_hash, ...})

      non_compliant: |
        # VIOLATION: No hash checking, always re-embeds
        vector = await embed(source_code)
        await upsert(vector, payload)

  service_method_usage:
    mandate: |
      ALL Qdrant operations MUST go through QdrantService methods.
      Direct client access (qdrant_service.client.method()) is PROHIBITED
      except when implementing new service methods.

    implementation:
      - "Use qdrant_service.upsert_symbol_vector() for upserting"
      - "Use qdrant_service.get_vector_by_id() for retrieval"
      - "Use qdrant_service.ensure_collection() for setup"
      - "Add new methods to QdrantService for new patterns"

    validation:
      - "Code review checks for .client. usage outside service"
      - "Linting rules flag direct client access"
      - "Audit reports non-service patterns"

    rationale: |
      Service methods provide:
      - Centralized error handling
      - Consistent logging
      - Payload validation
      - Connection pooling
      - Retry logic

      Direct client access bypasses all protections and creates
      inconsistent behavior across the codebase.

    examples:
      compliant: |
        # Use service method
        point_id = await qdrant_service.upsert_symbol_vector(
            point_id_str=symbol_id,
            vector=embedding,
            payload_data=payload
        )

      non_compliant: |
        # VIOLATION: Direct client access
        await qdrant_service.client.upsert(
            collection_name=collection,
            points=[PointStruct(...)]
        )

  standardized_payloads:
    mandate: |
      ALL vector payloads MUST conform to standardized schemas.
      Use EmbeddingPayload or domain-specific payload classes.
      Raw dictionaries are PROHIBITED.

    implementation:
      - "Use EmbeddingPayload for general embeddings"
      - "Create typed payload classes for domain-specific needs"
      - "Validate payloads before upserting"
      - "Document payload schema in docstrings"

    validation:
      - "Type checkers verify payload classes used"
      - "Runtime validation catches malformed payloads"
      - "Audit checks payload conformance"

    required_fields:
      all_payloads:
        - "content_sha256: str (SHA-256 hash of content)"
        - "model: str (embedding model name)"
        - "model_rev: str (model revision/version)"
        - "dim: int (vector dimension)"
        - "created_at: str (ISO timestamp)"

      code_vectors:
        - "source_path: str (relative path to source)"
        - "chunk_id: str (unique chunk identifier)"
        - "symbol: str (symbol identifier)"
        - "language: str (programming language)"

      constitutional_vectors:
        - "document_type: str (policy|pattern|schema)"
        - "document_id: str (unique document ID)"
        - "section_type: str (philosophy|rule|example)"
        - "section_path: str (YAML path to section)"

    examples:
      compliant: |
        @dataclass
        class CodePayload:
            content_sha256: str
            model: str
            model_rev: str
            dim: int
            created_at: str
            source_path: str
            chunk_id: str
            symbol: str
            language: str

        payload = CodePayload(
            content_sha256=hash_value,
            model=settings.EMBEDDING_MODEL,
            ...
        )
        await qdrant_service.upsert_symbol_vector(
            vector=embedding,
            payload_data=payload.to_dict()
        )

      non_compliant: |
        # VIOLATION: Raw dict with missing fields
        payload = {
            "symbol": symbol_name,
            "code": source_code  # Wrong: should be hash
        }
        await client.upsert(...)

  scroll_pagination:
    mandate: |
      ALL collection scanning MUST use proper pagination with scroll.
      Unbounded queries or limit-only queries are PROHIBITED for
      collections that may exceed 1000 points.

    implementation:
      - "Use scroll with offset tracking"
      - "Set reasonable page size (1000-10000)"
      - "Handle None offset as termination condition"
      - "Accumulate results across pages"

    validation:
      - "Large collection operations use scroll"
      - "Offset tracking is explicit"
      - "Loop termination is guaranteed"

    examples:
      compliant: |
        all_points = []
        offset = None

        while True:
            points, offset = await client.scroll(
                collection_name=collection,
                limit=10_000,
                offset=offset,
                with_payload=True,
                with_vectors=False
            )
            all_points.extend(points)

            if offset is None:
                break

        return all_points

      non_compliant: |
        # VIOLATION: No pagination, will fail >10k points
        points = await client.scroll(
            collection_name=collection,
            limit=100_000  # Single huge request
        )

  bidirectional_sync:
    mandate: |
      Vector store and PostgreSQL MUST maintain referential integrity.
      Orphaned vectors or dangling links are constitutional violations.

    implementation:
      - "symbol_vector_links table tracks vector_id <-> symbol_id"
      - "Sync operations clean orphaned vectors from Qdrant"
      - "Sync operations clean dangling links from PostgreSQL"
      - "Operations are atomic and ordered"

    validation:
      - "Orphaned vector count = 0"
      - "Dangling link count = 0"
      - "Sync operations are idempotent"

    sync_order:
      step_1: "Prune orphaned vectors from Qdrant (no DB link)"
      step_2: "Prune dangling links from PostgreSQL (no vector)"
      rationale: "Order prevents race conditions during cleanup"

    examples:
      compliant: |
        # Phase 1: Clean Qdrant
        orphaned = qdrant_ids - db_vector_ids
        await client.delete(points_selector=orphaned)

        # Phase 2: Clean PostgreSQL
        dangling = [link for link in db_links
                    if link.vector_id not in qdrant_ids]
        await delete_links(dangling)

      non_compliant: |
        # VIOLATION: Wrong order creates race condition
        await delete_links(dangling)  # DB first
        await client.delete(orphaned)  # Vector second

# =============================================================================
# COLLECTION STANDARDS
# =============================================================================

collection_standards:
  naming_convention:
    pattern: "^core-[a-z-]+$"
    examples:
      - "core-symbols"
      - "core-patterns"
      - "core-policies"

    prohibited:
      - "my-collection"  # Missing core- prefix
      - "core_symbols"   # Underscore instead of dash
      - "CoreSymbols"    # Not lowercase

  vector_configuration:
    distance_metric:
      value: "cosine"
      rationale: "Semantic similarity is directional"

    dimension_sources:
      default: 768  # nomic-embed-text
      openai_small: 1536  # text-embedding-3-small
      openai_large: 3072  # text-embedding-3-large

    payload_storage:
      value: "on_disk_payload: true"
      rationale: "Optimize memory, payloads accessed infrequently"

  lifecycle_management:
    creation: "Use ensure_collection() for idempotent setup"
    deletion: "Requires explicit approval and backup"
    migration: "Create new collection, migrate, swap, verify"

# =============================================================================
# VECTORIZER PATTERNS
# =============================================================================

vectorizer_patterns:
  standard_structure:
    class_name: "[Domain]Vectorizer"
    initialization: |
      def __init__(
          self,
          repo_root: Path,
          cognitive_service: CognitiveService,
          qdrant_service: QdrantService
      )

    core_methods:
      - "async def vectorize_all() -> dict[str, Any]"
      - "async def _vectorize_single(item) -> str | None"
      - "async def _get_stored_hashes() -> dict[str, str]"
      - "def _compute_hash(content: str) -> str"

  initialization_pattern:
    requirement: "Use ensure_collection() before vectorization"
    example: |
      async def vectorize_all(self):
          await self.qdrant_service.ensure_collection()

          # Get existing hashes
          stored_hashes = await self._get_stored_hashes()

          # Process items...

  hash_checking_pattern:
    requirement: "Check hash before embedding"
    example: |
      async def _vectorize_single(self, item):
          normalized = normalize_text(item.content)
          content_hash = hashlib.sha256(
              normalized.encode()
          ).hexdigest()

          # Check if already current
          existing_hash = stored_hashes.get(item.id)
          if existing_hash == content_hash:
              logger.debug(f"Skipping {item.id}, hash matches")
              return None

          # Generate embedding
          vector = await self.cognitive_service.get_embedding(normalized)

          # Upsert with hash
          await self.qdrant_service.upsert_symbol_vector(
              point_id_str=item.id,
              vector=vector,
              payload_data={
                  "content_sha256": content_hash,
                  ...
              }
          )

  error_handling_pattern:
    requirement: "Graceful degradation with logging"
    example: |
      try:
          result = await self._vectorize_single(item)
          success_count += 1
      except Exception as e:
          logger.error(f"Failed to vectorize {item.id}: {e}")
          failures.append({"item": item.id, "error": str(e)})
          continue  # Don't abort entire batch

# =============================================================================
# MIGRATION & ENFORCEMENT
# =============================================================================

migration:
  current_violations:
    pattern_vectorizer:
      - "Missing hash-based deduplication"
      - "Direct client.scroll() usage"

    policy_vectorizer:
      - "Missing hash tracking"
      - "Raw dict payloads"

    legacy_code:
      - "Multiple scroll patterns without pagination limits"

  migration_phases:
    phase_1_service_methods:
      duration: "1 week"
      tasks:
        - "Add missing methods to QdrantService"
        - "Update symbol_vectorizer to use new methods"
        - "Update pattern_vectorizer to use new methods"

      success_criteria:
        - "Zero direct client access outside QdrantService"
        - "All vectorizers use service methods"

    phase_2_hash_standards:
      duration: "1 week"
      tasks:
        - "Add hash computation to all vectorizers"
        - "Implement hash checking before embedding"
        - "Add content_sha256 to all payloads"

      success_criteria:
        - "100% of vectors have content_sha256"
        - "Hash checking prevents redundant embeddings"

    phase_3_payload_schemas:
      duration: "1 week"
      tasks:
        - "Create typed payload classes"
        - "Replace raw dicts with typed payloads"
        - "Add runtime validation"

      success_criteria:
        - "Zero raw dict payloads"
        - "Type checking passes"
        - "Runtime validation active"

enforcement:
  audit_checks:
    - id: "vector.hash_present"
      description: "All vectors must have content_sha256 in payload"
      severity: "error"
      check: "Query all points, verify hash field present"

    - id: "vector.service_usage"
      description: "No direct client access outside QdrantService"
      severity: "error"
      check: "Grep for '.client.' usage outside service file"

    - id: "vector.deterministic_ids"
      description: "No use of built-in hash() for IDs"
      severity: "error"
      check: "Scan for hash() calls in vectorizers"

    - id: "vector.typed_payloads"
      description: "All payloads use typed classes"
      severity: "error"
      check: "Type checker verification"

    - id: "vector.sync_integrity"
      description: "No orphaned vectors or dangling links"
      severity: "error"
      check: "Run bidirectional sync in dry-run mode"

  blocking_conditions:
    - "New vectorization code without hash checking"
    - "Direct client access in new code"
    - "Raw dict payloads in new code"
    - "Orphaned vectors > 0 in production"
    - "Use of non-deterministic hashing"

# =============================================================================
# TOOLING & AUTOMATION
# =============================================================================

tooling:
  cli_commands:
    - command: "core-admin manage vectors sync [target]"
      description: "Vectorize with hash-based deduplication"
      targets: ["policies", "patterns", "symbols", "all"]

    - command: "core-admin manage vectors verify"
      description: "Check vector-DB sync integrity"
      checks:
        - "Orphaned vectors count"
        - "Dangling links count"
        - "Hash coverage percentage"

    - command: "core-admin manage vectors prune"
      description: "Clean orphaned vectors and dangling links"
      safety: "Requires --confirm flag in production"

  monitoring:
    metrics:
      - "vector_count_by_collection"
      - "orphaned_vector_count"
      - "dangling_link_count"
      - "hash_coverage_percentage"
      - "embedding_api_calls_saved"

    alerts:
      - condition: "orphaned_vectors > 100"
        action: "Notify and trigger automatic cleanup"

      - condition: "hash_coverage < 95%"
        action: "Flag for migration attention"

      - condition: "embedding_cost_spike"
        action: "Check for missing hash deduplication"

# =============================================================================
# SUCCESS CRITERIA
# =============================================================================

success_criteria:
  consistency:
    - "All vectorizers use same service methods"
    - "All vectorizers implement hash checking"
    - "All vectorizers use typed payloads"

  efficiency:
    - "Redundant embeddings eliminated via hashing"
    - "Embedding API costs reduced by 60%+"
    - "Vectorization runs skip unchanged content"

  integrity:
    - "Zero orphaned vectors in Qdrant"
    - "Zero dangling links in PostgreSQL"
    - "Sync operations are idempotent"

  maintainability:
    - "New vectorizers follow standard pattern"
    - "Service layer provides all needed operations"
    - "Documentation is authoritative"

# =============================================================================
# REFERENCES
# =============================================================================

references:
  related_policies:
    - dependency_injection_policy.yaml
    - knowledge_graph_integrity.yaml
    - pattern_vectorization.yaml

  related_code:
    - "src/services/clients/qdrant_client.py"
    - "src/features/introspection/symbol_vectorizer.py"
    - "src/features/introspection/pattern_vectorizer.py"
    - "src/will/tools/policy_vectorizer.py"

  discovery_context:
    date: "2025-12-06"
    issue: "Inconsistent Qdrant interaction patterns"
    symptoms:
      - "Only symbol_vectorizer used hash checking"
      - "Mixed direct client vs service method usage"
      - "Different payload structures across vectorizers"

    resolution: "This policy standardizes all vector operations"
